rag,Rename `Tensor._storage` to `Tensor.untyped_storage` and update docs,Fixes CC(Deprecation warning in `Tensor.storage()` should suggest alternate API) ,2022-12-27T21:05:13Z,triaged open source module: deprecation Merged ciflow/trunk release notes: distributed (fsdp) topic: deprecation module: python frontend ciflow/periodic module: dynamo ciflow/inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/91414,Test failures are flaky.," merge f ""flaky test failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[skip ci] Trying to make nt passthrough with PT2 stack,Stack from ghstack:  CC([skip ci] Trying to make nt passthrough with PT2 stack) ,2022-12-27T20:28:01Z,Stale release notes: fx module: inductor module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/91411,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,PyObject preservation and resurrection for `StorageImpl`,"It may be useful for `StorageImpl` to support PyObject preservation and resurrection. TensorImpl has this behavior. When a reference count to the PyObject for a tensor goes to zero, but its associated TensorImpl is still being used, the ownership is flipped so that the TensorImpl hangs onto the PyObject, preventing it from getting deallocated. If the PyObject ever needs to be resurrected again, ownership is flipped again and the PyObject is given back to Python. The Python object will retain all of the properties it had before, since it was never deallocated. See CC(Preserve PyObject even when it is dead from Python side)  ",2022-12-26T23:08:25Z,module: internals triaged enhancement better-engineering,open,0,18,https://github.com/pytorch/pytorch/issues/91395,", since it is subtle but can work for any py object, not just in pytorch","Hey  , I'd like to better understand why this will be useful for storages. One possible use case is if someone wants to add a property to an UntypedStorage that remains alive even after all the Python references to it are deleted. Perhaps like this: ```python s = t._storage() s.__dict__['my_property'] = 'something' del s s = t._storage() assert s.my_property == 'something' ``` You also mentioned that PyObject preservation would allow us to make weakref work properly. In the podcast, you said that this is useful for creating caches that contain nonowning references to the cached objectsif the cache is the only reference to the object, then allow the object to be freed. Another use case is to store private fields for objects in a separate dictionary that is keyed by weak references, rather than attaching properties directly to the object like in the code example above. I can definitely see why someone would want to do these things with tensors. But I don't know how often people use storages in the first place, let alone if people want to use storages in these somewhat obscure ways. If you're sure that this will be useful, then greatI just want to make sure I understand the idea fully","So, this ask is primarily coming from . Let me dig up his use cases. * https://github.com/pytorch/pytorch/blob/0b255b3f80d9d365b5f4a064c88278270d6bfac4/torch/_subclasses/meta_utils.pyL147L153 we have to manually detect dead storages and run callbacks on them, because weakrefs don't work. * ""well, i mentioned to Greg wanting to calculate the max batch size symbolically. for that, ideally youd set a cleanup (finalize) function on fake tensors storage to tell you a deallocation has happened""; once again, can't do this because weakref doesn't work * ""similarly, for the cuda graph tapes impl (which we're deferring to post release, so no rush on review there or anything), i want to have a weakref to the tensor storage instead of the tensor itself"" (I'm not exactly sure why but it seems plausible) So this is all mostly about weakrefs. I don't think this feature would be useful for users, it'd mostly be a libraryside usage thing.","Got it, that makes sense, thanks!",I will start with writing the standalone example repo first,"I have a working standalone example repo that shows how PyObject preservation can be implemented: https://github.com/kurtamohler/pyobjectpreservation It still needs polishing and more comments in the code, but I think the main idea is there. Let me know what you think I'll go ahead and start implementing this for storages next","The overall structure looks good, I need to carefully read over MyClassBase.cpp but the broad strokes architecture looks great","There seem to be some errors in MyClassBase.cpp. The one I noticed is you're using shared pointer to point to the C++ type from MyClassBase PyObject. But this is not enough; you need to be able to toggle between owning and not owning from Python to C++, because when the ownership goes from Python to C++, having a shared ptr from C++ to Python means cycle aka you will never deallocate. It looks like the code is a little too much simplified from the conversion from intrusive pointer to shared pointer. I recommend preserving intrusive pointer and then also trying to keep more of the original code structure, including helper functions, e.g.","Thanks for the feedback, I'll make those changes",btw is doing the same thing as https://github.com/pytorch/pytorch/pull/81616 also part of this work?,"I'm having some trouble with adding a `PyObjectSlot` member to `c10::StorageImpl`. Here's my diff:  Click to expand ```cpp diff git a/c10/core/StorageImpl.h b/c10/core/StorageImpl.h index 1d80daed871..e208d88d710 100644  a/c10/core/StorageImpl.h +++ b/c10/core/StorageImpl.h @@ 3,6 +3,7 @@  include   include   include  +include   include  @@ 222,5 +223,8 @@ struct C10_API StorageImpl : public c10::intrusive_ptr_target {    // local to process cuda memory allocation    bool received_cuda_;    Allocator* allocator_; + + protected: +  impl::PyObjectSlot pyobj_slot_;  };  } // namespace c10 ```  When I try to build this, I get an error that I haven't been able to figure out yet:  Click to expand ``` Building wheel torch2.0.0a0+gitf2f42e5  Building version 2.0.0a0+gitf2f42e5 cmake build . target install config Release  j 24 [3/823] Building CXX object caffe2/CMakeFiles.../csrc/jit/runtime/static/memory_planner.cpp.o FAILED: caffe2/CMakeFiles/torch_cpu.dir/__/torch/csrc/jit/runtime/static/memory_planner.cpp.o  /usr/bin/ccache /home/kurtamohler/.conda/envs/pytorch3/bin/x86_64condalinuxgnuc++ DAT_PER_OPERATOR_HEADERS DCPUINFO_SUPPORTED_PLATFORM=1 DFMT_HEADER_ONLY=1 DFXDIV_USE_INLINE_ASSEMBLY=0 DHAVE_MALLOC_USABLE_SIZE=1 DHAVE_MMAP=1 DHAVE_SHM_OPEN=1 DHAVE_SHM_UNLINK=1 DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DNNP_CONVOLUTION_ONLY=0 DNNP_INFERENCE_ONLY=0 DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_FLASH_ATTENTION DUSE_RPC DUSE_TENSORPIPE D_FILE_OFFSET_BITS=64 Dtorch_cpu_EXPORTS I/work2/kurtamohler/development/pytorch3/build/aten/src I/work2/kurtamohler/development/pytorch3/aten/src I/work2/kurtamohler/development/pytorch3/build I/work2/kurtamohler/development/pytorch3 I/work2/kurtamohler/development/pytorch3/cmake/../third_party/benchmark/include I/work2/kurtamohler/development/pytorch3/third_party/onnx I/work2/kurtamohler/development/pytorch3/build/third_party/onnx I/work2/kurtamohler/development/pytorch3/third_party/foxi I/work2/kurtamohler/development/pytorch3/build/third_party/foxi I/work2/kurtamohler/development/pytorch3/torch/csrc/api I/work2/kurtamohler/development/pytorch3/torch/csrc/api/include I/work2/kurtamohler/development/pytorch3/caffe2/aten/src/TH I/work2/kurtamohler/development/pytorch3/build/caffe2/aten/src/TH I/work2/kurtamohler/development/pytorch3/build/caffe2/aten/src I/work2/kurtamohler/development/pytorch3/build/caffe2/../aten/src I/work2/kurtamohler/development/pytorch3/torch/csrc I/work2/kurtamohler/development/pytorch3/third_party/miniz2.1.0 I/work2/kurtamohler/development/pytorch3/third_party/kineto/libkineto/include I/work2/kurtamohler/development/pytorch3/aten/../third_party/catch/single_include I/work2/kurtamohler/development/pytorch3/aten/src/ATen/.. I/work2/kurtamohler/development/pytorch3/third_party/FXdiv/include I/work2/kurtamohler/development/pytorch3/c10/.. I/work2/kurtamohler/development/pytorch3/third_party/pthreadpool/include I/work2/kurtamohler/development/pytorch3/third_party/cpuinfo/include I/work2/kurtamohler/development/pytorch3/aten/src/ATen/native/quantized/cpu/qnnpack/include I/work2/kurtamohler/development/pytorch3/aten/src/ATen/native/quantized/cpu/qnnpack/src I/work2/kurtamohler/development/pytorch3/third_party/cpuinfo/deps/clog/include I/work2/kurtamohler/development/pytorch3/third_party/NNPACK/include I/work2/kurtamohler/development/pytorch3/third_party/fbgemm/include I/work2/kurtamohler/development/pytorch3/third_party/fbgemm I/work2/kurtamohler/development/pytorch3/third_party/fbgemm/third_party/asmjit/src I/work2/kurtamohler/development/pytorch3/third_party/ittapi/src/ittnotify I/work2/kurtamohler/development/pytorch3/third_party/FP16/include I/work2/kurtamohler/development/pytorch3/third_party/tensorpipe I/work2/kurtamohler/development/pytorch3/build/third_party/tensorpipe I/work2/kurtamohler/development/pytorch3/third_party/tensorpipe/third_party/libnop/include I/work2/kurtamohler/development/pytorch3/third_party/fmt/include I/work2/kurtamohler/development/pytorch3/third_party/flatbuffers/include isystem /work2/kurtamohler/development/pytorch3/build/third_party/gloo isystem /work2/kurtamohler/development/pytorch3/cmake/../third_party/gloo isystem /work2/kurtamohler/development/pytorch3/cmake/../third_party/googletest/googlemock/include isystem /work2/kurtamohler/development/pytorch3/cmake/../third_party/googletest/googletest/include isystem /work2/kurtamohler/development/pytorch3/third_party/protobuf/src isystem /work2/kurtamohler/development/pytorch3/third_party/XNNPACK/include isystem /work2/kurtamohler/development/pytorch3/third_party/ittapi/include isystem /work2/kurtamohler/development/pytorch3/cmake/../third_party/eigen isystem /work2/kurtamohler/development/pytorch3/build/include fvisibilityinlineshidden std=c++14 fmessagelength=0 march=nocona mtune=haswell ftreevectorize fPIC fstackprotectorstrong fnoplt O2 ffunctionsections pipe isystem /home/kurtamohler/.conda/envs/pytorch3/include D__STDC_FORMAT_MACROS g isystem /usr/local/cuda11.7.1/include L/usr/local/cuda11.7.1/lib64 L/home/kurtamohler/.conda/envs/pytorch3/lib D__STDC_FORMAT_MACROS Wnodeprecated fvisibilityinlineshidden DUSE_PTHREADPOOL fopenmp DNDEBUG DUSE_FBGEMM DUSE_PYTORCH_QNNPACK DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO O2 fPIC Wall Wextra Werror=returntype Werror=nonvirtualdtor Werror=booloperation Wnarrowing Wnomissingfieldinitializers Wnotypelimits Wnoarraybounds Wnounknownpragmas Wunusedlocaltypedefs Wnounusedparameter Wnounusedfunction Wnounusedresult Wnostrictoverflow Wnostrictaliasing Wnoerror=deprecateddeclarations Wnostringopoverflow Wnopsabi Wnoerror=pedantic Wnoerror=redundantdecls Wnoerror=oldstylecast fdiagnosticscolor=always falignednew Wnounusedbutsetvariable Wnomaybeuninitialized fnomatherrno fnotrappingmath Werror=format Werror=castfunctiontype Wnostringopoverflow DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION O3 DNDEBUG DNDEBUG fPIC DCAFFE2_USE_GLOO DTH_HAVE_THREAD Wall Wextra Wnounusedparameter Wnounusedfunction Wnounusedresult Wnomissingfieldinitializers Wnowritestrings Wnounknownpragmas Wnotypelimits Wnoarraybounds Wnosigncompare Wnostrictoverflow Wnostrictaliasing Wnoerror=deprecateddeclarations Wnomissingbraces Wnomaybeuninitialized fvisibility=hidden O2 fopenmp DCAFFE2_BUILD_MAIN_LIB pthread DASMJIT_STATIC std=gnu++17 MD MT caffe2/CMakeFiles/torch_cpu.dir/__/torch/csrc/jit/runtime/static/memory_planner.cpp.o MF caffe2/CMakeFiles/torch_cpu.dir/__/torch/csrc/jit/runtime/static/memory_planner.cpp.o.d o caffe2/CMakeFiles/torch_cpu.dir/__/torch/csrc/jit/runtime/static/memory_planner.cpp.o c /work2/kurtamohler/development/pytorch3/torch/csrc/jit/runtime/static/memory_planner.cpp In file included from /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/x86_64condalinuxgnu/bits/c++allocator.h:33,                  from /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/bits/allocator.h:46,                  from /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/string:41,                  from /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/stdexcept:39,                  from /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/array:39,                  from /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/tuple:39,                  from /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/functional:54,                  from /work2/kurtamohler/development/pytorch3/c10/util/C++17.h:7,                  from /work2/kurtamohler/development/pytorch3/c10/util/ArrayRef.h:18,                  from /work2/kurtamohler/development/pytorch3/c10/core/SymNodeImpl.h:4,                  from /work2/kurtamohler/development/pytorch3/c10/core/SymBool.h:3,                  from /work2/kurtamohler/development/pytorch3/c10/core/SymInt.h:3,                  from /work2/kurtamohler/development/pytorch3/c10/util/DimVector.h:3,                  from /work2/kurtamohler/development/pytorch3/aten/src/ATen/core/DimVector.h:2,                  from /work2/kurtamohler/development/pytorch3/aten/src/ATen/core/ivalue.h:3,                  from /work2/kurtamohler/development/pytorch3/torch/csrc/jit/runtime/static/impl.h:3,                  from /work2/kurtamohler/development/pytorch3/torch/csrc/jit/runtime/static/memory_planner.h:3,                  from /work2/kurtamohler/development/pytorch3/torch/csrc/jit/runtime/static/memory_planner.cpp:1: /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/ext/new_allocator.h: In instantiation of 'void __gnu_cxx::new_allocator::construct(_Up*, _Args&& ...) [with _Up = std::pair; _Args = {int, c10::StorageImpl}; _Tp = std::pair]': /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/bits/alloc_traits.h:483:4:   required from 'static void std::allocator_traits >::construct(std::allocator_traits >::allocator_type&, _Up*, _Args&& ...) [with _Up = std::pair; _Args = {int, c10::StorageImpl}; _Tp = std::pair; std::allocator_traits >::allocator_type = std::allocator >]' /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/bits/vector.tcc:115:30:   required from 'std::vector::reference std::vector::emplace_back(_Args&& ...) [with _Args = {int, c10::StorageImpl}; _Tp = std::pair; _Alloc = std::allocator >; std::vector::reference = std::pair&]' /work2/kurtamohler/development/pytorch3/torch/csrc/jit/runtime/static/memory_planner.cpp:429:44:   required from here /home/kurtamohler/.conda/envs/pytorch3/x86_64condalinuxgnu/include/c++/9.5.0/ext/new_allocator.h:146:4: error: no matching function for call to 'std::pair::pair(int, c10::StorageImpl)'   146  ```  , do you happen to know what the problem might be? This stackoverflow question must be relevant, since the error happens at an `emplace_back` call on a vector containing  `TensorImpls`, but adding the suggested constructor defs to `PyObjectSlot` didn't fix the error: https://stackoverflow.com/questions/62751522/resulttypemustbeconstructiblefromvaluetypeofinputrangewhencreating","Nevermind, I understand what was wrong. `std::atomic` is not movable, so I'll have to add something like ```cpp PyObjectSlot(PyObjectSlot&& other)   : pyobj_interpreter_(other.pyobj_interpreter_.load()) {} ```","fwiw, we deleted the move constructor/assignment on TensorImpl, and maybe you can also do that for storage. not sure, do it in a separate pr","Another build issue showed up because `PyObjectSlot` doesn't support `std::swap` yet due to the atomic. Seems like making a template specialization for the swap function should fix it. , do you see any problem with doing something like the following?  Click to expand ```cpp include  include  include  include  namespace c10 { class PyInterpreter { }; class PyObject { }; class PyObjectSlot; inline void swap(PyObjectSlot& lhs, PyObjectSlot& rhs); class PyObjectSlot {  public:   PyObjectSlot()     : pyobj_interpreter_(nullptr) {}   PyObjectSlot(PyObjectSlot&& other)     : pyobj_interpreter_(other.pyobj_interpreter_.load()) {}   PyObjectSlot& operator=(const PyObjectSlot& other) {     pyobj_interpreter_ = other.pyobj_interpreter_.load();     pyobj_ = other.pyobj_;     return *this;   }   friend void swap(PyObjectSlot& lhs, PyObjectSlot& rhs);   void set_pyobj_interpreter(PyInterpreter* pyobj_interpreter) {     pyobj_interpreter_ = pyobj_interpreter;   }   PyInterpreter* pyobj_interpreter() {     return pyobj_interpreter_;   }  private:   std::atomic pyobj_interpreter_;   PyObject* pyobj_; }; inline void swap(PyObjectSlot& lhs, PyObjectSlot& rhs) {   std::swap(lhs.pyobj_, rhs.pyobj_);   lhs.pyobj_interpreter_.exchange(     rhs.pyobj_interpreter_.exchange(lhs.pyobj_interpreter_)); } class StorageImpl {  private:   PyObjectSlot pyobj_slot_; }; } // namespace c10 int main() {   // Try moving PyObjectSlot   c10::PyObjectSlot a;   c10::PyInterpreter interp_a;   a.set_pyobj_interpreter(&interp_a);   c10::PyObjectSlot b(std::move(a));   std::vector> v;   v.emplace_back(0, std::move(b));   std::cout ","Swapping atomics is generally dicey business. I would still try to see if we can get rid of swapping on storage first, and discuss alternatives only if we can't easily","Ok makes sense, I'll try getting rid of swaps and moves. Maybe the bare StorageImpls in those places can be replaced with intrusive pointers",Could this mechanism be used with objects exposed with pybind11? We have this problem with PTD that allows for subclassing and it breaks badly.,This requires pretty involved control on the object and its type (see details in https://github.com/kurtamohler/pyobjectpreservation) so that won't work with pybind11 bound objects AFAIK
transformer,"memory location error:  init of LayerNorm with kaiming_uniform_(), xavier_uniform_()."," 🐛 Describe the bug When initializing the `named_parameters()`, the code raises memory error once it reaches `""norm1.weight""`.  It happens if we try to use `xavier_uniform_()` or `kaiming_uniform_()`. ``` class TestModuleImpl : public torch::nn::Module { public:     TestModuleImpl(int numTransformers, int transformerSizes){         for(int i=0; i& m ){         for(auto& p : m>named_parameters()){             if (p.key().find(""weight"") != std::string::npos){                 torch::nn::init::kaiming_uniform_(p.value()); //  _transformers; }; TORCH_MODULE(TestModule); ```  Versions build version 1.13.0+cu117   C++14 Debug and Release ",2022-12-25T15:45:17Z,module: cpp triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/91379,"Actually, it appears the issue is not with transformer, but with its LayerNorm, or any other LayerNorm. doing this in constructor causes exception to be raised: ``` torch::nn::LayerNormOptions lnOptions1({inputNumFeatures}); torch::nn::LayerNormOptions lnOptions2({inputNumFeatures}); _layerNorm1  = register_module(""LayerNorm1"", torch::nn::LayerNorm(lnOptions1)); _layerNorm2  = register_module(""LayerNorm2"", torch::nn::LayerNorm(lnOptions2)); void kaimingInit( const std::shared_ptr& m ){     for(auto& p : m>named_parameters()){         if (p.key().find(""weight"") != std::string::npos){             torch::Tensor& val = p.value();             torch::nn::init::kaiming_uniform_(p.value());  // <exception happens here, for LayerNorm         }         if (p.key().find(""bias"") != std::string::npos){  torch::nn::init::zeros_(p.value());  }     } } ```","Ok, I did an experiment and found a solid fix.  Creating a custom implementation of LayerNorm **still was causing this error**, which was super weird: ``` //constructor LayerNorm_harvImpl(int64_t features, double eps = 1e6) 		: _weight(register_parameter(""weight"", torch::ones({features }))), 		_bias(register_parameter(""bias"", torch::zeros({features }))), 		_eps(eps) {} ``` Changing it from {features} to {1, features} suddenly resolved all the memory crashes. I suspect the implementation in the Libtorch library has the same issue so it might be very easy to fix for you. ``` //constructor that works LayerNorm_CustomImpl(int64_t features, double eps = 1e6) 		: _weight(register_parameter(""weight"", torch::ones({ 1, features }))),  //< works fine now, no crash 		_bias(register_parameter(""bias"", torch::zeros({ 1, features }))), 		_eps(eps) {} ``` however, still crashes inside forward() method: ``` return torch::layer_norm(x, {x.size(1)}, _weight, _bias, _eps); //crashes here ```","For further visitors:  Make sure you are using `try{} catch(c10::Error& err)` It will tell you the error which is more descriptive than what seams like a memory crash :) After I wrapped the code in try/catch I found out that `kaiming_init()` indeed didn't like the dimensions of the `LayerNorm.weight`. The `err.what()` contained the message `""Fan in and fan out can not be computed for tensor with fewer than 2 dimensions""` And wrapping the `torch::layer_norm()` in try/catch revealed `""Expected weight to be of same shape as normalized_shape, but got weight of shape [1, 64] and normalized_shape = [64]""`.  This should get me back on track. I need to keep the weight shape as {features} rather than {1,features} and stop using kaiming init for the LayerNorm. ``` try {     return torch::layer_norm(x, {x.size(1)}, _weight, _bias, _eps, false); }catch(c10::Error& err){     std::string what = err.what();     LogConsole::get().ErrorBad(what);     return x; } ```"
transformer,Internal Assert failed," 🐛 Describe the bug I am using an EncoderDecoder model from hugging face with Bertbase uncased as both encoder and decoder. I am getting this by running this code.  from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer training_args = Seq2SeqTrainingArguments(     output_dir=""./"",     learning_rate=5e5,     evaluation_strategy=""steps"",     per_device_train_batch_size=4,     per_device_eval_batch_size=8,     predict_with_generate=True,     overwrite_output_dir=True,     save_total_limit=3,     fp16=True,  ) trainer = Seq2SeqTrainer(     model=bert2bert,     tokenizer=tokenizer,     args=training_args,     compute_metrics=compute_metrics,     train_dataset=train,     eval_dataset=test, ) RuntimeError: false INTERNAL ASSERT FAILED at ""../c10/cuda/CUDAGraphsC10Utils.h"":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus32508  Versions 20221224 21:02:31  https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com):443... connected. HTTP request sent, awaiting response... 200 OK Length: 17278 (17K) [text/plain] Saving to: ‘collect_env.py’ collect_env.py      100%[===================>]  16.87K  .KB/s    in 0s       20221224 21:02:32 (77.8 MB/s)  ‘collect_env.py’ saved [17278/17278] Collecting environment information... PyTorch version: 1.13.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.6 Libc version: glibc2.27 Python version: 3.8.16 (default, Dec  7 2022, 01:12:13)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.10.133+x86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.2.152 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 460.32.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.1.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.13.0+cu116 [pip3] torchaudio==0.13.0+cu116 [pip3] torchsummary==1.5.1 [pip3] torchtext==0.14.0 [pip3] torchvision==0.14.0+cu116 [conda] Could not collect ",2022-12-24T21:03:33Z,module: cuda triaged module: cuda graphs,open,0,0,https://github.com/pytorch/pytorch/issues/91375
rag,Support for saving multiple storages/tensors that view same data as different dtypes," 🚀 The feature, motivation and pitch Saving and loading multiple tensors or storages that view the same data with dfferent dtypes is not currently possible: ```python >>> import torch >>> t0 = torch.randn(10) >>> t1 = t0.view(torch.int) >>> torch.save([t0, t1], 'tmp.pt') ... RuntimeError: Cannot save multiple tensors or storages that view the same data as different types ``` In the past, it would have been pretty difficult to add support for this because of the way storage types were defined (different Python and C++ classes for each storage dtype). But now that storages have been refactored so that they all use the same untyped storage class underneath, enabling serialization of views of the same data with different dtypes should be much more straightforward now  Alternatives _No response_  Additional context _No response_  ",2022-12-22T20:17:51Z,feature module: serialization triaged,open,0,1,https://github.com/pytorch/pytorch/issues/91325,"As mentioned in  CC(zip file serialization endianness detection is broken)issuecomment1405080053, we should still disallow serializing views that would differ on systems that have different endianness"
transformer,[Torch2 CPU] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum," 🐛 Describe the bug I'm trying to compile a UniXcoder model (variation of BERT) from huggingface transformers on CPU. I use python version '3.8.16' and torch version '2.0.0.dev20221222+cpu'. When performing `model = torch.compile(model)` with the default mode, as well as `mode=reduceoverhead` on a machine with 8GB ram, I encounter the error provided below.  Any idea how to get through it? Thank you!  Error logs ``` [20221222 14:41:22,477] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum [20221222 14:41:35,196] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum [20221222 14:41:45,469] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum [20221222 14:41:56,081] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum [20221222 14:42:06,499] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum [20221222 14:42:19,296] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum [20221222 14:42:30,450] torch._inductor.ir: [WARNING] Using FallbackKernel: aten.cumsum Traceback (most recent call last):   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/output_graph.py"", line 676, in call_user_compiler     compiled_fn = compiler_fn(gm, self.fake_example_inputs())   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/debug_utils.py"", line 945, in debug_wrapper     compiled_gm = compiler_fn(gm, example_inputs, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/__init__.py"", line 1151, in __call__     return self.compile_fn(model_, inputs_)   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/compile_fx.py"", line 398, in compile_fx     return aot_autograd(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/optimizations/training.py"", line 78, in compiler_fn     cg = aot_module_simplified(gm, example_inputs, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_functorch/aot_autograd.py"", line 2353, in aot_module_simplified     compiled_fn = create_aot_dispatcher_function(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_functorch/aot_autograd.py"", line 2050, in create_aot_dispatcher_function     compiled_fn = compiler_fn(flat_fn, fake_flat_tensor_args, aot_config)   File ""/usr/local/lib/python3.8/distpackages/torch/_functorch/aot_autograd.py"", line 1305, in aot_wrapper_dedupe     return compiler_fn(flat_fn, leaf_flat_args, aot_config)   File ""/usr/local/lib/python3.8/distpackages/torch/_functorch/aot_autograd.py"", line 955, in aot_dispatch_base     compiled_fw = aot_config.fw_compiler(fw_module, flat_args)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/compile_fx.py"", line 373, in fw_compiler     return inner_compile(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/debug_utils.py"", line 507, in debug_wrapper     compiled_fn = compiler_fn(gm, example_inputs, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/debug.py"", line 223, in inner     return fn(*args, **kwargs)   File ""/usr/lib/python3.8/contextlib.py"", line 75, in inner     return func(*args, **kwds)   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/compile_fx.py"", line 140, in compile_fx_inner     compiled_fn = graph.compile_to_fn()   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/graph.py"", line 538, in compile_to_fn     return self.compile_to_module().call   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/graph.py"", line 527, in compile_to_module     mod = PyCodeCache.load(code)   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/codecache.py"", line 461, in load     exec(code, mod.__dict__, mod.__dict__)   File ""/tmp/torchinductor_root/ih/cihuzmkrufm4dzdsf7l5l6b7nhtybr7fexjtnk72btsrlnrnbtew.py"", line 6242, in      async_compile.wait(globals())   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/codecache.py"", line 656, in wait     scope[key] = result.result()   File ""/usr/lib/python3.8/concurrent/futures/_base.py"", line 444, in result     return self.__get_result()   File ""/usr/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result     raise self._exception   File ""/usr/lib/python3.8/concurrent/futures/thread.py"", line 57, in run     result = self.fn(*self.args, **self.kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/codecache.py"", line 633, in task     return CppCodeCache.load(source_code).kernel   File ""/usr/local/lib/python3.8/distpackages/torch/_inductor/codecache.py"", line 438, in load     subprocess.check_output(cmd, stderr=subprocess.STDOUT)   File ""/usr/lib/python3.8/subprocess.py"", line 415, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/lib/python3.8/subprocess.py"", line 493, in run     with Popen(*popenargs, **kwargs) as process:   File ""/usr/lib/python3.8/subprocess.py"", line 858, in __init__     self._execute_child(args, executable, preexec_fn, close_fds,   File ""/usr/lib/python3.8/subprocess.py"", line 1639, in _execute_child     self.pid = _posixsubprocess.fork_exec( OSError: [Errno 12] Cannot allocate memory The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""compile.py"", line 596, in      embeddings = get_embeddings(model, code_segments)   File ""compile.py"", line 582, in get_embeddings     _,code_embedding = model(source_ids)   File ""/usr/local/lib/python3.8/distpackages/torch/nn/modules/module.py"", line 1482, in _call_impl     return forward_call(*args, **kwargs)   File ""/unixcoder.py"", line 83, in forward     token_embeddings = self.model(source_ids,attention_mask = mask.unsqueeze(1) * mask.unsqueeze(2))[0]   File ""/usr/local/lib/python3.8/distpackages/torch/nn/modules/module.py"", line 1482, in _call_impl     return forward_call(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/eval_frame.py"", line 83, in forward     return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/eval_frame.py"", line 212, in _fn     return fn(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/eval_frame.py"", line 333, in catch_errors     return callback(frame, cache_size, hooks)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 480, in _convert_frame     result = inner_convert(frame, cache_size, hooks)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 103, in _fn     return fn(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 339, in _convert_frame_assert     return _compile(   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 400, in _compile     out_code = transform_code_object(code, transform)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/bytecode_transformation.py"", line 341, in transform_code_object     transformations(instructions, code_options)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/convert_frame.py"", line 387, in transform     tracer.run()   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/symbolic_convert.py"", line 1684, in run     super().run()   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/symbolic_convert.py"", line 538, in run     and self.step()   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/symbolic_convert.py"", line 501, in step     getattr(self, inst.opname)(inst)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/symbolic_convert.py"", line 1750, in RETURN_VALUE     self.output.compile_subgraph(self)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/output_graph.py"", line 553, in compile_subgraph     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/output_graph.py"", line 600, in compile_and_call_fx_graph     compiled_fn = self.call_user_compiler(gm)   File ""/usr/local/lib/python3.8/distpackages/torch/_dynamo/output_graph.py"", line 681, in call_user_compiler     raise BackendCompilerFailed(self.compiler_fn, e) from e torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised OSError: [Errno 12] Cannot allocate memory Set torch._dynamo.config.verbose=True for more information You can suppress this exception and fall back to eager by setting:     torch._dynamo.config.suppress_errors = True ```  Minified repro _No response_ ",2022-12-22T15:38:38Z,bug oncall: pt2,closed,0,9,https://github.com/pytorch/pytorch/issues/93495,"Do you get the same error when you try `backend=""eager""` or `backend=""aot_eager""` as an option in `torch.compile`?","> Do you get the same error when you try `backend=""eager""` or `backend=""aot_eager""` as an option in `torch.compile`? Thank you for replying. I tried them both, it actually doesn't crash anymore, but  the runtime performance is really bad. I compare the runtime of the original model (without compile) to the model compiled with both `eager` and `aot_eager` backends: **Without compile:** `Total time 28.479018211364746` **eager:** `Total time 104.0422477722168` **aot_eager:** `Total time 175.95780992507935` any idea why?", Do you mind share the reproducer code?,">  Do you mind share the reproducer code? Sure, in the attached zip you'll find:  unixcoder.py (as given by https://github.com/microsoft/CodeBERT/tree/master/UniXcoder)   code.py  simple python script that creates embedding for source code fragments. code.zip",Is there an update on this? I'm having the exact same issue.,"The issue is not caused by `[WARNING] Using FallbackKernel: aten.cumsum`. For `aten.cumsum`, using fallback is on purpose for low efficiency of decomposition  CC([inductor] Lower aten.cumsum). The error above is `Cannot allocate memory`. Actually, I could run the code with inductor mode without error and it seems that the error is due to inefficiency of inductor mode. I compared the performance in four modes.          mode  6.040424   Notice that the first time of compiling mode is usually timeconsuming and it is better to run repeatedly. I've run 10 times and excluded the first performance here. Details of data:         without compile 8.862784147 7.331763983 8.797510624 6.193632603 6.044025898 5.985580683 5.909735918 6.417853832 5.958158016 5.787334919 inductor 522.4309599 18.03655243 15.26723146 15.8309319 15.37905502 14.9801507 14.20492935 14.25522494 13.8550992 14.27873015 14.07980537 eager 76.17847276 5.888897419 5.793441057 5.563881159 5.665488482 5.629543304 6.018662691 5.815788269 5.927585363 6.117486238 6.155511618 aot_eager 142.1525922 6.320661306 5.981152534 5.77075839 5.898153543 5.977357864 5.99329257 6.369976044 6.220510244 5.851990461 6.020391941  ","For the reasons why inductor performs worse than eager, here are some updates. 1. 7500 `mkl_linear` of inductor cost an additional 1s compared to `aten::addmm` of eager; 2. Inductor kernels cannot do vectorization due to `to_type` (float, int, long) 1300 times; 3. Both using the same ops, it is strange that inductor is slower than eager. Also, the number of callings sometimes does not match.     * `aten::bmm`: inductor 629ms 2400; aten 525ms 2400     * `aten::as_strided`: inductor 41ms 13600; 9ms 27500     * `aten::empty`: inductor 35ms 7400; aten 10ms 9100  Profilings **Inductor**                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls           mkl::_mkl_linear        25.21%        4.348s        25.41%        4.383s     600.434us          7300                  aten::bmm         3.65%     628.859ms         3.65%     628.897ms     262.040us          2400           aten::as_strided         0.24%      41.000ms         0.24%      41.000ms       3.015us         13600                aten::empty         0.21%      35.677ms         0.21%      35.677ms       4.821us          7400       graph_7_kernel_cpp_4         0.13%      22.478ms         0.13%      22.478ms       4.496ms             5      graph_10_kernel_cpp_4         0.13%      22.447ms         0.13%      22.447ms       4.489ms             5      graph_7_kernel_cpp_60         0.13%      22.298ms         0.13%      22.298ms       4.460ms             5      graph_7_kernel_cpp_11         0.13%      22.153ms         0.13%      22.153ms       4.431ms             5      graph_7_kernel_cpp_46         0.13%      22.033ms         0.13%      22.033ms       4.407ms             5      graph_7_kernel_cpp_25         0.13%      22.026ms         0.13%      22.026ms       4.405ms             5      graph_7_kernel_cpp_39         0.13%      21.936ms         0.13%      21.936ms       4.387ms             5      graph_7_kernel_cpp_67         0.13%      21.891ms         0.13%      21.891ms       4.378ms             5      graph_7_kernel_cpp_74         0.13%      21.852ms         0.13%      21.852ms       4.370ms             5      graph_7_kernel_cpp_81         0.13%      21.704ms         0.13%      21.704ms       4.341ms             5      graph_7_kernel_cpp_32         0.13%      21.702ms         0.13%      21.702ms       4.340ms             5      graph_7_kernel_cpp_18         0.13%      21.650ms         0.13%      21.650ms       4.330ms             5     graph_10_kernel_cpp_11         0.13%      21.563ms         0.13%      21.563ms       4.313ms             5      graph_7_kernel_cpp_53         0.12%      21.488ms         0.12%      21.488ms       4.298ms             5     graph_10_kernel_cpp_32         0.12%      21.449ms         0.12%      21.449ms       4.290ms             5     graph_10_kernel_cpp_53         0.12%      21.269ms         0.12%      21.269ms       4.254ms             5     graph_10_kernel_cpp_74         0.12%      21.265ms         0.12%      21.265ms       4.253ms             5     graph_10_kernel_cpp_18         0.12%      21.226ms         0.12%      21.226ms       4.245ms             5     graph_10_kernel_cpp_60         0.12%      21.201ms         0.12%      21.201ms       4.240ms             5     graph_10_kernel_cpp_46         0.12%      21.122ms         0.12%      21.122ms       4.224ms             5     graph_10_kernel_cpp_81         0.12%      21.089ms         0.12%      21.089ms       4.218ms             5     graph_10_kernel_cpp_67         0.12%      20.947ms         0.12%      20.947ms       4.189ms             5     graph_10_kernel_cpp_39         0.12%      20.828ms         0.12%      20.828ms       4.166ms             5     graph_10_kernel_cpp_25         0.12%      20.755ms         0.12%      20.755ms       4.151ms             5     graph_29_kernel_cpp_18         0.12%      19.973ms         0.12%      19.973ms       3.995ms             5     graph_29_kernel_cpp_67         0.11%      19.611ms         0.11%      19.611ms       3.922ms             5     graph_29_kernel_cpp_53         0.11%      19.512ms         0.11%      19.512ms       3.902ms             5     graph_29_kernel_cpp_81         0.11%      19.491ms         0.11%      19.491ms       3.898ms             5     graph_29_kernel_cpp_11         0.11%      19.443ms         0.11%      19.443ms       3.889ms             5     graph_29_kernel_cpp_60         0.11%      19.437ms         0.11%      19.437ms       3.887ms             5     graph_29_kernel_cpp_39         0.11%      19.427ms         0.11%      19.427ms       3.885ms             5     graph_29_kernel_cpp_32         0.11%      19.388ms         0.11%      19.388ms       3.878ms             5      graph_29_kernel_cpp_4         0.11%      19.256ms         0.11%      19.256ms       3.851ms             5     graph_29_kernel_cpp_46         0.11%      19.079ms         0.11%      19.079ms       3.816ms             5     graph_29_kernel_cpp_74         0.11%      18.881ms         0.11%      18.881ms       3.776ms             5     graph_29_kernel_cpp_25         0.11%      18.763ms         0.11%      18.763ms       3.753ms             5      graph_11_kernel_cpp_4         0.11%      18.587ms         0.11%      18.587ms       4.647ms             4     graph_11_kernel_cpp_18         0.10%      17.638ms         0.10%      17.638ms       4.410ms             4     graph_11_kernel_cpp_60         0.10%      17.627ms         0.10%      17.627ms       4.407ms             4     graph_11_kernel_cpp_32         0.10%      17.560ms         0.10%      17.560ms       4.390ms             4     graph_11_kernel_cpp_39         0.10%      17.406ms         0.10%      17.406ms       4.351ms             4     graph_11_kernel_cpp_81         0.10%      17.397ms         0.10%      17.397ms       4.349ms             4     graph_11_kernel_cpp_74         0.10%      17.299ms         0.10%      17.299ms       4.325ms             4     graph_11_kernel_cpp_67         0.10%      17.143ms         0.10%      17.143ms       4.286ms             4     graph_11_kernel_cpp_11         0.10%      17.117ms         0.10%      17.117ms       4.279ms             4     graph_11_kernel_cpp_25         0.10%      16.873ms         0.10%      16.873ms       4.218ms             4     graph_11_kernel_cpp_53         0.10%      16.753ms         0.10%      16.753ms       4.188ms             4      graph_30_kernel_cpp_4         0.10%      16.701ms         0.10%      16.701ms       4.175ms             4     graph_11_kernel_cpp_46         0.10%      16.700ms         0.10%      16.700ms       4.175ms             4     graph_30_kernel_cpp_32         0.10%      16.577ms         0.10%      16.577ms       4.144ms             4     graph_30_kernel_cpp_46         0.10%      16.511ms         0.10%      16.511ms       4.128ms             4      graph_27_kernel_cpp_4         0.09%      16.361ms         0.09%      16.361ms       4.090ms             4     graph_30_kernel_cpp_67         0.09%      16.314ms         0.09%      16.314ms       4.079ms             4     graph_30_kernel_cpp_60         0.09%      16.040ms         0.09%      16.040ms       4.010ms             4                  aten::mul         0.09%      15.999ms         0.11%      19.498ms      97.490us           200     graph_27_kernel_cpp_53         0.09%      15.978ms         0.09%      15.978ms       3.994ms             4     graph_30_kernel_cpp_18         0.09%      15.946ms         0.09%      15.946ms       3.986ms             4     graph_30_kernel_cpp_81         0.09%      15.874ms         0.09%      15.874ms       3.969ms             4     graph_30_kernel_cpp_25         0.09%      15.857ms         0.09%      15.857ms       3.964ms             4     graph_27_kernel_cpp_74         0.09%      15.855ms         0.09%      15.855ms       3.964ms             4     graph_27_kernel_cpp_25         0.09%      15.853ms         0.09%      15.853ms       3.963ms             4     graph_30_kernel_cpp_74         0.09%      15.847ms         0.09%      15.847ms       3.962ms             4     graph_30_kernel_cpp_11         0.09%      15.841ms         0.09%      15.841ms       3.960ms             4     graph_27_kernel_cpp_46         0.09%      15.778ms         0.09%      15.778ms       3.945ms             4     graph_27_kernel_cpp_67         0.09%      15.759ms         0.09%      15.759ms       3.940ms             4     graph_27_kernel_cpp_81         0.09%      15.722ms         0.09%      15.722ms       3.930ms             4     graph_27_kernel_cpp_39         0.09%      15.717ms         0.09%      15.717ms       3.929ms             4     graph_27_kernel_cpp_18         0.09%      15.700ms         0.09%      15.700ms       3.925ms             4     graph_30_kernel_cpp_39         0.09%      15.699ms         0.09%      15.699ms       3.925ms             4     graph_27_kernel_cpp_32         0.09%      15.568ms         0.09%      15.568ms       3.892ms             4     graph_27_kernel_cpp_60         0.09%      15.539ms         0.09%      15.539ms       3.885ms             4     graph_27_kernel_cpp_11         0.09%      15.447ms         0.09%      15.447ms       3.862ms             4     graph_30_kernel_cpp_53         0.09%      15.394ms         0.09%      15.394ms       3.849ms             4       graph_1_kernel_cpp_4         0.08%      14.516ms         0.08%      14.516ms       4.839ms             3      graph_12_kernel_cpp_4         0.08%      14.502ms         0.08%      14.502ms       4.834ms             3       graph_2_kernel_cpp_4         0.08%      13.993ms         0.08%      13.993ms       4.664ms             3      graph_1_kernel_cpp_60         0.08%      13.971ms         0.08%      13.971ms       4.657ms             3      graph_2_kernel_cpp_18         0.08%      13.653ms         0.08%      13.653ms       4.551ms             3      graph_2_kernel_cpp_60         0.08%      13.610ms         0.08%      13.610ms       4.537ms             3      graph_2_kernel_cpp_11         0.08%      13.578ms         0.08%      13.578ms       4.526ms             3      graph_1_kernel_cpp_67         0.08%      13.546ms         0.08%      13.546ms       4.515ms             3     graph_12_kernel_cpp_18         0.08%      13.534ms         0.08%      13.534ms       4.511ms             3      graph_15_kernel_cpp_4         0.08%      13.534ms         0.08%      13.534ms       4.511ms             3      graph_2_kernel_cpp_46         0.08%      13.529ms         0.08%      13.529ms       4.510ms             3      graph_2_kernel_cpp_74         0.08%      13.517ms         0.08%      13.517ms       4.506ms             3      graph_1_kernel_cpp_46         0.08%      13.508ms         0.08%      13.508ms       4.503ms             3      graph_1_kernel_cpp_25         0.08%      13.467ms         0.08%      13.467ms       4.489ms             3      graph_5_kernel_cpp_18         0.08%      13.464ms         0.08%      13.464ms       4.488ms             3       graph_5_kernel_cpp_4         0.08%      13.456ms         0.08%      13.456ms       4.485ms             3      graph_1_kernel_cpp_81         0.08%      13.396ms         0.08%      13.396ms       4.465ms             3      graph_1_kernel_cpp_74         0.08%      13.355ms         0.08%      13.355ms       4.452ms             3      graph_1_kernel_cpp_53         0.08%      13.348ms         0.08%      13.348ms       4.449ms             3     graph_12_kernel_cpp_11         0.08%      13.332ms         0.08%      13.332ms       4.444ms             3     graph_15_kernel_cpp_32         0.08%      13.317ms         0.08%      13.317ms       4.439ms             3      graph_5_kernel_cpp_39         0.08%      13.305ms         0.08%      13.305ms       4.435ms             3      graph_2_kernel_cpp_53         0.08%      13.267ms         0.08%      13.267ms       4.422ms             3              Self CPU time total: 17.247s **Eager**                                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                 aten::addmm        52.64%        3.280s        58.06%        3.618s     495.552us          7300                   aten::bmm         8.43%     525.192ms         8.43%     525.192ms     218.830us          2400                   aten::add         7.74%     482.587ms         7.75%     483.054ms     123.860us          3900                   aten::div         6.84%     426.328ms         7.23%     450.471ms     346.516us          1300                 aten::copy_         6.53%     407.151ms         6.53%     407.151ms      37.699us         10800                  aten::gelu         5.66%     352.548ms         5.66%     352.548ms     293.790us          1200              aten::_softmax         4.21%     262.120ms         4.21%     262.120ms     218.433us          1200     aten::native_layer_norm         2.44%     152.096ms         2.56%     159.429ms      63.772us          2500                aten::linear         0.66%      41.319ms        59.65%        3.717s     509.139us          7300                aten::expand         0.47%      29.157ms         0.51%      31.720ms       2.600us         12200             aten::transpose         0.41%      25.438ms         0.48%      29.791ms       3.505us          8500                aten::matmul         0.41%      25.335ms         9.55%     595.245ms     248.019us          2400                  aten::view         0.35%      21.498ms         0.35%      21.498ms       0.877us         24500                     aten::t         0.31%      19.162ms         0.71%      43.998ms       6.027us          7300               aten::reshape         0.29%      17.767ms         0.39%      24.554ms       4.815us          5100               aten::permute         0.25%      15.397ms         0.27%      16.905ms       3.522us          4800          aten::index_select         0.24%      14.656ms         0.26%      15.906ms      53.020us           300                   aten::mul         0.22%      13.540ms         0.26%      16.038ms      40.095us           400              aten::_to_copy         0.20%      12.321ms         0.45%      28.054ms      12.197us          2300                 aten::clone         0.17%      10.861ms         1.44%      89.456ms      74.547us          1200                 aten::empty         0.16%       9.740ms         0.16%       9.740ms       1.070us          9100            aten::as_strided         0.15%       9.316ms         0.15%       9.316ms       0.339us         27500            aten::layer_norm         0.15%       9.231ms         2.71%     168.660ms      67.464us          2500               aten::softmax         0.14%       8.645ms         4.27%     265.976ms     221.647us          1200                    aten::to         0.13%       8.003ms         0.54%      33.950ms      13.580us          2500        aten::_reshape_alias         0.12%       7.332ms         0.12%       7.332ms       1.438us          5100                   aten::sum         0.11%       6.898ms         0.13%       8.194ms      40.970us           200         aten::empty_strided         0.09%       5.850ms         0.09%       5.850ms       2.543us          2300          aten::_unsafe_view         0.08%       4.827ms         0.08%       4.827ms       2.011us          2400                   aten::sub         0.06%       3.961ms         0.07%       4.541ms      45.410us           100            aten::contiguous         0.06%       3.736ms         1.49%      92.583ms      77.153us          1200            aten::empty_like         0.05%       3.253ms         0.10%       6.278ms       5.232us          1200                  aten::add_         0.04%       2.275ms         0.04%       2.275ms      22.750us           100                aten::select         0.03%       2.108ms         0.04%       2.192ms       3.131us           700             aten::embedding         0.03%       1.945ms         0.30%      18.883ms      62.943us           300                 aten::slice         0.03%       1.902ms         0.03%       1.996ms       3.327us           600                  aten::tanh         0.03%       1.873ms         0.03%       1.873ms      18.730us           100                    aten::ne         0.03%       1.745ms         0.03%       1.745ms       8.725us           200             aten::unsqueeze         0.02%       1.406ms         0.02%       1.537ms       3.074us           500                aten::cumsum         0.02%     975.000us         0.02%       1.542ms      15.420us           100                  aten::rsub         0.01%     645.000us         0.08%       5.186ms      51.860us           100               aten::detach_         0.01%     444.000us         0.01%     566.000us       5.660us           100                 aten::fill_         0.01%     328.000us         0.01%     328.000us       1.640us           200                     detach_         0.00%     162.000us         0.00%     162.000us       1.620us           100               aten::type_as         0.00%     139.000us         0.01%     614.000us       6.140us           100          aten::resolve_conj         0.00%      13.000us         0.00%      13.000us       0.001us         19400               aten::dropout         0.00%       9.000us         0.00%       9.000us       0.002us          3700            aten::lift_fresh         0.00%       3.000us         0.00%       3.000us       0.030us           100              Self CPU time total: 6.231s",Inductor mode generates 50 graphs and each one occupies about 1 GB of memory. It is found that inductor uses a high memory due to mkl linear weight prepack which would store an extra copy of the weight.,Perhaps we need a config to disable the weight prepacking for memory savings
transformer,[Torch 2.0] Fake Tensor Fails for codegen model," 🐛 Describe the bug ``` import torch from transformers import AutoModelForCausalLM,AutoTokenizer import torch._dynamo as dynamo from datetime import datetime from torch._dynamo.optimizations import backends tokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen6Bmono') model = AutoModelForCausalLM.from_pretrained(""Salesforce/codegen6Bmono"").to(device=""cuda:0"").eval() model = dynamo.optimize(""inductor"")(model)  This is the only line of code that we changed inputs = tokenizer(     ""func FormatInt(i int64, base int) string{      "",   Nvidia example prompt     add_special_tokens=True,     return_tensors=""pt"", ).to(device=""cuda:0"") p0 = datetime.utcnow() output = model(**inputs)  output = model.generate(inputs, max_new_tokens=256, max_time=20.0, do_sample=True,                                     temperature=0.8, top_p=0.95, use_cache=True,                                     pad_token_id=tokenizer.eos_token_id, num_return_sequences=1) p1 = datetime.utcnow() print(f""Time difference is {(p1  p0).total_seconds()} seconds"") print (output) ```  Error logs /home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/eval_frame.py:373: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled.Consider setting `torch.set_float32_matmul_precision('high')`   ""TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled."" Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/utils.py"", line 1054, in run_node     return node.target(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_subclasses/fake_tensor.py"", line 915, in __torch_dispatch__     r = func(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_ops.py"", line 284, in __call__     return self._op(*args, **kwargs or {})   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_prims_common/wrappers.py"", line 209, in _fn     result = fn(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_prims_common/wrappers.py"", line 119, in _fn     result = fn(**bound.arguments)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_refs/__init__.py"", line 1782, in where     lambda: f""expected predicate to be bool, got {pred.dtype}"",   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_prims_common/__init__.py"", line 1552, in check     raise exc_type(s()) RuntimeError: expected predicate to be bool, got torch.uint8 The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/utils.py"", line 1014, in get_fake_value     lambda: run_node(tx.output, node, args, kwargs, nnmodule)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/utils.py"", line 704, in wrap_fake_exception     return fn()   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/utils.py"", line 1014, in      lambda: run_node(tx.output, node, args, kwargs, nnmodule)   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/utils.py"", line 1065, in run_node     ) from e RuntimeError: Failed running call_function (*(FakeTensor(FakeTensor(..., device='meta', size=(1, 1, 14, 14), dtype=torch.uint8), cuda:0), FakeTensor(FakeTensor(..., device='meta', size=(1, 16, 14, 14), grad_fn=), cuda:0), FakeTensor(FakeTensor(..., device='meta', size=()), cuda:0)), **{}): expected predicate to be bool, got torch.uint8 (scroll up for backtrace)  Versions my environment: torch                     2.0.0.dev20221222+cu116 torchtensorrt        1.3.0 torchaudio                2.0.0.dev20221221+cu116 torchtriton               2.0.0+0d7e753227 torchvision               0.15.0.dev20221221+cu116 ",2022-12-22T11:33:54Z,bug oncall: pt2,closed,0,6,https://github.com/pytorch/pytorch/issues/91306,"It looks like this model calls `torch.where` with a uint8 conditional tensor, which is deprecated in eager mode(code), but doesn't currently work in our primtorch decomps (code). We should just allow it in the primtorch decomp, and emit the same deprecation warning.","I'm not sure we should allow it in primtorch, or use this as a forcing function to finally be able to deprecate it. ",fair point :)," you should modify your code to use a bool tensor instead of a byte tensor, do you need guidance on how to do so",">  you should modify your code to use a bool tensor instead of a byte tensor, do you need guidance on how to do so If you have a guide. Please send me. ",`dtype` here https://github.com/huggingface/transformers/blame/52dd2b61bff8af5b6409fdd5ec92a9b3114f3636/src/transformers/models/codegen/modeling_codegen.pyL101 has to be changed to `torch.bool`
transformer,`nn.TransformerEncoderLayer` fastpath (BetterTransformer) is slower than the normal path when no mask is provided," 🐛 Describe the bug Reproduction:    script ```python import argparse import torch import torch.nn as nn from tqdm import tqdm def get_parser():     parser = argparse.ArgumentParser()     parser.add_argument(         ""numbatches"",         type=int,         default=50,         help="""",     )     parser.add_argument(         ""batchsize"",         type=int,         default=64,         help="""",     )     parser.add_argument(         ""maxseqlen"",         type=int,         default=256,         help="""",     )     parser.add_argument(         ""usehalf"",         action=""store_true"",     )     parser.add_argument(         ""deviceid"",         type=int,         help=""GPU device id""     )     return parser def timing_cuda(model, num_batches, inputs):     start_event = torch.cuda.Event(enable_timing=True)     end_event = torch.cuda.Event(enable_timing=True)     start_event.record()     for _ in range(num_batches):         _ = model(inputs)     end_event.record()     torch.cuda.synchronize()     return (start_event.elapsed_time(end_event)) / num_batches def benchmark(num_batches: int, batch_size: int, max_seqlen: int, use_half: bool, device_id: int):     layers_vanilla = []     layers_bt = []     for i in range(12):         vanilla_layer = nn.TransformerEncoderLayer(d_model=768, nhead=12)   as bertbaseuncased         bt_layer = nn.TransformerEncoderLayer(d_model=768, nhead=12)         vanilla_layer.norm2.eps = 2e5   disable fastpath         assert vanilla_layer.norm1.eps != vanilla_layer.norm2.eps         layers_vanilla.append(vanilla_layer)         layers_bt.append(bt_layer)     vanilla_model = nn.Sequential(*layers_vanilla)     bt_model = nn.Sequential(*layers_bt)     inputs = torch.rand(batch_size, max_seqlen, 768)     if use_half is True:         vanilla_model = vanilla_model.half()         bt_model = bt_model.half()         inputs = inputs.half()     vanilla_model = vanilla_model.eval().to(f""cuda:{device_id}"")     bt_model = bt_model.eval().to(f""cuda:{device_id}"")     inputs = inputs.to(f""cuda:{device_id}"")      Warmup     _ = vanilla_model(inputs)     torch.cuda.synchronize()     _ = bt_model(inputs)     torch.cuda.synchronize()     vanilla_time = timing_cuda(vanilla_model, num_batches, inputs)     bt_time = timing_cuda(bt_model, num_batches, inputs)     return vanilla_time, bt_time if __name__ == ""__main__"":     parser = get_parser()     args = parser.parse_args()     BATCH_SIZES = [32, 64, 128]     SEQ_LEN = [16, 32, 64, 128, 256]     output_file = open(""log_transformerencoderlayer.py"", ""w"")     output_file.write(         ""num_batches, batch_size, seq_len, use half, Vanilla time (ms), BT time (ms), Speedup\n""     )     for bs in tqdm(BATCH_SIZES, desc=""batch size""):         for seq_len in tqdm(SEQ_LEN, desc=""sequence length""):             max_seqlen = seq_len             vanilla_time, bt_time = benchmark(                 args.num_batches,                 bs,                 max_seqlen,                 args.use_half,                 args.device_id             )             speedup = vanilla_time / bt_time             output_file.write(                 ""{},{},{},{},{},{},{}\n"".format(                     args.num_batches,                     bs,                     seq_len,                     args.use_half,                     f""{vanilla_time:.2f}"",                     f""{bt_time:.2f}"",                     f""{speedup:.3f}"",                 )             )     output_file.close() ```  Results (fp32):    Versions The above is run on a single A10080GB. PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.24.3 Libc version: glibc2.31 Python version: 3.9.13 (main, Oct 13 2022, 21:15:33)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.4.0125genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.0.221 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA DGX Display GPU 4: NVIDIA A100SXM480GB Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] torch==1.13.1 [pip3] torchaudio==2.0.0.dev20221220+cu116 [pip3] torchtriton==2.0.0+0d7e753227 [pip3] torchvision==0.15.0.dev20221220+cu116 [conda] numpy                     1.23.4                   pypi_0    pypi [conda] torch                     1.13.1                   pypi_0    pypi [conda] torchaudio                2.0.0.dev20221220+cu116          pypi_0    pypi [conda] torchtriton               2.0.0+0d7e753227          pypi_0    pypi [conda] torchvision               0.15.0.dev20221220+cu116          pypi_0    pypi",2022-12-22T11:09:19Z,module: nn triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/91305,"From looking at the optimum issue, is it safe to say that the slow down is not as severe in the Pytorch TransformerEncoderLayer. And the work around would be to not enable the BetterTransformer path? Better Transformer uses the NestedTensor subclass under the hood. This subclass excels at capitalizing on ragged sequence lengths. When no mask is provided then overhead of the conversion between Dense and NestedTensors can cause slow downs.   That all being said maybe we should guard against the fast_path if no mask is provided and fallback to the functional in that case.    ","Yes it's likely we have a bug / bad implementation on our side in optimum, the slowdown is more severe than here!",Thanks for your guidance    Seems to be fixed on optimum side now: https://github.com/huggingface/optimum/issues/626issuecomment1363412786 ,"Is this still an issue, or can we close this?   "
yi,Converting booleans into floats does not consistently yield exact 0 or 1," 🐛 Describe the bug **Updated**: Problem solved by using `.isclose()` I wrote a function to compute the fullsequence accuracy of predicted sequences against the target sequences. Part of the function looks like the following: ```python import torch def metrics(Y, Ypred):     pw_cmp = (Y == Ypred).float()      batchwise pairwise overlap rate     batch_overlap_rate = pw_cmp.mean(dim=0)      overlap_rate and absolute accuracy     overlap_rate = batch_overlap_rate.mean().item()     abs_correct = (batch_overlap_rate == 1.0)     abs_accu = abs_correct.float().mean().item() ``` One thing that is puzzling me is that sometimes `abs_correct` will contain all `False` even though most of the values inside `batch_overlap_rate` is 1. I do not know why I got the following. Is it an error? ```python print(batch_overlap_rate[0]) print(batch_overlap_rate2[0]) print(batch_overlap_rate[0] == torch.tensor(1.)) print(batch_overlap_rate2[0] == torch.tensor(1.))  tensor(1.0000, device='cuda:0')  tensor(1., device='cuda:0')   tensor(False, device='cuda:0')  tensor(True, device='cuda:0') ``` If I print `batch_overlap_rate` and `batch_overlap_rate2` out, essentially both are a torch.Tensor of `torch.Size([1000])` and consists of lots of 1.0s, but comparing the former with 1.0 yields only `False`. I checked the data types of both variables. Nothing different. The function is fixed, and this issue only occurs **occasionally** but in a very particular way. Only sequences of certain lengths lead to this problem. There are two other metrics inside this function (e.g., overlap rate), their values are always right.  **Updated**: The problem seems to be linked to floating precision. However, the output of `(Y == Ypred).float()` should always be either 0 or 1, right? ```python print(batch_overlap_rate[0]  1) print(batch_overlap_rate2[0]  1)  tensor(5.9605e08, device='cuda:0')  tensor(0., device='cuda:0') ```  Versions ``` 20221222 05:45:51 (9.46 MB/s)  ‘collect_env.py’ saved [17278/17278] Collecting environment information... PyTorch version: 1.13.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.6 Libc version: glibc2.27 Python version: 3.8.16 (default, Dec  7 2022, 01:12:13)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.10.133+x86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.2.152 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 460.32.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.1.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.1.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.13.0+cu116 [pip3] torchaudio==0.13.0+cu116 [pip3] torchsummary==1.5.1 [pip3] torchtext==0.14.0 [pip3] torchvision==0.14.0+cu116https://docs.github.com/github/writingongithub/gettingstartedwithwritingandformattingongithub/basicwritingandformattingsyntax [conda] Could not collect ```",2022-12-22T05:53:27Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/91294,I can not reproduce the issue you are providing. If you can produce a standalone script that reproduces the issue that would be very helpful. That being said the problem appears to be due to fp precision issues and any static casts that might be made when converting to and from tensors of different datatypes. I am going to close this issue for now. 
rag,Fix dtype mismatch for unallocated storage deserialization,Fixes CC(Saving and loading unallocated storage and tensor at the same time can result in incorrect dtype) ,2022-12-22T00:30:09Z,module: serialization triaged open source Merged ciflow/trunk,closed,1,10,https://github.com/pytorch/pytorch/issues/91285, I think you might want to check this?,"Yeah, I think when I do CC(Support for saving multiple storages/tensors that view same data as different dtypes) I'll hopefully be able to make the serialization code a little less insane", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / macos12py3arm64 / test (default, 2, 2, macosm112) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""unrelated failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,not able to import pipelines as torch.distributed is missing," 🐛 Describe the bug RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback): No module named 'torch.distributed' This is the error while im trying to import pipeline from transformer. My tensorflow is upraded to latesr version 2.11.0 and i am running this on windows 11 on visual code using python 3.10 Detailed Error if this helps: File c:\Users\varun\AppData\Local\Programs\Python\Python310\lib\sitepackages\transformers\utils\import_utils.py:1093, in _LazyModule._get_module(self, module_name)    1092 try: > 1093     return importlib.import_module(""."" + module_name, self.__name__)    1094 except Exception as e: File c:\Users\varun\AppData\Local\Programs\Python\Python310\lib\importlib\__init__.py:126, in import_module(name, package)     125         level += 1 > 126 return _bootstrap._gcd_import(name[level:], package, level) File :1050, in _gcd_import(name, package, level) File :1027, in _find_and_load(name, import_) File :1006, in _find_and_load_unlocked(name, import_) File :688, in _load_unlocked(spec) File :883, in exec_module(self, module) File :241, in _call_with_frames_removed(f, *args, **kwds) File c:\Users\varun\AppData\Local\Programs\Python\Python310\lib\sitepackages\transformers\pipelines\__init__.py:48      40 from ..utils import ( ...    1097         f"" traceback):\n{e}""    1098     ) from e  Versions 4.4.0 ",2022-12-20T16:22:11Z,oncall: distributed pipeline parallelism,open,0,2,https://github.com/pytorch/pytorch/issues/91173,How do you install torch?,"Hey sappa, +1 to  's question. And which version of PyTorch are you using? Default distributed support was not enabled until 1.7.0 release. If you are using an earlier version, you might need to build from source. "
finetuning,[FSDP] FSDP with CPU offload consumes `1.65X` more GPU memory when training models with most of the params frozen," 🐛 Describe the bug Context: We have more and more situations where a large part of the model that's being trained is frozen. As these are very large LLMs, we want to leverage FSDP with CPU offloading to fit such large model training with only a tiny fraction of training params on consumer GPUs. To this end, below is an example of finetuning `bigscience/mt0small` using LoRA parameter efficient finetuning method with/without FSDP. To finetune with FSDP:  1. Following https://github.com/huggingface/accelerate/issues/807, to avoid `AssertionError: expects all parameters to have same requires_grad`, created a custom `auto_wrap_policy` such that the layers with trainable params are in separate FSDP units than those which are frozen. The result model along with FSDP options are given below. We are using Accelerate's FSDP integration . the trainable params have `lora_` prefix: ``` FullyShardedDataParallelPlugin(sharding_strategy=, backward_prefetch=, mixed_precision_policy=None, auto_wrap_policy=functools.partial(, policies=[functools.partial(, lambda_fn=.lambda_policy_fn at 0x7f01ec31e3b0>), functools.partial(, transformer_layer_cls=(, , , ))]), cpu_offload=CPUOffload(offload_params=True), ignored_modules=None, state_dict_type=, state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=True), limit_all_gathers=False) FullyShardedDataParallel(   (_fsdp_wrapped_module): PETModelForSeq2SeqLM(     (base_model): LoRAModel(       (model): MT5ForConditionalGeneration(         (shared): Embedding(250112, 512)         (encoder): T5Stack(           (embed_tokens): Embedding(250112, 512)           (block): ModuleList(             (0): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                       (relative_attention_bias): Embedding(32, 6)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (1): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (2): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (3): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (4): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (5): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (6): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (7): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )           )           (final_layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )         (decoder): T5Stack(           (embed_tokens): Embedding(250112, 512)           (block): ModuleList(             (0): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                       (relative_attention_bias): Embedding(32, 6)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerCrossAttention(                     (EncDecAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (1): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerCrossAttention(                     (EncDecAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (2): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerCrossAttention(                     (EncDecAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (3): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerCrossAttention(                     (EncDecAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (4): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerCrossAttention(                     (EncDecAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (5): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerCrossAttention(                     (EncDecAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (6): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerCrossAttention(                     (EncDecAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (7): FullyShardedDataParallel(               (_fsdp_wrapped_module): T5Block(                 (layer): ModuleList(                   (0): T5LayerSelfAttention(                     (SelfAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): T5LayerCrossAttention(                     (EncDecAttention): T5Attention(                       (q): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (k): Linear(in_features=512, out_features=384, bias=False)                       (v): Linear(                         in_features=512, out_features=384, bias=False                         (lora_dropout): Dropout(p=0.1, inplace=False)                         (lora_A): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=512, out_features=8, bias=False)                         )                         (lora_B): FullyShardedDataParallel(                           (_fsdp_wrapped_module): Linear(in_features=8, out_features=384, bias=False)                         )                       )                       (o): Linear(in_features=384, out_features=512, bias=False)                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): T5LayerFF(                     (DenseReluDense): T5DenseGatedActDense(                       (wi_0): Linear(in_features=512, out_features=1024, bias=False)                       (wi_1): Linear(in_features=512, out_features=1024, bias=False)                       (wo): Linear(in_features=1024, out_features=512, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )           )           (final_layer_norm): FusedRMSNorm(torch.Size([512]), eps=1e06, elementwise_affine=True)           (dropout): Dropout(p=0.1, inplace=False)         )         (lm_head): Linear(in_features=512, out_features=250112, bias=False)       )     )   ) ) ``` 2. The number of trainable params are given below: ``` trainable params: 344064  trainable%: 0.11448923447676333 ``` 3. Now, the issue is that in comparison to **plain Pytorch**, FSDP consumes **1.65X more GPU memory** instead of reducing the same by a large amount (expectation) while also greatly increasing the memory consumed on CPU. Below are the screenshots for the same. The hardware used is 1 A100 80GB GPU. `Plain PyTorch`: !Screenshot 20221220 at 3 45 16 PM `FSDP Full Shard with CPU offloading`: !Screenshot 20221220 at 4 14 04 PM 4. **When trying to use FSDP with CPU offloading using `bigscience/mt0xxl` model (13B params) on a A100 80GB GPU it results in OOM GPU error whereas Plain Pytorch consumes 56GB GPU memory.** 5. **Expected behaviour**: Efficiently deal with frozen weights during training such that large models could be offloaded on CPUs/sharded across GPUs properly with storage of the optimizer state only for the trainable parameters, e.g., we can see that using plain PyTorch, mt0xxl (13B params) model takes up 56GB on GPU, now, it would be really helpful if one could do CPU offloading such that training could work on a 16GB or 24GB GPU using FSDP with CPU offloading.  Versions Collecting environment information... PyTorch version: 1.14.0.dev20221117+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.0125genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA A100SXM480GB GPU 1: NVIDIA A100SXM480GB GPU 2: NVIDIA A100SXM480GB GPU 3: NVIDIA DGX Display GPU 4: NVIDIA A100SXM480GB Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] msgpacknumpy==0.4.7.1 [pip3] numpy==1.23.4 [pip3] torch==1.14.0.dev20221117+cu117 [pip3] torchaudio==0.14.0.dev20221117 [pip3] torchtriton==2.0.0+0d7e753227 [pip3] torchvision==0.15.0.dev20221117 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0           py310h7f8727e_0   [conda] mkl_fft                   1.3.1           py310hd6ae3a3_0   [conda] mkl_random                1.2.2           py310h00e6091_0   [conda] msgpacknumpy             0.4.7.1                  pypi_0    pypi [conda] numpy                     1.23.4                   pypi_0    pypi [conda] pytorchcuda              11.7                 h67b0de4_0    pytorchnightly [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torch                     1.14.0.dev20221117+cu117          pypi_0    pypi [conda] torchaudio                0.14.0.dev20221117     py310_cu117    pytorchnightly [conda] torchtriton               2.0.0+0d7e753227          pypi_0    pypi [conda] torchvision               0.15.0.dev20221117     py310_cu117    pytorchnightly ",2022-12-20T10:49:54Z,high priority triage review oncall: distributed triaged module: fsdp,open,2,20,https://github.com/pytorch/pytorch/issues/91165,"Thanks for providing so many details! This helps us out a lot. We may need more time to investigate why CPU offload consumes so much more GPU memory. For now, have you guys tried setting `limit_all_gathers=True`? We plan to default to this soon. Without this, the memory usage may be unexpectedly high (as long as the CPU thread runs faster than the GPU execution, which is almost always). This may or may not be related to this issue. IIUC, CPU offload was required for the 1T parameter training run (cc: varma ), so the memory savings were proven there. Conceptually, I am not seeing how frozen parameters/layers affect the GPU memory usage aside from the optimizer state as you mentioned. In that case, if you think it is helpful, could you verify what optimizer states are being created when using FSDP? Are you seeing that optimizer states are being created even for `FlatParameter`s that do not require gradient?","Hello , thanks for your quick reply. Apart from the optimizer states only for only the tiny portion of trainable params, offloading to CPU should further reduce GPU memory usage. In case of CPU offloading, the expected behaviour would be to only move model shards (T5 block one at a time as they belong to separate FSDP units) from CPU to GPU, do the forward pass computation and then GPU to CPU (similar to B/w pass), **hence the max memory consumption on GPU should be that of a single T5 block only**. Let me know if there are gaps in my understanding.", Do you have a recommendation for how to try to reproduce this?," From the source code, it seems that another problem of FSDP is that the freezed parameters are not deallocated until the end of the backward. If most parameters are frozen in this case, then the GPU would have a nearly whole copy of the model parameters. ",">  From the source code, it seems that another problem of FSDP is that the freezed parameters are not deallocated until the end of the backward. If most parameters are frozen in this case, then the GPU would have a nearly whole copy of the model parameters. This is unfortunately a known problem :( We would need to add special hooks to reshard the frozen parameters after they are used for gradient computation. We have not had any bandwidth to look into this though.","> >  From the source code, it seems that another problem of FSDP is that the freezed parameters are not deallocated until the end of the backward. If most parameters are frozen in this case, then the GPU would have a nearly whole copy of the model parameters. >  > This is unfortunately a known problem :( >  > We would need to add special hooks to reshard the frozen parameters after they are used for gradient computation. We have not had any bandwidth to look into this though. Is it possible to use a `sized_policy` to deal with those frozen parameters?"," I do not think so. The issue is more fundamental than simply choosing the wrapping policy and comes from when a parameter corresponds to a module whose output activations require gradient but itself does not require gradient  in that case, FSDP's prebackward hook runs but its postbackward hook does not."," thanks for the quick reply. u r right, wrapping policy only effects how to shard the model. hope it'll be optimized in the next version.;)","> We would need to add special hooks to reshard the frozen parameters after they are used for gradient computation. We have not had any bandwidth to look into this though.  any idea about how to add the hook? e.g., torch.autograd.Function."," I am testing https://github.com/pytorch/pytorch/pull/101982/ out. I am seeing memory savings in my experiments, but   is not at the moment. We are investigating."," , just to update the thread as we discussed offline, I could also observe the memory savings as well.","Since https://github.com/pytorch/pytorch/pull/101982 landed, I would recommend people try things out again (e.g. with a nightly). There should be memory savings now.","Thank you  and . Will be trying this out this week and report back. This should work with CPU offloading too, right?"," Yes, it should work with CPU offloading. (It turns out that CPU offloading is connected to ""freeing the parameters"", so once we fixed ""freeing the parameters"", the parameters should be offloaded to CPU as well if enabled.)","> When trying to use FSDP with CPU offloading using bigscience/mt0xxl model (13B params) on a A100 80GB GPU it results in OOM GPU error whereas Plain Pytorch consumes 56GB GPU memory. Tried the latest nightly '2.1.0.dev20230620' Using 2 A100 GPUs, able to run `bigscience/mt0xxl` but doesn't result in any significant memory savings at all compared to Plain PyTorch while consuming a whole lot of CPU memory. ``` GPU Memory before entering the train : 0 GPU Memory consumed at the end of the train (endbegin): 315 GPU Peak Memory consumed during the train (maxbegin): 53360 GPU Total Peak Memory consumed during the train (max): 53360 CPU Memory before entering the train : 26816 CPU Memory consumed at the end of the train (endbegin): 33817 CPU Peak Memory consumed during the train (maxbegin): 57895 CPU Total Peak Memory consumed during the train (max): 84711 ```  As I stated earlier, the expected behaviour is > Expected behaviour: Efficiently deal with frozen weights during training such that large models could be offloaded on  CPUs/sharded across GPUs properly with storage of the optimizer state only for the trainable parameters, e.g., we can see that using plain PyTorch, mt0xxl (13B params) model takes up 56GB on GPU, now, it would be really helpful if one could do CPU offloading such that training could work on a 16GB or 24GB GPU using FSDP with CPU offloading. Steps to reproduce: 1. Tried the latest nightly '2.1.0.dev20230620' 2. install peft, transformers and accelerate from source. 3. remove line 1316 from `accelerator.py` as FSDP in nightly doesn't support it. ```diff kwargs = {                         ""sharding_strategy"": fsdp_plugin.sharding_strategy,                         ""cpu_offload"": fsdp_plugin.cpu_offload,                         ""auto_wrap_policy"": fsdp_plugin.auto_wrap_policy,                         ""mixed_precision"": fsdp_plugin.mixed_precision_policy,                         ""sync_module_states"": fsdp_plugin.sync_module_states,                         ""backward_prefetch"": fsdp_plugin.backward_prefetch,                         ""forward_prefetch"": fsdp_plugin.forward_prefetch,                         ""use_orig_params"": fsdp_plugin.use_orig_params,                         ""param_init_fn"": fsdp_plugin.param_init_fn,                         ""ignored_modules"": fsdp_plugin.ignored_modules,                         ""ignored_parameters"": fsdp_plugin.ignored_parameters,                         ""limit_all_gathers"": fsdp_plugin.limit_all_gathers,                         ""device_id"": self.device,                     }                     model = FSDP(model, **kwargs) ``` 4. Follow https://github.com/huggingface/peftcaveats related to FSDP. Use the below example to track GPU and CPU memory usage: FSDP config: ``` compute_environment: LOCAL_MACHINE distributed_type: FSDP downcast_bf16: 'no' fsdp_config:   fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP   fsdp_backward_prefetch_policy: BACKWARD_PRE   fsdp_forward_prefetch: false   fsdp_offload_params: true   fsdp_sharding_strategy: 1   fsdp_state_dict_type: FULL_STATE_DICT   fsdp_sync_module_states: false   fsdp_transformer_layer_cls_to_wrap: MT5Block   fsdp_use_orig_params: true machine_rank: 0 main_training_function: main mixed_precision: 'no' num_machines: 1 num_processes: 2 rdzv_backend: static same_network: true tpu_env: [] tpu_use_cluster: false tpu_use_sudo: false use_cpu: false ``` Code ``` import gc import os import sys import threading import numpy as np import psutil import torch from accelerate import Accelerator from datasets import load_dataset from torch.utils.data import DataLoader from tqdm import tqdm from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed from peft import LoraConfig, TaskType, get_peft_model from peft.utils.other import fsdp_auto_wrap_policy  Converting Bytes to Megabytes def b2mb(x):     return int(x / 2**20)  This context manager is used to track the peak memory usage of the process class TorchTracemalloc:     def __enter__(self):         gc.collect()         torch.cuda.empty_cache()         torch.cuda.reset_max_memory_allocated()   reset the peak gauge to zero         self.begin = torch.cuda.memory_allocated()         self.process = psutil.Process()         self.cpu_begin = self.cpu_mem_used()         self.peak_monitoring = True         peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)         peak_monitor_thread.daemon = True         peak_monitor_thread.start()         return self     def cpu_mem_used(self):         """"""get resident set size memory for the current process""""""         return self.process.memory_info().rss     def peak_monitor_func(self):         self.cpu_peak = 1         while True:             self.cpu_peak = max(self.cpu_mem_used(), self.cpu_peak)              can't sleep or will not catch the peak right (this comment is here on purpose)              time.sleep(0.001)  1msec             if not self.peak_monitoring:                 break     def __exit__(self, *exc):         self.peak_monitoring = False         gc.collect()         torch.cuda.empty_cache()         self.end = torch.cuda.memory_allocated()         self.peak = torch.cuda.max_memory_allocated()         self.used = b2mb(self.end  self.begin)         self.peaked = b2mb(self.peak  self.begin)         self.cpu_end = self.cpu_mem_used()         self.cpu_used = b2mb(self.cpu_end  self.cpu_begin)         self.cpu_peaked = b2mb(self.cpu_peak  self.cpu_begin)          print(f""delta used/peak {self.used:4d}/{self.peaked:4d}"") def main():     accelerator = Accelerator()      model_name_or_path = ""bigscience/T0_3B""     model_name_or_path = ""bigscience/mt0xxl""""facebook/bartlarge""     dataset_name = ""twitter_complaints""     peft_config = LoraConfig(         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1     )     text_column = ""Tweet text""     label_column = ""text_label""     lr = 3e3     num_epochs = 5     batch_size = 8     seed = 42     do_test = False     set_seed(seed)     dataset = load_dataset(""ought/raft"", dataset_name)     classes = [k.replace(""_"", "" "") for k in dataset[""train""].features[""Label""].names]     dataset = dataset.map(         lambda x: {""text_label"": [classes[label] for label in x[""Label""]]},         batched=True,         num_proc=1,     )     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)     target_max_length = max([len(tokenizer(class_label)[""input_ids""]) for class_label in classes])     def preprocess_function(examples):         inputs = examples[text_column]         targets = examples[label_column]         model_inputs = tokenizer(inputs, truncation=True)         labels = tokenizer(             targets, max_length=target_max_length, padding=""max_length"", truncation=True, return_tensors=""pt""         )         labels = labels[""input_ids""]         labels[labels == tokenizer.pad_token_id] = 100         model_inputs[""labels""] = labels         return model_inputs     with accelerator.main_process_first():         processed_datasets = dataset.map(             preprocess_function,             batched=True,             num_proc=1,             remove_columns=dataset[""train""].column_names,             load_from_cache_file=True,             desc=""Running tokenizer on dataset"",         )     accelerator.wait_for_everyone()     train_dataset = processed_datasets[""train""]     eval_dataset = processed_datasets[""train""]     test_dataset = processed_datasets[""test""]     def collate_fn(examples):         return tokenizer.pad(examples, padding=""longest"", return_tensors=""pt"")     train_dataloader = DataLoader(         train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True     )     eval_dataloader = DataLoader(eval_dataset, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True)     test_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True)      creating model     model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)     model = get_peft_model(model, peft_config)     model.print_trainable_parameters()      optimizer     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)      lr scheduler     lr_scheduler = get_linear_schedule_with_warmup(         optimizer=optimizer,         num_warmup_steps=0,         num_training_steps=(len(train_dataloader) * num_epochs),     )     if getattr(accelerator.state, ""fsdp_plugin"", None) is not None:         accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)     model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(         model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler     )     accelerator.print(model)     for epoch in range(num_epochs):         with TorchTracemalloc() as tracemalloc:             model.train()             total_loss = 0             for step, batch in enumerate(tqdm(train_dataloader)):                 outputs = model(**batch)                 loss = outputs.loss                 total_loss += loss.detach().float()                 accelerator.backward(loss)                 optimizer.step()                 lr_scheduler.step()                 optimizer.zero_grad()          Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage         accelerator.print(""GPU Memory before entering the train : {}"".format(b2mb(tracemalloc.begin)))         accelerator.print(""GPU Memory consumed at the end of the train (endbegin): {}"".format(tracemalloc.used))         accelerator.print(""GPU Peak Memory consumed during the train (maxbegin): {}"".format(tracemalloc.peaked))         accelerator.print(             ""GPU Total Peak Memory consumed during the train (max): {}"".format(                 tracemalloc.peaked + b2mb(tracemalloc.begin)             )         )         accelerator.print(""CPU Memory before entering the train : {}"".format(b2mb(tracemalloc.cpu_begin)))         accelerator.print(""CPU Memory consumed at the end of the train (endbegin): {}"".format(tracemalloc.cpu_used))         accelerator.print(""CPU Peak Memory consumed during the train (maxbegin): {}"".format(tracemalloc.cpu_peaked))         accelerator.print(             ""CPU Total Peak Memory consumed during the train (max): {}"".format(                 tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)             )         )         train_epoch_loss = total_loss / len(train_dataloader)         train_ppl = torch.exp(train_epoch_loss)         accelerator.print(f""{epoch=}: {train_ppl=} {train_epoch_loss=}"") if __name__ == ""__main__"":     main() ``` output logs: ``` trainable params: 9,437,184  2/2 [00:00<00:00, 1349.52it/s] FSDP Warning: When using FSDP, it is efficient and recommended to call prepare for the model before creating the optimizer      FullyShardedDataParallel(                                                                                                         (_fsdp_wrapped_module): PeftModelForSeq2SeqLM(     (base_model): LoraModel(       (model): MT5ForConditionalGeneration(         (shared): Embedding(250112, 4096)         (encoder): MT5Stack(           (embed_tokens): Embedding(250112, 4096)           (block): ModuleList(             (0): FullyShardedDataParallel(               (_fsdp_wrapped_module): MT5Block(                 (layer): ModuleList(                   (0): MT5LayerSelfAttention(                     (SelfAttention): MT5Attention(                       (q): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (k): Linear(in_features=4096, out_features=4096, bias=False)                       (v): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (o): Linear(in_features=4096, out_features=4096, bias=False)                       (relative_attention_bias): Embedding(32, 64)                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): MT5LayerFF(                     (DenseReluDense): MT5DenseGatedActDense(                       (wi_0): Linear(in_features=4096, out_features=10240, bias=False)                       (wi_1): Linear(in_features=4096, out_features=10240, bias=False)                       (wo): Linear(in_features=10240, out_features=4096, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (123): 23 x FullyShardedDataParallel(               (_fsdp_wrapped_module): MT5Block(                 (layer): ModuleList(                   (0): MT5LayerSelfAttention(                     (SelfAttention): MT5Attention(                       (q): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (k): Linear(in_features=4096, out_features=4096, bias=False)                       (v): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (o): Linear(in_features=4096, out_features=4096, bias=False)                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): MT5LayerFF(                     (DenseReluDense): MT5DenseGatedActDense(                       (wi_0): Linear(in_features=4096, out_features=10240, bias=False)                       (wi_1): Linear(in_features=4096, out_features=10240, bias=False)                       (wo): Linear(in_features=10240, out_features=4096, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )           )           (final_layer_norm): MT5LayerNorm()           (dropout): Dropout(p=0.1, inplace=False)         )         (decoder): MT5Stack(           (embed_tokens): Embedding(250112, 4096)           (block): ModuleList(             (0): FullyShardedDataParallel(               (_fsdp_wrapped_module): MT5Block(                 (layer): ModuleList(                   (0): MT5LayerSelfAttention(                     (SelfAttention): MT5Attention(                       (q): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (k): Linear(in_features=4096, out_features=4096, bias=False)                       (v): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (o): Linear(in_features=4096, out_features=4096, bias=False)                       (relative_attention_bias): Embedding(32, 64)                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): MT5LayerCrossAttention(                     (EncDecAttention): MT5Attention(                       (q): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (k): Linear(in_features=4096, out_features=4096, bias=False)                       (v): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (o): Linear(in_features=4096, out_features=4096, bias=False)                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): MT5LayerFF(                     (DenseReluDense): MT5DenseGatedActDense(                       (wi_0): Linear(in_features=4096, out_features=10240, bias=False)                       (wi_1): Linear(in_features=4096, out_features=10240, bias=False)                       (wo): Linear(in_features=10240, out_features=4096, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )             (123): 23 x FullyShardedDataParallel(               (_fsdp_wrapped_module): MT5Block(                 (layer): ModuleList(                   (0): MT5LayerSelfAttention(                     (SelfAttention): MT5Attention(                       (q): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (k): Linear(in_features=4096, out_features=4096, bias=False)                       (v): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (o): Linear(in_features=4096, out_features=4096, bias=False)                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (1): MT5LayerCrossAttention(                     (EncDecAttention): MT5Attention(                       (q): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (k): Linear(in_features=4096, out_features=4096, bias=False)                       (v): Linear(                         in_features=4096, out_features=4096, bias=False                         (lora_dropout): ModuleDict(                           (default): Dropout(p=0.1, inplace=False)                         )                         (lora_A): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=4096, out_features=8, bias=False)                           )                         )                         (lora_B): ModuleDict(                           (default): FullyShardedDataParallel(                             (_fsdp_wrapped_module): Linear(in_features=8, out_features=4096, bias=False)                           )                         )                         (lora_embedding_A): ParameterDict()                         (lora_embedding_B): ParameterDict()                       )                       (o): Linear(in_features=4096, out_features=4096, bias=False)                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                   (2): MT5LayerFF(                     (DenseReluDense): MT5DenseGatedActDense(                       (wi_0): Linear(in_features=4096, out_features=10240, bias=False)                       (wi_1): Linear(in_features=4096, out_features=10240, bias=False)                       (wo): Linear(in_features=10240, out_features=4096, bias=False)                       (dropout): Dropout(p=0.1, inplace=False)                       (act): NewGELUActivation()                     )                     (layer_norm): MT5LayerNorm()                     (dropout): Dropout(p=0.1, inplace=False)                   )                 )               )             )           )           (final_layer_norm): MT5LayerNorm()           (dropout): Dropout(p=0.1, inplace=False)         )         (lm_head): Linear(in_features=4096, out_features=250112, bias=False)       )     )   ) ) /home/sourab/miniconda3/envs/ml/lib/python3.11/sitepackages/torch/cuda/memory.py:307: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.   warnings.warn( /home/sourab/miniconda3/envs/ml/lib/python3.11/sitepackages/torch/cuda/memory.py:307: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.   warnings.warn( GPU Memory before entering the train : 0 GPU Memory consumed at the end of the train (endbegin): 315 GPU Peak Memory consumed during the train (maxbegin): 53360 GPU Total Peak Memory consumed during the train (max): 53360 CPU Memory before entering the train : 26816 CPU Memory consumed at the end of the train (endbegin): 33817 CPU Peak Memory consumed during the train (maxbegin): 57895 CPU Total Peak Memory consumed during the train (max): 84711 epoch=0: train_ppl=tensor(16.8360, device='cuda:0') train_epoch_loss=tensor(2.8235, device='cuda:0') ```", Can you please share the manually warp code for lora？ I encounter the same problem. Many Thanks!,  Is this issue solved in recent pytorch versions? Should I not bother with trying FSDP for reducing memory in a finetuning task with most parameters frozen?,Still having OOM issue with CPU offload on a 7b model in mixed precision,"Hello, come across this important discussion. In the latest PyTorch Nightlies (for example in the LLaMAreceipes implementation),  runing PEFT with FSDP indeeds saves memory. For example, when using a minibatch size of 4 to train 13B model on 4 GPU (with pure BF16), VRAM per GPU is 25G (PEFT) vs 55g (full model). Could we confirm this issue has resolved in the latest PyTorch Nightlies? Many thanks. Also note to future readers, there are some excellent discussion on this led by :  CC([FSDP] ValueError: `FlatParameter` requires uniform `requires_grad`)  CC(when i try to use FullyShardedDataParallel to finetune a LLM with huggingface peft library, pytorch raise error ValueError: `FlatParameter` requires uniform `requires_grad`)  CC([FSDP] ValueError: `FlatParameter` requires uniform `requires_grad`) Also looking at LLaMArecipes implementation, seems they largely followed the rec from this post). They will use the customized lambda_policy_fn only when using peft. "," There's a lot of issues open regarding FSDP + PEFT. I've been looking at a lot of these discussion threads and unfortunately even with CPU Offloading and a wrapping policy suggested hereissuecomment1626894957), I'm still facing persistent OOM issues. Thisissuecomment1544607463) was another issue we'd like to hear updates on. Is it possible to consolidate everything regarding this topic (current workarounds, PT nightlies, current problems, WIP if any) either on this thread or in a new issue on this repo? That would really help a lot of us currently struggling to evade OOMs with FSDP + PEFT."
transformer,"`torch.compile` with `""inductor""` backend is much slower than `torch._dynamo.optimize`"," 🐛 Describe the bug I don't quite understand the difference, to me it's only the `_TorchCompileInductorWrapper`. With `torch._dynamo.optimize()`: !image With `torch.compile()`: !image Reproduce using this Dockerfile: ``` FROM nvidia/cuda:11.7.1cudnn8develubuntu22.04  build with `docker build f Dockerfile t containertorchdynamo .`  run with `docker run gpus device=4 it v $(pwd)/scripts:/workspace containertorchdynamo:latest python benchmark.py usecuda` ENV PATH=""/root/miniconda3/bin:${PATH}"" ARG PATH=""/root/miniconda3/bin:${PATH}"" RUN aptget update && aptget upgrade y RUN aptget install y wget && rm rf /var/lib/apt/lists/* RUN wget \     https://repo.anaconda.com/miniconda/Miniconda3latestLinuxx86_64.sh \     && mkdir /root/.conda \     && bash Miniconda3latestLinuxx86_64.sh b \     && rm f Miniconda3latestLinuxx86_64.sh RUN conda init bash RUN aptget update && aptget upgrade y && aptget install y noinstallrecommends \     git && \     aptget clean RUN pip install upgrade numpy RUN pip install pre torch extraindexurl https://download.pytorch.org/whl/nightly/cu117 RUN pip install transformers tqdm WORKDIR /workspace ``` And this script, by commenting the lines ```python     dynamo_model = torch.compile(model_copy, backend=""inductor"")     dynamo_model = dynamo.optimize(""inductor"")(model_copy) ``` to use one or the other: ```python import torch torch.set_float32_matmul_precision(""high"") torch.backends.cuda.matmul.allow_tf32 = True import torch._dynamo as dynamo import argparse import copy from tqdm import tqdm from transformers import AutoModel def get_parser():     parser = argparse.ArgumentParser()     parser.add_argument(         ""numbatches"",         type=int,         default=100,         help="""",     )     parser.add_argument(         ""batchsize"",         type=int,         default=64,         help="""",     )     parser.add_argument(         ""maxseqlen"",         type=int,         default=256,         help="""",     )     parser.add_argument(         ""modelname"",         type=str,         default=""bertbaseuncased"",         help="""",     )     parser.add_argument(         ""usecuda"",         action=""store_true"",     )     parser.add_argument(         ""usehalf"",         action=""store_true"",     )     parser.add_argument(         ""usemask"",         action=""store_true"",     )     return parser def timing_cuda(model, num_batches, input_ids):     print(""model device:"", model.device)     print(""input device:"", input_ids.device)     print(type(model))     start_event = torch.cuda.Event(enable_timing=True)     end_event = torch.cuda.Event(enable_timing=True)     start_event.record()     with torch.no_grad():         for i in range(num_batches):             _ = model(input_ids)     end_event.record()     torch.cuda.synchronize()     return (start_event.elapsed_time(end_event) * 1.0e3) / num_batches def benchmark(model_name, num_batches, batch_size, sequence_length, is_cuda, is_half, use_mask):     print(""Loading model {}"".format(model_name))     hf_model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16 if is_half else None).eval()     if is_cuda:         hf_model = hf_model.to(0)     model_copy = copy.deepcopy(hf_model)     dynamo_model = torch.compile(model_copy, backend=""inductor"")     dynamo_model = dynamo.optimize(""inductor"")(model_copy)     hf_model.eval()     dynamo_model.eval()     vocab_size = 30522  TODO: generalize     input_ids = torch.randint(vocab_size, (batch_size, sequence_length), dtype=torch.long)     if is_cuda:         input_ids = input_ids.to(0)     if use_mask:         raise NotImplementedError()      Warmup     _ = hf_model(input_ids)     torch.cuda.synchronize()     _ = dynamo_model(input_ids)     torch.cuda.synchronize()     print(""input_ids:"", input_ids)     print(""input_ids shape:"", input_ids.shape)     total_hf_time = timing_cuda(hf_model, num_batches, input_ids)     total_dynamo_time = timing_cuda(dynamo_model, num_batches, input_ids)     return total_dynamo_time, total_hf_time if __name__ == ""__main__"":     parser = get_parser()     args = parser.parse_args()     BATCH_SIZES = [8, 16]     SEQ_LEN = [64, 128]     BATCH_SIZES = [8]     SEQ_LEN = [128]     output_file = open(""log_{}_compile.csv"".format(args.model_name.replace(""/"", """")), ""w"")     output_file.write(         ""num_batches, batch_size, seq_len, is cuda, is half, HF time, dynamo time, Speedup\n""     )     for batch_size in tqdm(BATCH_SIZES, desc=""Batch size""):         for sequence_length in tqdm(SEQ_LEN, desc=""Sequence length""):             total_dynamo_time, total_hf_time = benchmark(                 args.model_name,                 args.num_batches,                 batch_size,                 sequence_length,                 args.use_cuda,                 args.use_half,                 args.use_mask,             )             speedup = total_hf_time / total_dynamo_time             output_file.write(                 ""{},{},{},{},{},{},{},{}\n"".format(                     args.num_batches,                     batch_size,                     sequence_length,                     args.use_cuda,                     args.use_half,                     total_hf_time,                     total_dynamo_time,                     speedup,                 )             )     output_file.close() ```  Versions ``` PyTorch version: 2.0.0.dev20221216+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.35 Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.0125genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A100SXM480GB Nvidia driver version: 515.65.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==2.0.0.dev20221216+cu117 [pip3] torchtriton==2.0.0+0d7e753227 [conda] numpy                     1.23.5                   pypi_0    pypi [conda] torch                     2.0.0.dev20221216+cu117          pypi_0    pypi [conda] torchtriton               2.0.0+0d7e753227          pypi_0    pypi ```  (the , sorry)",2022-12-19T13:06:33Z,oncall: pt2 module: inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/91095,"LIkely the difference is in enabling/disabling cudagraphs, you can do it with `torch.compile` by passing `mode=reduce_overhead` to torch.compile","Can confirm the difference is in passing `mode=""reduceoverhead""` versus `model=""default""`. Thank you, feel free to close!"
rag,[MPS] Fix tensor with non-zero storage offset graph gathering,"Previously, the ""can slice"" flag in Placeholder constructor in `OperationUtils.mm` is conditioned on whether the numbers of dimensions of base shape and view shape are the same. This doesn't consider the situation that a view tensor could be the base tensor's sliced and then unsqueezed version, resulting in different num of dims. For example, if we want to stack `y_mps` and `x_mps` on the last dim: ``` t_mps = torch.tensor([1, 2, 3, 4], device=""mps"") x_mps = t_mps[2:]   [3, 4] y_mps = t_mps[:2]   [1, 2] res_mps = torch.stack((y_mps, x_mps), dim=1) ``` the kernel will unsqueeze both of them on the last dim and then concatenate them, which is equivalent to: ``` res_mps = torch.cat((y_mps.unsqueeze(1), x_mps.unsqueeze(1)), dim=1) ``` `x_mps.unsqueeze(1)` is an unsqueezed and contiguous tensor with a storage offset, this kind of tensors should be sliceable without cloning its storage. Fixes CC(torch.stack gives wrong results on MPS ) Fixes CC([MPS] Incorrect results of two multiplied chunked tensors produced by `unsafe_chunk`) ",2022-12-18T04:31:46Z,triaged open source Merged ciflow/trunk module: mps release notes: mps ciflow/mps keep-going,closed,0,9,https://github.com/pytorch/pytorch/issues/91071," label ""module: mps"""," label ""ciflow/trunk"""," label ""keepgoing""","Modulo the nit, changes look good.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 jobs have failed, first few of them are: trunk / macos12py3x8664 / test (default, 1, 3, macos12) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,lint transformer.py,Stack from ghstack:  CC(lint transformer.py),2022-12-16T23:37:11Z,Merged topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/91048," merge f ""fixes a lint that is broken on master"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,Trying to extract a concrete int out of a symbolic int," 🐛 Describe the bug ```python import torch import torch._dynamo torch._dynamo.config.verbose = True .compile(dynamic=True) def foo(a):     return torch.zeros_like(a) x = torch.randint(0, 1024, size=(100,)) r = foo(x) ```  Error logs ```python /tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py:372: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but  not enabled.Consider setting `torch.set_float32_matmul_precision('high')`   warnings.warn( Traceback (most recent call last):   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/sympy/core/cache.py"", line 70, in wrapper     retval = cfunc(*args, **kwargs) TypeError: unhashable type: 'SymInt' During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/output_graph.py"", line 659, in call_user_compiler     compiled_fn = compiler_fn(gm, self.fake_example_inputs())   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/debug_utils.py"", line 934, in debug_wrapper     compiled_gm = compiler_fn(gm, example_inputs, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/__init__.py"", line 1153, in __call__     return self.compile_fn(model_, inputs_)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/compile_fx.py"", line 398, in compile_fx     return aot_autograd(   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/optimizations/training.py"", line 78, in compiler_fn     cg = aot_module_simplified(gm, example_inputs, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py"", line 2333, in aot_module_simplified     compiled_fn = create_aot_dispatcher_function(   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py"", line 2030, in create_aot_dispatcher_function     compiled_fn = compiler_fn(flat_fn, fake_flat_tensor_args, aot_config)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py"", line 1297, in aot_wrapper_dedupe     return compiler_fn(flat_fn, leaf_flat_args, aot_config)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py"", line 954, in aot_dispatch_base     compiled_fw = aot_config.fw_compiler(fw_module, flat_args)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/compile_fx.py"", line 373, in fw_compiler     return inner_compile(   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/debug_utils.py"", line 507, in debug_wrapper     compiled_fn = compiler_fn(gm, example_inputs, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/debug.py"", line 223, in inner     return fn(*args, **kwargs)   File ""/usr/lib/python3.8/contextlib.py"", line 75, in inner     return func(*args, **kwds)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/compile_fx.py"", line 139, in compile_fx_inner     graph.run(*example_inputs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/graph.py"", line 168, in run     return super().run(*args)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/fx/interpreter.py"", line 130, in run     self.env[node] = self.run_node(node)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/graph.py"", line 367, in run_node     result = super().run_node(n)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/fx/interpreter.py"", line 171, in run_node     return getattr(self, n.op)(n.target, args, kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/graph.py"", line 253, in placeholder     sizes, strides = self.static_sizes_strides(example)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/graph.py"", line 78, in static_sizes_strides     size = [sympy.Integer(i) for i in ex.size()]   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_inductor/graph.py"", line 78, in      size = [sympy.Integer(i) for i in ex.size()]   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/sympy/core/cache.py"", line 74, in wrapper     retval = func(*args, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/sympy/core/numbers.py"", line 2095, in __new__     ival = int(i)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/__init__.py"", line 242, in __int__     return self.node.int_()   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/fx/experimental/symbolic_shapes.py"", line 214, in int_     raise RuntimeError(""Trying to extract a concrete int out of a symbolic int"") RuntimeError: Trying to extract a concrete int out of a symbolic int While executing %arg0_1 : [users=1] = placeholder[target=arg0_1] Original traceback: None The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""repro.py"", line 11, in      r = foo(x)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 212, in _fn     return fn(*args, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 333, in catch_errors     return callback(frame, cache_size, hooks)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 480, in _convert_frame     result = inner_convert(frame, cache_size, hooks)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 103, in _fn     return fn(*args, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 339, in _convert_frame_assert     return _compile(   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 400, in _compile     out_code = transform_code_object(code, transform)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/bytecode_transformation.py"", line 341, in transform_code_object     transformations(instructions, code_options)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/convert_frame.py"", line 387, in transform     tracer.run()   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1687, in run     super().run()   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/symbolic_convert.py"", line 538, in run     and self.step()   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/symbolic_convert.py"", line 501, in step     getattr(self, inst.opname)(inst)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/symbolic_convert.py"", line 1753, in RETURN_VALUE     self.output.compile_subgraph(self)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/output_graph.py"", line 511, in compile_subgraph     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/output_graph.py"", line 583, in compile_and_call_fx_graph     compiled_fn = self.call_user_compiler(gm)   File ""/tmp/tmp.t12rN541On/.venv/lib/python3.8/sitepackages/torch/_dynamo/output_graph.py"", line 664, in call_user_compiler     raise BackendCompilerFailed(self.compiler_fn, e) from e torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised RuntimeError: Trying to extract a concrete int out of a symbolic int While executing %arg0_1 : [users=1] = placeholder[target=arg0_1] Original traceback: None You can suppress this exception and fall back to eager by setting:     torch._dynamo.config.suppress_errors = True ```  Minified repro I am not sure, if I still need to submit a minified repro since the example I provided is already pretty small and the minified examples don't seem to work for me (`TORCHDYNAMO_REPRO_AFTER=""aot""` says `Input graph did not fail the tester` and `TORCHDYNAMO_REPRO_AFTER=""dynamo""` fails with a `name 's0' is not defined`). Here's a `collect_env` instead: ``` Collecting environment information... PyTorch version: 2.0.0.dev20221216+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0135genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA A40 GPU 1: NVIDIA A40 GPU 2: NVIDIA A40 GPU 3: NVIDIA A40 GPU 4: NVIDIA A40 GPU 5: NVIDIA A40 GPU 6: NVIDIA A40 GPU 7: NVIDIA A40 GPU 8: NVIDIA A40 GPU 9: NVIDIA A40 Nvidia driver version: 515.86.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==2.0.0.dev20221216+cu117 [pip3] torchtriton==2.0.0+0d7e753227 [conda] Could not collect ```",2022-12-16T22:35:19Z,triaged bug,closed,0,4,https://github.com/pytorch/pytorch/issues/93487,This is inductor doesn't work with symbolic shapes on master.  plz submit your PR lol,Was the PR submitted/merged for this issue? I am getting this error on `2.0.0.dev20230116+cu116` (nightly).,No still not yet,is fixed now.
rag,*_scatter ops should preserve input stride/storage_offset,"It turns out that we *do* need to update *_scatter ops to return the exact same strides as their inputs. I added a test to `test/test_functionalization.py`, which now trips thanks to Ed's functionalization stride debugging check. It only actually ends up tripping silent correctness if you try to .backward() on that function.   CC(AOT Autograd refactor + cleanup, handle intermediate views of bases, use view replay, fix nontensor input handling)  CC([test] inductor should take storage_offset() into account when cloning inputs)  CC(*_scatter ops should preserve input stride/storage_offset) ",2022-12-16T19:47:50Z,Merged ciflow/trunk release notes: python_frontend topic: bc breaking module: dynamo ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/91029,"Done. I also had to update a ton of expect tests: eagerly applying a mutation back to the base during functionalization was also needed to get that test passing, and causes our graphs to wobble.","This is also (unfortunately) technically BCbreaking, although hopefully: (1) very few people are actually using these *_scatter ops (2) even fewer people rely on the old behavior of `*_scatter(non_contiguous_input)` > contiguous output", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,A Simple Function Causing Graph Break," 🐛 Describe the bug An error raised when calling `torch._dynamo.optimize` when optimizing huggingface GPT2 and enforcing a single optimized graph for the whole model. ```   File ""conda/dexport/lib/python3.9/sitepackages/torch/_dynamo/variables/user_defined.py"", line 243, in call_function     return super().call_function(tx, args, kwargs)   File ""conda/dexport/lib/python3.9/sitepackages/torch/_dynamo/variables/base.py"", line 230, in call_function     unimplemented(f""call_function {self} {args} {kwargs}"")   File ""conda/dexport/lib/python3.9/sitepackages/torch/_dynamo/exc.py"", line 67, in unimplemented     raise Unsupported(msg) torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(_lru_cache_wrapper) [] {} from user code:    File ""dexport/tests/loop.py"", line 12, in forward     if transformers.is_torch_tpu_available(): Set torch._dynamo.config.verbose=True for more information You can suppress this exception and fall back to eager by setting:     torch._dynamo.config.suppress_errors = True ``` Below is a minimal repro. If `transformers.is_torch_tpu_available()` is removed, then the code works fine. ```python import torch import torch._dynamo from torch import nn import transformers class ToyModel(nn.Module):     def __init__(self):         super().__init__()         self.linear = nn.Linear(10, 10)     def forward(self, x):         if transformers.is_torch_tpu_available():             x = x + 1.0         else:             x = x + 2.0         return self.linear(x) model = ToyModel() x = torch.rand(10, 10, dtype=torch.float32) y = model(x) class GraphCaptureCompiler:     def __init__(self):         self.captured_graph = None         self.captured_graph_count =  0     def compile(self, gm, _):         assert self.captured_graph_count == 0         self.captured_graph = gm         self.captured_graph_count += 1         return gm compiler = GraphCaptureCompiler() y_optimized = torch._dynamo.optimize(compiler.compile, nopython=True)(model)(x) ``` I'd expect dynamo can just trace through this simple function.  Error logs _No response_  Minified repro _No response_",2022-12-16T18:42:16Z,triaged bug,open,0,1,https://github.com/pytorch/pytorch/issues/93486,The decorator `._dynamo.assume_constant_result` https://github.com/pytorch/pytorch/blob/b0cda0b38cf5e0516269e38314b1290fc1c26779/torch/_dynamo/eval_frame.pyL697 Might be able to work around this issue.  In this case it looks like `transformers.is_torch_tpu_available()` is using `functools.lru_cache`. We should add support for `functools.lru_cache` in cases where a function takes no arguments.  In those cases it should be pretty easy for dynamo to infer that the result of the function is a constant automatically.
transformer,CUDA 12 Support," 🚀 The feature, motivation and pitch CUDA was released on December 12 2022, with support for FP8 operations. Getting PyTorch to build using CUDA 12 would unlock performance gains, especially for Lovelace and Hopper architectures, e.g. using Transformer Engine. Magma may need to be updated as CUDA 12 dropped deprecated functions. Relevant issue: Magma: Can't build with CUDA 12 cc:   Related:  CC(Feature request: INT4 format support) (cc: )  Alternatives _No response_  Additional context _No response_ ",2022-12-16T07:13:23Z,module: cuda,closed,0,7,https://github.com/pytorch/pytorch/issues/90988,We are working on the CUDA12 bringup internally and will follow up with needed code changes.,: awesome! Is there a rough timeline on when it'll be available?,> We are working on the CUDA12 bringup internally and will follow up with needed code changes. and python 3.11 please :P ,Is cuda 11.8 getting skipped?, NVIDIA's PyTorch docker container uses CUDA 11.8: https://docs.nvidia.com/deeplearning/frameworks/pytorchreleasenotes/rel2211.htmlrel2211,"> Is cuda 11.8 getting skipped? No, we are working to enable nightly builds for 11.8 here: https://github.com/pytorch/pytorch/pull/90826 > Is there a rough timeline on when it'll be available? We are working on some critical issues we need to fix before enabling the support. ",Closing this in favor of  CC([CUDA][CUDA 12] CUDA 12 Support Tracking Issue) which has a lot more details on the plan.
transformer,TorchDynamo doesn't inline modified nn.Modules forward - Fails with Huggingface Accelerate," 🐛 Describe the bug Error was raised when running the huggingface BLOOM model with accelerate support for large model after applying `dynamo.optimize`. The example model and code were taken from the PyTorch conference demo. Note that the standard version, without arguments `device_map` and `offload_state_dict`, works fine.  Requirements ``` torch              2.0.0a0+gitfa946ae transformers       4.26.0.dev0 accelerate         0.15.0  ```  Error logs ``` Traceback (most recent call last):   File ""/home/bowbao/pytorch/torch/_dynamo/utils.py"", line 1092, in run_node     return nnmodule(*args, **kwargs)   File ""/home/bowbao/pytorch/torch/nn/modules/module.py"", line 1482, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/bowbao/stable_diffusion/lib/python3.8/sitepackages/accelerate/hooks.py"", line 156, in new_forward     output = old_forward(*args, **kwargs)   File ""/home/bowbao/pytorch/torch/nn/modules/sparse.py"", line 162, in forward     return F.embedding(   File ""/home/bowbao/pytorch/torch/nn/functional.py"", line 2210, in embedding     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)   File ""/home/bowbao/pytorch/torch/_subclasses/fake_tensor.py"", line 812, in __torch_dispatch__     raise Exception( Exception: Invoking operators with nonFake Tensor inputs in FakeTensorMode is not yet supported. Please convert all Tensors to FakeTensors first. Found in aten.embedding.default(*(Parameter containing: tensor([[0.0099, 0.0048, 0.0111,  ..., 0.0426,  0.0099,  0.0212],         [ 0.0048, 0.0127,  0.0138,  ..., 0.0448,  0.0003, 0.0120],         [ 0.0065,  0.0239,  0.0050,  ..., 0.0431, 0.0067,  0.0137],         ...,         [0.0028, 0.0038, 0.0012,  ..., 0.0252,  0.0013,  0.0012],         [0.0028, 0.0038, 0.0012,  ..., 0.0252,  0.0013,  0.0012],         [0.0028, 0.0038, 0.0012,  ..., 0.0252,  0.0013,  0.0012]],        device='cuda:3', dtype=torch.float16, requires_grad=True), FakeTensor(FakeTensor(..., device='meta', size=(1, 14), dtype=torch.int64), cuda:3)), **{}) The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/bowbao/pytorch/torch/_dynamo/utils.py"", line 1046, in get_fake_value     return wrap_fake_exception(   File ""/home/bowbao/pytorch/torch/_dynamo/utils.py"", line 712, in wrap_fake_exception     return fn()   File ""/home/bowbao/pytorch/torch/_dynamo/utils.py"", line 1047, in      lambda: run_node(tx.output, node, args, kwargs, nnmodule)   File ""/home/bowbao/pytorch/torch/_dynamo/utils.py"", line 1096, in run_node     raise RuntimeError( RuntimeError: Failed running call_module self_transformer_word_embeddings(*(FakeTensor(FakeTensor(..., device='meta', size=(1, 14), dtype=torch.int64), cuda:3),), **{}): Invoking operators with nonFake Tensor inputs in FakeTensorMode is not yet supported. Please convert all Tensors to FakeTensors first. Found in aten.embedding.default(*(Parameter containing: tensor([[0.0099, 0.0048, 0.0111,  ..., 0.0426,  0.0099,  0.0212],         [ 0.0048, 0.0127,  0.0138,  ..., 0.0448,  0.0003, 0.0120],         [ 0.0065,  0.0239,  0.0050,  ..., 0.0431, 0.0067,  0.0137],         ...,         [0.0028, 0.0038, 0.0012,  ..., 0.0252,  0.0013,  0.0012],         [0.0028, 0.0038, 0.0012,  ..., 0.0252,  0.0013,  0.0012],         [0.0028, 0.0038, 0.0012,  ..., 0.0252,  0.0013,  0.0012]],        device='cuda:3', dtype=torch.float16, requires_grad=True), FakeTensor(FakeTensor(..., device='meta', size=(1, 14), dtype=torch.int64), cuda:3)), **{}) (scroll up for backtrace) The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""repro_dynamo.py"", line 56, in      run_dynamo(model, inputs, tokenizer)   File ""repro_dynamo.py"", line 41, in run_dynamo     run_model(opt_model, inputs, tokenizer)   File ""repro_dynamo.py"", line 30, in run_model     outputs = model(**inputs, return_dict=False)   File ""/home/bowbao/pytorch/torch/nn/modules/module.py"", line 1482, in _call_impl     return forward_call(*args, **kwargs)   File ""/home/bowbao/pytorch/torch/_dynamo/eval_frame.py"", line 82, in forward     return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)   File ""/home/bowbao/pytorch/torch/_dynamo/eval_frame.py"", line 211, in _fn     return fn(*args, **kwargs)   File ""/home/bowbao/stable_diffusion/lib/python3.8/sitepackages/accelerate/hooks.py"", line 151, in new_forward     args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)   File ""/home/bowbao/stable_diffusion/lib/python3.8/sitepackages/accelerate/hooks.py"", line 156, in      output = old_forward(*args, **kwargs)   File ""/home/bowbao/pytorch/torch/_dynamo/eval_frame.py"", line 332, in catch_errors     return callback(frame, cache_size, hooks)   File ""/home/bowbao/pytorch/torch/_dynamo/convert_frame.py"", line 479, in _convert_frame     result = inner_convert(frame, cache_size, hooks)   File ""/home/bowbao/pytorch/torch/_dynamo/convert_frame.py"", line 103, in _fn     return fn(*args, **kwargs)   File ""/home/bowbao/pytorch/torch/_dynamo/utils.py"", line 90, in time_wrapper     r = func(*args, **kwargs)   File ""/home/bowbao/pytorch/torch/_dynamo/convert_frame.py"", line 339, in _convert_frame_assert     return _compile(   File ""/home/bowbao/pytorch/torch/_dynamo/convert_frame.py"", line 398, in _compile     out_code = transform_code_object(code, transform)   File ""/home/bowbao/pytorch/torch/_dynamo/bytecode_transformation.py"", line 341, in transform_code_object     transformations(instructions, code_options)   File ""/home/bowbao/pytorch/torch/_dynamo/convert_frame.py"", line 385, in transform     tracer.run()   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 1686, in run     super().run()   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 537, in run     and self.step()   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 500, in step     getattr(self, inst.opname)(inst)   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 306, in wrapper     return inner_fn(self, inst)   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 1014, in CALL_FUNCTION_KW     self.call_function(fn, args, kwargs)   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 434, in call_function     self.push(fn.call_function(self, args, kwargs))   File ""/home/bowbao/pytorch/torch/_dynamo/variables/nn_module.py"", line 220, in call_function     return tx.inline_user_function_return(   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 470, in inline_user_function_return     result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 1764, in inline_call     return cls.inline_call_(parent, func, args, kwargs)   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 1819, in inline_call_     tracer.run()   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 537, in run     and self.step()   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 500, in step     getattr(self, inst.opname)(inst)   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 306, in wrapper     return inner_fn(self, inst)   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 965, in CALL_FUNCTION     self.call_function(fn, args, {})   File ""/home/bowbao/pytorch/torch/_dynamo/symbolic_convert.py"", line 434, in call_function     self.push(fn.call_function(self, args, kwargs))   File ""/home/bowbao/pytorch/torch/_dynamo/variables/nn_module.py"", line 201, in call_function     return wrap_fx_proxy(   File ""/home/bowbao/pytorch/torch/_dynamo/variables/builder.py"", line 731, in wrap_fx_proxy     return wrap_fx_proxy_cls(   File ""/home/bowbao/pytorch/torch/_dynamo/variables/builder.py"", line 768, in wrap_fx_proxy_cls     example_value = get_fake_value(proxy.node, tx)   File ""/home/bowbao/pytorch/torch/_dynamo/utils.py"", line 1066, in get_fake_value     raise TorchRuntimeError() from e torch._dynamo.exc.TorchRuntimeError: from user code:    File ""/home/bowbao/transformers/src/transformers/models/bloom/modeling_bloom.py"", line 903, in forward     transformer_outputs = self.transformer(   File ""/home/bowbao/transformers/src/transformers/models/bloom/modeling_bloom.py"", line 729, in forward     inputs_embeds = self.word_embeddings(input_ids) Set torch._dynamo.config.verbose=True for more information You can suppress this exception and fall back to eager by setting:     torch._dynamo.config.suppress_errors = True ```  Minified repro ```python import torch from transformers import AutoModelForCausalLM, AutoTokenizer sentence = ""Question: Can I run BLOOM on a single GPU? Answer:""  Load model def load_model(model_name: str = ""bigscience/bloom560m""):     model = AutoModelForCausalLM.from_pretrained(         model_name,         torch_dtype=torch.float16,         device_map=""auto"",         offload_state_dict=True,     )     tokenizer = AutoTokenizer.from_pretrained(model_name)     inputs = tokenizer(sentence, return_tensors=""pt"").to(0)     print(inputs.keys())     return model, inputs, tokenizer  Inference in PyTorch def run_model(model, inputs, tokenizer):     with torch.no_grad():         outputs = model(**inputs, return_dict=False)     token_id = outputs[0][0][1].argmax()     answer = tokenizer.decode([token_id])     print(f""{sentence}\n{answer}"")  Inference in dynamo def run_dynamo(model, inputs, tokenizer):     from torch import _dynamo as torchdynamo     opt_model = torchdynamo.optimize(""eager"")(model)     run_model(opt_model, inputs, tokenizer) model, inputs, tokenizer = load_model() run_model(model, inputs, tokenizer)   this works run_dynamo(model, inputs, tokenizer)   this fails ``` ",2022-12-16T01:43:34Z,high priority triaged bug oncall: pt2,closed,0,9,https://github.com/pytorch/pytorch/issues/93484,"The root cause of this issue is that we're attempting to run the hooks directly of this module. https://github.com/huggingface/accelerate/blob/ca6505a6a86163dd2aa62ad5b6f24c87079ae372/src/accelerate/hooks.pyL151 we call `old_forward` which is that forward for of the prior `nnmodule` before it has been copied to fake tensors.  I think this is a dynamo issue  we should be inlining and compiling `nn.Module.forward`  when it has been modified, instead of invoking it.   ,  ,    maybe related to : https://github.com/pytorch/pytorch/pull/91018/","I think for ```nn.Module.forward``` call, we are inlining already: https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/variables/nn_module.pyL220. It seems we doesn't support hooks yet. But anyway, to support HF accelerate is important, I'll take a look. ",Any update on this ? I am hoping this makes it into PT 2.0 official release...,"i don't have much context on this issue, but i wonder if it has been fixed by https://github.com/pytorch/pytorch/pull/92125? It's possible that even if the approach in 92125 helps, it's not enough until we also land https://github.com/pytorch/pytorch/pull/94951 which is blocked by some other ongoing changes. After these changes, we should be tracing module.__call__ and then inlining both the hooks and the .forward.  We should be noticing and recompiling if forward got changed.  (but only if forward of the _original_ module got changed, not OptimizedModule.forward.", has 92125 been merged to mainline ? I can rebuild and check if it fixes my issue.,"yea, merged last week.","> yea, merged last week.   The FakeTensor errors for BLOOM HF are still observed with TOT pytorch (both ROCm and CUDA) **Repro code:** ``` import torch from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom560m') model = AutoModelForCausalLM.from_pretrained(""bigscience/bloom560m"", device_map='auto') model = torch.compile(model)  This is the only line of code that we changed text = ""Replace me by any text you'd like."" encoded_input = tokenizer(text, return_tensors='pt').to(device=""cuda:0"") output = model(**encoded_input) ``` One finding we have made is that if we do not use HF's `device_map=""auto""` arguement and move the model to a single GPU then the model will complete e.g. ``` import torch from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom560m') model = AutoModelForCausalLM.from_pretrained(""bigscience/bloom560m"").to(""cuda:0"") model = torch.compile(model)  This is the only line of code that we changed text = ""Replace me by any text you'd like."" encoded_input = tokenizer(text, return_tensors='pt').to(device=""cuda:0"") output = model(**encoded_input) ``` cc: amd  ", are you still planning on working on this?,"Repros don't give the same error anymore, e.g. I get: ``` Traceback (most recent call last):   File ""/data/users/williamwen/pytorch/playground3.py"", line 39, in      run_model(model, inputs, tokenizer)   this works   File ""/data/users/williamwen/pytorch/playground3.py"", line 24, in run_model     outputs = model(**inputs, return_dict=False)   File ""/data/users/williamwen/pytorch/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/data/users/williamwen/pytorch/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/data/users/williamwen/transformers/src/transformers/models/bloom/modeling_bloom.py"", line 858, in forward     transformer_outputs = self.transformer(   File ""/data/users/williamwen/pytorch/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/data/users/williamwen/pytorch/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/data/users/williamwen/transformers/src/transformers/models/bloom/modeling_bloom.py"", line 670, in forward     inputs_embeds = self.word_embeddings(input_ids)   File ""/data/users/williamwen/pytorch/torch/nn/modules/module.py"", line 1511, in _wrapped_call_impl     return self._call_impl(*args, **kwargs)   File ""/data/users/williamwen/pytorch/torch/nn/modules/module.py"", line 1520, in _call_impl     return forward_call(*args, **kwargs)   File ""/data/users/williamwen/pytorch/torch/nn/modules/sparse.py"", line 163, in forward     return F.embedding(   File ""/data/users/williamwen/pytorch/torch/nn/functional.py"", line 2237, in embedding     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select) ``` If I use `CUDA_VISIBLE_DEVICES=0`, then the repros complete. Closing as the original issue seems fixed."
transformer,Support for Transformer Models on Android with Vulkan Backend," 🚀 The feature, motivation and pitch Hello We are currently using a number of different transformer models (plain BERT encoders with attached classification head) on Android. In order to increase the performance of the inference on mobile devices, we're currently evaluating the Pytorch Vulkan backend. We therefore compiled Pytorch from source for desktop to export our model for mobile and specifically for the Vulkan backend. Afterwards, we compiled Pytorch with Vulkan backend for Android, included the resulting aars in our TestingApp and tried running the model. The following error occured: ``` com.facebook.jni.CppException: Vulkan vk_format(): no corresponding format for dtype ``` After some digging around in the source code, we found that the dtype for which there is no corresponding format is `Long`. We assume that the model requires a `Long` as index to the embedding bag. To make sure that nothing was wrong with our custom build, we also tried to run a basic mobilenet v2 model on Android with Vulkan backend and it seemed to work as expected. We therefore assume that this is an unsupported feature.  Just out of curiosity we also tried passing `Int` and `Float` values to the model, just to see what would happen. With `Int` we received the same error whereas with `Float` the following error occured: ``` com.facebook.jni.CppException: Could not run 'aten::as_strided' with arguments from the 'Vulkan' backend. This could be because the operator doesn't exist for this backend ``` Can you please confirm that this is in fact an unsupported feature? Is there currently some kind of workaround for this? Is there a plan to support this in the future? Thank you in advance for your reply and have a great day!  Alternatives _No response_  Additional context _No response_",2022-12-15T14:26:43Z,triaged module: vulkan,open,8,4,https://github.com/pytorch/pytorch/issues/90920, do you have an idea here? :),"JIA  sorry for pinging, but do you have insights on this?","The Vulkan backend doesn't support strides, see: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.hL6",">  🚀 The feature, motivation and pitch > Hello >  > We are currently using a number of different transformer models (plain BERT encoders with attached classification head) on Android. In order to increase the performance of the inference on mobile devices, we're currently evaluating the Pytorch Vulkan backend. We therefore compiled Pytorch from source for desktop to export our model for mobile and specifically for the Vulkan backend. Afterwards, we compiled Pytorch with Vulkan backend for Android, included the resulting aars in our TestingApp and tried running the model. The following error occured: >  > ``` > com.facebook.jni.CppException: Vulkan vk_format(): no corresponding format for dtype > ``` >  > After some digging around in the source code, we found that the dtype for which there is no corresponding format is `Long`. We assume that the model requires a `Long` as index to the embedding bag. To make sure that nothing was wrong with our custom build, we also tried to run a basic mobilenet v2 model on Android with Vulkan backend and it seemed to work as expected. We therefore assume that this is an unsupported feature. >  > Just out of curiosity we also tried passing `Int` and `Float` values to the model, just to see what would happen. With `Int` we received the same error whereas with `Float` the following error occured: >  > ``` > com.facebook.jni.CppException: Could not run 'aten::as_strided' with arguments from the 'Vulkan' backend. This could be because the operator doesn't exist for this backend > ``` >  > Can you please confirm that this is in fact an unsupported feature? Is there currently some kind of workaround for this? Is there a plan to support this in the future? >  > Thank you in advance for your reply and have a great day! >  >  Alternatives > _No response_ >  >  Additional context > _No response_ Actually, I encountered the same question, can you please support us ? The Index that embedding need is Long. "
rag,[test] inductor should take storage_offset() into account when cloning inputs,"The problem is that for code like this (existing test: `TestInductorOpInfoCUDA.test_comprehensive_as_strided_partial_views_cuda_float64`) ``` .compile def f(x):     return x.as_strided(shape, stride) base = torch.ones(20) slice = base[5:15] out = f(base) ``` Inductor has some logic to clone the inputs, but it doesn't respect the storage offset of said inputs. The above example breaks, because we will put torch.ops.aten.as_strided(x, size, stride) into the graph, and the storage_offset will be inferred from the input, to (incorrectly) be 0 instead of 5. We are also forced to use  This used to work before the current refactor, because aot autograd was not passing outputthatviewinput ops into the graph at all, and instead, it would just dump the (hardcoded) sizes/strides of views as outputs into the graph. With the changes in the aot autograd PR, we're now trusting inductor to return us an output alias with the correct metadata. Which... we kinda need to, since we talked a few weeks ago about how inductor can choose its own output memory format, so we need inductor to tell us what the strides of the outputs are in general.   CC(AOT Autograd refactor + cleanup, handle intermediate views of bases, use view replay, fix nontensor input handling)  CC([test] inductor should take storage_offset() into account when cloning inputs) ",2022-12-14T22:36:06Z,Stale ciflow/trunk module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/90870," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."," , I'm interested if you have any thoughts. This fixed an `as_strided` test involving partial views in the inductor op info test suite, but it looks like it's breaking an inductor + distributed test. ``` python test/distributed/test_dynamo_distributed.py TestDistributedMultiProc.test_hf_bert_fsdp ``` Which now fails with a CUDA misaligned address (full paste here) ```   File ""/scratch/hirsheybar2/work/pytorch/torch/_inductor/triton_ops/autotune.py"", line 84, in _precompile_config     torch.cuda.synchronize(torch.cuda.current_device())   File ""/scratch/hirsheybar2/work/pytorch/torch/cuda/__init__.py"", line 577, in synchronize     return torch._C._cuda_synchronize() RuntimeError: CUDA error: misaligned address ``` I'm wondering: (1) do you think that morally, inductor should respect storage offset when its clones inputs? It definitely needs to for this test, although user code with .as_strided() on an existing input view is definitely a corner case that we can xfail. (2) If that's the right fix, then I'm pretty stumped by the distributed test failure. Do you have any tips on where to look?"," is your theory that the change to inductor to respect original offsets is correct, but leads to unaligned addresses for some views?  And these unaligned addresses only upset inductor (but worked when the program ran in eager)?  (And that such cases are happening in FSDP somewhere?)  Any more details you can provide?","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Support arbitrary masks for _nested_tensor_from_mask in nn.TransformerEncoder," 🚀 The feature, motivation and pitch I am working on transformers and am using the Better Transformer. To enjoy the speed improvement, we need to pass a src_key_padding_mask to the nn.TransformerEncoder. The problem is that, the src_key_padding_mask has to be leftaligned, which means all True values must be in front of all False values. That is the common case in NLP as sentences have different lengths, but in computer vision, the True values in a mask are usually arbitrarily dispersed. I am wondering if you could support transforming a tensor with an arbitrary mask to a nested tensor, and also support transforming the nested tensor back to a normal tensor with 0 paddings according to the same mask. This feature would be very useful for vision transformers when people want to use masks and enjoy the inference speed improvement for whatever their tasks, for example, dynamic pruning, etc.  Alternatives _No response_  Additional context _No response_ ",2022-12-14T21:37:02Z,module: nn triaged module: nestedtensor,open,0,1,https://github.com/pytorch/pytorch/issues/90866,"Hi  , thanks for the feature request, I have added this to our op coverage tracker CC(General NestedTensor op coverage tracking issue)! "
rag,"Revert ""Revert ""[functorch] Refactor life handle storage (#90317)""""","  CC(Revert ""Revert ""[functorch] Refactor life handle storage (90317)"""") Adds the fix for Wsigncompare. See original PR (https://github.com/pytorch/pytorch/pull/90317) for commit message",2022-12-14T19:50:18Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/90856," merge f ""test failures are unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,FYI: script for counting external PRs,"``` python import os import urllib.parse from typing import (     Any,     Callable,     Dict,     Iterable,     List,     Optional,     Pattern,     Tuple,     Union,     cast,     NamedTuple ) import json from urllib.error import HTTPError from urllib.request import Request, urlopen import datetime import time def _fetch_url(url: str, *,                headers: Optional[Dict[str, str]] = None,                data: Optional[Dict[str, Any]] = None,                method: Optional[str] = None,                reader: Callable[[Any], Any] = lambda x: x.read()) > Any:     if headers is None:         headers = {}     token = os.environ.get(""GITHUB_TOKEN"")     if token is not None and url.startswith('https://api.github.com/'):         headers['Authorization'] = f'token {token}'     data_ = json.dumps(data).encode() if data is not None else None     try:         with urlopen(Request(url, headers=headers, data=data_, method=method)) as conn:             return reader(conn)     except HTTPError as err:         print(err.reason)         print(err.headers)         if err.code == 403 and all(key in err.headers for key in ['XRateLimitLimit', 'XRateLimitUsed']):             print(f""Rate limit exceeded: {err.headers['XRateLimitUsed']}/{err.headers['XRateLimitLimit']}"")         raise def fetch_json(url: str,                params: Optional[Dict[str, Any]] = None,                data: Optional[Dict[str, Any]] = None) > List[Dict[str, Any]]:     headers = {'Accept': 'application/vnd.github.v3+json'}     if params is not None and len(params) > 0:         url += '?' + '&'.join(f""{name}={urllib.parse.quote(str(val))}"" for name, val in params.items())     return cast(List[Dict[str, Any]], _fetch_url(url, headers=headers, data=data, reader=json.load)) def main():     FILTER_OUT_USERS = set([""pytorchmergebot"", ""facebookgithubbot""])     period_begin_date = datetime.date(2022, 6, 5)     period_begin_date = datetime.date(2022, 11, 20)     while period_begin_date <= datetime.date(2022, 12, 11):         period_end_date = period_begin_date + datetime.timedelta(days=6)         response = cast(             Dict[str, Any],             fetch_json(                 ""https://api.github.com/search/issues"",                 params={""q"": f'repo:pytorch/pytorch is:pr is:closed label:""open source"" label:Merged label:Reverted closed:{period_begin_date}..{period_end_date}', ""per_page"": '100'},             ),         )         pr_count = 0         users = set([])         for item in response[""items""]:             u = item[""user""][""login""]             if u not in FILTER_OUT_USERS:                 pr_count += 1                 users.add(u)         user_count = len(users)         print(f'{period_begin_date},{period_end_date},{pr_count},{user_count}')         print(users)         period_begin_date = period_end_date + datetime.timedelta(days=1)         time.sleep(10) if __name__ == ""__main__"":     main() ```",2022-12-13T21:04:41Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/90794
gpt,Experiment with gpt2,Fixes ISSUE_NUMBER,2022-12-13T18:30:14Z,open source release notes: onnx,closed,0,0,https://github.com/pytorch/pytorch/issues/90779
gpt,Experiment with gpt2,Fixes ISSUE_NUMBER,2022-12-13T18:25:55Z,release notes: onnx,closed,0,0,https://github.com/pytorch/pytorch/issues/90778
transformer,In distributed get SIGTERM and run crash," 🐛 Describe the bug Hi everyone, I'm tying to finetune GPT2small on the OpenWebText dataset (A big dataset consist of 40GB+), by running a slightly modified HuggingFace run_clm.py script in distributed. However, I get SIGTERM in different states of the training/inference without further explanation. Didn't find a similar reported issue with a working solution yet so decided to post and ask for your help.  Scheme of how I run my code:     torchrun \     standalone \     nnodes=1 \     nproc_per_node=${NUM_GPU} \     run_clm.py     ddp_timeout 3240000 \     ddp_find_unused_parameters False \     arg_1 arg_2 ... arg_n  My errors:  Error on finetune: When I tried to finetune my model, I suffered from error midway while tokenizing my loaded dataset after successfully tokenized chunk of data, but errored in the next chunk: > Running tokenizer on dataset:   1% 1/81 [00:04     sys.exit(main())   File ""/venv/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/venv/lib/python3.8/sitepackages/torch/distributed/run.py"", line 719, in main     run(args)   File ""/venv/lib/python3.8/sitepackages/torch/distributed/run.py"", line 710, in run     elastic_launch(   File ""/venv/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/venv/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 259, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ============================================================ ./code/gpt2/ModelCompressionResearchPackage/examples/transformers/languagemodeling/run_clm.py FAILED Failures:    Root Cause (first observed failure): [0]:   time      : 20221212_08:13:08   host      : openwebtextpcfz22zz8p   rank      : 0 (local_rank: 0)   exitcode  : 9 (pid: 463)   error_file:    traceback : Signal 9 (SIGKILL) received by PID 463 ============================================================  Error on inference: I was running my model in inference mode only, it took 3.5 hours to load, process the validation dataset (about 310MB of data) and run inference on those examples, and after it finished all steps, errored again the same noninformative SIGTERM error.  Notes:  I'm using high value for `ddp_timeout` in order to avoid timeout when loading and processing my huge dataset.   I track my memory usage and it doesn't seem like OOM errors.  It doesn't seem to error due to timeout, since I dealt with it when placing high values for `ddp_timeout`  The error doesn't appear in single GPU.  The error appear much sooner when I use all my 8 GPUs (still 1 node), rather than when I use only few of them. Would really appreciate any help on the issue!  Versions Ubuntu version = 20.04.4 LTS (Focal Fossa) Python = 3.8.10 transformers = 4.24.0 torch = 1.10.2+cu113 datasets = 2.7.0 tokenizers = 0.13.2 accelerate = 0.14.0 evaluate = 0.3.0 numpy = 1.22.3",2022-12-12T08:55:18Z,oncall: distributed,open,0,0,https://github.com/pytorch/pytorch/issues/90688
rag,[FSDP][Easy] Move to `_storage()` in test file,"Stack from ghstack:  CC([FSDP][Perf] Preallocate full prec sharded param) [FSDP][Perf] Preallocate full prec sharded param  CC([FSDP][Easy][BE] Minor cleanup of postbackward logic) [FSDP][Easy][BE] Minor cleanup of postbackward logic  CC([FSDP][Perf] Preallocate sharded grad in default stream; save one copy when downcasting grad) [FSDP][Perf] Save one copy when downcasting grad  CC([FSDP][Perf] Preallocate padded unsharded grad in default stream) [FSDP][Perf] Preallocate padded unsharded grad in default stream  CC([FSDP] Sanitize `HandleConfig` for mixed precision) [FSDP] Sanitize `HandleConfig` for mixed precision  CC([FSDP] Tighten postbwd cast to `reduce_dtype`) [FSDP] Tighten postbwd cast to `reduce_dtype` * * CC([FSDP][Easy] Move to `_storage()` in test file) [FSDP][Easy] Move to `_storage()` in test file**  CC([FSDP] Save `_stream_to_name` for debugging) [FSDP] Save `_stream_to_name` for debugging  CC([Reland][FSDP] Another fix for `DTensor`, `use_orig_params=True`) [Reland][FSDP] Another fix for `DTensor`, `use_orig_params=True` This is to silence some deprecation warnings.",2022-12-10T16:07:10Z,Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/90622,"> How come this was not throwing before? It is just that they added deprecation warnings now (no error), so I wanted to migrate.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Fix straggler logging.CODE,"  CC(Handle tensor default func args when inlining)  CC(Fix straggler logging.CODE)  logging.CODE was entirely removed, this was left behind ",2022-12-09T22:06:28Z,ciflow/trunk topic: not user facing module: dynamo ciflow/inductor,closed,0,11,https://github.com/pytorch/pytorch/issues/90572, accept2ship, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: inductor ,inductor / cuda11.6py3.10gcc7sm86 / test (inductor_timm, 1, 2, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""flaky CI"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 04a3639cb59a8775d78eb82286d8bc832d063495` returned nonzero exit code 1 ``` Automerging torch/_dynamo/output_graph.py CONFLICT (content): Merge conflict in torch/_dynamo/output_graph.py error: could not apply 04a3639cb5... Fix straggler logging.CODE hint: After resolving the conflicts, mark them with hint: ""git add/rm "", then run hint: ""git cherrypick continue"". hint: You can instead skip this commit with ""git cherrypick skip"". hint: To abort and get back to the state before ""git cherrypick"", hint: run ""git cherrypick abort"". ``` Details for Dev Infra team Raised by workflow job ",someone else fixed it first
transformer,Bugs about BART of Hugging Face using Pytorch 2.0," 🐛 Describe the bug This is the code of using BART of Hugging Face with Pytorch 2.0: ```python import torch from transformers import BartTokenizer, BartForConditionalGeneration device = torch.device('cuda') tokenizer = BartTokenizer.from_pretrained(""facebook/bartbase"") model = BartForConditionalGeneration.from_pretrained(""facebook/bartbase"") model = model.to(device) model = torch.compile(model) inputs = tokenizer(     ""Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons."",     return_tensors=""pt"", ) labels = tokenizer(""Bad Reasons To Quit Your Job"", return_tensors=""pt"")[""input_ids""] input_ids = inputs[""input_ids""].to(device) attention_mask = inputs[""attention_mask""].to(device) labels = labels.to(device) loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss ``` When using Pytorch 2.0 without `torch.compile` with `device=torch.device('cpu')` or `device=torch.device('cuda')`, it works well without any warning. When using Pytorch 2.0 with `torch.compile` and `device=torch.device('cpu')`, it works well but with the following warning: ``` /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions   warnings.warn(""functional_call was passed multiple values for tied weights. "" /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions   warnings.warn(""functional_call was passed multiple values for tied weights. "" /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions   warnings.warn(""functional_call was passed multiple values for tied weights. "" ``` When using Pytorch 2.0 with `torch.compile` and `device=torch.device('cuda')`, it doesn't work well with the following errors: ``` /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions   warnings.warn(""functional_call was passed multiple values for tied weights. "" /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions   warnings.warn(""functional_call was passed multiple values for tied weights. "" /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/nn/utils/stateless.py:44: UserWarning: functional_call was passed multiple values for tied weights. This behavior is deprecated and will be an error in future versions   warnings.warn(""functional_call was passed multiple values for tied weights. "" ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮ │ :1 in                                                                             │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/nn/modules/module.py:1480 in       │ │ _call_impl                                                                                       │ │                                                                                                  │ │   1477 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │ │   1478 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │ │   1479 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │ │ ❱ 1480 │   │   │   return forward_call(*args, **kwargs)                                          │ │   1481 │   │    Do not call functions when jit is used                                          │ │   1482 │   │   full_backward_hooks, non_full_backward_hooks = [], []                             │ │   1483 │   │   backward_pre_hooks = []                                                           │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py:82 in        │ │ forward                                                                                          │ │                                                                                                  │ │    79 │   │   return getattr(self._orig_mod, name)                                               │ │    80 │                                                                                          │ │    81 │   def forward(self, *args, **kwargs):                                                    │ │ ❱  82 │   │   return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)                    │ │    83                                                                                            │ │    84                                                                                            │ │    85 def remove_from_cache(f):                                                                  │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py:211 in _fn   │ │                                                                                                  │ │   208 │   │   │   dynamic_ctx = enable_dynamic(self.dynamic)                                     │ │   209 │   │   │   dynamic_ctx.__enter__()                                                        │ │   210 │   │   │   try:                                                                           │ │ ❱ 211 │   │   │   │   return fn(*args, **kwargs)                                                 │ │   212 │   │   │   finally:                                                                       │ │   213 │   │   │   │   set_eval_frame(prior)                                                      │ │   214 │   │   │   │   dynamic_ctx.__exit__(None, None, None)                                     │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/transformers/models/bart/modeling_bart.p │ │ y:1328 in forward                                                                                │ │                                                                                                  │ │   1325 │   def set_output_embeddings(self, new_embeddings):                                      │ │   1326 │   │   self.lm_head = new_embeddings                                                     │ │   1327 │                                                                                         │ │ ❱ 1328 │   (BART_INPUTS_DOCSTRING)                         │ │   1329 │   (output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC  │ │   1330 │   (BART_GENERATION_EXAMPLE)                                          │ │   1331 │   def forward(                                                                          │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py:211 in _fn   │ │                                                                                                  │ │   208 │   │   │   dynamic_ctx = enable_dynamic(self.dynamic)                                     │ │   209 │   │   │   dynamic_ctx.__enter__()                                                        │ │   210 │   │   │   try:                                                                           │ │ ❱ 211 │   │   │   │   return fn(*args, **kwargs)                                                 │ │   212 │   │   │   finally:                                                                       │ │   213 │   │   │   │   set_eval_frame(prior)                                                      │ │   214 │   │   │   │   dynamic_ctx.__exit__(None, None, None)                                     │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py:2107 in │ │ forward                                                                                          │ │                                                                                                  │ │   2104 │   │   full_args = []                                                                    │ │   2105 │   │   full_args.extend(params_flat)                                                     │ │   2106 │   │   full_args.extend(runtime_args)                                                    │ │ ❱ 2107 │   │   return compiled_fn(full_args)                                                     │ │   2108 │                                                                                         │ │   2109 │    Just for convenience                                                                │ │   2110 │   forward.zero_grad = mod.zero_grad                                                     │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py:811 in  │ │ g                                                                                                │ │                                                                                                  │ │    808                                                                                           │ │    809 def make_boxed_func(f):                                                                   │ │    810 │   def g(args):                                                                          │ │ ❱  811 │   │   return f(*args)                                                                   │ │    812 │                                                                                         │ │    813 │   g._boxed_call = True                                                                  │ │    814 │   return g                                                                              │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py:1687 in │ │ debug_compiled_function                                                                          │ │                                                                                                  │ │   1684 │   │   │   │   │   f""{describe_input(i, aot_config)} would not require grad""             │ │   1685 │   │   │   │   )                                                                         │ │   1686 │   │                                                                                     │ │ ❱ 1687 │   │   return compiled_function(*args)                                                   │ │   1688 │                                                                                         │ │   1689 │   return debug_compiled_function                                                        │ │   1690                                                                                           │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py:1551 in │ │ compiled_function                                                                                │ │                                                                                                  │ │   1548 │   │   else:                                                                             │ │   1549 │   │   │   args_with_synthetic_bases = args                                              │ │   1550 │   │                                                                                     │ │ ❱ 1551 │   │   all_outs = CompiledFunction.apply(*args_with_synthetic_bases)                     │ │   1552 │   │   if CompiledFunction.num_aliasing_metadata_outs > 0:                               │ │   1553 │   │   │   outs = all_outs[:CompiledFunction.num_aliasing_metadata_outs]                │ │   1554 │   │   │   aliasing_metadata_outs = all_outs[CompiledFunction.num_aliasing_metadata_ou  │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py:1455 in │ │ forward                                                                                          │ │                                                                                                  │ │   1452 │   │   │    (*mutated_data_inputs, *non_aliased_fw_outs, *saved_tensors, *saved_symint  │ │   1453 │   │   │     Note that in the synthetic bases case, mutated_inputs will correspond to  │ │   1454 │   │   │      of the original view, and not the synthetic base                          │ │ ❱ 1455 │   │   │   fw_outs = call_func_with_args(                                                │ │   1456 │   │   │   │   CompiledFunction.compiled_fw, deduped_flat_tensor_args, disable_amp=disa  │ │   1457 │   │   │   )                                                                             │ │   1458                                                                                           │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_functorch/aot_autograd.py:836 in  │ │ call_func_with_args                                                                              │ │                                                                                                  │ │    833 │   │   guard = torch._C._DisableAutocast()                                               │ │    834 │   try:                                                                                  │ │    835 │   │   if hasattr(f, ""_boxed_call""):                                                     │ │ ❱  836 │   │   │   out = normalize_as_list(f(args))                                              │ │    837 │   │   else:                                                                             │ │    838 │   │   │    TODO: Please remove soon                                                    │ │    839 │   │   │    https://github.com/pytorch/pytorch/pull/83137issuecomment1211320670       │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_inductor/compile_fx.py:199 in run │ │                                                                                                  │ │   196 │   │   for i in check_inputs:                                                             │ │   197 │   │   │   if new_inputs[i].data_ptr() % ALIGNMENT:                                       │ │   198 │   │   │   │   new_inputs[i] = clone_preserve_strides(new_inputs[i])                      │ │ ❱ 199 │   │   return model(new_inputs)                                                           │ │   200 │                                                                                          │ │   201 │   return run                                                                             │ │   202                                                                                            │ │                                                                                                  │ │ /tmp/torchinductor_tangtianyi/mt/cmtrlxnsp7o7om6qxzj7wbwah2qxc6ker2iphq6omeo3332peb3m.py:1184 in │ │ call                                                                                             │ │                                                                                                  │ │   1181 │   buf4 = empty_strided((1, 31, 768), (23808, 768, 1), device='cuda', dtype=torch.float  │ │   1182 │   buf471 = empty_strided((1, 31, 1), (31, 1, 31), device='cuda', dtype=torch.float32)   │ │   1183 │   stream3 = get_cuda_stream(3)                                                          │ │ ❱ 1184 │   triton_fused_add_add_1_add_2_add_3_arange_div_50_embedding_embedding_1_expand_mul_0.  │ │   1185 │   del primals_1                                                                         │ │   1186 │   del primals_3                                                                         │ │   1187 │   buf5 = empty_strided((31, 768), (768, 1), device='cuda', dtype=torch.float32)         │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_inductor/triton_ops/autotune.py:1 │ │ 69 in run                                                                                        │ │                                                                                                  │ │   166 │   │   │   if len(self.launchers) == 0:                                                   │ │   167 │   │   │   │   self.precompile()                                                          │ │   168 │   │   │   if len(self.launchers) > 1:                                                    │ │ ❱ 169 │   │   │   │   self.autotune_to_one_config(*args, grid=grid)                              │ │   170 │   │                                                                                      │ │   171 │   │   (launcher,) = self.launchers                                                       │ │   172 │   │   if launcher.config.pre_hook is not None:                                           │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_dynamo/utils.py:90 in             │ │ time_wrapper                                                                                     │ │                                                                                                  │ │     87 │   │   if key not in compilation_metrics:                                                │ │     88 │   │   │   compilation_metrics[key] = []                                                 │ │     89 │   │   t0 = time.time()                                                                  │ │ ❱   90 │   │   r = func(*args, **kwargs)                                                         │ │     91 │   │   latency = time.time()  t0                                                        │ │     92 │   │    print(f""Dynamo timer: key={key}, latency={latency:.2f} sec"")                    │ │     93 │   │   compilation_metrics[key].append(latency)                                          │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_inductor/triton_ops/autotune.py:1 │ │ 56 in autotune_to_one_config                                                                     │ │                                                                                                  │ │   153 │   │   │   else:                                                                          │ │   154 │   │   │   │   cloned_args.append(arg)                                                    │ │   155 │   │                                                                                      │ │ ❱ 156 │   │   timings = {                                                                        │ │   157 │   │   │   launcher: self.bench(launcher, *cloned_args, **kwargs)                         │ │   158 │   │   │   for launcher in self.launchers                                                 │ │   159 │   │   }                                                                                  │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_inductor/triton_ops/autotune.py:1 │ │ 57 in                                                                                  │ │                                                                                                  │ │   154 │   │   │   │   cloned_args.append(arg)                                                    │ │   155 │   │                                                                                      │ │   156 │   │   timings = {                                                                        │ │ ❱ 157 │   │   │   launcher: self.bench(launcher, *cloned_args, **kwargs)                         │ │   158 │   │   │   for launcher in self.launchers                                                 │ │   159 │   │   }                                                                                  │ │   160 │   │   self.launchers = [builtins.min(timings, key=timings.get)]                          │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_inductor/triton_ops/autotune.py:1 │ │ 38 in bench                                                                                      │ │                                                                                                  │ │   135 │   │                                                                                      │ │   136 │   │   from triton.testing import do_bench                                                │ │   137 │   │                                                                                      │ │ ❱ 138 │   │   return do_bench(kernel_call, rep=40, fast_flush=True)                              │ │   139 │                                                                                          │ │   140 │   .dynamo_timed                                                             │ │   141 │   def autotune_to_one_config(self, *args, **kwargs):                                     │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/triton/testing.py:140 in do_bench        │ │                                                                                                  │ │   137 │   """"""                                                                                    │ │   138 │                                                                                          │ │   139 │    Estimate the runtime of the function                                                 │ │ ❱ 140 │   fn()                                                                                   │ │   141 │   torch.cuda.synchronize()                                                               │ │   142 │   start_event = torch.cuda.Event(enable_timing=True)                                     │ │   143 │   end_event = torch.cuda.Event(enable_timing=True)                                       │ │                                                                                                  │ │ /home/tangtianyi/miniconda3/lib/python3.8/sitepackages/torch/_inductor/triton_ops/autotune.py:1 │ │ 30 in kernel_call                                                                                │ │                                                                                                  │ │   127 │   │   │   │   launcher.config.pre_hook(                                                  │ │   128 │   │   │   │   │   {**zip(self.arg_names, args), **launcher.config.kwargs}                │ │   129 │   │   │   │   )                                                                          │ │ ❱ 130 │   │   │   launcher(                                                                      │ │   131 │   │   │   │   *args,                                                                     │ │   132 │   │   │   │   grid=grid,                                                                 │ │   133 │   │   │   │   stream=stream,                                                             │ │ :4 in launcher                                                                           │ ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯ RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered ```  Versions PyTorch version: 1.14.0.dev20221208+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.15.041genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 GPU 3: NVIDIA GeForce RTX 3090 GPU 4: NVIDIA GeForce RTX 3090 GPU 5: NVIDIA GeForce RTX 3090 GPU 6: NVIDIA GeForce RTX 3090 GPU 7: NVIDIA GeForce RTX 3090 Nvidia driver version: 510.54 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.24.0rc2 [pip3] pytorchtransformers==1.0.0 [pip3] torch==1.14.0.dev20221208+cu116 [pip3] torchaudio==0.14.0.dev20221208+cu116 [pip3] torchtriton==2.0.0+0d7e753227 [pip3] torchvision==0.15.0.dev20221208+cu116 [pip3] torchviz==0.0.2 [conda] blas                      1.0                         mkl    defaults [conda] cudatoolkit               11.3.1               h2bc3f7f_2    defaults [conda] faissgpu                 1.7.2           py3.8_h28a55e0_0_cuda11.3    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch [conda] ffmpeg                    4.3                  hf484d3e_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch [conda] libfaiss                  1.7.2           hfc2d529_0_cuda11.3    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch [conda] mkl                       2021.4.0           h06a4308_640    defaults [conda] mklservice               2.4.0            py38h7f8727e_0    defaults [conda] mkl_fft                   1.3.1            py38hd3c417c_0    defaults [conda] mkl_random                1.2.2            py38h51133e4_0    defaults [conda] numpy                     1.24.0rc2                pypi_0    pypi [conda] numpybase                1.21.5           py38hf524024_2    defaults [conda] pytorch                   1.13.0          py3.8_cuda11.6_cudnn8.3.2_0    pytorch [conda] pytorchcuda              11.6                 h867d48c_0    pytorch [conda] pytorchmutex             1.0                        cuda    https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch [conda] pytorchtransformers      1.0.0                    pypi_0    pypi [conda] torch                     1.14.0.dev20221208+cu116          pypi_0    pypi [conda] torchaudio                0.14.0.dev20221208+cu116          pypi_0    pypi [conda] torchtriton               2.0.0+0d7e753227          pypi_0    pypi [conda] torchvision               0.15.0.dev20221208+cu116          pypi_0    pypi [conda] torchviz                  0.0.2                    pypi_0    pypi ",2022-12-09T10:02:27Z,module: crash triaged bug oncall: pt2 module: inductor,closed,0,3,https://github.com/pytorch/pytorch/issues/90537,"  i'm seeing what looks like a really long compile time when i run the above repro on tip of master. dynamo log level DEBUG stops here and hangs or takes a long time (>5min) ``` ... [20221209 18:21:51,088] torch._inductor.scheduler: [DEBUG] remove_buffer('buf425') [20221209 18:21:51,088] torch._inductor.scheduler: [DEBUG] remove_buffer('buf426') [20221209 18:21:51,704] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0 [20221209 18:21:51,708] torch._dynamo.output_graph: [INFO] Step 2: done compiler function _compile_fn [20221209 18:21:51,713] torch._dynamo.eval_frame: [DEBUG] skipping _fn /scratch/whc/work/pytorch/torch/_dynamo/eval_frame.py [20221209 18:21:51,713] torch._dynamo.eval_frame: [DEBUG] skipping nothing /scratch/whc/work/pytorch/torch/_dynamo/eval_frame.py ``` also if I ctrl+c, it doesn't exit.",", I see BartForConditionalGeneration pass on the dashboard, do you know what's the difference? ","Closed as error is fixed. Repro with: ```python import torch from transformers import BartTokenizer, BartForConditionalGeneration device = torch.device('cuda') tokenizer = BartTokenizer.from_pretrained(""facebook/bartbase"") model = BartForConditionalGeneration.from_pretrained(""facebook/bartbase"") model = model.to(device) model = torch.compile(model) inputs = tokenizer(     ""Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons."",     return_tensors=""pt"", ) labels = tokenizer(""Bad Reasons To Quit Your Job"", return_tensors=""pt"")[""input_ids""] input_ids = inputs[""input_ids""].to(device) attention_mask = inputs[""attention_mask""].to(device) labels = labels.to(device) loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss print(loss) ``` Log: ``` /home/yidi/local/pytorch/torch/_inductor/compile_fx.py:140: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.   warnings.warn( (repro) ~/local/pytorch$ python repro6.py  /home/yidi/local/pytorch/torch/_inductor/compile_fx.py:140: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.   warnings.warn( tensor(6.2411, device='cuda:0', grad_fn=) ```"
transformer,TransformerEncoderLayer produce different result with padding," 🐛 Describe the bug ```python import torch d_model = 10 n_head = 5 batch_size = 5 _record = 50 torch.manual_seed(0) _encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head, batch_first=True) torch.manual_seed(0) feature_value = torch.randn(batch_size, _record, d_model) _mask = torch.ones(batch_size, _record) _mask[:, 1] = 0.0              mask the last row for each batch _encoder_layer.eval()  prediction include the last row but mask it result1 = _encoder_layer(feature_value, src_key_padding_mask=_mask)      prediction without mask and exclude the last row result2 = _encoder_layer(feature_value[:, :1, :])            print(result1[0, 0]) print(result2[0, 0])     """""" tensor([0.8711, 0.8349,  0.0806,  0.4240,  1.5906,  1.3947, 0.1622, 1.5281,          0.8241, 0.9176], grad_fn=) tensor([0.8746, 0.8340,  0.0819,  0.4208,  1.5915,  1.3968, 0.1670, 1.5278,          0.8238, 0.9116], grad_fn=) """""" ``` Ideally, this two cases should produce exactly same result, but they are different. If I change the _record value bigger, like 5k, the difference shrink. I also checked if adding src_mask will aviod the difference, but it produce the same result without scr_mask. I'm wondering if this is the desired result. Thanks for your help!  Versions PyTorch version: 1.12.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: CentOS Stream 9 (x86_64) GCC version: (GCC) 11.3.1 20220421 (Red Hat 11.3.12) Clang version: 14.0.5 (Red Hat 14.0.51.el9) CMake version: version 3.20.2 Libc version: glibc2.10 Python version: 3.7.6 (default, Jan  8 2020, 19:59:22)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.14.0142.el9.x86_64x86_64withcentos9 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3070 GPU 1: NVIDIA GeForce RTX 3070 GPU 2: NVIDIA GeForce RTX 3070 GPU 3: NVIDIA GeForce RTX 3070 GPU 4: NVIDIA GeForce RTX 3070 Nvidia driver version: 515.43.04 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.19.5 [pip3] numpydoc==0.9.2 [pip3] torch==1.12.0+cu116 [pip3] torchaudio==0.12.0+cu116 [pip3] torchvision==0.13.0+cu116 [conda] blas                      1.0                         mkl    condaforge [conda] mkl                       2021.2.0                 pypi_0    pypi [conda] mklservice               2.3.0            py37he904b0f_0   [conda] mkl_fft                   1.0.15           py37ha843d7b_0   [conda] mkl_random                1.1.0            py37hd6b4f25_0   [conda] numpy                     1.18.1           py37h4f9e942_0   [conda] numpybase                1.18.1           py37hde5b4d6_1   [conda] numpydoc                  0.9.2                      py_0    condaforge ",2022-12-09T09:31:55Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/90534,"I think the difference comes from the multiattention part. According to the paper ""Attention Is All You Need“, the attention matrix will be divided by sqrt(d_k), which is the sequence len. So adding a padding record will change the value of d_k, thus affect the final result. I think the more reasonable way is to divide the matrix by the sqrt of the length of the unpadding data.",This issue is reproducible.,I made a mistate about src_key_padding_mask. The mask matrix should take True as padding value.
yi,[Dynamo] torch._dynamo.exc.BackendCompilerFailed: tvm raised AttributeError: Can't pickle local object 'WeakValueDictionary.__init__.<locals>.remove'," 🐛 Describe the bug This looks like subgraph not able to pickle whatever output eager mode is returning. And if I remove `torch.save(result, filename)` at line 111 of `_dynamo/optimization/subgraph.py`, a seg fault is thrown. My repro: ``` .optimize(""tvm"") def toy_example(a, b):     a = torch.cos(a)     return a * torch.sin(b) for _ in range(100):     toy_example(torch.randn(10), torch.randn(10)) ``` complete stacktrace: ``` Traceback (most recent call last):   File ""/home/yj/pytorch/torch/_dynamo/output_graph.py"", line 586, in call_user_compiler     compiled_fn = compiler_fn(gm, self.fake_example_inputs())   File ""/home/yj/pytorch/torch/_dynamo/debug_utils.py"", line 915, in debug_wrapper     compiled_gm = compiler_fn(gm, example_inputs, **kwargs)   File ""/home/yj/pytorch/torch/_dynamo/optimizations/backends.py"", line 51, in inner     return inner(SubGraph(model, example_inputs, tmp), **kwargs)   File ""/home/yj/pytorch/torch/_dynamo/optimizations/backends.py"", line 56, in inner     return fn(model, **kwargs)   File ""/home/yj/pytorch/torch/_dynamo/optimizations/backends.py"", line 536, in tvm     return subgraph.wrap_returns(   File ""/home/yj/pytorch/torch/_dynamo/optimizations/subgraph.py"", line 201, in wrap_returns     expected = self.example_outputs   File ""/home/yj/pytorch/torch/_dynamo/optimizations/subgraph.py"", line 25, in inner     result = fn(self)   File ""/home/yj/pytorch/torch/_dynamo/optimizations/subgraph.py"", line 111, in example_outputs     torch.save(result, filename)   File ""/home/yj/pytorch/torch/serialization.py"", line 436, in save     _save(obj, opened_zipfile, pickle_module, pickle_protocol)   File ""/home/yj/pytorch/torch/serialization.py"", line 648, in _save     pickler.dump(obj) AttributeError: Can't pickle local object 'WeakValueDictionary.__init__..remove' ```  Versions PyTorch version: 1.14.0a0+gitd957829 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.15 (default, Nov  4 2022, 20:59:55)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.053genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Nvidia driver version: 510.85.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.6.0 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_adv_train.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.4.1 /usr/local/cuda11.6/targets/x86_64linux/lib/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] bertpytorch==0.0.1a4 [pip3] clipanytorch==2.5.0 [pip3] CoCapytorch==0.0.6 [pip3] dalle2pytorch==1.10.5 [pip3] emapytorch==0.0.10 [pip3] functorch==1.14.0a0+408bcf1 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.5 [pip3] pytorchtransformers==1.2.0 [pip3] pytorchwarmup==0.1.1 [pip3] rotaryembeddingtorch==0.1.5 [pip3] torch==1.14.0a0+git1b1301f [pip3] torchfidelity==0.3.0 [pip3] torchstruct==0.5 [pip3] torchaudio==0.14.0a0+8ba323b [pip3] torchdata==0.6.0a0+d218f79 [pip3] torchmetrics==0.10.3 [pip3] torchrecnightly==2022.11.21 [pip3] torchtext==0.15.0a0+ed78e3b [pip3] torchvision==0.15.0a0+4a310f2 [pip3] torchx==0.3.0 [pip3] vectorquantizepytorch==0.10.11 [conda] bertpytorch              0.0.1a4                   dev_0     [conda] blas                      1.0                         mkl   [conda] clipanytorch             2.5.0                    pypi_0    pypi [conda] cocapytorch              0.0.6                    pypi_0    pypi [conda] dalle2pytorch            1.10.5                   pypi_0    pypi [conda] emapytorch               0.0.10                   pypi_0    pypi [conda] functorch                 1.14.0a0+408bcf1          pypi_0    pypi [conda] magmacuda116             2.6.1                         1    pytorch [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklinclude               2022.1.0           h06a4308_224   [conda] mklservice               2.4.0            py38h7f8727e_0   [conda] mkl_fft                   1.3.1            py38hd3c417c_0   [conda] mkl_random                1.2.2            py38h51133e4_0   [conda] numpy                     1.23.5                   pypi_0    pypi [conda] pytorchtransformers      1.2.0                    pypi_0    pypi [conda] pytorchwarmup            0.1.1                    pypi_0    pypi [conda] rotaryembeddingtorch    0.1.5                    pypi_0    pypi [conda] torch                     1.14.0a0+git1b1301f           dev_0     [conda] torchfidelity            0.3.0                    pypi_0    pypi [conda] torchstruct              0.5                      pypi_0    pypi [conda] torchaudio                0.14.0a0+8ba323b          pypi_0    pypi [conda] torchdata                 0.6.0a0+d218f79          pypi_0    pypi [conda] torchmetrics              0.10.3                   pypi_0    pypi [conda] torchrecnightly          2022.11.21               pypi_0    pypi [conda] torchtext                 0.15.0a0+ed78e3b          pypi_0    pypi [conda] torchvision               0.15.0a0+4a310f2          pypi_0    pypi [conda] torchx                    0.3.0                    pypi_0    pypi [conda] vectorquantizepytorch   0.10.11                  pypi_0    pypi ",2022-12-08T23:55:03Z,triaged oncall: pt2 module: dynamo,closed,0,6,https://github.com/pytorch/pytorch/issues/90511,Related to  CC(`make_fx` makes tensors unpickleable) ?," Thanks for the reply! It is possible that this issue is related to CC(`make_fx` makes tensors unpickleable). One odd thing is that this error seems to be a problem of saving `example_outputs` from eager mode while running `tvm` as a backend. But if that is the case, then all other noneager mode backends should have the same error but I can't get a repro on inductor or ts on the same workload."," inductor never tries to pickle the example_inputs/example_outputs, so it won't hit pickling errors. We should fix pickling, but possible workarounds are: 1) Don't save the tensors to disk 2) Copy to a fresh tensor before saving Looks like  has a fix for the other pickling issue in CC(Don't put tracing state on Tensor)  can you test if that fixes this issue too?", Thanks for the reply! I saw that  's PR is merged so I did a rebase but unfortunately this error persists. Should we put in a workaround for non eager mode saving backends for dynamo for now?,"Ideally we would fix pickling. Either way, let's just make SubGraph not save example_outputs() to disk.  I'm not convinced that caching optimization is worth it.",close as this is fixed
transformer,Introduce causal mask,"Summary: Introduce causal mask This PR introduces a causal mask option is_causal, since current custom kernels do not support arbitrary masks. (Also, building these masks is expensive, and avoiding to build them is a significant opportunity since causal mask is extremely common for training Transformer models.) Test Plan: sandcastle & github ci/cd Differential Revision: D41723137",2022-12-08T23:43:49Z,fb-exported Merged release notes: AO frontend,closed,0,41,https://github.com/pytorch/pytorch/issues/90508,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137,This pull request was **exported** from Phabricator. Differential Revision: D41723137
rag,Saving and loading unallocated storage and tensor at the same time can result in incorrect dtype,"In CC(move TypedStorage handling to assertEqual), it was shown that the following checks https://github.com/pytorch/pytorch/blob/b0bd5c4508a8923685965614c2c74e6a8c82f7ba/test/test_serialization.pyL688L689 did not previously check if a loaded storage's dtype matches the saved storage, and indeed there are some cases where they do not match. I will look into it and fix it ",2022-12-08T21:33:50Z,module: serialization triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/90497
dspy,"Failure in guard_fail_fn callback - raising here will cause a NULL Error on guard eval Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/site-packages/torch/_dynamo/guards.py"", line 897, in guard_fail_hook     guard_fn.guard_fail_fn(GuardFail(reason, orig_code_map[code])) "," 🐛 Describe the bug `.compile() class model():      def __init__():           .....      def forward():           return process()      def process():           .... ` [20221209 01:13:39,987] torch._dynamo.guards: [ERROR] Failure in guard_fail_fn callback  raising here will cause a NULL Error on guard eval Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/guards.py"", line 897, in guard_fail_hook     guard_fn.guard_fail_fn(GuardFail(reason, orig_code_map[code])) TypeError: 'NoneType' object is not callable  Versions [20221209 01:13:39,987] torch._dynamo.guards: [ERROR] Failure in guard_fail_fn callback  raising here will cause a NULL Error on guard eval Traceback (most recent call last):   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_dynamo/guards.py"", line 897, in guard_fail_hook     guard_fn.guard_fail_fn(GuardFail(reason, orig_code_map[code])) TypeError: 'NoneType' object is not callable ",2022-12-08T17:22:47Z,high priority bug oncall: pt2 module: dynamo,closed,0,1,https://github.com/pytorch/pytorch/issues/90480,I think this issue has already been fixed by https://github.com/pytorch/pytorch/pull/89731 could you reopen if not fixed after tonight's nightly?
yi,[stateless] add weight tying support,  CC([stateless] add weight tying support) ,2022-12-08T16:27:51Z,Merged ciflow/trunk release notes: nn topic: bc breaking topic: deprecation module: functorch,closed,0,5,https://github.com/pytorch/pytorch/issues/90477,"This changed the algorithm from make_functional and from the original PR CC([release] Add warning to stateless.functional_call for deprecated behavior), so some new numbers from that In running a script like this on resnet18 (no tied parameters), this went from 58% slower than vanilla (not using functional_call at all) to 63% slower. This is a 3.5% slowdown from the functional_call before this PR Raw numbers in case anyone wants to see: ","If I'm understanding the numbers, they are measuring some fixed number of iterations of resnet18 using the functional_call API. Shouldn't resnet be faster on GPU than CPU?","> If I'm understanding the numbers, they are measuring some fixed number of iterations of resnet18 using the functional_call API Yep! They're running it 10 times and then this is the average time as reported > Shouldn't resnet be faster on GPU than CPU? Yeah I didn't explain these tables well. From the docs, my understanding is that this reports the amount of time used by the CPU and GPU separately. In this case, what we're worried about is the CPU time since the code for the stateless call happens on CPU"," merge f ""failures from flaky test and unrelated mps test"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
agent,Strange issue with tensor asyncio and RPC," 🐛 Describe the bug Hi Folks, I have a bizarre issue and can't narrow down a problem.  I'll update a case if you can provide more debug commands for this issue.  platform and system    * Latest torch version, CUDA 11.7, Python 3.10 and 3.11 ( test both)    * Test CPU , CUDA , Linux , MacOs, WSL2 , Docker. Consider on the same host, communicating over RPC, mainly to address the issue of torch.mp. i.e., I need one process to detach the tensor from CUDA to the CPU, send  it to the agent and upload it back to GPU. Thus,  Agent  Observer.   (Observer takes the tensors and sends via async RPC on request from Agent, it async call  and Observer responds as torch.future). ``` Observer:     agent will issue this call, and foobar_rpc put tensor take a tensor from cuda      to CPU and send.    foobar_rpc():       tensor.clone().cpu() ``` The agent takes the torch future and puts the asyncio queue. Now in the asyncio dequeue phase (i.e, asyncio wait on queue),  does  ``` Agent    async def the_rpc_call()           fu = rpc_async().foobar()  args etc. removed to make it clear          await THE_QUEUE.put(the_fu) ``` ``` async def foo_consumer()          skip code for a loop, not relevant.          the_future = THE_QUEUE.get()            left_hand = xsum.clone()      < error         left_hand = copy_ same error etc.          if I try to do anything else, it either           block or crash, or I see an error in RPC         await some_foo(x, y)        no errors. at all  ``` It looks like somewhere in downstream code, it either has a lock, but on readonly, it works; on lefthand assignment, it fails for tensors. But if that is the case, the last call some_foo (x, y) should produce the same behavior. i.e., if I pass x and y to downstream code, I have an issue now. Does RPC logic in torch, do anything specific with the tensor it receives somewhere deep in a C library with the tensors it receives? Thank you,  Versions Linux ``` Collecting environment information... PyTorch version: 1.13.0 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 22.10 (x86_64) GCC version: (condaforge gcc 10.4.019) 10.4.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.36 Python version: 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0] (64bit runtime) Python platform: Linux5.19.026genericx86_64withglibc2.36 Is CUDA available: True CUDA runtime version: 11.7.99 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 520.61.05 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] torch==1.13.0 [pip3] torchvision==0.14.0 [pip3] torchviz==0.0.2 [conda] blas                      1.0                         mkl [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklservice               2.4.0           py310h7f8727e_0 [conda] mkl_fft                   1.3.1           py310hd6ae3a3_0 [conda] mkl_random                1.2.2           py310h00e6091_0 [conda] numpy                     1.23.4          py310hd5efca6_0 [conda] numpybase                1.23.4          py310h8e6c178_0 [conda] pytorch                   1.13.0          py3.10_cuda11.7_cudnn8.5.0_0    pytorch [conda] pytorchcuda              11.7                 h67b0de4_0    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torchvision               0.14.0              py310_cu117    pytorch [conda] torchviz                  0.0.2                    pypi_0    pypi ``` This is on mac 1.12 ``` Collecting environment information... PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 13.0.1 (x86_64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.202) CMake version: version 3.24.3 Libc version: N/A Python version: 3.8.13 (default, Mar 28 2022, 06:16:26)  [Clang 12.0.0 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] blas                      1.0                         mkl [conda] mkl                       2021.4.0           hecd8cb5_637 [conda] mklservice               2.4.0            py38h9ed2024_0 [conda] mkl_fft                   1.3.1            py38h4ab4a9b_0 [conda] mkl_random                1.2.2            py38hb2f4e1b_0 [conda] numpy                     1.22.4                   pypi_0    pypi [conda] numpybase                1.22.3           py38h3b1a694_0 [conda] torch                     1.12.0                   pypi_0    pypi [conda] torchaudio                0.12.0                   pypi_0    pypi [conda] torchvision               0.13.0                   pypi_0    pypi ``` ",2022-12-08T10:36:56Z,oncall: distributed,open,0,0,https://github.com/pytorch/pytorch/issues/90459
yi,`torch.linalg.solve` yields much lower precisions in `1.13.0` than previous versions," 🐛 Describe the bug After upgrading to `torch 1.13.0`, `torch.linalg.solve` suddenly gives solutions with much lower precisions, **regardless of device (`cpu` or `gpu`) or type (`float64` or `float32`)**. The errors quickly escalate in my numerical calculations and break down my simulations. Take the following data as an example (I know it is somewhat illconditioned, but the changes in behaviors are real) ```python import torch torch.set_default_dtype(torch.float64) torch.backends.cuda.matmul.allow_tf32 = False A = torch.tensor([     [ 3.8025705376834739e07, 9.1719365342788720e07, 6.7124337949782264e06, 6.4837019110456791e05, 7.0869999797614066e04, 1.0694859984690733e02, 3.2912231531790004e01, 6.6347339870464399e+00, 8.2509761085708249e+01,  0.0000000000000000e+00],     [ 0.0000000000000000e+00,  4.4000124553730829e07, 5.5080918253708871e07, 5.1498277032055974e06, 5.7818057148617599e05, 9.1226448867859551e04, 2.2619326362175465e02, 4.4038788530099793e01, 5.1992675801721502e+00,  0.0000000000000000e+00],     [ 0.0000000000000000e+00, 1.0669700681643825e10,  4.3768558191229986e07, 4.3974816153203019e07, 4.8865127972067992e06, 7.8116560507683326e05, 1.7589402883070333e03, 3.3666362131922367e02, 3.8659142733749491e01,  0.0000000000000000e+00],     [ 0.0000000000000000e+00, 7.8216940301197729e12, 1.5895421888461478e10,  4.3542984469163267e07, 4.0043248885844276e07, 6.6798905178796823e06, 1.3761857019311234e04, 2.5943507621790695e03, 2.9003633389177604e02,  0.0000000000000000e+00],     [ 0.0000000000000000e+00, 2.4603969583879200e13, 6.0925772512004975e12, 1.9886454656863128e10,  4.3370279880257098e07, 5.6639032522315289e07, 1.0649799471193429e05, 1.9808440853565822e04, 2.1583707954594099e03,  0.0000000000000000e+00],     [ 0.0000000000000000e+00, 1.4999959257460881e15, 3.2831398418930186e14, 8.8714562886788080e13, 4.3280772005187299e11,  4.4148762039828565e07, 6.8089481270669943e07, 1.4575015323337058e05, 1.5597848962814291e04,  0.0000000000000000e+00],     [ 0.0000000000000000e+00, 3.6858575028157790e16, 7.2036090445864899e15, 1.4349791509103240e13, 2.9849302443991965e12,  6.3914122655929791e10,  4.6448551809896547e07, 6.8453604307207769e07, 1.0332761488908590e05,  0.0000000000000000e+00],     [ 0.0000000000000000e+00, 3.7045642770024088e17, 7.2015144280333478e16, 1.4158860652466324e14, 2.8662564585632735e13, 6.2285079180541528e12,  1.5090963357302090e09,  4.8979817748389458e07, 1.2863401745116974e07,  0.0000000000000000e+00],     [ 0.0000000000000000e+00, 2.3760629594245614e18, 4.6007155546998113e17, 8.9513844792609796e16, 1.7640414722799569e14, 3.5935860384434572e13, 7.9429359080595169e12,  2.0146206213869421e09,  4.7959403001188342e07,  0.0000000000000000e+00],     [ 0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  3.8025705376834739e07] ]) b = torch.tensor(     [ 6.9677181015078851e+04,  3.9337825712781823e+03,  2.7914109655787729e+02,  1.9895852311404216e+01,  1.3819016836738420e+00,  7.5229947004102571e02,  1.3433804143281360e03, 3.1421146091483441e04, 2.8076324348838071e05,  0.0000000000000000e+00] ) ``` With `torch 1.12.1`, the relative errors are around machineprecision (a few 1e16), which is *consistent with the precision obtained from `numpy` or `cupy`* ```python In [1]: (A @ torch.linalg.solve(A, b)  b) / b tensor([ 0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  1.6068046486108669e16,         3.6894317650011501e16,  0.0000000000000000e+00, 0.0000000000000000e+00,  3.6202728109145290e16,                     nan]) ``` However, **with `torch 1.13.0`, the relative errors are huge (max at 5e11)** ```python In [2]: (A @ torch.linalg.solve(A, b)  b) / b tensor([2.0884764590602007e16,  4.6240212075443264e16,  0.0000000000000000e+00, 1.7856554337026822e16, 4.1776920863882539e15,         8.7255061242277206e14,  5.0944524844510106e11, 2.0456409676328997e11, 4.9269499441339466e12,                     nan]) ```  Below are more comparisons using `torch.float64` and `cuda` ```python In [1]: A = torch.tensor([ ... ], device=torch.device('cuda')) In [2]: b = torch.tensor([ ... ], device=torch.device('cuda')) In [3]: (A @ torch.linalg.solve(A, b)  b) / b   with torch 1.12.1 tensor([ 0.0000e+00,  1.1560e16,  0.0000e+00,  0.0000e+00,  0.0000e+00,         1.8447e16,  0.0000e+00,  1.7253e16,  3.6203e16,         nan],        device='cuda:0') In [4]: (A @ torch.linalg.solve(A, b)  b) / b   with torch 1.13.0 tensor([2.0885e16,  0.0000e+00, 2.0364e16, 7.1426e16, 1.7675e15,          4.1875e14,  4.4228e11, 1.2897e11, 3.0743e12,         nan],        device='cuda:0') ``` And more comparisons using `torch.float32` and `cpu` ```python In [1]: torch.set_default_dtype(torch.float32) In [2]: torch.backends.cuda.matmul.allow_tf32 = True In [3]: (A @ torch.linalg.solve(A, b)  b) / b   with torch 1.12.1 tensor([1.1212e07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.6265e08,          1.9807e07, 8.6658e08,  9.2625e08, 0.0000e+00,         nan]) In [4]: (A @ torch.linalg.solve(A, b)  b) / b   with torch 1.13.0 tensor([1.1212e07,  6.2063e08, 1.0933e07, 9.5867e08, 2.3291e06,         4.0902e05, 2.2294e02, 2.5929e03, 1.9909e03,         nan]) ```  Versions For tests with `torch 1.12.1`, the output is ```bash Collecting environment information... PyTorch version: 1.12.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] (64bit runtime) Python platform: Linux5.15.79.1microsoftstandardWSL2x86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU Nvidia driver version: 527.37 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.5 [pip3] pytorchmemlab==0.2.4 [pip3] torch==1.12.1+cu116 [pip3] torchaudio==0.12.1+cu116 [pip3] torchvision==0.13.1+cu116 [pip3] xitorch==0.3.0 [conda] No relevant packages ``` For tests with `torch 1.13.0`, the output is ```bash PyTorch version: 1.13.0+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.79.1microsoftstandardWSL2x86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.8.89 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU Nvidia driver version: 527.37 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.5 [pip3] torch==1.13.0 [pip3] torchaudio==0.13.0 [pip3] torchvision==0.14.0 [conda] No relevant packages ``` ",2022-12-08T06:04:45Z,triaged module: linear algebra,closed,0,11,https://github.com/pytorch/pytorch/issues/90453,cc:  who is a linalg guru,"That matrix is very badly conditioned: ```python >>> torch.linalg.svdvals(A) tensor([8.2942e+01, 2.2421e02, 9.6433e07, 5.4287e07, 4.8071e07, 4.6181e07,         3.8026e07, 2.1794e07, 2.7241e12, 3.0295e15]) >>> torch.linalg.cond(A) tensor(2.7378e+16) ``` as such errors of that order are expected.  See https://pytorch.org/docs/master/notes/numerical_accuracy.htmlextremalvaluesinlinalg","> That matrix is very badly conditioned > as such errors of that order are expected. See https://pytorch.org/docs/master/notes/numerical_accuracy.htmlextremalvaluesinlinalg Yes, they are illconditioned but are helplessly, physically motivated. Do you mind elaborating a bit what have changed in `0.13.0`? Are there some settings that I can use to make `torch` produce results similar to previous versions?  "," I tried to find what have changed in the blog posts but didn't find any clue behind this change of behaviors.  With `torch 1.12.1` or earlier, I can obtain the same answer in my test use cases from either `torch`, `numpy`, or `cupy` (including the example given above). And I've already spent quite some time to adapt my code from using `numpy` to `torch` to take advantage of the GPU acceleration.  Thus, I do wonder if this precision change is somewhat permanent or is there some hope on revival of old behaviors?"," I compiled PyTorch from source and can confirm that the commit 54949a5abc9890143de4b5dd2f13ff98446376a3 is responsible for this behavior change. **Here I want to argue that, even if the matrix is illconditioned, the solution should be around machineprecision** (one may question the correctness of the solution, but that's another issue).  Using `torch` with one commit prior (65a37923f9b14c7c9e80535d771ef9e4e92d0502) gives the expected results ```python In [4]: (A @ torch.linalg.solve(A, b)  b) / b Out[4]:  tensor([ 2.0885e16,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.6068e16,         3.6894e16,  0.0000e+00, 0.0000e+00,  3.6203e16,         nan]) ``` Using `torch` with 54949a5abc9890143de4b5dd2f13ff98446376a3 gives the new results ```python In [4]: (A @ torch.linalg.solve(A, b)  b) / b Out[4]:  tensor([2.0885e16, 6.9360e16, 2.0364e16,  0.0000e+00, 1.7675e15,         5.3312e13,  1.5038e11,  1.3604e11,  1.3794e12,         nan]) ``` However, my current knowledge is not enough to understand what had changed in the commit 54949a5abc9890143de4b5dd2f13ff98446376a3. Any ideas/suggestions would be greatly appreciated!","That commit is mostly a better engineering commit. Now, it introduces an optimisation. To avoid performing a copy of the matrix `A`, if `A` is Ccontiguous (rowmajor), we factorise `A^T` instead and then call `lu_solve` with `adjoint=True`. It may be the case that the backend library (the relevant BLAS implementation that you use) is a bit less precise when called with `adjoint=True`. In my opinion, errors of that order are reasonable for linear algebra. As discussed in the note linked in  CC(`torch.linalg.solve` yields much lower precisions in `1.13.0` than previous versions)issuecomment, it is expected that different different devices and different backends give marginally different solutions. What I would suggest if you do care about these is that you try to find a small repro in C / C++ and you report the accuracy mismatch to the relevant BLAS implementation that you are using."," Thanks a lot for the explanations. Sorry if I misunderstand something, but I am not using a specific BLAS implementation  the error seems to be quite consistent across *very* different devices/platforms and BLAS backends (`mkl/cuda/generic/open`, see tests below). Is there any way that I may confirm the issue is due to some BLAS implementation? Thanks you again in advance.  **1**. The test results in the first comment of this issue were from **WSL2 Ubuntu 20.04**, a laptop with Intel i711800H + RTX 3070 Mobile, where the `cpu` results used `MKL` and the `cuda` results used (I think) `cuBLAS` (if not, I guess it is something in `MAGMA`). Below shows the config: ```python >>> print(torch.__config__.show()) PyTorch built with:    GCC 9.3    C++ Version: 201402    Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications    Intel(R) MKLDNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)    OpenMP 201511 (a.k.a. OpenMP 4.5)    LAPACK is enabled (usually provided by MKL)    NNPACK is enabled    CPU capability usage: AVX2    CUDA Runtime 11.7    NVCC architecture flags: gencode;arch=compute_37,code=sm_37;gencode;arch=compute_50,code=sm_50;gencode;arch=compute_60,code=sm_60;gencode;arch=compute_70,code=sm_70;gencode;arch=compute_75,code=sm_75;gencode;arch=compute_80,code=sm_80;gencode;arch=compute_86,code=sm_86    CuDNN 8.5    Magma 2.6.1    Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset9/root/usr/bin/c++, CXX_FLAGS= fabiversion=11 Wnodeprecated fvisibilityinlineshidden DUSE_PTHREADPOOL fopenmp DNDEBUG DUSE_KINETO DUSE_FBGEMM DUSE_QNNPACK DUSE_PYTORCH_QNNPACK DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO O2 fPIC Wnonarrowing Wall Wextra Werror=returntype Werror=nonvirtualdtor Wnomissingfieldinitializers Wnotypelimits Wnoarraybounds Wnounknownpragmas Wunusedlocaltypedefs Wnounusedparameter Wnounusedfunction Wnounusedresult Wnostrictoverflow Wnostrictaliasing Wnoerror=deprecateddeclarations Wnostringopoverflow Wnopsabi Wnoerror=pedantic Wnoerror=redundantdecls Wnoerror=oldstylecast fdiagnosticscolor=always falignednew Wnounusedbutsetvariable Wnomaybeuninitialized fnomatherrno fnotrappingmath Werror=format Werror=castfunctiontype Wnostringopoverflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, ``` **2**. The test results in the sixth comment (where I found the exact commit responsible for this issue) were from a **CentOS 7 server** with Intel(R) Xeon(R) CPU E52650 v2, where I only (compiled and) used `cpu` with `BLAS_INFO=generic` and `USE_MKL=OFF`. Below shows the config: ```python >>> print(torch.__config__.show()) PyTorch built with:    GCC 8.3    C++ Version: 201402    Intel(R) MKLDNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)    OpenMP 201511 (a.k.a. OpenMP 4.5)    LAPACK is enabled (usually provided by MKL)    NNPACK is enabled    CPU capability usage: NO AVX    Build settings: BLAS_INFO=generic, BUILD_TYPE=Release, CXX_COMPILER=/opt/ohpc/pub/compiler/gcc/8.3.0/bin/c++, CXX_FLAGS= Wnodeprecated fvisibilityinlineshidden DUSE_PTHREADPOOL fopenmp DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_QNNPACK DUSE_PYTORCH_QNNPACK DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO O2 fPIC Wnonarrowing Wall Wextra Werror=returntype Wnomissingfieldinitializers Wnotypelimits Wnoarraybounds Wnounknownpragmas Wnounusedparameter Wnounusedfunction Wnounusedresult Wnostrictoverflow Wnostrictaliasing Wnoerror=deprecateddeclarations Wnostringopoverflow Wnopsabi Wnoerror=pedantic Wnoerror=redundantdecls Wnoerror=oldstylecast fdiagnosticscolor=always falignednew Wnounusedbutsetvariable Wnomaybeuninitialized fnomatherrno fnotrappingmath Werror=format Werror=castfunctiontype Wnostringopoverflow, FORCE_FALLBACK_CUDA_MPI=1, LAPACK_INFO=generic, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=ON, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, ``` **3**. On a **MacBook Pro with M1 Pro**, I can also reproduce the errors (from either `torch` installed via `pip` or `torch` compiled by `MacPorts`) ```python >>>  ===== on CPU since MPS doesn't support linalg.solve yet ===== >>> (A @ torch.linalg.solve(A, b)  b) / b tensor([ 0.0000e+00,  2.3120e16, 2.0364e16, 1.7857e16, 4.0170e15,         2.1768e14,  5.9627e11, 1.7222e11, 5.9982e12,         nan]) >>>  ===== config ===== >>> print(torch.__config__.show()) PyTorch built with:    GCC 4.2    C++ Version: 201402    clang 14.0.0    LAPACK is enabled (usually provided by MKL)    NNPACK is enabled    CPU capability usage: NO AVX    Build settings: BLAS_INFO=accelerate, BUILD_TYPE=Release, CXX_COMPILER=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang++, CXX_FLAGS=Wnoerror=bitwiseinsteadoflogical fvisibilityinlineshidden Wnodeprecateddeclarations DUSE_PTHREADPOOL DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_PYTORCH_QNNPACK DUSE_XNNPACK DUSE_PYTORCH_METAL DUSE_PYTORCH_METAL_EXPORT DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO O2 fPIC Wnonarrowing Wall Wextra Werror=returntype Werror=nonvirtualdtor Wnomissingfieldinitializers Wnotypelimits Wnoarraybounds Wnounknownpragmas Wunusedlocaltypedefs Wnounusedparameter Wnounusedfunction Wnounusedresult Wnostrictoverflow Wnostrictaliasing Wnoerror=deprecateddeclarations Wvlaextension Wnorangeloopanalysis Wnopassfailed Wnoerror=pedantic Wnoerror=redundantdecls Wnoerror=oldstylecast Wconstantconversion Wnoinvalidpartialspecialization Wnotypedefredefinition Wnounusedprivatefield Wnoinconsistentmissingoverride Wnoc++14extensions Wnoconstexprnotconst Wnomissingbraces Wunusedlambdacapture Wunusedlocaltypedef Qunusedarguments fcolordiagnostics fdiagnosticscolor=always fnomatherrno fnotrappingmath Werror=format Werror=castfunctiontype DUSE_MPS fnoobjcarc Wnounguardedavailabilitynew Wnounusedprivatefield Wnomissingbraces Wnoc++14extensions Wnoconstexprnotconst, LAPACK_INFO=accelerate, TORCH_VERSION=1.13.0, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=ON, USE_GLOG=ON, USE_LITE_PROTO=1, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=OFF, USE_ROCM=OFF, ``` **4**. On a **Ubuntu 20.04 server with ARM NeoverseN1**, I can also reproduce the errors with `OpenBLAS` ```python >>>  ===== also only on CPU ===== >>> (A @ torch.linalg.solve(A, b)  b) / b tensor([ 2.0885e16, 1.1560e16, 2.0364e16, 1.0714e15,  0.0000e+00,         5.1468e14,  7.4451e12,  1.6115e11, 1.4156e12,         nan]) >>>  ===== config (note that `BLAS_INFO=open`) ===== >>> print(torch.__config__.show()) PyTorch built with:    GCC 10.2    C++ Version: 201402    Intel(R) MKLDNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)    OpenMP 201511 (a.k.a. OpenMP 4.5)    LAPACK is enabled (usually provided by MKL)    NNPACK is enabled    CPU capability usage: NO AVX    Build settings: BLAS_INFO=open, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset10/root/usr/bin/c++, CXX_FLAGS= Wnodeprecated fvisibilityinlineshidden DUSE_PTHREADPOOL fopenmp DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_QNNPACK DUSE_PYTORCH_QNNPACK DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO O2 fPIC Wnonarrowing Wall Wextra Werror=returntype Werror=nonvirtualdtor Wnomissingfieldinitializers Wnotypelimits Wnoarraybounds Wnounknownpragmas Wunusedlocaltypedefs Wnounusedparameter Wnounusedfunction Wnounusedresult Wnostrictoverflow Wnostrictaliasing Wnoerror=deprecateddeclarations Wnostringopoverflow Wnopsabi Wnoerror=pedantic Wnoerror=redundantdecls Wnoerror=oldstylecast fdiagnosticscolor=always falignednew Wnounusedbutsetvariable Wnomaybeuninitialized fnomatherrno fnotrappingmath Werror=format Werror=castfunctiontype Wnostringopoverflow, LAPACK_INFO=open, TORCH_VERSION=1.13.0, USE_CUDA=OFF, USE_CUDNN=OFF, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, ``` **5**. On a CentOS 7.9 server **but with AMD EPYC 7552 + Nvidia Tesla V100SPCIE32GB**, I can also reproduce the errors ```python >>>  ===== on CPU ===== >>> (A @ torch.linalg.solve(A, b)  b) / b tensor([ 0.0000e+00, 6.9360e16, 4.0727e16,  5.3570e16, 1.2854e15,         3.3205e13,  1.7105e11,  1.4818e11,  8.1637e13,         nan]) >>>  ===== on GPU ===== >>> (A @ torch.linalg.solve(A, b)  b) / b tensor([ 0.0000e+00, 2.3120e16,  0.0000e+00, 3.5713e16, 2.2495e15,          7.2682e14,  5.9152e11, 7.6224e12, 4.7471e12,         nan],        device='cuda:0') >>>  ===== config ===== >>> print(torch.__config__.show()) PyTorch built with:    GCC 9.4    C++ Version: 201402    Intel(R) Math Kernel Library Version 2020.0.4 Product Build 20200917 for Intel(R) 64 architecture applications    Intel(R) MKLDNN v2.6.0 (Git Hash N/A)    OpenMP 201511 (a.k.a. OpenMP 4.5)    LAPACK is enabled (usually provided by MKL)    NNPACK is enabled    CPU capability usage: AVX2    CUDA Runtime 11.7    NVCC architecture flags: gencode;arch=compute_52,code=sm_52;gencode;arch=compute_60,code=sm_60;gencode;arch=compute_61,code=sm_61;gencode;arch=compute_70,code=sm_70;gencode;arch=compute_75,code=sm_75;gencode;arch=compute_80,code=sm_80;gencode;arch=compute_86,code=sm_86;gencode;arch=compute_86,code=compute_86    CuDNN 8.4.1  (built against CUDA 11.6)    Magma 2.6.2    Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.4.1, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS=fnognuunique fvisibilityinlineshidden DUSE_PTHREADPOOL fopenmp DNDEBUG DUSE_KINETO DUSE_FBGEMM DUSE_QNNPACK DUSE_PYTORCH_QNNPACK DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO O2 fPIC Wnonarrowing Wall Wextra Werror=returntype Wnomissingfieldinitializers Wnotypelimits Wnoarraybounds Wnounknownpragmas Wnounusedparameter Wnounusedfunction Wnounusedresult Wnostrictoverflow Wnostrictaliasing Wnoerror=deprecateddeclarations Wnostringopoverflow Wnopsabi Wnoerror=pedantic Wnoerror=redundantdecls Wnoerror=oldstylecast fdiagnosticscolor=always falignednew Wnounusedbutsetvariable Wnomaybeuninitialized fnomatherrno fnotrappingmath Werror=format Werror=castfunctiontype Wnostringopoverflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, ``` I can do more tests on more platforms/backends if necessary (if these turn out to be useful tests).","Thank you for the rather comprehensive dissection! Alas, I still think that this error is coming from the backend implementation. As you can see in that PR, we dispatch to the same backends, only that in the initial PR, when the matrix A is rowmajor, we copied it into a column major matrix (which is slow) and then called the backend `getrs` with `trans='N'`. What that PR introduces is the optimisation of, rather than copying `A` into a columnmajor matrix, we simply call `getrs` with `trans='T`'. This is mathematically equivalent, but it's clear from your experiments that this codepath may not be as numerically stable when called with illconditioned matrices as the `trans='N'` path. Again, here we are simply calling the given backends and perform a semanticpreserving optimisation as per the backend docs. I think this particular discrepancy should be reported to the relevant backends, given that you have a concrete reproducer. Note that having less wellmaintained paths in some of this libraries is not uncommon. We have found during the years many bugs in different libraries. For example, for MAGMA we discovered that the path `trans='T'`in `getrs` was buggy and I had to implement this option in terms of the `trans='N'` option in https://github.com/pytorch/pytorch/pull/77634/. See in particular https://github.com/pytorch/pytorch/blob/983d4f6fbb51381faa4af651f49142f45c834f8d/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cppL2460L2462","Yo can also see the bounds in the `getrs` docs about the guarantees given by the CPU backend. Given the condition number of the given matrix, I would not be surprised that the provided solutions are indeed within the range specified there. Now, if you want to recover the previous implementation, what you can do is to bypass the optimisation that that PR added altogether by making the matrix columnmajor as per: ```python A = A.mT.contiguous().mT ``` You can see that by doing this you get the same results as with PyTorch 1.12. Now, as in PyTorch 1.12 (before that speed optimisation was implemented, you have to copy the data once.  ",Thank you very much for the explanations and the workaround👍! My apologies for the late response.  Lesson learned that there are various codepaths maintained at different levels (indeed fascinating 😂).,Closing this. Feel free to reopen if you have further issues.
rag,RuntimeError: Placeholder storage has not been allocated on MPS device!," 🐛 Describe the bug I get an error every time I attempt to use MPS to train a model on my M1 Mac. The error occurs at first training step (so first call of `model(x)`). MRE: ``` import torch from torch import nn from torch.utils.data import DataLoader, Dataset import pandas as pd import numpy as np device = torch.device('mps') class MyLSTM(nn.Module):     def __init__(self, hidden_size, num_layers, output_size, input_dim):         super().__init__()         self.hidden_size = hidden_size         self.num_layers = num_layers         self.input_dim = input_dim         self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)         self.fc = nn.Linear(hidden_size, output_size)     def forward(self, x):         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)         out, _ = self.lstm(x, (h0, c0))         out = self.fc(out[:, 1, :])         return out def train_step(model, criterion, optimizer, x, y):     model.train()     optimizer.zero_grad()     y_pred = model(x)     loss = criterion(y_pred, y)     loss.backward()     optimizer.step()     return loss.item() def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=100):     train_losses = []     for epoch in range(epochs):         print(""Epoch"", epoch)         train_loss = 0         for x, y in train_loader:             train_loss += train_step(model, criterion, optimizer, x, y)         train_loss /= len(train_loader)         train_losses.append(train_loss)         print(""Train loss:"", train_loss)     return train_losses class MyDataset(Dataset):     def __init__(self, df, window_size):         self.df = df         self.window_size = window_size         self.data = []         self.labels = []         for i in range(len(df)  window_size):             x = torch.tensor(df.iloc[i:i+window_size].values, dtype=torch.float, device=device)             y = torch.tensor(df.iloc[i+window_size].values, dtype=torch.float, device=device)             self.data.append(x)             self.labels.append(y)     def __len__(self):         return len(self.data)     def __getitem__(self, idx):         return self.data[idx], self.labels[idx] class MyDataLoader(DataLoader):     def __init__(self, dataset, window_size, batch_size, shuffle=True):         self.dataset = dataset         super().__init__(self.dataset, batch_size=batch_size, shuffle=shuffle) df = pd.DataFrame(np.random.randint(0,100,size=(100, 1))) model = MyLSTM(1, 1, 1, 1) model.to(device) train_data = MyDataset(df, 5) train_loader = MyDataLoader(train_data, 5, 16) criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01) train_losses = train_model(model, criterion, optimizer, train_loader, None, epochs=10) ``` I receive the following traceback: ``` Traceback (most recent call last):   File ""min_mps.py"", line 83, in      train_losses = train_model(model, criterion, optimizer, train_loader, None, epochs=10)   File ""min_mps.py"", line 44, in train_model     train_loss += train_step(model, criterion, optimizer, x, y)   File ""min_mps.py"", line 32, in train_step     y_pred = model(x)   File ""~/miniconda3/envs/jaxenv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1480, in _call_impl     return forward_call(*args, **kwargs)   File ""min_mps.py"", line 24, in forward     out, _ = self.lstm(x, (h0, c0))   File ""~/miniconda3/envs/jaxenv/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1480, in _call_impl     return forward_call(*args, **kwargs)   File ""~/miniconda3/envs/jaxenv/lib/python3.10/sitepackages/torch/nn/modules/rnn.py"", line 776, in forward     result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers, RuntimeError: Placeholder storage has not been allocated on MPS device! ```  Versions ``` Python version: 3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ] (64bit runtime) Python platform: macOS13.0arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] torch==1.14.0.dev20221207 [pip3] torchaudio==0.14.0.dev20221207 [pip3] torchvision==0.15.0.dev20221207 [conda] numpy                     1.22.4                   pypi_0    pypi [conda] torch                     1.14.0.dev20221207          pypi_0    pypi [conda] torchaudio                0.14.0.dev20221207          pypi_0    pypi [conda] torchvision               0.15.0.dev20221207          pypi_0    pypi ``` Also note if relevant I'm running Mac OS 13.0. I also have tried this on the 1.13 stable release, same issue. ",2022-12-08T03:04:44Z,triaged module: mps,open,8,5,https://github.com/pytorch/pytorch/issues/90440,"I don't think this is a bug in PyTorch! You haven't allocated your torch.zeros to the device in the forward pass. If you do that, it runs, at least for me. `    def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=device) c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=device) out, _ = self.lstm(x, (h0, c0)) out = self.fc(out[:, 1, :]) return out`","> I don't think this is a bug in PyTorch! You haven't allocated your torch.zeros to the device in the forward pass. If you do that, it runs, at least for me. > ` def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=device) c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=device) out, _ = self.lstm(x, (h0, c0)) out = self.fc(out[:, 1, :]) return out` That’s indeed correct . We see these errors when all the tensors are not mapped to the device . Also there were some bugs in LSTM layer which got fixed in 2.0 release. I would recommend  to try that latest release with MacOS 13.3 OS version ","Dear Kulin, Thank you! He needs to normalise his training loss too, but ... The error that is of most interest to me right now is the *""aten::empty.memory_format""* that seems to arise whenever I switch a model that works perfectly well (but slowly) on the CPU (intel i9 I 8core; running *whisper*) to the ""mps"" device (*AMD Radeon Pro 5500M*). It throws an error somewhere reported as lying around lines 114345 of the *module.py* buried deep in the anaconda3/lib/ .../sitepackages/ storage, but there is no reference to this ""*aten*"" process anywhere in that file. My supposition is that I have done something stupid/ignorant in innocence, or there is some kind of hardware (memory) limitation, but nobody seems to have an answer. It may be to do with *SparseMPS*, but I frankly doubt it. So I started reading and wonder whether this hypothesis has any value: this error arises for me running whisper, and I am loading the models programmatically, but when I am using the ""mps"" device it will try to load them *using the mps*, and if there is some element of quantization involved in this, that will create an error because *quantization is supposed to be done on the CPU*. That I am trying to run these models on a 16GB MacOS Ventura 13.3 makes me wonder whether there is some kind of ""*automatic quantization optimization*"" going on that tries to make the models smaller because I don't have enough RAM. I hesitated to post this because I don't know enough about quantization, or indeed anything else, but is it possible that this is the issue and that it is somehow registering as a different error couched in terms of ""aten::...."" because the code at module.py 114345 looks more like a quantization process than some sort of backprop differentiation? If this has enough legs to merit being posted as an issue, I'll happily do so, but my default assumption is that I am an idiot! Apologies if this just confirms my stupidity! Best, John On Wed, Apr 12, 2023 at 2:20 PM Kulin Seth ***@***.***> wrote: > I don't think this is a bug in PyTorch! You haven't allocated your > torch.zeros to the device in the forward pass. If you do that, it runs, at > least for me. > def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), > self.hidden_size, device=device) c0 = torch.zeros(self.num_layers, > x.size(0), self.hidden_size, device=device) out, _ = self.lstm(x, (h0, c0)) > out = self.fc(out[:, 1, :]) return out > > That’s indeed correct . We see these errors when all the tensors are not > mapped to the device . Also there were some bugs in LSTM layer which got > fixed in 2.0 release. I would recommend  >  to try that latest release with MacOS > 13.3 OS version > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you commented.Message ID: > ***@***.***> >","I fix it by adding device before when create each network, device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"") self.lstm = nn.LSTM(..., device=device)","I ran into a similar issue, got the same error message ""RuntimeError: Placeholder storage has not been allocated on MPS device!"" when using a LSTM model. All tensors and the model were correctly mapped to the device in the code. However, my code worked fine when I updated torch to version 2.2.1 (I was using version 2.0.1) I have M1 Mac and was using the ""mps"" device. Before I updated torch, I tried to run on ""cpu"" device and then the output tensor from the forward pass contained NaN. I didn't look into it since this issue is fixed in latest versions of torch :)"
transformer,Fakify params and weights under private config,"  CC(Fakify params and weights under private config) Previously, we planned to lift the parameters and weights while exporting and implement our own transformer to ""unlift"" the lifted weights and params back to the graph as attributes. But this is bit challenging because:  We need to maintain correct ordering for weights and parameters that are passed as inputs so that we know how to map them back.   Some weights are unused in the graph, so our transformer needs to be aware of which weights and parameters are not used in the graph. And we need to distinguish which are real user input and which are parameters.   There can be more edge cases we haven't seen in other models yet.  I am aware that   and  mentioned that functionalization won't work with faketensor attributes but this is fine for the short term as we don't expect users to be modifying weights and params in inference mode. In fact, we explicitly disable attribute mutation in torchdynamo export mode right now.  Given above condition, it might be ok to just fakify params when we need. I use a flag to guard against this change. Differential Revision: D41891201",2022-12-07T21:55:58Z,Merged ciflow/trunk release notes: fx ciflow/inductor,closed,0,15,https://github.com/pytorch/pytorch/issues/90417,"Yeah I think this will be helpful to unblock all our use cases, including ASR and DPE models, since all models have weights. Right now it's nonscalable on our side to unlift params and buffers, because we have to handle so many corner cases.","I think I've mentioned (to  ?  ?) that I'm open to the idea of automatically fakeifying real tensors when in fake tensor mode in general, and then we would not have to guard this under a config flag. To understand if this is safe or not, we have to understand why we didn't add support for this initially. Consider the following program: ``` x = torch.empty(0) with FakeTensorMode():     x.resize_(10)     y = x * 2 ``` What exactly is this program supposed to do? Naively, we would perform a resize_ on the original tensor x, but this is problematic, because the entire point of fake tensor mode is to avoid doing real compute, and a x.resize_(10) would cause real memory to be allocated. However, nor can we *not* do a resize_ on the original tensor x, because the subsequent computation on x needs the updated metadata from the resize call. It sounds like you don't need this case. I think we'd be OK with throwing if this happens. Can we do this, instead of guarding this in private config?",We should also consider the api for if the user will want to make the lifted inputs static or dynamic,"> I think checking for inplace views is sufficient here > I think I've mentioned (to  ?  ?) that I'm open to the idea of automatically fakeifying real tensors when in fake tensor mode in general, and then we would not have to guard this under a config flag. To understand if this is safe or not, we have to understand why we didn't add support for this initially. >  > Consider the following program: >  > ``` > x = torch.empty(0) > with FakeTensorMode(): >     x.resize_(10) >     y = x * 2 > ``` >  > What exactly is this program supposed to do? Naively, we would perform a resize_ on the original tensor x, but this is problematic, because the entire point of fake tensor mode is to avoid doing real compute, and a x.resize_(10) would cause real memory to be allocated. However, nor can we _not_ do a resize_ on the original tensor x, because the subsequent computation on x needs the updated metadata from the resize call. >  > It sounds like you don't need this case. I think we'd be OK with throwing if this happens. Can we do this, instead of guarding this in private config? Are you referring to mutation on global variable? I think local mutations on the inputs should be handled by functionalization right? ","Mutation on a global variable, but in this case it refers to parameters which you didn't fakeify.","i'm going to let  drive the review, holler if you need second opinion"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", rebase , successfully started a rebase job. Check the current status here,"Successfully rebased `gh/tugsbayasgalan/83/orig` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/90417`)"," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Make Transformers compilable by C++17,"`register` keyword is removed in C++17, but keeping it there under ifdef as I have not measured the perf implication on older compiler, though there shouldn't be any: all modern compilers supposed to downright ignore it. This code originates from https://github.com/facebookresearch/xformers/pull/375 will propose similar PR to remove register keyword usage to that repo. Yet another thing discovered while working on https://github.com/pytorch/pytorch/pull/85969",2022-12-07T16:03:08Z,Merged ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/90389," merge f ""All CUDA builds are green"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[Vulkan] Enable copying QInt8 and QInt32 tensors from cpu to vulkan.,"Summary: Copying QInt8 and QInt32 from cpu to vulkan:   Added shader nchw_to_image_int8   Added shader nchw_to_image_int32 Copying QInt8 and QInt32 from vulkan to cpu Note: This functionality is currently disabled until issues on Android are resolved.  Added shader image_to_nchw_int32  QInt8 works with the same existing image_to_nchw_quantized shaders Added multiple tests for each supported dtype:  cpu_to_vulkan_and_dequantize: These tests check the correctness of copying quantized cpu tensor to vulkan by comparing the output of the following:    cpu float tensor > quantize > to vulkan > dequantize > to cpu    cpu float tensor > quantize > dequantize  cpu_to_vulkan_and_vulkan_to_cpu (currently disabled until copying vulkan quantized to cpu is enabled): These tests check the correctness of copying from cpu to vulkan and from vulkan to cpu by creating a random cpu float tensor, quantizing it, then copying it to vulkan, then back to cpu and comparing the output tensor to the original quantized tensor.  quantize_per_tensor_and_vulkan_to_cpu (currently disabled until copying vulkan quantized to cpu is enabled): These tests check the correctness of copying quantized tensor from vulkan to cpu by comparing the output of the following:    cpu float tensor > to vulkan > quantize > to cpu    cpu float tensor > quantize Test Plan: On Mac ``` cd ~/fbsource buck1 run c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\macosxarm64 ``` On Android ``` cd ~/fbsource buck1 build c ndk.custom_libcxx=false c pt.enable_qpl=0 c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAndroid\androidarm64 showoutput adb push buckout/gen/xplat/caffe2/pt_vulkan_quantized_api_test_binAndroid\androidarm64 /data/local/tmp/vulkan_quantized_api_test adb shell ""/data/local/tmp/vulkan_quantized_api_test"" ``` Reviewed By: kimishpatel Differential Revision: D41654287",2022-12-07T04:34:15Z,fb-exported Merged ciflow/trunk release notes: vulkan,closed,0,3,https://github.com/pytorch/pytorch/issues/90357,This pull request was **exported** from Phabricator. Differential Revision: D41654287, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[functorch] Refactor life handle storage,"Stack from ghstack:  CC([functorch] Refactor life handle storage) A ""life handle"" is a pointertoboolean that says whether or not a TensorWrapper is alive. A TensorWrapper is alive if we are currently inside of its corresponding transform. An Interpreter is alive if we are currently inside of its corresponding transform. I.e., for vmap(f)(x), the BatchedTensor(x, level=1) is alive inside of the execution of f; and the corresponding VmapInterpreter is alive inside of f. Previously, there was a global map of level to life handle. It is possible to get into a state where we have multiple levels that refer to different Interpreters (if the implementation of an operator calls into functorch) and that messes up the global map. This PR changes it so that  every Interpreter holds a life handle that says if it is alive  to construct a TensorWrapper, one must either (a) directly pass it a life handle, or (b) one must create the TensorWrapper when the corresponding Interpreter is on the stack (and we will automatically grab the life handle by indexing into the DynamicLayerStack with the level) (a) is more robust so I changed most of our C++ callsites to do that. (b) feels a bit hacky to me, but it seems fine for now:  It'll raise a nice error message if the interpreter isn't on the stack  all of our Python callsites already follow this convention (we construct TensorWrappers after pushing the Interpreter onto the stack). The alternative to (b) is that we always do (a), which we can do in the future if (b) runs us into any problems. Test Plan:  all functorch tests",2022-12-06T20:34:05Z,Merged Reverted ciflow/trunk topic: not user facing,closed,0,13,https://github.com/pytorch/pytorch/issues/90317,"> Should we just pass the interpreter instead? This might also make it easier to change the callsites for _make_tensor_wrapper. However, if we want to this all could also be done separately I agree that would be better. I'll file an issue for followup and merge this first to unblock the autograd.Function work. The reason why I didn't want to handle changing the callsites for `_make_tensor_wrapper` in this PR is that we would need to do some redesign of the functorch pythonside API that might rabbithole into changing the pythonside API to look like modes:  How does the developer grab the interpreter? They could grab from the top of the stack (like how pyfunctorch is doing it), or `_grad_increment_nesting` could return an interpreter instead of a level.  The ideal API for entering/exiting a level is not to use `_grad_increment_nesting/_grad_decrement_nesting`, it is to create a GradInterpreter object (mode context manager?) and then `__enter__` it. To create a GradTensorWrapper, we'd probably just call some method on the GradInterpreter (GradInterpreter.lift) instead of having a function `_wrap_for_grad(Tensor, Interpreter)`.","Sorry, I think I misunderstood your comment. To clarify, are you suggesting that in C++, we change the makeTensorWrapper API to accept an Interpreter instead of a (current_level, life_handle) pair because it's possible that the life handle is from the wrong level?","> To clarify, are you suggesting that in C++, we change the makeTensorWrapper API to accept an Interpreter instead of a (current_level, life_handle) pair because it's possible that the life handle is from the wrong level? Ahh yes so there were two parts baked in there. The main suggestion was at the C++ have `makeTensorWrapper` accept an interpreter so that (current_level, life_handle) can never be from different interpreters. At the Python level, I was thinking that this could mean `_grad_increment_nesting ` could return an interpreter instead of an int (until we get modes). This would mean exposing interpreters to the python API which could be done safely but definitely might be more of a pybind headache than it's worth given that we want to move to modes soon","> Ahh yes so there were two parts baked in there. The main suggestion was at the C++ have makeTensorWrapper accept an interpreter so that (current_level, life_handle) can never be from different interpreters. Cool, yeah, I agree this is good to avoid footguns. I've added that in the latest update. > At the Python level, I was thinking that this could mean _grad_increment_nesting could return an interpreter instead of an int (until we get modes). This would mean exposing interpreters to the python API which could be done safely but definitely might be more of a pybind headache than it's worth given that we want to move to modes soon Punting this to the future, I'll file an issue to track it"," merge f ""failures are unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","This seems to be causing all internal contbuilds to fail with the following signcompare issue: ``` [20221213T16:40:07.59008:00] Stderr: fbcode/caffe2/aten/src/ATen/functorch/DynamicLayer.cpp:163:32: error: comparison of integers of different signs: 'std::vector::size_type' (aka 'unsigned long') and 'int64_t' (aka 'long') [Werror,Wsigncompare] [CONTEXT] [20221213T16:40:07.59008:00]       dynamicLayerStack.size() >= level && level >= 1, [CONTEXT] [20221213T16:40:07.59008:00]       ~~~~~~~~~~~~~~~~~~~~~~~~ ^  ~~~~~ ``` More details in D42019543. "," revert m ""Causing contbuilds to fail when pytorch is built with Wsigncompare internally  details in D42019543"" c ghfirst", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,   can we consider turning on Wsigncompare in the OSS PyTorch build to avoid things like this in the future?,">    can we consider turning on Wsigncompare in the OSS PyTorch build to avoid things like this in the future? I would love to do this. There are *a lot* of violations currently though and we'd need a library that can handle sign mismatch arithmetic correctly. I believe there's something proposed for c++20 that we can get a backport of. Basically it will be no cost when it can be (e.g. uint32 compared with int64), but will have a predictable branch when doing things like int32 vs uint64.. https://en.cppreference.com/w/cpp/utility/intcmp In practice, we can turn this on incrementally throughout the codebase to make it a bit more tractable.",> There are a lot of violations currently though Is there a reason why the other violations aren't triggering the Wsigncompare ?
transformer,inductor(CPU): support vectorization of scalar tensor add when the scalar tensor dtype is not float,"  CC(inductor(CPU): support vectorization of scalar tensor add when the scalar tensor dtype is not float) For HF transformer model, there has an NumPy operator which introduces an add: float tensor + double scalar tensor: ``` class MockModule(torch.nn.Module):     def __init__(self):         super().__init__()         self.temperature = np.power(128, 0.5)     def forward(self, x):         return x + self.temperature ``` and inductor always convert the NumPy.number to a scalartensor which has a double dtype, and it  generates a nonvec code: ``` kernel_cpp_0 = async_compile.cpp(''' include ""/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h"" extern ""C"" void kernel(const float* __restrict__ in_ptr0,                        const double* __restrict__ in_ptr1,                        float* __restrict__ out_ptr0) {     pragma GCC ivdep     for(long i0=0; i0(tmp1);                 auto tmp3 = tmp0 + tmp2;                 out_ptr0[i0] = tmp3;             }         }     } } ''') async_compile.wait(globals()) del async_compile def call(args):     arg0_1, arg1_1 = args     args.clear()     buf0 = empty_strided((128, 3, 7, 7), (147, 1, 21, 3), device='cpu', dtype=torch.float32)     kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(arg1_1.data_ptr()), c_void_p(buf0.data_ptr()))     del arg0_1     del arg1_1     return (buf0, ) ``` After this PR: ``` kernel_cpp_0 = async_compile.cpp(''' include ""/tmp/torchinductor_xiaobing/tu/ctukqeudfyyazw7deqmkrt5gv5wz5mnyl5tzcgkkz4djjjjbiai7.h"" extern ""C"" void kernel(const float* __restrict__ in_ptr0,                        const double* __restrict__ in_ptr1,                        float* __restrict__ out_ptr0) {     pragma omp parallel num_threads(40)     {         pragma omp for         for(long i0=0; i0::loadu(in_ptr0 + 16*i0);             float g_tmp_buffer_in_ptr1[1] = {0};             to_float(in_ptr1, g_tmp_buffer_in_ptr1, 1);             auto tmp1 = at::vec::Vectorized(g_tmp_buffer_in_ptr1[0]);             auto tmp2 = (tmp1);             auto tmp3 = tmp0 + tmp2;             tmp3.store(out_ptr0 + 16*i0);         }         pragma omp for simd simdlen(8)         for(long i0=18816; i0(tmp1);             auto tmp3 = tmp0 + tmp2;             out_ptr0[i0] = tmp3;         }     } } ''') async_compile.wait(globals()) del async_compile def call(args):     arg0_1, arg1_1 = args     args.clear()     buf0 = empty_strided((128, 3, 7, 7), (147, 49, 7, 1), device='cpu', dtype=torch.float32)     kernel_cpp_0(c_void_p(arg0_1.data_ptr()), c_void_p(arg1_1.data_ptr()), c_void_p(buf0.data_ptr()))     del arg0_1     del arg1_1     return (buf0, ) if __name__ == ""__main__"":     from torch._dynamo.testing import rand_strided     from torch._inductor.utils import print_performance     arg0_1 = rand_strided((128, 3, 7, 7), (147, 49, 7, 1), device='cpu', dtype=torch.float32)     arg1_1 = rand_strided((), (), device='cpu', dtype=torch.float64)     print_performance(lambda: call([arg0_1, arg1_1])) ``` the current solution is that manually cast double to float, but I think a better method is that to fully support vectorization of dtype conversion, i.e. make **to_dtype** can be vectorized as aten side. ",2022-12-06T14:23:24Z,open source intel module: inductor ciflow/inductor release notes: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/90283," , we should add a pass to convert the double to float. I'm thinking of adding a dedicated optimization pass to apply potential optimizations one by one. Currently, the conversion from double to float is a candidate and vectorization is another candidate and the two optimizations should be independent. We need to decouple the different optimizations.","Otherwise, I'm kind of concerned it is not sustainable and hard to be maintained.","> Otherwise, I'm kind of concerned it is not sustainable and hard to be maintained. Yes, the vectorization of **to_dtype** is the next step.", Please request my review when the PR is ready.,"  please note: today is last day for cherrypicking, we will have to remove this PR from milestones"
yi,[Vulkan] Partially fix and then disable copying of vulkan quantized tensors to cpu,"Summary: Before this diff, copying of vulkan quantized tensors to cpu was broken. This was mainly caused because the shader only works properly with specific global and local work group sizes, and those specific sizes had been modified in earlier refactoring. As part of this fix, an optimized version of the shader that performs the copying was written, to take advantage of the special case when the plane size (x*y) is multiple of 4). After fixing this, and writing comprehensive tests, it was discovered that the copying still has issues on Android for specific input sizes, e.g. [1, 1, 11, 17]. These issues are currently unresolved, so, copying of quantized vulkan tensors to cpu has been disabled. What is contained in this diff?  Fix for existing issue  New optimized shader (image_to_nchw_quantized_mul4)  New comprehensive tests (which have been disabled)  Disable the copying of quantized vulkan tensors to cpu until issues on Android are fixed. Test Plan: On Mac ``` cd ~/fbsource buck1 run c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\macosxarm64 ``` On Android ``` cd ~/fbsource buck1 build c ndk.custom_libcxx=false c pt.enable_qpl=0 c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAndroid\androidarm64 showoutput adb push buckout/gen/xplat/caffe2/pt_vulkan_quantized_api_test_binAndroid\androidarm64 /data/local/tmp/vulkan_quantized_api_test adb shell ""/data/local/tmp/vulkan_quantized_api_test"" ``` Reviewed By: kimishpatel Differential Revision: D41047098",2022-12-06T11:42:22Z,fb-exported Merged ciflow/trunk release notes: vulkan,closed,0,5,https://github.com/pytorch/pytorch/issues/90275,This pull request was **exported** from Phabricator. Differential Revision: D41047098,This pull request was **exported** from Phabricator. Differential Revision: D41047098,This pull request was **exported** from Phabricator. Differential Revision: D41047098, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Add `TORCH_FAKE_TENSOR_DEBUG` use it to enable storage of traces on fake tensors at init time,  CC(Add `TORCH_FAKE_TENSOR_DEBUG` use it to enable storage of traces on fake tensors at init time) ,2022-12-05T20:38:08Z,Merged topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/90215," merge f ""unrelated XLA failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,Godel Model fails to compile (python 3.11)," 🐛 Describe the bug I was trying out PyTorch 1.14/2.0 and installed the latest nightly build. I then used the new `torch.compile()` method and integrated it into my code like described here:  Here is a small code snipit that reproduces the error with GODEL (the model I'm working with): ```python import torch from transformers import AutoTokenizer, AutoModelForSeq2SeqLM tokenizer = AutoTokenizer.from_pretrained(""microsoft/GODELv1_1largeseq2seq"") model = AutoModelForSeq2SeqLM.from_pretrained(""microsoft/GODELv1_1largeseq2seq"") new_model = torch.compile(model) ``` Fails with the error: ```bash Traceback (most recent call last):   File ""/var/home/riley/test.py"", line 7, in      new_model = torch.compile(model)                 ^^^^^^^^^^^^^^^^^^^^   File ""/var/home/riley/.local/lib/python3.11/sitepackages/torch/__init__.py"", line 1164, in compile     import torch._dynamo   File ""/var/home/riley/.local/lib/python3.11/sitepackages/torch/_dynamo/__init__.py"", line 1, in      from . import allowed_functions, convert_frame, eval_frame, resume_execution   File ""/var/home/riley/.local/lib/python3.11/sitepackages/torch/_dynamo/convert_frame.py"", line 16, in      from .bytecode_analysis import remove_dead_code, remove_pointless_jumps   File ""/var/home/riley/.local/lib/python3.11/sitepackages/torch/_dynamo/bytecode_analysis.py"", line 8, in      dis.opmap[""JUMP_ABSOLUTE""],     ~~~~~~~~~^^^^^^^^^^^^^^^^^ KeyError: 'JUMP_ABSOLUTE' ```  Versions ``` Collecting environment information... PyTorch version: 1.14.0.dev20221205+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Fedora Linux 37 (Container Image) (x86_64) GCC version: (GCC) 12.2.1 20221121 (Red Hat 12.2.14) Clang version: 15.0.4 (Fedora 15.0.41.fc37) CMake version: version 3.24.2 Libc version: glibc2.36 Python version: 3.11.0 (main, Oct 24 2022, 00:00:00) [GCC 12.2.1 20220819 (Red Hat 12.2.12)] (64bit runtime) Python platform: Linux6.0.11300.fc37.x86_64x86_64withglibc2.36 Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.0 [pip3] torch==1.14.0.dev20221205+cpu [conda] Could not collect ``` ",2022-12-05T19:22:18Z,triaged has workaround oncall: pt2,closed,0,6,https://github.com/pytorch/pytorch/issues/90203,"would you be able to run the minifier and post the results for us? https://pytorch.org/docs/master/dynamo/troubleshooting.html if so that helps triage, if not we can still try to repro based on the above. thanks in advance! (see section ""Minifier for any backend"" since this looks like a dynamo issue not a backend issue) set environment variable `TORCHDYNAMO_REPRO_AFTER=""dynamo""`","It could be related to python version. We are not even running the model, the error happens during the import. Is it possible to open a python prompt and run these commands? ~~~ >>> import dis >>> dis.opmap[""JUMP_ABSOLUTE""] 110 >>> dis.opmap {'POP_TOP': 1, 'ROT_TWO': 2, 'ROT_THREE': 3, 'DUP_TOP': 4, 'DUP_TOP_TWO': 5, 'ROT_FOUR': 6, 'NOP': 9, 'UNARY_POSITIVE': 10, 'UNARY_NEGATIVE': 11, 'UNARY_NOT': 12, 'UNARY_INVERT': 15, 'BINARY_MATRIX_MULTIPLY': 16, 'INPLACE_MATRIX_MULTIPLY': 17, 'BINARY_POWER': 19, 'BINARY_MULTIPLY': 20, 'BINARY_MODULO': 22, 'BINARY_ADD': 23, 'BINARY_SUBTRACT': 24, 'BINARY_SUBSCR': 25, 'BINARY_FLOOR_DIVIDE': 26, 'BINARY_TRUE_DIVIDE': 27, 'INPLACE_FLOOR_DIVIDE': 28, 'INPLACE_TRUE_DIVIDE': 29, 'RERAISE': 48, 'WITH_EXCEPT_START': 49, 'GET_AITER': 50, 'GET_ANEXT': 51, 'BEFORE_ASYNC_WITH': 52, 'END_ASYNC_FOR': 54, 'INPLACE_ADD': 55, 'INPLACE_SUBTRACT': 56, 'INPLACE_MULTIPLY': 57, 'INPLACE_MODULO': 59, 'STORE_SUBSCR': 60, 'DELETE_SUBSCR': 61, 'BINARY_LSHIFT': 62, 'BINARY_RSHIFT': 63, 'BINARY_AND': 64, 'BINARY_XOR': 65, 'BINARY_OR': 66, 'INPLACE_POWER': 67, 'GET_ITER': 68, 'GET_YIELD_FROM_ITER': 69, 'PRINT_EXPR': 70, 'LOAD_BUILD_CLASS': 71, 'YIELD_FROM': 72, 'GET_AWAITABLE': 73, 'LOAD_ASSERTION_ERROR': 74, 'INPLACE_LSHIFT': 75, 'INPLACE_RSHIFT': 76, 'INPLACE_AND': 77, 'INPLACE_XOR': 78, 'INPLACE_OR': 79, 'LIST_TO_TUPLE': 82, 'RETURN_VALUE': 83, 'IMPORT_STAR': 84, 'SETUP_ANNOTATIONS': 85, 'YIELD_VALUE': 86, 'POP_BLOCK': 87, 'POP_EXCEPT': 89, 'STORE_NAME': 90, 'DELETE_NAME': 91, 'UNPACK_SEQUENCE': 92, 'FOR_ITER': 93, 'UNPACK_EX': 94, 'STORE_ATTR': 95, 'DELETE_ATTR': 96, 'STORE_GLOBAL': 97, 'DELETE_GLOBAL': 98, 'LOAD_CONST': 100, 'LOAD_NAME': 101, 'BUILD_TUPLE': 102, 'BUILD_LIST': 103, 'BUILD_SET': 104, 'BUILD_MAP': 105, 'LOAD_ATTR': 106, 'COMPARE_OP': 107, 'IMPORT_NAME': 108, 'IMPORT_FROM': 109, 'JUMP_FORWARD': 110, 'JUMP_IF_FALSE_OR_POP': 111, 'JUMP_IF_TRUE_OR_POP': 112, 'JUMP_ABSOLUTE': 113, 'POP_JUMP_IF_FALSE': 114, 'POP_JUMP_IF_TRUE': 115, 'LOAD_GLOBAL': 116, 'IS_OP': 117, 'CONTAINS_OP': 118, 'JUMP_IF_NOT_EXC_MATCH': 121, 'SETUP_FINALLY': 122, 'LOAD_FAST': 124, 'STORE_FAST': 125, 'DELETE_FAST': 126, 'RAISE_VARARGS': 130, 'CALL_FUNCTION': 131, 'MAKE_FUNCTION': 132, 'BUILD_SLICE': 133, 'LOAD_CLOSURE': 135, 'LOAD_DEREF': 136, 'STORE_DEREF': 137, 'DELETE_DEREF': 138, 'CALL_FUNCTION_KW': 141, 'CALL_FUNCTION_EX': 142, 'SETUP_WITH': 143, 'LIST_APPEND': 145, 'SET_ADD': 146, 'MAP_ADD': 147, 'LOAD_CLASSDEREF': 148, 'EXTENDED_ARG': 144, 'SETUP_ASYNC_WITH': 154, 'FORMAT_VALUE': 155, 'BUILD_CONST_KEY_MAP': 156, 'BUILD_STRING': 157, 'LOAD_METHOD': 160, 'CALL_METHOD': 161, 'LIST_EXTEND': 162, 'SET_UPDATE': 163, 'DICT_MERGE': 164, 'DICT_UPDATE': 165} ~~~ Otherwise, I will try to build 3.11 python locally and try this out.","Yes, actually I think this is because many jumprelated bytecode ops have been replaced in the Python 3.11 version. This is the relevant snapshot !image Link  https://docs.python.org/3/whatsnew/3.11.htmlreplacedopcodes  This might be a good task to start diving deeper into TorchDynamo bytecode analysis if you are interested.","We don't currently support py 3.11, but we're working on it.  In the time being, please try a supported version if possible (3.83.10)","> We don't currently support py 3.11, but we're working on it. In the time being, please try a supported version if possible (3.83.10) Ok, thank you so much!","Closing this, as this is currently tracked at  CC([dynamo] Python 3.11 support)"
transformer,wav2vec2 model: error trying to do inference," 🐛 Describe the bug I tried to do torch.compile() on the second most downloaded model of facebook on huggingface: https://huggingface.co/facebook/wav2vec2large960hlv60self I took the example of usage they provide and just added the torch.compile() step. I also tried with an another custom audio sample earlier, and strangely, it ran, but very slowly compared to the original model... But it had the same prediction result  Error logs Here is the warnings and the traceback: ``` Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2large960hlv60self and are newly initialized: ['wav2vec2.masked_spec_embed'] You should probably TRAIN this model on a downstream task to be able to use it for predictions and inference. No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda' Found cached dataset librispeech_asr_dummy (/home/noetits/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc) It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug. /home/noetits/miniconda3/envs/.../lib/python3.10/sitepackages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   warnings.warn(message, UserWarning) /home/noetits/miniconda3/envs/.../lib/python3.10/sitepackages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   warnings.warn(message, UserWarning)  AssertionError                            Traceback (most recent call last) Cell In [1], line 17      14 input_values = processor(ds[0][""audio""][""array""], return_tensors=""pt"", padding=""longest"").input_values      16  retrieve logits > 17 logits = model(input_values).logits      19  take argmax and decode      20 predicted_ids = torch.argmax(logits, dim=1) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/torch/nn/modules/module.py:1480, in Module._call_impl(self, *args, **kwargs)    1475  If we don't have any hooks, we want to skip the rest of the logic in    1476  this function, and just call forward.    1477 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks    1478         or _global_backward_pre_hooks or _global_backward_hooks    1479         or _global_forward_hooks or _global_forward_pre_hooks): > 1480     return forward_call(*args, **kwargs)    1481  Do not call functions when jit is used    1482 full_backward_hooks, non_full_backward_hooks = [], [] File ~/miniconda3/envs/.../lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py:80, in OptimizedModule.forward(self, *args, **kwargs)      79 def forward(self, *args, **kwargs): > 80     return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py:209, in _TorchDynamoContext.__call__.._fn(*args, **kwargs)     207 dynamic_ctx.__enter__()     208 try: > 209     return fn(*args, **kwargs)     210 finally:     211     set_eval_frame(prior) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/transformers/models/wav2vec2/modeling_wav2vec2.py:1742, in Wav2Vec2ForCTC.forward(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)    1732 r""""""    1733 labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):    1734     Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to    (...)    1737     config.vocab_size  1]`.    1738 """"""    1740 return_dict = return_dict if return_dict is not None else self.config.use_return_dict > 1742 outputs = self.wav2vec2(    1743     input_values,    1744     attention_mask=attention_mask,    1745     output_attentions=output_attentions,    1746     output_hidden_states=output_hidden_states,    1747     return_dict=return_dict,    1748 )    1750 hidden_states = outputs[0]    1751 hidden_states = self.dropout(hidden_states) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/torch/nn/modules/module.py:1480, in Module._call_impl(self, *args, **kwargs)    1475  If we don't have any hooks, we want to skip the rest of the logic in    1476  this function, and just call forward.    1477 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks    1478         or _global_backward_pre_hooks or _global_backward_hooks    1479         or _global_forward_hooks or _global_forward_pre_hooks): > 1480     return forward_call(*args, **kwargs)    1481  Do not call functions when jit is used    1482 full_backward_hooks, non_full_backward_hooks = [], [] File ~/miniconda3/envs/.../lib/python3.10/sitepackages/transformers/models/wav2vec2/modeling_wav2vec2.py:1308, in Wav2Vec2Model.forward(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)    1304         hidden_states[mask_feature_indices] = 0    1306     return hidden_states > 1308 (WAV_2_VEC_2_INPUTS_DOCSTRING)    1309 (    1310     processor_class=_PROCESSOR_FOR_DOC,    1311     checkpoint=_CHECKPOINT_FOR_DOC,    1312     output_type=Wav2Vec2BaseModelOutput,    1313     config_class=_CONFIG_FOR_DOC,    1314     modality=""audio"",    1315     expected_output=_EXPECTED_OUTPUT_SHAPE,    1316 )    1317 def forward(    1318     self,    1319     input_values,    1320     attention_mask=None,    1321     mask_time_indices=None,    1322     output_attentions=None,    1323     output_hidden_states=None,    1324     return_dict=None,    1325 ):    1326     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions    1327     output_hidden_states = (    1328         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states    1329     ) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/torch/_dynamo/eval_frame.py:209, in _TorchDynamoContext.__call__.._fn(*args, **kwargs)     207 dynamic_ctx.__enter__()     208 try: > 209     return fn(*args, **kwargs)     210 finally:     211     set_eval_frame(prior) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/functorch/_src/aot_autograd.py:2107, in aot_module_simplified..forward(*runtime_args)    2105 full_args.extend(params_flat)    2106 full_args.extend(runtime_args) > 2107 return compiled_fn(full_args) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/functorch/_src/aot_autograd.py:811, in make_boxed_func..g(args)     810 def g(args): > 811     return f(*args) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/functorch/_src/aot_autograd.py:1687, in aot_dispatch_autograd..debug_compiled_function(*args)    1681     elif not can_require_grad:    1682         assert not a.requires_grad, format_guard_bug_msg(    1683             aot_config,    1684             f""{describe_input(i, aot_config)} would not require grad""    1685         ) > 1687 return compiled_function(*args) File ~/miniconda3/envs/.../lib/python3.10/sitepackages/functorch/_src/aot_autograd.py:1551, in aot_dispatch_autograd..compiled_function(*args)    1548 else:    1549     args_with_synthetic_bases = args > 1551 all_outs = CompiledFunction.apply(*args_with_synthetic_bases)    1552 if CompiledFunction.num_aliasing_metadata_outs > 0:    1553     outs = all_outs[:CompiledFunction.num_aliasing_metadata_outs] File ~/miniconda3/envs/.../lib/python3.10/sitepackages/functorch/_src/aot_autograd.py:1455, in aot_dispatch_autograd..CompiledFunction.forward(ctx, *deduped_flat_tensor_args)    1447     1448 def forward(ctx, *deduped_flat_tensor_args):    1449     (...)    1453       Note that in the synthetic bases case, mutated_inputs will correspond to an updated version    1454        of the original view, and not the synthetic base > 1455     fw_outs = call_func_with_args(    1456         CompiledFunction.compiled_fw, deduped_flat_tensor_args, disable_amp=disable_amp    1457     )    1459     num_non_aliased_outs = CompiledFunction.num_non_aliased_outs    1460     num_aliasing_metadata_outs = CompiledFunction.num_aliasing_metadata_outs File ~/miniconda3/envs/.../lib/python3.10/sitepackages/functorch/_src/aot_autograd.py:836, in call_func_with_args(f, args, steal_args, disable_amp)     834 try:     835     if hasattr(f, ""_boxed_call""): > 836         out = normalize_as_list(f(args))     837     else:     838          TODO: Please remove soon     839          https://github.com/pytorch/pytorch/pull/83137issuecomment1211320670     840         warnings.warn(     841             ""Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. ""     842             ""Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. ""     843             ""See https://github.com/pytorch/pytorch/pull/83137issuecomment1211320670 for rationale.""     844         ) File /tmp/torchinductor_noetits/dh/cdhukqo6kp2i5wvu22fwo63insaj3b7qi4fq4a6rr52eycvrif32.py:1270, in call(args)    1268 del primals_33    1269 buf1 = aten.convolution(buf0, primals_1, primals_2, (5,), (0,), (1,), False, (0,), 1) > 1270 assert_size_stride(buf1, (1, 512, 14879), (7618048, 14879, 1))    1271 del primals_2    1272 buf2 = empty_strided((1, 14879, 1), (14879, 1, 14879), device='cpu', dtype=torch.float32) AssertionError: expected size 512==512, stride 1==14879 at dim=1 ```  Minified repro I didn't try the minifier, but with the huggingface example, if you add just the `torch.compile()`, I assume you can reproduce the problem Versions: Working on a WSL UBUNTU 22.04 LTS torch                   1.14.0.dev20221204+cpu torchaudio              0.14.0.dev20221204+cpu torchvision             0.15.0.dev20221204+cpu ```  from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC  from datasets import load_dataset  import torch   load model and processor  processor = Wav2Vec2Processor.from_pretrained(""facebook/wav2vec2large960hlv60self"")  model = Wav2Vec2ForCTC.from_pretrained(""facebook/wav2vec2large960hlv60self"")  model=torch.compile(model)   load dummy dataset and read soundfiles  ds = load_dataset(""patrickvonplaten/librispeech_asr_dummy"", ""clean"", split=""validation"")   tokenize  input_values = processor(ds[0][""audio""][""array""], return_tensors=""pt"", padding=""longest"").input_values   retrieve logits  logits = model(input_values).logits   take argmax and decode  predicted_ids = torch.argmax(logits, dim=1)  transcription = processor.batch_decode(predicted_ids) ```",2022-12-05T14:18:16Z,triaged bug,open,0,4,https://github.com/pytorch/pytorch/issues/93464,"working on setting up a repro.  ~(Btw if you can run the minifier, it would help us.)~ I wasn't able to get minifier to work for this model, digging deeper",we've gotten a minimum repro and are in the process of fixing it see updates here:  CC(`unsqueeze` and `expand` metas produce wrong strides),"actually is this issue fixed? if I trust myself from the past, CC(`unsqueeze` and `expand` metas produce wrong strides) was supposed to be a repro of this and has since been closed.   ","I just tried to paste my original example in colab, and it runs. However, inference seem to be way slower with compiled version than with original version, trying with a couple example from the dummy_asr dataset"
yi,Trying pytorch 2.0 snippet from the website - sm89 error," 🐛 Describe the bug trying the snippet from the website ``` import torch import torchvision.models as models model = models.resnet18().cuda() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) compiled_model = torch.compile(model) x = torch.randn(16, 3, 224, 224).cuda() optimizer.zero_grad() out = compiled_model(x) out.sum().backward() optimizer.step() ``` I got Value 'sm_89' is not defined for option 'gpuname' I am indeed using a rtx4090 and cuda 11.6 BUT works fine without torch.compile  Versions Collecting environment information... PyTorch version: 1.14.0.dev20221205+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.25.0 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.056genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Nvidia driver version: 525.60.11 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.6.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.6.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] pytorchlightning==1.7.7 [pip3] torch==1.14.0.dev20221205+cu116 [pip3] torchtbprofiler==0.4.0 [pip3] torchaudio==0.14.0.dev20221204+cu116 [pip3] torchtext==0.14.0 [pip3] torchtriton==2.0.0+0d7e753227 [pip3] torchvision==0.15.0.dev20221204+cpu [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] faissgpu                 1.7.2           py3.8_h28a55e0_0_cuda11.3    pytorch [conda] libfaiss                  1.7.2           hfc2d529_0_cuda11.3    pytorch [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0            py38h7f8727e_0   [conda] mkl_fft                   1.3.1            py38hd3c417c_0   [conda] mkl_random                1.2.2            py38h51133e4_0   [conda] numpy                     1.23.4           py38h14f4228_0   [conda] numpybase                1.23.4           py38h31eccc5_0   [conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torch                     1.14.0.dev20221204+cu117          pypi_0    pypi [conda] torchaudio                0.14.0.dev20221204+cu117          pypi_0    pypi [conda] torchtriton               2.0.0+0d7e753227          pypi_0    pypi [conda] torchvision               0.15.0.dev20221204+cpu          pypi_0    pypi ",2022-12-05T11:41:23Z,high priority triaged module: third_party module: inductor,closed,0,16,https://github.com/pytorch/pytorch/issues/90170,"By default `compile` uses `inductor` as backend. Currently, inductor supports Volta and Ampere devices only. Your RTX 4090 is based on Ada Lovelace architecture, which is not supported by the torch inductor.  There are other backends available in `torch.compile`.  For example: ``` compiled_model = torch.compile(model, fullgraph=True, backend='nvprims_aten') ``` worked on my 3080. You may get a list of available backends like so: ``` import torch._dynamo as dynamo print(dynamo.list_backends()) ```","The issue likely in Triton, that needs to limit the compilation to latest architecture supported by toolkit rather than GPU sm arch. Updating cudatoolkit might solve the problem as well (as 11.6 to the best of my knowledge does not support Lovelace) ","The error comes from triton, your cuda version (and thus ptxas version) is too old to support 4090, if you install cuda 11.8 on your system and set `TRITON_PTXAS_PATH` to point to it that should work. ",But if I install cuda 11.8 will I be able to install pytorch from wheels ? I think it offers only 11.6 and 11.7 at the moment.,Using backend='nvprims_aten' worked.,"Yeah if you install cuda 11.8 you can still use pytorch compiled with any other cuda version, you just need 11.8 ptxas, not even necessarily in your PATH, just discoverable by `TRITON_PTXAS_PATH`","no luck.  Installed cuda 11.8 tried to set `TRITON_PTXAS_PATH` to `/usr/local/cuda11.8` or `/usr/local/cuda11.8/bin` (where ptxas is) and I am now getting this message: ``` Traceback (most recent call last):   File ""/usr/lib/python3.8/concurrent/futures/process.py"", line 239, in _process_worker     r = call_item.fn(*call_item.args, **call_item.kwargs)   File ""/home/vincent/.local/lib/python3.8/sitepackages/torch/_inductor/codecache.py"", line 488, in _worker_compile     kernel.precompile(warm_cache_only_with_cc=cc)   File ""/home/vincent/.local/lib/python3.8/sitepackages/torch/_inductor/triton_ops/autotune.py"", line 59, in precompile     self.launchers = [   File ""/home/vincent/.local/lib/python3.8/sitepackages/torch/_inductor/triton_ops/autotune.py"", line 60, in      self._precompile_config(c, warm_cache_only_with_cc)   File ""/home/vincent/.local/lib/python3.8/sitepackages/torch/_inductor/triton_ops/autotune.py"", line 74, in _precompile_config     triton.compile(   File ""/home/vincent/.local/lib/python3.8/sitepackages/triton/compiler.py"", line 1256, in compile     asm, shared, kernel_name = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages,   File ""/home/vincent/.local/lib/python3.8/sitepackages/triton/compiler.py"", line 901, in _compile     name, asm, shared_mem = _triton.code_gen.compile_ttir(backend, module, device, num_warps, num_stages, extern_libs, cc) RuntimeError: Internal Triton PTX codegen error:  ptxas /tmp/fileZVMk93, line 6; error   : PTX .version 7.4 does not support .target sm_89 ptxas fatal   : Ptx assembly aborted due to errors ``` plus subsequent more related errors but always due to .target sm_89","Ok, sorry about that, the fix should be on the triton side, we'll work with them to enable it. ",But in essence why don't you guys release a wheel for cuda 11.8 ? I noticed there are always a shift of one minor release vs Cuda last one.,We are working on it. ,"I took a stab at it with https://github.com/openai/triton/pull/1038. I've got a Docker image I'm building using pytorch with triton's new MLIR backend on my 4090 and it seems to be working reasonably well. If anyone is blocked by the lack of support for newer architectures, let me know and I can post it somewhere.",Hi ! Thank you for the work on newer architectures! Could you please post it for us to try the new features🤩?,> Hi ! Thank you for the work on newer architectures! > Could you please post it for us to try the new features🤩? Here you go: https://gist.github.com/ConnorBaker/10988159828943187d75737566c7e342 I'm working on a version which uses some of the optimizations Polly provides but I don't know if they'll make an impact/I'll be able to get it working.,closing since https://github.com/openai/triton/pull/1039 is merged.," the code example in your original post won't work unless the user installs Triton from master or cherrypicks that commit into their own build, or PyTorch updates the version of Triton they're using to do the same. The version of Triton PyTorch uses is pinned here: https://github.com/pytorch/pytorch/blob/master/.github/ci_commit_pins/triton.txt I understand that they're holding off on updating to newer Triton releases until the Triton team has ironed out more of the performance regressions caused by the MLIR rewrite.","Right, good to clarify. I had just quick hacked the llvm.. I am stuck by dynamic shapes anyway, so have to wait longer ... :)"
yi,Simplify by using yield from,,2022-12-05T07:15:25Z,better-engineering Merged topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/90160,The XLA failure is preexisting.," merge f ""Preexisting XLA failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[Feature Proposal] Extend torch hub to better support cloud serving and edge deployment," 🚀 The feature, motivation and pitch **TL;DR**: Extend torch hub to better support cloud serving and edge deployment by: 1. Extend the `hubconf.py` as an entrypoint for serving and deployment, like HuggingFace Inferece Handler 2. (Optional) Allow thirdparty infra providers to integrate model deployment and performance reporting to the hub website.  Background I was a PhD researcher a while ago and am now an AI practitioner in the industry. From my personal experience and observation, the pytorch hub provides incremental value to researchers, because: 1. Researchers usually rely on conferences and their own networks to find the latest models. 2. Researchers are familiar with cloning a repo, peeking into the source and doing modifications. The simple `torch.hub.load` interface is not very much helpful to them. However, industry community usually don't have the advantages mentioned above, therefore the hub can be a valuable information source to them. But when they choose a model, they not only want an interface to run some sample data, deployment easiness and model performance are also important factors to consider.  That's why huggingface transformers, yoloseries, and mmdetection are very popular.  But not all researchers like to submit their models to those repos due to the extensive customizations needed, therefore, a lightweight hubconf.py could just reduce the gap between research and production need.  Proposed changes `hubconf.py`: provides a reference format to share models, and acts as an interface for deployment, benchmarking and serving. A very rough example is shown below, just for discussion purpose. ``` __model__ = [""MyModel""] def MyModel:   def __init__(self, pretrained=False, **kwargs):      self.model = build_model(pretrained=pretrained)   def forward(self, x: Union[Tuple[Any, ...], torch.Tensor]) > Union[Tuple[Any, ...], torch.Tensor]:        Core tensorin, tensorout model function     return self.model(x)   def get_input(self):       get sample data     return preprocess(cv2.imread('assets/img1.png'))   def inference(self, data: Dict[str, Any]) > Dict[str, Any]:      optional, for model serving only     output = self.model(preprocess(data['input']))     return {""predictions"": postprocess(output)} ``` Hub website: ""Discover and publish models to a pretrained model repository designed for research exploration."" > ""For research exploration and production use"". And in the future, allow thirdparty providers to plug in model statistics based on `hubconf.py` **Disclaimer** Our company is currently working on an MLOps platform to facilitate the production of PyTorch models. We see a clear benefit that a shared protocol could further reduce the friction between research and industry communities. We would like to hear feedback from PyTorch team and we are also open to take the initial effort to extend the hubconf and adopt opensource repos to fit this extended format.  Alternatives _No response_  Additional context _No response_ ",2022-12-04T23:28:48Z,feature triaged module: hub,open,0,1,https://github.com/pytorch/pytorch/issues/90147,"Hi , sorry for the delay in responding to you. Thanks for posting this. I'm trying to understand precisely what you're proposing, feel free to correct me if I'm making wrong assumptions.  Are you suggesting that every model registered in torch.hub should be made compliant with this structure, or would it be optional? In the former case it would be a bit difficult to bring in, as it'd require us to go to multiple contributors and ask them to change their code accordingly. I would not bet on many of them doing it. Also, we provide a hub for models provided by third parties without forcing them to give a pretrained version (although many do provide this). In this context it's not obvious what a benchmark execution would mean (since the model is not tied to a specific dataset or task). Same for `self.get_input`: what would this mean without a precise context? Let me know if I got anything wrong, I'm not sure I clearly understood the kind of changes you were suggesting."
rag,Fix exception cause in storage.py,This change causes the correct message to be shown between the two tracebacks when an error is shown. More context here: https://blog.ram.rachum.com/post/621791438475296768/improvingpythonexceptionchainingwith,2022-12-03T18:23:08Z,open source Merged,closed,0,4,https://github.com/pytorch/pytorch/issues/90118,The committers listed above are authorized under a signed CLA.:white_check_mark: login: coolRR / name: Ram Rachum  (516c00fb9b649e8247ff195d816f1a925ad121a6),The CI failure doesn't have anything to do with my change. ," merge f ""Irrelevant XLA failure"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[reland][quant] Explictly set default quantized engine instead of relying on the order of supported_qengines (#89804),  CC([reland][quant] Explictly set default quantized engine instead of relying on the order of supported_qengines (89804)) Summary: Fixes:  CC(Use better default QEngine based on the environment) Test Plan: ossci + sandcastle Reviewers: Subscribers: Tasks: Tags:,2022-12-02T00:31:05Z,Merged ciflow/trunk release notes: quantization topic: improvements topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/90036, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,For test_transformer_FuseConvBNNoConvBias set deadline to None,Summary: I am on the current oncall  for aml_ai_platform and this is a fix for one of the failing tests. Test Plan: The function fails the deadline on the first run but passes on subsequent. Implemented the suggested solution of setting deadline=None. Differential Revision: D41669143,2022-12-01T23:52:15Z,caffe2 fb-exported topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/90032,This pull request was **exported** from Phabricator. Differential Revision: D41669143,This pull request was **exported** from Phabricator. Differential Revision: D41669143,no need
yi,libtorch_python.so: undefined symbol: PyInstanceMethod_Type," 🐛 Describe the bug I run into this error vvv ``` C++ exception with description ""/var/lib/jenkins/multipy/multipy/runtime/loader.cpp:780: libnvfuser_python.so: could not load library, dlopen says: /opt/conda/lib/python3.10/sitepackages/torch/lib/libtorch_python.so: undefined symbol: PyInstanceMethod_Type"" thrown in the test body. ``` link of failing CI: https://github.com/pytorch/pytorch/actions/runs/3586511045/jobs/6036274959 cmake lines related to the object: https://github.com/pytorch/pytorch/blob/d8335704a54f89e93f562cda7b49048e1eab1ebb/torch/csrc/jit/codegen/cuda/CMakeLists.txtL165L200 It's a little strange to me how the error was thrown from libtorch_python.so. This feels like a pytorch build issue. I'm also a little bit uncertain how this is invoked. In the setup, I have `libnvfuser_python.so` which depends on both `libnvfuser_codegen.so` and `libtorch_python.so`. While `libnvfuser_codegen.so` is dlopened by torch and does not have dependency on `libnvfuser_python.so`.  Versions This is failing on CI job: linuxbioniccuda11.6py3.10gcc7 / test (deploy, 1, 1, linux.4xlarge.nvidia.gpu) ",2022-12-01T18:22:11Z,module: build triaged topic: build,closed,0,6,https://github.com/pytorch/pytorch/issues/90016,"One thing I did notice is that for linuxbioniccuda11.6py3.10gcc7  CI build uses `Found PythonLibs: /opt/conda/lib/libpython3.10.a (found version ""3.10.4"")` and tests runs uses 3.10.8 ``` 20221130T20:20:41.8143520Z     libpythonstatic3.10.8           haa1d7c7_0        26.8 MB ``` But minor version should be compatible...",Bumping threads~ since the `libtorch_python.so` issue is the only failing test on my PR. :cry: ,"bumping threads again: Just realized that the failing config is `deploy` and the test somehow is dlopen `libnvfuser_python.so`. which has python dependency and likely that's what's missing there?!?! I'm not sure where those tests are defined, and why are they explicitly loading `libnvfuser_python.so`? ``` [  FAILED  ] 20 tests, listed below: [  FAILED  ] TorchpyTest.InitManagerBasic [  FAILED  ] TorchpyTest.InitTwice [  FAILED  ] TorchpyTest.DifferentInterps [  FAILED  ] TorchpyTest.SimpleModel [  FAILED  ] TorchpyTest.ResNet [  FAILED  ] TorchpyTest.Movable [  FAILED  ] TorchpyTest.MultiSerialSimpleModel [  FAILED  ] TorchpyTest.ThreadedSimpleModel [  FAILED  ] TorchpyTest.ErrorsReplicatingObj [  FAILED  ] TorchpyTest.ThrowsSafely [  FAILED  ] TorchpyTest.AcquireMultipleSessionsInTheSamePackage [  FAILED  ] TorchpyTest.AcquireMultipleSessionsInDifferentPackages [  FAILED  ] TorchpyTest.TensorSharingNotAllowed [  FAILED  ] TorchpyTest.TaggingRace [  FAILED  ] TorchpyTest.DisarmHook [  FAILED  ] TorchpyTest.RegisterModule [  FAILED  ] TorchpyTest.TensorSerializationSharing [  FAILED  ] TorchpyTest.UsesDistributed [  FAILED  ] TorchpyTest.Autograd [  FAILED  ] TorchpyTest.PrintInstruction ```",from https://github.com/pytorch/pytorch/blob/797544f1c456035db1d3fbbd9141d592a834c60a/.jenkins/pytorch/common_utils.shL147L169 the multipy loader.cpp seems to be https://github.com/pytorch/multipy/blob/60d9ef2df2950eb7cbaed671eb1fe4eb1e02525b/multipy/runtime/loader.cpp which meticulously handles `libtorch_python.so` https://github.com/pytorch/multipy/blob/60d9ef2df2950eb7cbaed671eb1fe4eb1e02525b/multipy/runtime/loader.cppL720L723,Thx for that. I would need to add an entry to skip libnvfuser_python.so as well on deploy.,"I think the issue here is that we are loading libnvfuser_python.so because it's a python extension under `torch._C_nvfuser`. I think there's something I did wrong, that I'm not handling the load properly. I don't understand how this is supposed to work in the deploy tests... I guess I should magically strip libtorch_python.so dependency for the test to pass, but that doesn't work for nvfuser python interface to work independently. Anyway, I decided to work towards a different direction and take nvfuser out of torch package. This worked in my PR :tada:  I'm closing this one as it is a user error."
rag,ShapeEnv.create_symbolic_sizes_strides_storage_offset,"  CC(Keep track of source name on all allocated SymInts)  CC(Rewrite dynamo cond() handling to not recursively call export)  CC(Type torch._dynamo.side_effects)  CC(Convert InstructionTranslatorGraphState and OutputGraphState to NamedTuple)  CC(Type torch._dynamo.symbolic_convert)  CC(Add missing mypynofollow.ini)  CC(Ensure that we fakeify tensor subclasses when they are initially tracked)  CC(ShapeEnv.create_symbolic_sizes_strides_storage_offset) Instead of having storage offset hang out on its own, allocate all of these symbols all in one go. Signedoffby: Edward Z. Yang ",2022-11-30T21:28:07Z,Merged ciflow/trunk release notes: fx ciflow/inductor,closed,0,6,https://github.com/pytorch/pytorch/issues/89962,"> I guess this is towards being able to specialize differently storage offset from the other symints? Yeah, it makes it easier to associate storage offset with the tensor in question", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 3 additional jobs have failed, first few of them are: linuxbinarylibtorchcxx11abi ,linuxbinarylibtorchprecxx11 ,trunk Details for Dev Infra team Raised by workflow job "," merge f ""merge bot is WRONG"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,"When dealing with dupe arguments, prefer leafifying if possible","  CC(When dealing with dupe arguments, prefer leafifying if possible) See code comment for details. I also had to do some extra fixes: * `run_functionalized_fw_and_collect_metadata` now is able to handle duplicated arguments * `aot_wrapper_dedupe` now always returns boxed compiled functions * `aot_wrapper_dedupe` is now applied to inference compiler along with autograd compiler (preexisting) Fixes https://github.com/pytorch/torchdynamo/issues/1939 Fixes DebertaV2ForQuestionAnswering DebertaForMaskedLM DebertaForQuestionAnswering DebertaV2ForMaskedLM Repro command: ``` python benchmarks/dynamo/huggingface.py performance float32 dcuda training inductor noskip dashboard only DebertaForQuestionAnswering cold_start_latency ``` Signedoffby: Edward Z. Yang  ",2022-11-30T05:39:04Z,Merged ciflow/trunk topic: bug fixes module: functorch module: dynamo ciflow/inductor release notes: dynamo,closed,0,5,https://github.com/pytorch/pytorch/issues/89896, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""lint failure only with fix"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
transformer,[RFC] PyTorch Tensor Parallel(TP) User API for Distributed Training," 🚀 The feature, motivation and pitch  🚀 Feature Provide a detailed API design for highlevel PyTorch Tensor Parallelism API design. This is an evolvement of PyTorch Sharding introduced in  CC([RFC] PyTorch Sharder for distributed training) and is directly built on top of DTensor proposed in  CC([RFC] PyTorch DistributedTensor). We want users to only focus on how their modules to be distributed and hide all other details. (Caveat, for now, we only support linear/transformer based models).   Motivation To scale the large model training, especially transformer based model training, multiple parallelism paradigms are proposed and considered. Among them, model parallelism like MegatronLM is getting popular together with 3D parallelism. We have already proposed a standardized sharding api in the past ( CC([RFC] PyTorch Sharder for distributed training)). Now to enable more generic data distributions more than sharding across hosts, we have proposed a new design of Distributed Tensor (DTensor) in  CC([RFC] PyTorch DistributedTensor) and we want to not only provide similar functionality of model parallelism as Megatron on top of DTensor, but also provide better usability so that users don't need to change their model to use tensor parallelism directly.  Pitch We are proposing APIs which cover three different use cases during module annotation. These APIs not only include the TPonly case, it also covers 2D parallel and 3D parallel down the road.  One base Parallel Style Class and three children inhouse parallel style. This is extendible so that users can create their own parallel styles if the inhouse ones do not meet their requirements. ```python class ParallelStyle(ABC):     """"""     The parallel style user wants the module or submodule to be parallelized.  We can add more in future, but this seems sufficient for immediate needs.  Users can extend this class to build their own parallel style with customized input/output preparations.     """"""     _prepare_input: Callable[[Union[Tensor, DTensor], Optional[DeviceMesh], Optional[Int]], DTensor]     _prepare_output: Callable[[DTensor, Optional[DeviceMesh], Optional[Int]], Union[Tensor, DTensor]]      class RowwiseParallel(ParallelStyle):     """"""     Partitioning the row of a module.  We assume the input to be a sharded DTensor and output to be a replicated DTensor.     """"""         def __init__(self):                  super().__init__(MakeInputShard, MakeOutputReplicated) Class ColwiseParallel(ParallelStyle):     """"""    Partitioning the column of a tensor or module.  We assume the input to be a Replicated DTensor and output to be a Replicated DTensor.  """"""          def __init__(self):          super().__init__(MakeInputReplicated, MakeOutputReplicated) class PairwiseParallel(ParallelStyle):     """"""     We concatenate colwise and rowwise styles as a fixed pair like what MegatronLM(https://arxiv.org/abs/1909.08053) is doing.  We assume both input and output to a Replicated DTensor.  We now only support Multihead Attention, MLP and transformer for this style.     We also need to assume the input is a nn.Multihead Attention, nn.Transformer or evennumber layers of nn.Linear for now.    """"""     def __init__(self):         super().__init__(MakeInputReplicated(), MakeOutputReplicated()) ```  One API for module level parallel and the user needs to specify what parallel style they want to apply to the whole module or users can specify parallel style per the module path. For PairwiseParallelStyle, we only support it for MHA, MLP and transformer models for now. ```python def parallelize_module(     module: nn.Module,     device_mesh: DeviceMesh,     parallelize_plan: Union[ParallelStyle, Dict[str, ParallelStyle]],     tp_mesh_dim: int=0, ) > None:     '''     This function converts all module parameters to distributed tensor parameters according to the `parallelize_plan` specified.     Users can always use FSDP or DDP as a fallback if the model does not fall into the type we support here.     Args:         module (nn.Module): user module to be partitioned.         parallel_plan (ParallelPlan): the parallel plan which the user wants.         device_mesh (DeviceMesh): the device mesh to place the module.         tp_mesh_dim (int): the dimension of TP in the device mesh.     '''  Code example is shown as following import torch import torch.distributed.tensor_parallel as tp from torch.distributed.fsdp import FullyShardedDataParallel as FSDP from torch.distributed import DeviceMesh  initialize a new device mesh for TP for the given tp world size device_mesh = DeviceMesh(""cuda"", torch.arange(world_size))  colwise parallel of a Linear module layer_one = torch.nn.Linear(8,16) tp.parallelize_module(layer_one, tp.ColwiseParallel(), device_mesh)  rowwise parallel of a Linear module layer_two = torch.nn.Linear(16,8) tp.parallelize_module(layer_two, tp.RowwiseParallel(), device_mesh)  MegatronLM style pairwise parallel for a transformer model  Users do not need to specify col/row wise parallel for each module or parameter.  transformer_model = torch.nn.Transformer(nhead=16, num_encoder_layers=12) pairwise_style = tp.PairwiseParallelStyle() tp.parallelize_module(transformer_model, pairwise_style, device_mesh)  Customized module class DemoModel(torch.nn.Module):     def __init__(self):         super(SimpleModel, self).__init__()         self.attn = AttentionModule(...)  Defined by user.         self.layer_norm = LayerNorm(...)         self.mlp = CustomizedMLP(...)  Defined by user.     def forward(self, x):         return self.mlp(self.layer_norm(self.attn(x))) customized_model = DemoModel(...) tp.parallelize_module(customized_model, {“attn”: pairwise_style, “mlp”: pairwise_style}, device_mesh) ```  For 2D parallel, the code is similar. To recap how we do 2D parallelism with FSDP. We will first parallelize modules within 8 GPUs on each host and then wrap the module with FSDP. Basically TP first shards the weight of a module and then FSDP shards the local tensor of TPsharded weights. And another common practice of 2D parallel is to perform it on each layer of a transformer encoder or decoder rather than directly applying it to the whole model directly. ```python  Below is another example showing 2D parallel with FSDP.  initialize a new device mesh for 2D parallel for the given world size device_mesh_2D = DeviceMesh(""cuda"", torch.arange(world_size).reshape(dp_size, tp_size))  Pairwise parallelize a transformer model transformer_model = torch.nn.Transformer(nhead=16, num_encoder_layers=12) parallelize_module(transformer_model, tp_style, device_mesh_2D, tp_mesh_dim=1)  Wrap the transformer with FSDP dp_pg = device_mesh_2D.get_dim_groups()[0] transformer_model = FSDP(transformer_model, pg=dp_pg) ```  Lowlevel API for TP: We also want to build some lowlevel APIs to provide more flexibility and usability for users as we continue to build more highlevel TP features. ```python def _parallelize_mlp(     module: nn.Module,     device_mesh: DeviceMesh,     parallel_style: ParallelStyle=PairwiseParallelStyle(),     tp_mesh_dim: int=0, ) > None:     '''     This function assumes the input module is a sequence of nn.Linear and we parallelize the module based on the given parallel style.     Args:         module (nn.Module): user module to be partitioned.         device_mesh (DeviceMesh): the device mesh to place the module.         parallel_style (ParallelStyle): Parallel style with input/output preparation.         tp_mesh_dim (int): the dimension of TP in the device mesh.     ''' def _parallelize_multihead_attn(     module: nn.Module,     device_mesh: DeviceMesh,     parallel_style: ParallelStyle=PairwiseParallelStyle(),     tp_mesh_dim: int=0, ) > None:     '''     This function assumes the input module is a class of nn.MultiheadAttention or a customized multihead attention. We will replace it with our own version of the multihead attention module.     We directly assume the input module will be a nn.MultiheadAttention or module which has a similar structure.     Args:         module (nn.Module): user module to be partitioned.         device_mesh (DeviceMesh): the device mesh to place the module.         parallel_style (ParallelStyle): Parallel style with input/output preparation.         tp_mesh_dim (int): the dimension of TP in the device mesh.     ''' def _parallelize_linear(     module: nn.Module,     device_mesh: DeviceMesh,     parallel_style: ParallelStyle=ColwiseParallel(),     tp_mesh_dim: int=0, ) > None:     '''     This function assumes the input module is a class of nn.Linear.     We directly assume the input module will be a nn.Linear.     Args:         module (nn.Module): user module to be partitioned.         device_mesh (DeviceMesh): the device mesh to place the module.         parallel_style (ParallelStyle): Parallel style with input/output preparation.         tp_mesh_dim (int): the dimension of TP in the device mesh.     '''",2022-11-30T00:29:54Z,triaged module: dtensor,open,10,5,https://github.com/pytorch/pytorch/issues/89884,"  It seems that the output(MakeOutputReplicated) in the code is not aligned with the comment (output should be a sharded DTensor).  ``` Class ColwiseParallel(ParallelStyle):     """"""    Partitioning the column of a tensor or module.  We assume the input to be a Replicated DTensor and output to be a sharded DTensor.  """"""          def __init__(self):          super().__init__(MakeInputReplicated, MakeOutputReplicated) ```", Thanks! I have corrected the comment.,"1. I want to learn more about the recommended workflow for applying tensor parallelism to custom `nn.Module`s that are not covered by the highlevel API. Could you provide some examples or guidance?      For example, if the user has a custom attention module that is not equivalent to the one used in `_parallelize_multihead_attn`, then how should the user apply tensor parallelism to the custom attention module? Should the user look into any of the other lowlevel APIs beyond the ones mentioned(`_parallelize_mlp`, `_parallelize_multihead_attn`, and `_parallelize_linear`)? 2. Could you give an example of the model checkpointing workflow for a TPonly module (i.e. no FSDP)?",Has there been any development on this? ," CC([RFC] PyTorch Sharder for distributed training) talked about extending the `PlacementSpec` to accommodate for use cases where tensor need to be placed on UVM/SSD. Although this issue is an evolution of CC([RFC] PyTorch Sharder for distributed training), I don't see a similar description here. Is there plan to make `PlacementSpec` extensible? If there are any public examples that show how to do this, please point me them."
rag,Guarantee symbol allocation for all sizes/strides/storage offset,"  CC(ShapeEnv.create_symbolic_sizes_strides_storage_offset)  CC(Type torch._dynamo.guards)  CC(Guarantee symbol allocation for all sizes/strides/storage offset)  CC(Add definitely_not_01 set to ShapeEnv.) We may need to express guards on the size/stride/storage offset of a tensor, but we cannot do this if it's already been duck sized. This PR guarantees that we allocate a symbol (or negation of the symbol) whenever we ask to create a SymInt, and propagates this symbol to SymNode so that Dynamo can look at it (not in this PR). This PR doesn't actually add guards, nor does Dynamo do anything with these symbols. Signedoffby: Edward Z. Yang ",2022-11-29T23:32:13Z,ciflow/trunk release notes: composability release notes: fx topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/89879
rag,Fix some typed storage is deprecated warnings.,  CC(Fix some typed storage is deprecated warnings.) Signedoffby: Edward Z. Yang  ,2022-11-29T21:19:58Z,Merged ciflow/trunk topic: not user facing module: functorch ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/89867, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: inductor ,inductor / cuda11.6py3.10gcc7sm86 / test (inductor_timm, 2, 2, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""flaky ci"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,[replacing D41588356] Trying to fix issues in TP serialization (#89765),Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/89765 Differential Revision: D41588356 LaMa Project: L1142320,2022-11-29T19:11:19Z,fb-exported Stale release notes: package/deploy,closed,0,4,https://github.com/pytorch/pytorch/issues/89861,This pull request was **exported** from Phabricator. Differential Revision: D41588356," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D41588356,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,update transformer init function,Fixes CC(update transformer init function) ,2022-11-29T09:00:33Z,triaged open source Stale,closed,0,7,https://github.com/pytorch/pytorch/issues/89830,"This seems reasonable (assuming tests pass).  An alternative workaround is getting these fields from lower level components. For example, you can find dim_feedforward by going Transformer>TransformerEncoder>feed forward linear layers. What do you think?","> This seems reasonable (assuming tests pass). >  > An alternative workaround is getting these fields from lower level components. For example, you can find dim_feedforward by going Transformer>TransformerEncoder>feed forward linear layers. What do you think? Hi, this is where the problem lies, you have to dig into your model, and I strongly assume that **Transformer>TransformerEncoder>feed forward linear layers** path not exist, because maybe there is a custom TransformerEncoder in user's Transformer module, or TransformerEncoder has 0 TransformerEncoderLayer. it's messed up.  ","  codes modified in a more elegant way. Here, I use property decorator to dynamically get attributes and modules.", Do you think this may be related to  CC(Quantization issue in transformers) ?,">  Do you think this may be related to CC(Quantization issue in transformers) ?  I don't think they are related, and I cannot reproduce your issue on torch version **'1.13.0+cu117'**",  can this be merged?,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,update transformer init function," 🚀 The feature, motivation and pitch Once a transformer instance is created, like below: ` model_pytorch = nn.Transformer(nhead=16, num_encoder_layers=2, num_decoder_layers=2) ` I cannot access it's num_encoder_layers by **model_pytorch.num_encoder_layers**, TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer alike.   This is extremely suffering when doing quantization, like below: ` Transformer.from_torch(model_pytorch) ` say I have a selfdefined Transformer class, it's really hard to create my instance from original pytorch instance, because I cannot  access init value elegantly.  Alternatives _No response_  Additional context _No response_ ",2022-11-29T08:58:37Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/89829
,Macro Dynamo Progress Bar,"Video demo if add a 1s sleep: https://www.loom.com/share/436f988653a4455ba53b0b5067b15ab8 This is the simplest progress bar I could think of, goal is to have something that we can enable by default and merge quickly whereas more detailed progress bars could be enabled at a later date CC(Dynamo, FX, Inductor Progress Bars)  An inductor compilation goes through 3 steps whereas most (all) other backends seem to go with just 2 so this meant I had a few choices 1. Since I figured users would be happier if something ends sooner rather than something going from 66% to 100% I opted for having the total always be 3 if triton is installed. This is not ideal if it so happens that triton is installed but if someone is not using the inductor backend 2. Another option would be to have a separate compilation bar just for inductor with 1 step I feel like 1 bar for now makes the most sense and we can have 2 bars when I instrument the inductor passes. I tried 2 and it wasn't super helpful  !Screen Shot 20221128 at 7 16 09 PM",2022-11-29T03:00:46Z,topic: not user facing module: dynamo ciflow/inductor,closed,0,0,https://github.com/pytorch/pytorch/issues/89818
yi,[quant] Explictly set default quantized engine instead of relying on the order of supported_qengines,  CC([quant] Explictly set default quantized engine instead of relying on the order of supported_qengines) Summary: Fixes:  CC(Use better default QEngine based on the environment) Test Plan: ossci + sandcastle Reviewers: Subscribers: Tasks: Tags: Differential Revision: D41635738,2022-11-28T23:42:52Z,Merged Reverted ciflow/trunk release notes: quantization topic: improvements,closed,0,7,https://github.com/pytorch/pytorch/issues/89804," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," revert m ""breaking tests https://hud.pytorch.org/pytorch/pytorch/commit/607ff6f4c10914a2a46bab90577cd083a6b3d46d https://github.com/pytorch/pytorch/actions/runs/3596841274/jobs/6058297637 trunk label didnt kick off workflows fast enough"" c nosignal", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,">  revert m ""breaking tests https://hud.pytorch.org/pytorch/pytorch/commit/607ff6f4c10914a2a46bab90577cd083a6b3d46d https://github.com/pytorch/pytorch/actions/runs/3596841274/jobs/6058297637 trunk label didnt kick off workflows fast enough"" c nosignal I remember checked the CI before actually, should this error block landing?"
yi,[benchmarks][dynamo] Trying CI - Set train() for TIMM models accuracy tests,Moving to train mode for TIMM models and also raising batch size for accuracy testing. Raising batch size seems to remove a lot of noise/instability coming from batch_norm decomposition.  ,2022-11-28T20:19:21Z,Merged ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,12,https://github.com/pytorch/pytorch/issues/89780,LGTM, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , merge,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , rebase b master, successfully started a rebase job. Check the current status here,"Successfully rebased `timmtrain` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `git checkout timmtrain && git pull rebase`)", Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Trying to fix issues in TP serialization,"Summary: several error comes up last wk for TP:  Issue during deserialization: P555070876 Test Plan: buck run mode/opt //scripts/pls331/aimp:raw_inference_model_dumper  model_entity_id 355981394 config_version_id 3 Reviewed By: dracifer, houseroad Differential Revision: D41391267",2022-11-28T17:46:13Z,fb-exported release notes: package/deploy,closed,0,6,https://github.com/pytorch/pytorch/issues/89765,"   :x: The commit (994ba186e72a8671b46c9b2ee9dbd99f32d87bb0). This user is missing the User's ID, preventing the EasyCLA check. Consult GitHub Help to resolve.For further assistance with EasyCLA, please submit a support request ticket.",This pull request was **exported** from Phabricator. Differential Revision: D41391267,This pull request was **exported** from Phabricator. Differential Revision: D41391267,This pull request was **exported** from Phabricator. Differential Revision: D41391267,This pull request was **exported** from Phabricator. Differential Revision: D41391267,This pull request was **exported** from Phabricator. Differential Revision: D41391267
yi,`torch.Tensor.flatten` Trigger Segmentation Fault when trying to provide and output named dim," 🐛 Describe the bug A test for `torch.Tensor.flatten` triggers segmentation fault when named dim is provided.  Test ```python import torch def test():     tensor = torch.rand([2, 3, 5, 7, 11], dtype=torch.float32)     _ = torch.Tensor.flatten(tensor, 2, 9, 'features') test() ```  Error log `segmentation fault`  Versions ``` PyTorch version: 1.14.0a0+gitbdc9911 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: 11.1.06 CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0] (64bit runtime) Python platform: Linux5.15.052genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce RTX 3090 GPU 2: NVIDIA GeForce RTX 3090 Nvidia driver version: 515.65.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.1 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.5 [pip3] torch==1.14.0a0+gitbdc9911 [pip3] torchvision==0.13.1 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0            py39h7f8727e_0   [conda] mkl_fft                   1.3.1            py39hd3c417c_0   [conda] mkl_random                1.2.2            py39h51133e4_0   [conda] numpy                     1.21.5           py39he7a7128_1   [conda] numpybase                1.21.5           py39hf524024_1   [conda] numpydoc                  1.2                pyhd3eb1b0_0   [conda] torch                     1.14.0a0+gitce2f870          pypi_0    pypi ``` ",2022-11-27T09:54:16Z,module: crash triaged module: named tensor,open,0,2,https://github.com/pytorch/pytorch/issues/89718,The last argument is a name for the named Tensor feature which is not actively worked on.,"torch.Tensor.flatten() takes 2 named dim, start dim, and end dim where only dimensions starting with start_dim and ending with end_dim are flattened. But in the above example dimension provided is out of range (expected to be in the range of [5, 4], but got 9)"
transformer,DDP hangs on forward pass of transformer," 🐛 Describe the bug I'm facing an issue where DDP is hanging sometimes. A relevant code snippet is below. The setup is fairly simple: I have an encoder, and I want to do a forward pass on one GPU but not the other. Based on which GPU I let it do the forward pass on, I see varying results as to what processes complete. I'm running the file as follows: ``` export CUDA_VISIBLE_DEVICES=5,6 python m torch.distributed.launch nproc_per_node=2 ddp.py ``` Here is the file ddp.py: ```python import torch import argparse from transformers import RobertaModel from torch.nn.parallel import DistributedDataParallel as DDP def main():     parser = argparse.ArgumentParser()     parser.add_argument(""local_rank"", type=int, default=1,                         help=""For distributed training: local_rank"")        args = parser.parse_args()     torch.cuda.set_device(args.local_rank)     device = torch.device(""cuda"", args.local_rank)     torch.distributed.init_process_group(backend='nccl')     args.device = device     encoder = RobertaModel.from_pretrained(""microsoft/codebertbase"")     encoder.to(device)     encoder = DDP(encoder, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=False)     source_ids = torch.zeros([8, 256]).long().to(device)     source_mask = torch.ones([8, 256]).long().to(device)      devices = [1, 1]  0 started, 1 started, 1 done, unexpected     devices = [1, 0]  0 started, 1 started, unexpected      devices = [1, 0, 1]  0 started, 0 started, 0 done, 0 done, expected     if args.local_rank not in devices:         print(""process 1 started"")         output = encoder(source_ids, attention_mask=source_mask)         torch.distributed.barrier()         print(""process 1 done"")     if args.local_rank in devices:         print(""process 0 started"")         torch.distributed.barrier()         print(""process 0 done"") if __name__ == ""__main__"":     main() ```  Versions Collecting environment information... PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 16.04.7 LTS (x86_64) GCC version: (Ubuntu 5.5.012ubuntu1~16.04) 5.5.0 20171010 Clang version: Could not collect CMake version: version 3.24.2 Libc version: glibc2.23 Python version: 3.8.15 (default, Nov 24 2022, 15:19:38)  [GCC 11.2.0] (64bit runtime) Python platform: Linux4.4.0210genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: GeForce GTX 1080 Ti GPU 1: GeForce GTX 1080 Ti GPU 2: GeForce GTX 1080 Ti GPU 3: GeForce GTX 1080 Ti GPU 4: GeForce GTX 1080 Ti GPU 5: GeForce GTX 1080 Ti GPU 6: GeForce GTX 1080 Ti GPU 7: GeForce GTX 1080 Ti Nvidia driver version: 460.27.04 cuDNN version: /usr/lib/x86_64linuxgnu/libcudnn.so.6.0.21 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] torch==1.12.1 [pip3] torchaudio==0.12.1 [pip3] torchvision==0.13.1 [pip3] torchviz==0.0.2 [conda] mkl                       2022.2.1                 pypi_0    pypi [conda] mklfft                   1.3.1                    pypi_0    pypi [conda] mklrandom                1.2.2                    pypi_0    pypi [conda] mklservice               2.4.0                    pypi_0    pypi [conda] numpy                     1.22.4                   pypi_0    pypi [conda] torch                     1.12.1                   pypi_0    pypi [conda] torchaudio                0.12.1                   pypi_0    pypi [conda] torchvision               0.13.1                   pypi_0    pypi [conda] torchviz                  0.0.2                    pypi_0    pypi ",2022-11-27T06:08:57Z,oncall: distributed,open,0,3,https://github.com/pytorch/pytorch/issues/89716,"Hi, I would like to help but have a question:  If you are doing just forward pass, why do you need DDP?  DDP is used for summing gradients among multiple devices in the backward pass.","Also, for certain buffers in the model, DDP might trigger a collective in the forward pass. So there can be a collective mismatch.","Hi , thanks for the reply. To answer your question: I would like to use DDP to speed up the training process, but I'd like to do evaluation while traning. This is just showing the evaluation (inference) part, I omitted the training part.  I didn't exactly understand what you mean by ""trigger a collective"" and having a ""collective mismatch,"" could you please elaborate? Is there a way to fix the issue? Thanks!"
rag,move TypedStorage handling to assertEqual," CC(Deprecate TypedStorage, its derived classes, and all of their public methods) added a patch to `torch.testing.assert_close` to handle `torch.storage.TypedStorage`'s. This change is not reflected in the docs and is not intended for the public API. This PR removes the patch ones again and moves the behavior to `TestCase.assertEqual` instead. Meaning, `TypedStorage`'s are again not supported by the public API, but the behavior is the same for all internal use cases.  ",2022-11-23T10:36:25Z,module: mkldnn open source Merged module: testing ciflow/trunk topic: not user facing,closed,0,9,https://github.com/pytorch/pytorch/issues/89557,"As for your errors: > Unless I'm mistaken we should simply add `exact_dtype=False` to the `assertEqual` calls. I think we want the dtypes to match when we perform the comparisonotherwise it wouldn't be possible to ensure an exact match, right? I'm not sure why the dtypes disagree. The loaded storage's dtype at least matches the saved storage's dtype in `test/test_serialization.py` in `save_load_check`, right? We definitely need that to be true > The problem is only occurs for the quantized dtypes. It happens, because you can't instantiate a new tensor with such a dtype directly, but our logic currently tries to. You may be able to use something like this trick to interpret the quantized type as the appropriate nonquantized type: https://github.com/pytorch/pytorch/blob/7322f73c8f25caea79509805635cd05fd67dc607/torch/storage.pyL530L543","> I think we want the dtypes to match when we perform the comparisonotherwise it wouldn't be possible to ensure an exact match, right? I'm not sure why the dtypes disagree. The loaded storage's dtype at least matches the saved storage's dtype in `test/test_serialization.py` in `save_load_check`, right? We definitely need that to be true I've looked more closely into the test and as is it makes little sense. Since we operate without any data https://github.com/pytorch/pytorch/blob/b0bd5c4508a8923685965614c2c74e6a8c82f7ba/test/test_serialization.pyL693 the only thing we could be checking with https://github.com/pytorch/pytorch/blob/b0bd5c4508a8923685965614c2c74e6a8c82f7ba/test/test_serialization.pyL688L689 is the dtype. However, due to some quirks in `assertEqual` we aren't doing that either: for CC(use `torch.testing.assert_equal` in `TestCase.assertEqual`) we agreed offline (I'm too lazy now to find the specific commit in the PR that added this) that storages should be compared as if they were a list of Python Scalars. This is why before this PR we had them listed in `sequence_types`. Meaning, an empty tensor will be turned into an empty list and no check is happening at all: ```pycon >>> a = torch.tensor([], dtype=torch.int) >>> b = a.float() >>> torch.testing.assert_close(a, b) AssertionError: The values for attribute 'dtype' do not match: torch.int32 != torch.float32. >>> torch.testing.assert_close(a.tolist(), b.tolist()) ``` Even if we had some data before, the check wouldn't have been doing a dtype check due to some legacy quirks in `assertEqual` https://github.com/pytorch/pytorch/blob/8b0847811501cd452b131b6153f28c254a3ee44f/torch/testing/_internal/common_utils.pyL1491L1496 In conclusion, before this PR `assertEqual` will only check the values, but never the dtypes of typed storages. Since there are no values, the test is doing nothing for typed storages.  Now for the part why this is failing after this PR. A minimal reproduction is ```py import io import torch dtype = torch.float64 other_dtype = torch.float32 t = torch.tensor([], dtype=dtype) s = torch.TypedStorage(wrap_storage=t.storage().untyped(), dtype=other_dtype) with io.BytesIO() as buffer:     torch.save([t, s], buffer)     buffer.seek(0)     *_, s_loaded = torch.load(buffer) assert s_loaded.dtype == s.dtype ``` Is the mismatch between `s.dtype` and `t.dtype` intentional? We create `s` from `t.storage()` that has `dtype`, but we explicitly set `dtype=other_dtype`. If yes, the failing test in this PR actually is a valid failure, since `s_loaded` has `dtype` and not `other_dtype` like `s`. Interestingly, the behavior is correct when only serializing `s`, i.e. `torch.save([s], buffer)`. Plus, if we actually try to do the same with some data, i.e. `t = torch.tensor([0], dtype=dtype)`, we are not even allowed to serialize both at the same time: ``` RuntimeError: Cannot save multiple tensors or storages that view the same data as different types ```","Ah sorry, I forgot about this until now. Yes the mismatch between `s.dtype` and `a.dtype` is intentional. When I wrote `test_save_different_dtype_unallocated`, I should have added a comment to explain it. Historically, it has not been possible to save/load multiple tensors/storages that point to the same location (though I think it may be fairly straightforward to add that feature now that all the storage refactoring is complete). In `torch.save` we check the `data_ptr` of each tensor and storage and throw an error if two have the same `data_ptr` but different dtypes. However, if a tensor and storage are unallocated (as in `tensor([])`), their `data_ptr`s are both 0, and the tensor and storage are not actually pointing to the same memory location, since they're not pointing at any memory location. This means that doing this ```python a = torch.tensor([]) s = a.storage() ``` and this ```python a = torch.tensor([]) s = torch.Storage() ``` produce exactly the same `a` and `s`. We do allow saving and loading unallocated tensors/storages of different dtypes. The only reason why the test for this exists is because I broke this behavior at one point and had to fix it. The history of this is here:  CC(Cannot `torch.load` tensors of different dtypes that view the same data) It probably would have been better if I had not used `a.storage()` in this test, and instead created the storage separately, so that it's clear that `a` and `s` aren't actually linked at all.","And it appears that the dtype mismatch between the saved and loaded storage is an upstream bug. If I change the test to do this: ```python self.assertEqual(a.dtype, a_loaded.dtype) self.assertEqual(b.dtype, b_loaded.dtype) ``` it fails. I had assumed `assertEqual` checks the dtype. I will look into why this failure happens",> I had assumed `assertEqual` checks the dtype. It will after this PR. Should we maybe xfail this test in this PR and merge it so you can have a testing ground?,"> Should we maybe xfail this test  I took the liberty to do that in 822f6b29f35204a92c810545f59778b5285a075e. In case you want to go another round, I'll revert later.",Expected failure seems like a good idea to me, merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,cuFFT error raised after trying FFT," 🐛 Describe the bug ``` import torch device = torch.device(""cuda:0"") aa = torch.ones(60,60).to(device) torch.fft.fft2(aa) ``` returns ```  RuntimeError                              Traceback (most recent call last) Cell In [14], line 2       1 aa = torch.ones(60,60).to(device) > 2 torch.fft.fft2(aa) RuntimeError: cuFFT error: CUFFT_INTERNAL_ERROR ``` I'm using cuda 11.6, nvidia driver version 520.56.06, RTX 4090... with Ubuntu 22.04 I have reinstalled CUDA and NVIDIA drivers several times, but the problem is repeating. I think there is no solution in the google..  please help me . thanks.  Versions ``` Collecting environment information... PyTorch version: 1.13.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.35 Python version: 3.9.15 (main, Oct 12 2022, 19:14:37)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.053genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: 11.6.124 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 Nvidia driver version: 520.56.06 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.4 [pip3] torch==1.13.0+cu116 [pip3] torchaudio==0.13.0+cu116 [pip3] torchvision==0.14.0+cu116 [conda] Could not collect ``` ",2022-11-23T10:31:10Z,high priority triage review module: cuda triaged module: fft,closed,0,2,https://github.com/pytorch/pytorch/issues/89556,This is a duplicate of  CC(CUFFT_INTERNAL_ERROR on RTX 4090),我遇到了一样的问题，请问怎么解决的啊？非常需要帮助！！
transformer,[Inductor] [CPU] Vectorization not supporting python pass-in scalar double in speech_transformer," Description Comparing performances of `speech_transformer` with backends inductor and IPEX, inductor is 0.68 IPEX. The main reason is that vectorization does not support python passin scalar double.  Profiling and Code snippet !image ``` kernel_cpp_8 = async_compile.cpp(''' include ""/tmp/tmp8ofgbidl/rp/crpdeql3xwpfmcyakwtqpzihz525if6mt25mozau77xvmnh7vqyu.h"" extern ""C"" void kernel(float* __restrict__ in_out_ptr0,                        const bool* __restrict__ in_ptr0,                        const double* __restrict__ in_ptr2,                        const float* __restrict__ in_ptr3,                        float* __restrict__ out_ptr0,                        float* __restrict__ out_ptr2,                        float* __restrict__ out_ptr4) {     auto in_ptr1 = in_out_ptr0;     auto out_ptr1 = in_out_ptr0;     auto out_ptr3 = in_out_ptr0;     pragma omp parallel num_threads(28)     {         pragma omp for          for(long i0=0; i0::infinity();                         for(long i2=0; i2::infinity();                                 auto tmp4 = static_cast(tmp3);                                 auto tmp5 = tmp2 / tmp4;                                 auto tmp6 = tmp0 ? tmp1 : tmp5;                                 tmp7 = std::max(tmp7, tmp6);                             }                         }                         out_ptr0[i1 + (204*i0)] = tmp7;                     }                 }             }         }         pragma omp for          for(long i0=0; i0::infinity();                             auto tmp4 = static_cast(tmp3);                             auto tmp5 = tmp2 / tmp4;                             auto tmp6 = tmp0 ? tmp1 : tmp5;                             auto tmp8 = tmp6  tmp7;                             auto tmp9 = std::exp(tmp8);                             out_ptr1[i2 + (204*i1) + (41616*i0)] = tmp9;                         }                     }                 }             }         }         pragma omp for          for(long i0=0; i0:omp_out += omp_in) initializer(omp_priv={{0}})                 float tmp1 = 0;                 auto tmp1_vec = at::vec::Vectorized(tmp1);                 for(long i1=0; i1::loadu(out_ptr1 + (16*i1) + (204*i0));                     tmp1_vec += tmp0;                 }                 tmp1 = at::vec::vec_reduce_all([](at::vec::Vectorized& x, at::vec::Vectorized&y) {return x + y;}, tmp1_vec);                 pragma omp simd simdlen(8)  reduction(+:tmp1)                 for(long i1=192; i1::loadu(out_ptr1 + (16*i1) + (204*i0));                 auto tmp1 = at::vec::Vectorized(out_ptr2[i0]);                 auto tmp2 = tmp0 / tmp1;                 tmp2.store(out_ptr3 + (16*i1) + (204*i0));             }             pragma omp simd simdlen(8)              for(long i1=192; i1::loadu(in_ptr3 + (16*i2) + (64*i0) + (512*i1));                     tmp0.store(out_ptr4 + (16*i2) + (64*i1) + (130560*i0));                 }                 pragma omp simd simdlen(8)                  for(long i2=64; i2<64; i2+=1)                 {                     auto tmp0 = in_ptr3[i2 + (64*i0) + (512*i1)];                     out_ptr4[i2 + (64*i1) + (130560*i0)] = tmp0;                 }             }         }     } } ''') ``` According to the profiling analysis, bottlenecks are `kernel_cpp_8, kernel_cpp_14, kernel_cpp_20, kernel_cpp_2, kernel_cpp_32 and kernel_cpp_26`, which are the implementations for the same Python code snippet: ``` attn = attn / self.temperature attn = attn.masked_fill(mask, np.inf) code: attn = self.softmax(attn) ``` As `self.temperature` in Python, a.k.a. `__restrict__ in_ptr2` in C++, is a double scalar, vectorization is not applied.  Minified repro `python benchmarks/dynamo/torchbench.py performance float32 dcpu n50 inductor noskip dashboard k ""speech_transformer"" cold_start_latency channelslast`  ",2022-11-23T07:37:23Z,triaged,open,2,0,https://github.com/pytorch/pytorch/issues/93446
rag,as_strided: Fix default storage_offset for reference implementation,"  CC(as_strided: Fix default storage_offset for reference implementation) This fixes the default storage_offset to take it from the input. This was previously untested, so I've also added a new OpInfo which includes samples with nonzero storage_offsets on the input tensor.",2022-11-22T18:48:08Z,open source Merged Reverted ciflow/trunk topic: not user facing,closed,0,26,https://github.com/pytorch/pytorch/issues/89513,wow that's a lot of xfails lol, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / linuxbionicpy3.7clang9slow / test (slow, 1, 1, linux.2xlarge) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / winvs2019cuda11.6py3 / test (default, 3, 5, windows.8xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 STARTUP failures reported, please check workflows syntax! trunk Details for Dev Infra team Raised by workflow job "," merge f ""pip failure is unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c landrace m ""broke master""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted., merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""land bot is not working"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert m ""Broke multiple workflows, 2 unexpected successes for autograd tests"" c weird", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.,">  merge f ""land bot is not working"" PR has been reopened, so there is no way for me to debug what went wrong :(", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,as_strided: Enable sample inputs with non-zero storage_offset,  CC(as_strided: Fix default storage_offset for reference implementation)  CC(as_strided: Enable sample inputs with nonzero storage_offset),2022-11-22T18:48:03Z,open source release notes: python_frontend,closed,0,0,https://github.com/pytorch/pytorch/issues/89512
yi,Remove TORCH_API from inline at::internal::lazy_init_num_thread,"The function signature in its current state is ambiguous.  Its an inline function that is also declared to be imported from the DLL. which leaves it subject to compilers decision to choose one or the other and depending on what the compiler/linker may choose we may get one of the two behaviors for the `aten::init_num_threads` call:  1. Onceperdllinathread (if its inlined) 2. Onceperthread (if its imported) I suspect onceperdllinathread is already the case currently because it being tagged inline So removing the inline will simply make it a little more consistent and clear. The function exists to avoid repeated calls to aten::init_num_threads.  Being in an ""internal"" namespace, the function isnt expected to be called by external plugins which means that the ""onceperdllinathread"" behavior isn't that much of a problem anyway",2022-11-22T18:26:16Z,triaged open source Merged ciflow/trunk ciflow/periodic,closed,0,13,https://github.com/pytorch/pytorch/issues/89511,The committers listed above are authorized under a signed CLA.:white_check_mark: login: ankurvdev / name: Ankur Verma  (a7822dbf92273716d4313c6d31ddda6f2e00dca7),  Please take a look.,Note that this VS 17.4.0 regression is being tracked as thread\_local causing fatal error LNK1161: invalid export specification on VS 2022  Visual Studio Feedback,> Note that this VS 17.4.0 regression is being tracked as thread_local causing fatal error LNK1161: invalid export specification on VS 2022  Visual Studio Feedback  Is it possible to make the change affect only that specific compiler version? Otherwise the change seems to be too broad as it will affect every compiler on every platform.,"Thanks for providing the reference to the bug.  IMO it's probably not worth wrapping this in a compiler version specific change.  If the VS team is ready to roll out a fix for this, then I'm happy to wait it out. However, that said, IMO the semantics this piece of code tries to express (inlining as well as importing) are extremely weird. And that should probably be changed regardless. The code change here would simply imply that `aten::init_num_threads` is called onceperdllinathread (that call the lazy function) as opposed to onceperthread.  Given that plugin dlls rarely call this directly .... thats probably never going to happen and even if it does, I suspect multiple calls to init_num_threads wont be a huge performance hit (why is it exported anyway?) Even so ... if there are concerns  I can move this function into a cpp file so we can reliably switch to onceperthread. The semantics in its current state make it unclear which mode we'll get 1. Onceperdllinathread 2. Onceperthread And depending on the compiler version it could be either. I suspect onceperdllinathread is already the case currently because it being tagged inline. So choosing one way or the other should be done regardless of the VS bugfix. Thoughts ?","`lazy_init_num_thread` was introduced in https://github.com/pytorch/pytorch/pull/37461 for the purpose of reducing number of calls to the heavier `at::init_num_threads` function, but on the other hand, if its called repeatedly is not an end of the world and just a minor performance overhead. With that in mind, removing `TORCH_API` decorator sounds reasonable to me, but please change PR title to something like: ""Remove `TORCH_API"" from inline `at::internal::lazy_init_num_thread`"" and in PR description explain, that behavior of inline functions with external linkage is not defined across shared library boundary and very likely does not result in a singleton. Imo a better change , would be to move `thread_local bool init` to the namespace level and rename it to something like `extern TORCH_API thread_local bool _num_thread_initialized` and reference it from the inline function. This way it will have an unambiguous callonceperthread semantic across all shared libraries.","FYI  a MS C++ bug fix is underway, but it may not be included until version 17.4.4. ",Ping ...  I changed the title and description as requested. Is there any other change needed before this can merge. ?, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `vs2022_17_4_compilation_fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout vs2022_17_4_compilation_fix && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Fix retrying logic for successful unittest tests under --rerun-disabled-tests mode,"When looking into Rockset data for disabled test unittest, for example `testAdd`, I see that it's rerun only 3 times instead of 50+ times as expected under rerundisabled test mode ``` [   {     ""name"": ""testAdd"",     ""classname"": ""TestLazyReuseIr"",     ""filename"": ""lazy/test_reuse_ir.py"",     ""flaky"": false,     ""num_green"": 3,     ""num_red"": 0   } ] ``` It turns out that I made a mistake mixing `RERUN_DISABLED_TESTS` and `report_only` into `(RERUN_DISABLED_TESTS or report_only) and num_retries_left < MAX_NUM_RETRIES` in https://github.com/pytorch/pytorch/pull/88646.  The retrying logic for successful tests under rerundisabledtests mode is never executed because num_retries_left would be equal to MAX_NUM_RETRIES (not smaller) if the very first run successes. Thus, the sample test `testAdd` finishes right away (1 success count) * `report_only` and `RERUN_DISABLED_TESTS` are 2 different things and shouldn't be mixed together. RERUN_DISABLED_TESTS has the higher priority. * We also don't want to retry skipped tests under rerundisabledtests mode because they are only skipped due to `check_if_enable` check `Test is enabled but rerundisabledtests verification mode is set, so only disabled tests are run`  Testing * CI https://github.com/pytorch/pytorch/actions/runs/3518228784 generates https://ghaartifacts.s3.amazonaws.com/pytorch/pytorch/3518228784/1/artifact/testreportstestdefault44linux.4xlarge.nvidia.gpu_9627285587.zip in which `testAdd` is correctly called multiple times and `TestLazyReuseIr` is skipped correctly * Locally ```  export CI=1  export PYTORCH_RETRY_TEST_CASES=1  export PYTORCH_OVERRIDE_FLAKY_SIGNAL=1  export PYTORCH_TEST_RERUN_DISABLED_TESTS=1 $ python test/run_test.py verbose i lazy/test_reuse_ir Ignoring disabled issues:  [] Selected tests:  lazy/test_reuse_ir Prioritized test from test file changes. reordering tests for PR: prioritized: [] the rest: ['lazy/test_reuse_ir'] Downloading https://raw.githubusercontent.com/pytorch/testinfra/generatedstats/stats/slowtests.json to /Users/huydo/Storage/mine/pytorch/test/.pytorchslowtests.json Downloading https://raw.githubusercontent.com/pytorch/testinfra/generatedstats/stats/disabledtestscondensed.json to /Users/huydo/Storage/mine/pytorch/test/.pytorchdisabledtests.json parallel (file granularity) tests:  lazy/test_reuse_ir serial (file granularity) tests: Ignoring disabled issues:  [] Ignoring disabled issues:  [] Running lazy/test_reuse_ir ... [20221121 13:21:07.165877] Executing ['/Users/huydo/miniconda3/envs/py3.9/bin/python', 'bb', 'lazy/test_reuse_ir.py', 'v', 'importslowtests', 'importdisabledtests', 'rerundisabledtests'] ... [20221121 13:21:07.166279] Expand the folded group to see the log file of lazy/test_reuse_ir [group]PRINTING LOG FILE of lazy/test_reuse_ir (/Users/huydo/Storage/mine/pytorch/test/testreports/lazytest_reuse_ir_6cf_dxa1) Running tests...  Test results will be stored in testreports/pythonunittest/lazy.test_reuse_ir   testAdd (__main__.TestLazyReuseIr) ... ok (1.215s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 50 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 49 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 48 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 47 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 46 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 45 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 44 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 43 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 42 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 41 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 40 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 39 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 38 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 37 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 36 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 35 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 34 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 33 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 32 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 31 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 30 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 29 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 28 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 27 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 26 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 25 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 24 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 23 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 22 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 21 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 20 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 19 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 18 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 17 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 16 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 15 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 14 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 13 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 12 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 11 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 10 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 9 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 8 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 7 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 6 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 5 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 4 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 3 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 2 ok (0.001s)   testAdd (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 1 ok (0.001s)   testAddSub (__main__.TestLazyReuseIr) ...     testAdd succeeded  num_retries_left: 0 skip: Test is enabled but rerundisabledtests verification mode is set, so only disabled tests are run (0.001s)   testAddSubFallback (__main__.TestLazyReuseIr) ... skip: Test is enabled but rerundisabledtests verification mode is set, so only disabled tests are run (0.001s)   testBatchNorm (__main__.TestLazyReuseIr) ... skip: Test is enabled but rerundisabledtests verification mode is set, so only disabled tests are run (0.001s)  Ran 54 tests in 1.264s OK (skipped=3) ``` Here is the sample rockset query ``` WITH added_row_number AS (   SELECT     *,     ROW_NUMBER() OVER(PARTITION BY name, classname, filename ORDER BY _event_time DESC) AS row_number   FROM     commons.rerun_disabled_tests ) SELECT   name,   classname,   filename,   flaky,   num_green,   num_red FROM   added_row_number WHERE   row_number = 1   AND name = 'testAdd' ```",2022-11-21T21:22:21Z,Merged ciflow/trunk release notes: releng test-config/default,closed,0,2,https://github.com/pytorch/pytorch/issues/89454, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,torch.nn.TransformerEncoderLayer missing exception description information.," 🐛 Describe the bug     import torch     encoder_layer = torch.nn.TransformerEncoderLayer(d_model=52, nhead=1)     src = torch.rand(10, 32, 512)     out = encoder_layer(src) When running this code, an AssertionError is thrown directly without giving effective information about the exception. The real error is that d_model is not equal to the third value in src. It is recommended to use the pytorch1.8.0 version. When this happens, throw Display the corresponding exception information.  Versions pytorch: 1.8.0 Python version: 3.8 CUDA/cuDNN version: cuDNN 11.1 GPU models and configuration: RTX3060 Operating System：Windows ",2022-11-21T02:34:36Z,module: nn module: error checking triaged,open,0,2,https://github.com/pytorch/pytorch/issues/89394,"Are you strongly attached to 1.8.0? On 1.13, it gives a more informative error: ```   File ""/Users/samdow/anaconda3/envs/1.13/lib/python3.10/sitepackages/torch/nn/functional.py"", line 5046, in multi_head_attention_forward     assert embed_dim == embed_dim_to_check, \ AssertionError: was expecting embedding dimension of 52, but got 512 ``` since d_model is the number of expected features of the input according to the docs, it and the final dim of src must match","> TransformerEncoderLayer Currently doing a test experiment, need to use pytorch1.8 version, and then use TransformerEncoderLayer, this problem is triggered, I feel very confused, I will test with a higher version later, and found that the higher version has provided the corresponding information description, thank you ."
yi,"Reland 2 ""Towards unifying symbolic and non symbolic fake tensor (#89038) (#89143)""","  CC(Reland 2 ""Towards unifying symbolic and non symbolic fake tensor (89038) (89143)"") This reverts commit 8e4c9828f4c990f439179912159086aaed790493. ",2022-11-19T14:51:41Z,Merged ciflow/trunk release notes: composability topic: not user facing module: dynamo ciflow/inductor,closed,0,2,https://github.com/pytorch/pytorch/issues/89346," merge f ""flaky SIGIOT on jit"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
yi,torch.cholesky_inverse: The result accuracy was inconsistent on the CPU and GPU," 🐛 Describe the bug     import torch     torch.manual_seed(5)      input = torch.rand([3, 3], dtype=torch.float32)     resCPU = torch.cholesky_inverse(input)     print(resCPU)     input = input.cuda()     resGPU = torch.cholesky_inverse(input)     print(resGPU) code result:         tensor([[ 321.4903, 431.7014,  536.8290],                 [431.7014,  582.4526, 724.7808],                 [ 536.8290, 724.7808,  903.7211]])         tensor([[ 321.4903, 431.7013,  536.8291],                 [431.7013,  582.4526, 724.7809],                 [ 536.8290, 724.7808,  903.7211]], device='cuda:0') As can be seen, the results on the CPU and GPU are inconsistent, resulting in accuracy problems.  Versions pytorch: 1.12.1 Python version: 3.8 CUDA/cuDNN version: cuDNN 11.3 GPU models and configuration: RTX3060",2022-11-19T03:27:08Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/89335
gemma,fbgemm_avx512 build failure," 🐛 Describe the bug I'm building git master with the same Arch recipe. My CPU is Ryzen 2 and does NOT support AVX512. fbgemm is programmed wrongly and demands `fbgemm_avx512` even when the main project has disabled it: ```  Found OpenMP: TRUE    CMake Warning at third_party/fbgemm/CMakeLists.txt:74 (message):   OpenMP found! OpenMP_C_INCLUDE_DIRS =   File """", line 1     exec(open('defs.bzl').read());print(';'.join(get_fbgemm_avx2_srcs(msvc=)))                                                                            ^ SyntaxError: invalid syntax   File """", line 1     exec(open('defs.bzl').read());print(';'.join(get_fbgemm_inline_avx2_srcs(msvc=)))                                                                                   ^ SyntaxError: invalid syntax   File """", line 1     exec(open('defs.bzl').read());print(';'.join(get_fbgemm_avx512_srcs(msvc=)))                                                                              ^ SyntaxError: invalid syntax   File """", line 1     exec(open('defs.bzl').read());print(';'.join(get_fbgemm_inline_avx512_srcs(msvc=)))                                                                                     ^ SyntaxError: invalid syntax CMake Error at third_party/fbgemm/CMakeLists.txt:135 (target_compile_options):   Cannot specify compile options for target ""fbgemm_avx512"" which is not   built by this project. CMake Warning at third_party/fbgemm/CMakeLists.txt:170 (message):   ========== CMake Warning at third_party/fbgemm/CMakeLists.txt:171 (message):   CMAKE_BUILD_TYPE = Release CMake Warning at third_party/fbgemm/CMakeLists.txt:172 (message):   CMAKE_CXX_FLAGS_DEBUG is g CMake Warning at third_party/fbgemm/CMakeLists.txt:173 (message):   CMAKE_CXX_FLAGS_RELEASE is O3 DNDEBUG CMake Warning at third_party/fbgemm/CMakeLists.txt:174 (message):   ========== ** AsmJit Summary **    ASMJIT_DIR=./src/pytorch/third_party/fbgemm/third_party/asmjit    ASMJIT_TEST=FALSE    ASMJIT_TARGET_TYPE=STATIC    ASMJIT_DEPS=pthread;rt    ASMJIT_LIBS=asmjit;pthread;rt    ASMJIT_CFLAGS=DASMJIT_STATIC    ASMJIT_PRIVATE_CFLAGS=Wall;Wextra;Wconversion;fnomatherrno;fnothreadsafestatics;fnosemanticinterposition;DASMJIT_STATIC    ASMJIT_PRIVATE_CFLAGS_DBG=    ASMJIT_PRIVATE_CFLAGS_REL=O2;fmergeallconstants;fnoenforceehspecs CMake Error at third_party/fbgemm/CMakeLists.txt:235 (target_include_directories):   Cannot specify include directories for target ""fbgemm_avx512"" which is not   built by this project. CMake Error at third_party/fbgemm/CMakeLists.txt:262 (target_compile_definitions):   Cannot specify compile definitions for target ""fbgemm_avx512"" which is not   built by this project. CMake Error at cmake/Dependencies.cmake:826 (set_property):   set_property could not find TARGET fbgemm_avx512.  Perhaps it has not yet   been created. Call Stack (most recent call first):   CMakeLists.txt:719 (include) ```  Versions ``` PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: Arch Linux (x86_64) GCC version: (GCC) 12.2.0 Clang version: 14.0.6 CMake version: version 3.25.0 Libc version: glibc2.36 Python version: 3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0] (64bit runtime) Python platform: Linux6.1.01rc5mainlinex86_64withglibc2.36 Is CUDA available: N/A CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: N/A GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A Versions of relevant libraries: [pip3] mypy==0.982 [pip3] mypyextensions==0.4.3 [pip3] mypyprotobuf==2.9 [pip3] numpy==1.23.4 [pip3] numpyquaternion==2022.4.2 [pip3] numpydoc==1.5.0 [pip3] oldestsupportednumpy==2022.5.28 [pip3] pytorchlightning==1.7.7 [pip3] torchaudio==0.12.1+58da317 [pip3] torchfile==0.1.0 [pip3] torchmetrics==0.10.0 [pip3] torchvision==0.14.0a0 [conda] Could not collect ``` ",2022-11-18T17:54:18Z,module: build triaged,open,0,0,https://github.com/pytorch/pytorch/issues/89293
yi,Delete .pyi files in torch/nn/parallel folder to enable mypy type checking,Summary: As title. Context in https://fburl.com/4irjskbe Test Plan: CI Differential Revision: D41148641,2022-11-18T14:55:14Z,better-engineering fb-exported Stale topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/89286,This pull request was **exported** from Phabricator. Differential Revision: D41148641,This pull request was **exported** from Phabricator. Differential Revision: D41148641,This pull request was **exported** from Phabricator. Differential Revision: D41148641,You should restore the pyi annotations into the py files as part of this,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
agent,[Inductor CI] Use string format for cuda-arch-list input to prevent 8.0/9.0/10.0 etc from being interpreted as 8/9/10,"Currently or in future whenever we change the cudaarchlist to num.0, github action or some agent would pass just num to TORCH_CUDA_ARCH_LIST This num is not regex matched during cuda arch analysis phase. (here: https://github.com/pytorch/pytorch/blob/c5fafb4e1694f141d8a1a31142cce4049d9057ed/cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmakeL229) Example failure: https://github.com/weiwangmeta/pytorch/actions/runs/3495656108/jobs/5852735299   Unknown CUDA Architecture Name 8 in CUDA_SELECT_NVCC_ARCH_FLAGS This change reminds us to use e.g. '8.0', '9.0', '10.0' etc instead of 8.0, 9.0, 10.0 as GHA or some other agent may erroneously truncate it to pure numbers. ",2022-11-18T09:48:20Z,Merged ciflow/trunk topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/89279, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 additional jobs have failed, first few of them are: trunk Details for Dev Infra team Raised by workflow job ", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,Support masked_fill to address the GPT2 performance issue,  CC(Support masked_fill to address the GPT2 performance issue)  CC(Redefine the simdlen semantic) ,2022-11-18T07:23:44Z,open source Merged ciflow/trunk topic: not user facing intel module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/89274,Just reland this PR.,"The CI failure looks irrelevant, see https://github.com/pytorch/pytorch/pull/89281", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
gpt,Support masked_fill to address the GPT2 performance issue,  CC(Support masked_fill to address the GPT2 performance issue)  CC(Redefine the simdlen semantic) ,2022-11-18T07:23:44Z,open source Merged ciflow/trunk topic: not user facing intel module: inductor ciflow/inductor,closed,0,4,https://github.com/pytorch/pytorch/issues/89274,Just reland this PR.,"The CI failure looks irrelevant, see https://github.com/pytorch/pytorch/pull/89281", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,[Dynamo] Fix args bug in TensorVariable.call_method,"bug in 7k github models: ``` run_dynamo error from ./generated/test_BloodAxe_pytorch_toolbelt.py:BinaryLovaszLoss  pytest ./generated/test_BloodAxe_pytorch_toolbelt.py k test_008 Traceback (most recent call last):   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/convert_frame.py"", line 404, in _compile     out_code = transform_code_object(code, transform)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/bytecode_transformation.py"", line 341, in transform_code_object     transformations(instructions, code_options)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/convert_frame.py"", line 392, in transform     tracer.run()   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 1523, in run     super().run()   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 389, in run     and self.step()   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 359, in step     getattr(self, inst.opname)(inst)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 193, in wrapper     return inner_fn(self, inst)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 865, in CALL_FUNCTION_KW     self.call_function(fn, args, kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 301, in call_function     self.push(fn.call_function(self, args, kwargs))   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/functions.py"", line 194, in call_function     return super(UserFunctionVariable, self).call_function(tx, args, kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/functions.py"", line 65, in call_function     return tx.inline_user_function_return(   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 330, in inline_user_function_return     result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 1595, in inline_call     return cls.inline_call_(parent, func, args, kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 1649, in inline_call_     tracer.run()   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 389, in run     and self.step()   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 359, in step     getattr(self, inst.opname)(inst)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 193, in wrapper     return inner_fn(self, inst)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 853, in CALL_FUNCTION_EX     self.call_function(fn, argsvars.items, kwargsvars.items)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 301, in call_function     self.push(fn.call_function(self, args, kwargs))   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/functions.py"", line 194, in call_function     return super(UserFunctionVariable, self).call_function(tx, args, kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/functions.py"", line 65, in call_function     return tx.inline_user_function_return(   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 330, in inline_user_function_return     result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 1595, in inline_call     return cls.inline_call_(parent, func, args, kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 1649, in inline_call_     tracer.run()   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 389, in run     and self.step()   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 359, in step     getattr(self, inst.opname)(inst)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 193, in wrapper     return inner_fn(self, inst)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/symbolic_convert.py"", line 119, in impl     self.push(fn_var.call_function(self, self.popn(nargs), {}))   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/builtin.py"", line 355, in call_function     result = handler(tx, *args, **kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/builtin.py"", line 612, in call_getitem     return args[0].call_method(tx, ""__getitem__"", args[1:], kwargs)   File ""/scratch/ybliang/work/repos/pytorch/torch/_dynamo/variables/tensor.py"", line 332, in call_method     *proxy_args_kwargs([self] + args, kwargs), TypeError: can only concatenate list (not ""tuple"") to list ``` ",2022-11-18T01:29:55Z,module: dynamo ciflow/inductor,closed,0,1,https://github.com/pytorch/pytorch/issues/89258,"> add a test? Yes, I decided to merge this change into https://github.com/pytorch/pytorch/pull/89257, as they can be covered by the same tests. Thanks for your reviewing."
rag,Deprecation warning in `Tensor.storage()` should suggest alternate API,"Since CC(Deprecate TypedStorage, its derived classes, and all of their public methods) deprecated `TypedStorage`, calling `Tensor.storage()` now raises a warning about the deprecation because it returns a `TypedStorage`. The warning should be updated to mention how to get an `UntypedStorage` from the tensor. At the moment, this is done with `Tensor._storage()`, but it should be renamed to `Tensor.untyped_storage()` so that it's public, and there should be documentation for it. It might be good for the warning to mention in which cases `UntypedStorage` and `TypedStorage` are not interchangeable. ~We should also have a function that takes an `UntypedStorage` and a `torch.dtype` and returns a tensor that wraps the storage.~ EDIT: `torch.tensor` does this ",2022-11-17T18:08:55Z,triaged module: deprecation module: python frontend,closed,2,2,https://github.com/pytorch/pytorch/issues/89224,That sounds good. The new untyped_storage API sounds good. Not sure if we really need an API to build a Storage again? The APIs that we have today that take Storage also accept UntypedStorages right? It would be good as well to provide 11 correspondance for python storage object with c++ storage like we do for Tensor.  do you think it is worth reproducing all the complexity we have for Tensor here?, has been asking for it and I think is reasonable 
yi,"Reland ""Towards unifying symbolic and non symbolic fake tensor (#89038)""","  CC(Reland ""Towards unifying symbolic and non symbolic fake tensor (89038)"") This reverts commit cf6003f0469ae1440d4a8585860c2c5f4c738707. Differential Revision: D41363992",2022-11-16T17:00:10Z,Merged Reverted ciflow/trunk release notes: composability topic: not user facing ciflow/inductor,closed,0,9,https://github.com/pytorch/pytorch/issues/89143," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ," merge f ""flaky ci only"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c 'nosignal' m ""This seems to be causing the test_make_fx_symbolic_exhaustive_rad2deg_cpu_float32 and test_make_fx_symbolic_exhaustive_inplace_rad2deg_cpu_float32 test to fail across multiple jobs"" Logs: https://hud.pytorch.org/pytorch/pytorch/commit/e686b8c3ba93cb7caa314c78bf84dbd2d7df9683", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
transformer,AssertionError: Dynamo input and output is a strict subset of traced input/output," 🐛 Describe the bug Trying `torchdynamo.export` on Huggingface transformers BertModel. Please let me know if I'm not using this api correctly. torch version: built from master on commit https://github.com/pytorch/pytorch/commit/f1a5044de0639180f667d212800aa43f34026b3c  Error logs ``` Traceback (most recent call last):   File ""test_bert_dynamo_export.py"", line 22, in      fx_module, _ = dynamo.export(model, *args, aten_graph=False, tracing_mode=""real"")   this fails   File ""/home/bowbao/pytorch_dev/torch/_dynamo/eval_frame.py"", line 539, in export     matched_output_elements_positions = produce_matching(flat_both, flat_results_traced)   File ""/home/bowbao/pytorch_dev/torch/_dynamo/eval_frame.py"", line 489, in produce_matching     ), ""Dynamo input and output is a strict subset of traced input/output"" AssertionError: Dynamo input and output is a strict subset of traced input/output ```  Minified repro Pasting raw repro code since it's about `torchdynamo.export` ```python from transformers import BertModel, BertTokenizer tokenizer = BertTokenizer.from_pretrained(""bertbaseuncased"") model = BertModel.from_pretrained(""bertbaseuncased"") text = ""Test sentence for transformers bert model."" encoded_input = tokenizer(text, return_tensors=""pt"") print(encoded_input) args = (     encoded_input[""input_ids""],     encoded_input[""attention_mask""],     encoded_input[""token_type_ids""], ) output = model(**encoded_input)   this works output = model(*args)   this works from torch import _dynamo as dynamo fx_module, _ = dynamo.export(model, *args)   this fails fx_module, _ = dynamo.export(model, **encoded_input)   this fails fx_module.print_readable() ``` Edit: versions: transformers                  4.25.0 ",2022-11-15T20:01:45Z,bug oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/93422,Friendly ping.  Do you happen to know the context of this error? I'm happy to contribute. We are observing this on other huggingface transformer models too., might have more context here.,Should be fixed by https://github.com/pytorch/pytorch/pull/92013
transformer,Transformers model tracing not working," 🐛 Describe the bug When using torch.neuron.trace to compile a transformer model, an error appears : torch.jit.trace failed with following error: 0INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":607, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Code to reproduce ``` import torch   from torch import nn import torch_neuron from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(             pretrained_model_name_or_path='shtoshni/longformer_coreference_joint', use_fast=True         ) model = AutoModel.from_pretrained(             pretrained_model_name_or_path='shtoshni/longformer_coreference_joint',             output_hidden_states=False,             add_pooling_layer=False,             return_dict = False,         ) model.eval() model wrapper class Mymodel(nn.Module):     def __init__(self):         super(Mymodel,self).__init__()         self.pretrained = AutoModel.from_pretrained(             pretrained_model_name_or_path='shtoshni/longformer_coreference_joint',             output_hidden_states=False,             add_pooling_layer=False,             return_dict = False,         )         self.pretrained.eval()     def forward(self,*x):         self.pretrained.eval()         x = list(x)         y = self.pretrained(*x)         return y[0] sequence_0 = ""The company HuggingFace is based in New York City"" sequence_1 = ""HuggingFace's headquarters are situated in Manhattan"" max_length=128 paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, max_length=max_length, padding='max_length', truncation=True, return_tensors=""pt"") example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'] mymodel = Mymodel() mymodel.eval() model_neuron = torch.neuron.trace(mymodel, example_inputs_paraphrase) ```  Error message ``` RuntimeError                              Traceback (most recent call last) ~/anaconda3/lib/python3.7/sitepackages/torch_neuron/decorators.py in torch_trace_error_handler()    1526     try: > 1527         yield    1528     except RuntimeError as e: ~/anaconda3/lib/python3.7/sitepackages/torch_neuron/convert.py in to_graph(func_or_mod, example_inputs, return_trace, **kwargs)     218     with track_script() as script_tracker, torch_trace_error_handler(): > 219         jit_trace = torch.jit.trace(func_or_mod, example_inputs, **kwargs)     220     original_name = getattr(jit_trace, 'original_name', ~/anaconda3/lib/python3.7/sitepackages/torch/jit/_trace.py in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)     749             _force_outplace, > 750             _module_class,     751         ) ~/anaconda3/lib/python3.7/sitepackages/torch/jit/_trace.py in trace_module(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)     964                 _force_outplace, > 965                 argument_names,     966             ) RuntimeError: 0INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":607, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool,  Candidates: ... torch.jit.trace failed with following error: 0INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":607, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool,  Candidates: 	aten::constant_pad_nd(Tensor self, int[] pad, Scalar value=0) > (Tensor) ```  Versions PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.21.3 Libc version: glibc2.10 Python version: 3.7.10  (default, Feb 19 2021, 16:07:37)  [GCC 9.3.0] (64bit runtime) Python platform: Linux5.4.01088awsx86_64withdebianbustersid Is CUDA available: True CUDA runtime version: 10.0.130 CUDA_MODULE_LOADING set to:  GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 450.142.00 cuDNN version: Probably one of the following: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda10.2/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_adv_train.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_ops_train.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_train.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_ops_train.so.8.0.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.19.2 [pip3] numpydoc==1.1.0 [pip3] torch==1.11.0 [pip3] torchmodelarchiver==0.6.0 [pip3] torchneuron==1.11.0.2.3.0.0 [pip3] torchserve==0.6.0 [conda] blas                      1.0                         mkl   [conda] mkl                       2020.2                      256   [conda] mklservice               2.3.0            py37he8ac12f_0   [conda] mkl_fft                   1.2.1            py37h54f3939_0   [conda] mkl_random                1.1.1            py37h0573a6f_0   [conda] numpy                     1.19.2           py37h54aff64_0   [conda] numpybase                1.19.2           py37hfa32c7d_0   [conda] numpydoc                  1.1.0              pyhd3eb1b0_1   [conda] torch                     1.11.0                   pypi_0    pypi [conda] torchmodelarchiver      0.6.0                    pypi_0    pypi [conda] torchneuron              1.11.0.2.3.0.0           pypi_0    pypi [conda] torchserve                0.6.0                    pypi_0    pypi ",2022-11-15T17:30:08Z,oncall: jit,open,0,2,https://github.com/pytorch/pytorch/issues/89076,"Hi  , Thanks for raising this issue. May I know which transformers version you use? As discussed in https://github.com/huggingface/transformers/issues/13126, this issue shall be fixed by https://github.com/huggingface/transformers/pull/17176 in transformers repo (since v4.22.0 release) ~3 months ago. To confirm this, I try with transformers v4.21.1 (without the fix) and the issue is reproduced, and when I switch the latest transformers (with the fix), the issue does have been solved and PyTorch tracing works fine;","Hi gu, Thank you for your reply. It works after uploading transformers to v4.22.0."
yi,Towards unifying symbolic and non symbolic fake tensor,"  CC(Towards unifying symbolic and non symbolic fake tensor)  CC(SymIntify convolution backend calculation)  CC(SymIntArrayRef type caster)  CC(Add int64_t, SymInt overloads for all binary operators in C++)  CC(Move ConvParams methods directly on struct)  CC(Hide ConvParams struct from ConvUtils.h) Fake tensor behaves pretty differently depending on if you have symbolic shapes or not.  This leads to bugs; for example, we weren't getting correct convolution_backward strides because we bypassed the correct stride logic in fake tensor on symbolic shapes. This PR attempts to unify the two codepaths.  I don't manage to unify everything, but I get most of it.  The algorithm is delicate and I'm still hosing down test failures. Signedoffby: Edward Z. Yang ",2022-11-15T04:49:39Z,Merged Reverted ciflow/trunk release notes: composability topic: not user facing ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/89038," merge f ""lint only, previous ci was clean"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "," revert c ghfirst m ""executorch segfaults""", successfully started a revert job. Check the current status here. Questions? Feedback? Please reach out to the PyTorch DevX Team, your PR has been successfully reverted.
transformer,[torch] [analytics] add pytorch event logger callsites to transformers and encoder/decoders,Differential Revision: D41227275,2022-11-11T18:44:20Z,fb-exported Merged ciflow/trunk,closed,0,10,https://github.com/pytorch/pytorch/issues/88896,This pull request was **exported** from Phabricator. Differential Revision: D41227275, merge g, Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ,This pull request was **exported** from Phabricator. Differential Revision: D41227275,This pull request was **exported** from Phabricator. Differential Revision: D41227275,This pull request was **exported** from Phabricator. Differential Revision: D41227275,This pull request was **exported** from Phabricator. Differential Revision: D41227275,This pull request was **exported** from Phabricator. Differential Revision: D41227275, merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,[Inductor] Input Buffers Should Be Representable As Storage And Layout," 🐛 Describe the bug InputBuffer's currently do not have an associated storage. This prevents correct handling of aliased inputs  see TODO here.  When calling require_stride_order, because `is_storage_and_layout` returns False for InputBuffers we fall through to an unnecessary copy.  A shorter term fix would be to handle InputBuffers in `require_stride_order`. ",2022-11-11T01:22:15Z,feature triaged oncall: pt2 module: inductor internal ramp-up task,open,0,0,https://github.com/pytorch/pytorch/issues/93617
rag,module 'torch.cuda' has no attribute '_UntypedStorage'," 🐛 Describe the bug On a machine with PyTorch version: 1.12.1+cu116, running the following code gets error message `module 'torch.cuda' has no attribute '_UntypedStorage'`. However, the error disappears if not using cuda. The same code can run correctly on a different machine with PyTorch version: 1.8.2+cu111  ```python import time import torch import torch.multiprocessing as mp def set_device():      Note: the code can run if the following two lines are commented out     if torch.cuda.is_available():         torch.set_default_tensor_type(torch.cuda.FloatTensor)     return def worker(job_queue: mp.Queue, done_queue: mp.Queue, result_queue: mp.Queue):     set_device()     para = torch.zeros((100, 100))     try:         while True:             result = para + torch.randn_like(para)             if not job_queue.empty():                 job_queue.get()                 break             if result_queue.full():                 time.sleep(0.1)                 continue             result_queue.put(result)         done_queue.put(None)         result_queue.cancel_join_thread()     except Exception as e:         print(f'{mp.current_process().name}  {e}') def test_queue():     set_device()     ctx = mp.get_context('spawn')     job_queue = ctx.Queue()     result_queue = ctx.Queue(100)     done_queue = ctx.Queue()     proc = ctx.Process(target=worker, args=(job_queue, done_queue, result_queue))     proc.start()     for i in range(10):         result = result_queue.get()         for j in range(100):             if not result_queue.empty():                 result = result_queue.get()             else:                 break         print(""result: "", result.sum().item())         time.sleep(0.1)     job_queue.put(None)     proc.join() if __name__ == '__main__':     test_queue() ``` Error message: ``` Traceback (most recent call last):   File ""test_queue.py"", line 53, in      test_queue()   File ""test_queue.py"", line 44, in test_queue     result = result_queue.get()   File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 116, in get     return _ForkingPickler.loads(res)   File ""/home/weixu/venvs/python3.8/lib/python3.8/sitepackages/torch/multiprocessing/reductions.py"", line 122, in rebuild_cuda_tensor     shared_cache[(storage_handle, storage_offset_bytes)] = StorageWeakRef(storage)   File ""/home/weixu/venvs/python3.8/lib/python3.8/sitepackages/torch/multiprocessing/reductions.py"", line 65, in __setitem__     self.free_dead_references()   File ""/home/weixu/venvs/python3.8/lib/python3.8/sitepackages/torch/multiprocessing/reductions.py"", line 70, in free_dead_references     if storage_ref.expired():   File ""/home/weixu/venvs/python3.8/lib/python3.8/sitepackages/torch/multiprocessing/reductions.py"", line 35, in expired     return torch.Storage._expired(self.cdata)   type: ignore[attrdefined]   File ""/home/weixu/venvs/python3.8/lib/python3.8/sitepackages/torch/storage.py"", line 757, in _expired     return eval(cls.__module__)._UntypedStorage._expired(*args, **kwargs) AttributeError: module 'torch.cuda' has no attribute '_UntypedStorage' ```  Versions Collecting environment information... PyTorch version: 1.12.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.01ubuntu1~22.04) 11.3.0 Clang version: Could not collect CMake version: version 3.22.1 Libc version: glibc2.35 Python version: 3.8.15 (default, Oct 12 2022, 19:15:16)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.15.052genericx86_64withglibc2.35 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to:  GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 510.47.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.4 [pip3] torch==1.12.1+cu116 [pip3] torchaudio==0.12.1+cu116 [pip3] torchvision==0.13.1+cu116 [conda] Could not collect",2022-11-10T20:23:29Z,,closed,0,7,https://github.com/pytorch/pytorch/issues/88839, ,"This problem doesn't exist in the newer pytorch 1.13. I could fix this on the 1.12 branch, but will there be a 1.12.2 release?","No, 1.13 is out, thanks for confirming .  please reopen if error repros on pytorch 1.13","Hi ,  , I'm stuck with this issue and the problem is I cannot use the latest version of pytorch (currently using 1.12+cu11.3). Is there a workaround? or can I please get some context of why this is occuring?",I ran into this problem as well. At this moment we are not planning to move to pytorch 1.13 yet. Can we reopen this issue and maybe get a backport to 1.12? Thanks!,Pytorch doesn't do backport fixes. ,Thanks for the clarification.
transformer,[RFC] PyTorch DistributedTensor," 🚀 The feature, motivation and pitch  RFC: PyTorch DistributedTensor We have been developing a DistributedTensor (a.k.a DTensor) concept under the pytorch/tau repo in the past few months, and now we are moving the implementation over to pytorch with the stack https://github.com/pytorch/pytorch/pull/88180. This RFC proposes the DistributedTensor to torch.distributed. Any early feedbacks are welcomed! **Update**:  DTensor now available in PyTorch 2.0 and nightly build! You can now play around with DTensor even in a colab Notebook! see a quick e2e tutorial here https://colab.research.google.com/drive/12Pl5fvh0eLPUrcVO7s6yY4n2_RZo8pLRscrollTo=stYPKb9Beq4e  Introduction We propose distributed tensor primitives to allow easier distributed computation authoring in SPMD(Single Program Multiple Devices) paradigm. The primitives are simple but powerful when used to express tensor distributions with both sharding and replication parallelism strategies. This could empower native Tensor parallelism among other advanced parallelism explorations. For example, to shard a big tensor across devices with 3 lines of code: ```python  torchrun standalone nnodes=1 nprocpernode=4 dtensor_example.py import torch   from torch.distributed.device_mesh import init_device_mesh from torch.distributed._tensor import Shard, distribute_tensor    Create a mesh topology with the available devices. mesh = init_device_mesh(""cuda"", (int(os.environ[""WORLD_SIZE""]),)) big_tensor = torch.randn(100000, 88)   Shard this tensor over the mesh by sharding `big_tensor`'s 0th dimension over the 0th dimension of `mesh`. my_dtensor = distribute_tensor(big_tensor, mesh, [Shard(dim=0)]) ```  Motivation Today there are mainly three ways to scale up distributed training: Data Parallel, Tensor Parallel and Pipeline Parallel. Each of them works on a separate dimension where solutions have been built independently (i.e. PyTorch DDP, FSDP, ShardedTensor, PiPPy, etc.). When training really large models, users would like to use these technologies together (i.e. 3D Parallelism), while the interoperability of the existing solutions are not great and often hard to use (i.e. users might want arbitrary combinations of the data parallel, tensor parallel and pipeline parallel). This is becoming an issue for users and one of the biggest reasons is that there’s no common abstractions that build the bridge between different parallelism strategies. An ideal scenario is that users could just build their models like in a single node/device, without worrying about how to do distributed training in a cluster, and our solutions could help them run distributed training in an efficient manner. For example, researchers just need to build their big transformer model, and PyTorch Distributed automatically figures out how to split the model and run pipeline parallel across different nodes, how to run data parallel and tensor parallel within each node. In order to achieve this, we need some common abstractions to represent data distribution and run the distributed computation. There're many recent works that working on tensor level parallelism to provide common abstractions, see the `Related Works` in the last section for more details. Inspired by GSPMD, Oneflow and TF’s DTensor, we introduce a DistributedTensor concept to represent generic data distributions across hosts. DistributedTensor is the next evolution of ShardedTensor and provides basic abstractions to distribute storage and compute. It serves as one of the basic building blocks for distributed program translations and describes the layout of a distributed training program. With the DistributedTensor abstraction, we can seamlessly build parallelism strategies such as tensor parallelism, DDP and FSDP.  Value Propsition DistributedTensor primarily:    Offers a uniform way to save/load state dict during checkpointing, even when there’re complex data distribution strategies such as combining tensor parallelism with parameter sharding in FSDP.    Could natively offer Tensor Parallelism solution in eager mode, just like our current ShardedTensor solution. Moreover, it gives additional flexibility for advanced users who want to mix sharding and replication.    Could be the entry point of a SPMD programming model for ML System Engineers, providing good UX to mix up different types of parallelism, and could be used as a fundamental building block of a compiler based distributed training.  PyTorch DistributedTensor  DistributedTensor API We offer both a lower level DistributedTensor API and a module level API to create a `nn.Module` with “distributed” parameters.  Basic DistributedTensor API Examples Here are some basic DistributedTensor API examples that showcase:  1. How to construct a DistributedTensor directly, to represent different types of sharding, replication, sharding + replication strategies. 2. How to create DistributedTensor from a local `torch.Tensor`. 3. How to “reshard” an existing DistributedTensor to a different DistributedTensor with modified placement strategy or world size. ```python  torchrun standalone nnodes=1 nprocpernode=4 dtensor_example.py import torch import torch.distributed._tensor as dtensor from torch.distributed.device_mesh import init_device_mesh from torch.distributed._tensor import DTensor, Shard, Replicate, distribute_tensor    construct a device mesh with available devices (multihost or single host) world_size = int(os.environ[""WORLD_SIZE""]) device_mesh = init_device_mesh(""cuda"", (world_size,))  if we want to do rowwise sharding   rowwise_placement=[Shard(0)]    if we want to do colwise sharding   colwise_placement=[Shard(1)]    distributed tensor returned will be sharded across the dimension specified in placements   rowwise_dtensor = dtensor.empty((8, 12), device_mesh=device_mesh, placements=rowwise_placement)  shard the torch.Tensor on rank 0 and scatter the shards to all ranks colwise_dtensor = distribute_tensor(torch.randn(8, 12), device_mesh, colwise_placement)  if we want to do replication across a certain device list   replica_placement = [Replicate()]    distributed tensor will be replicated to all four GPUs.   dtensor.empty((8, 12), device_mesh=device_mesh, placements=replica_placement)    if we want to distributed a tensor with both replication and sharding   device_mesh = init_device_mesh(device_type=""cuda"", (world_size // 2, 2))  replicate across the first dimension of device mesh, then sharding on the second dimension of device mesh   spec=[Replicate(), Shard(0)]   dtensor.empty((8, 8), device_mesh=device_mesh, placements=spec)    create a DistributedTensor that shards on dim 0, from a local torch.Tensor   local_tensor = torch.randn((8, 8), requires_grad=True)   rowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)    reshard the current rowise tensor to a colwise tensor or replicate tensor   colwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)   replica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement) ```  High level User Facing APIs Users can use DistributedTensor tensor constructors directly to create a distributed tensor (i.e. `distributed.ones/empty`), but for existing modules like nn.Linear that are already having torch.Tensor as parameters, how to make them distributed parameters? We offer a way to directly distribute a torch.Tensor and a module level APIs to directly distribute the module parameters. Below is the high level API we introduce: ```python def distribute_tensor(tensor: torch.Tensor, device_mesh: DeviceMesh=None, placements: List[Placement]=None):       '''     distribute the tensor according to device_mesh and placements, `tensor` could be a ""meta"" tensor.       '''   def distribute_module(       module: nn.Module,       device_mesh: DeviceMesh=None,       partition_fn: Callable[[str, nn.Module, DeviceMesh], ...]=None,     input_fn: Callable[...., None]=None,       output_fn: Callable[...., None]=None,   ):       '''       This function converts all module parameters to distributed tensor parameters according to the `partition_fn` specified.       It could also control the input/output of the module by specifying the `input_fn` and `output_fn`.      ''' ```  High level API examples: ```python from torch.distributed._tensor import distribute_module def MyModule(nn.Module):       def __init__(self):           super.__init__()           self.fc1 = nn.Linear(8, 8)           self.fc2 = nn.Linear(8, 8)           self.relu = nn.ReLU()       def forward(self, input):           return self.relu(self.fc1(input) + self.fc2(input))   mesh = init_device_mesh(device_type=""cuda"", [[0, 1], [2, 3]]) def shard_params(mod_name, mod, mesh):       rowwise_placement = [Shard(0)]     def to_dist_tensor(t): return distribute_tensor(t, mesh, rowwise_placement)       mod._apply(to_dist_tensor)   sharded_module = distribute_module(model, device_mesh, partition_fn=shard_params)   def shard_fc(mod_name, mod, mesh):       rowwise_placement = [Shard(0)]     if mod_name == ""fc1"":           mod.weight = torch.nn.Parameter(distribute_tensor(mod.weight, mesh, rowwise_placement)) sharded_module = distribute_module(model, device_mesh, partition_fn=shard_fc) ```  Compiler and DistributedTensor DistributedTensor provides efficient solutions for cases like Tensor Parallelism. But when using the DTensor's replication in a data parallel fashion, it might become observably slow compared to our existing solutions like DDP/FSDP. This is mainly because existing solutions like DDP/FSDP could have the global view of entire model architecture, thus could optimize for data parallel specifically, i.e. collective fusion and computation overlap, etc. DistributedTensor itself is only a Tensorlike object and only knows its local computation operation, it does not know the subsequent operations that happened afterwards. In order to make the performance on par when using DistributedTensor directly to do data parallel training, DistributedTensor also needs the global view to do things like communication optimization. We are exploring a compiler based solution accompanied with DistributedTensor so that we could run optimizations on top of it, which will be shared later.  Related Works This work is mainly inspired by GSPMD, Oneflow and TF’s DTensor. All of these three works use a single “distributed tensor” concept for both replication and sharding, and the solutions could enable users to build up their distributed training program in a uniform SPMD programming model. Specifically: GSPMD:     GSPMD is now the fundamental component of JAX/TensorFlow distributed training and enables various optimizations with the XLA compiler to allow users to train their models efficiently in a large scale setting.     Fundamentally, GSPMD have three types of sharding strategies within a tensor: “tiled”, “replicated”, “partially tiled” to represent sharding and replication.    At the core of GSPMD Partitioner, it utilizes the XLA compiler to do advanced optimizations, i.e. sharding propagation and compiler based fusion.     XLA mark_sharding API: PyTorch XLA’s mark_sharding API uses XLAShardedTensor abstraction (i.e. sharding specs) in PyTorch/XLA. Under the hood XLAShardedTensor is utilizing the GPSMD partitioner to enable SPMD style training on TPU. OneFlow GlobalTensor:    OneFlow is building up their own solution of the “GlobalTensor” concept, which is a variant form of GSPMD sharding, allowing users to explore different parallel strategies with GlobalTensor.    OneFlow also has three types of tensor, but they are slightly different from GSPMD: “split”, “broadcast”, and “partial sum”. They don’t use partially tiled and instead have a concept of partial sum to partition the values. TensorFlow DTensor:    DTensor Concepts is an extension of TensorFlow synchronous distributed training. its sharding API, supported features and its compilation passes with MLIR.    DTensor also allows sharding and replication on an nd mesh like device network.    DTensor implements MLIR passes to do propagation and operator implementations. There are also several cutting edge research fields that embeds tensor sharding as part of the system, i.e. MegatronLM for tensor parallelism on Transformer based models. DeepSpeed for training large scale models with different optimization techniques on top of tensor sharding.  Alternatives In PyTorch, we have existing ShardedTensor work in the prototype stage, which introduces basic PyTorch sharding primitives as our Tensor Parallelism solution. But ShardedTensor only has tensor sharding support, which makes it hard to be used by users to describe other data distributions strategies like replication or replication + sharding. As a distributed system developer who wants to explore more parallelism patterns, it’s crucial to have a basic building block that describes the data distribution in a uniform way. This DistributedTensor RFC aims at solving this and provide a fundamental abstraction for distributed training.  Additional context We are gathering early feedbacks about this proposal. We have also posted this RFC to the devdiscuss forum, please feel free to comment directly in this issue or in the forum post. To see a complete design doc with additional details about this proposal, please refer to this doc  ",2022-11-10T19:27:21Z,oncall: distributed module: dtensor,open,33,47,https://github.com/pytorch/pytorch/issues/88838,"Wondering if there are applications here for inmemory sharded _datasets_, a usecase not mentioned above. Example: such that you could randomly access into a massive XTB dataset from any node (assuming a solid IB network). Example: if a satellite image covering the entire world is 8TB  if you need to randomly index into this, instead of obtaining multiple jpeg crops from your disk, i could see this implemented as ona huge sharded tensor, preloaded into the nodes, and any node who needs a slice of this can transparently index into this worldtensor. Would eliminate disk access and use rdma  good for throughput and latency.","> Wondering if there are applications here for inmemory sharded _datasets_, a usecase not mentioned above. Example: such that you could randomly access into a massive XTB dataset from any node (assuming a solid IB network). Example: if a satellite image covering the entire world is 8TB  if you need to randomly index into this, instead of obtaining multiple jpeg crops from your disk, i could see this implemented as ona huge sharded tensor, preloaded into the nodes, and any node who needs a slice of this can transparently index into this worldtensor. Would eliminate disk access and use rdma  good for throughput and latency.  Yeah I think this is entirely possible, for indexing into a massive in memory data set which have the same length per imagine, it looks to me a sharded embedding look up on images, which can be easily implemented with DTensor, underlying we will call into necessary collectives to get a imagine slice according to the index.","Nit: why post here instead of https://github.com/pytorch/rfcs? Edit: never mind, found https://github.com/pytorch/rfcs/pull/44 :)","How much control will users have over the sharding and materialization? E.g., to get decent throughput for parameter sharding or CPU offloading, we'll need some sort of prefetching mechanism. Also, what's the story for saving + loading large, sharded models? Torch Snapshot?","I wanna ask when will models such as DeviceMesh, Shard, distribute_tensor can be used? Will documentation be provided for these models? I want to use the tensordot function to realize the largescale tensor contraction of distributed storage, so how can I code to realize it?","> How much control will users have over the sharding and materialization? E.g., to get decent throughput for parameter sharding or CPU offloading, we'll need some sort of prefetching mechanism.  user will have control about how to implement a operator in a distributed fashion, if you want to apply prefetching mechanism on top, I think it would just running this operator on DTensor with a separate CUDA stream (if it's cuda) or do async offloading just like you do with torch.Tensor > Also, what's the story for saving + loading large, sharded models? Torch Snapshot? To save/load large sharded models, we are working on releasing `torch.distributed.checkpoint` to beta for large scale model save/load as part of next release. Right now the functionality is there, but it's under `torch.distributed._shard.checkpoint` but we plan to make it a dedicated subpackage under torch.distributed https://github.com/pytorch/pytorch/pull/88698  ","> I wanna ask when will models such as DeviceMesh, Shard, distribute_tensor can be used? Will documentation be provided for these models? I want to use the tensordot function to realize the largescale tensor contraction of distributed storage, so how can I code to realize it?  We are working on landing it to pytorch soon, you can subscribe to this stack of PRs https://github.com/pytorch/pytorch/pull/88180 and once those are merged (hopefully this week or next week), you should be able to use it immediately in master or nightly build 12 days later. We plan to release DTensor as a prototype feature in the next pytorch release, which we might add more documentation in the code APIs directly, but not in official https://docs.pytorch.org/ yet. It will be added to the doc website when it releases beta. Note that we will release `DeviceMesh` as a beta feature in the next release, and will add documentations on it and demonstrate how to use it :) Feel free to try it out and submit issues to pytorch/pytorch directly or even contributing once it's there :)","Thanks for the great work! I have several questions. 1. How would existing data loader work with the DTensor? Would some DTensoraware wrapper and sampler be needed here? 2. What is needed to make a custom op support DTensor?  3. Have you considered a DTensor abstraction for heterogeneous devices, e.g., some data on CPU and some data on GPU?"," Thanks for all the questions! > 1. How would existing data loader work with the DTensor? Would some DTensoraware wrapper and sampler be needed here? We haven't plan on letting data loader directly generate DTensor, but for data coming from data loader, you can easily mark the data on each rank as a DTensor by using `DTensor.from_local(tensor, mesh, placements)` (i.e. usually data loader would generate a minibatch data on each rank, and we can create a DTensor that shards on dim 0 (minibatch dim). > 2. What is needed to make a custom op support DTensor? Do you mean a custom registered op in pytorch? In order for a custom registered op to work in DTensor, you might need to register a custom rule to describe how to perform ""distributed"" computation for this op, we will open up a API for user to write the rules. > 3. Have you considered a DTensor abstraction for heterogeneous devices, e.g., some data on CPU and some data on GPU? We have been following the concept of torch.Tensor whenever possible in the design of DTensor, which means a Tensor could either be on CPU or GPU, but it can't be partially be on CPU and partially be on GPU, this is currently also true for DTensor. Could you share more details about your use case of heterogeneous devices on a tensor level abstraction?","> We have been following the concept of torch.Tensor whenever possible in the design of DTensor, which means a Tensor could either be on CPU or GPU, but it can't be partially be on CPU and partially be on GPU, this is currently also true for DTensor. Could you share more details about your use case of heterogeneous devices on a tensor level abstraction? One example is to run inference or training on both CPU and GPU and apply various parallelism schemes just as how it is support on either CPU or GPU via DTensor. With distributed tensor abstraction, users can run such heterogeneous compute without the need of changing the original models.",">  Thanks for all the questions! > > 1. How would existing data loader work with the DTensor? Would some DTensoraware wrapper and sampler be needed here? >  > We haven't plan on letting data loader directly generate DTensor, but for data coming from data loader, you can easily mark the data on each rank as a DTensor by using `DTensor.from_local(tensor, mesh, placements)` (i.e. usually data loader would generate a minibatch data on each rank, and we can create a DTensor that shards on dim 0 (minibatch dim). One nice feature (if not already planned) would be to have a zerocopy API that allows data tiles/fragments loaded into separate training processes to be fused into a single, larger DTensor. This could be useful for large images (e.g., medical domains), long sequence lengths in NLP, etc. ","In the spirit of helping with the design process, here are some more concerns around prefetching / user control. > user will have control about how to implement a operator in a distributed fashion, if you want to apply prefetching mechanism on top I could be misunderstanding, but I don't think this solves the problem, for two reasons. First, running an op asynchronously isn't the same as prefetching. Let's say I have this code: ```python def forward(self, X):   for layer in self.layers:     X = layer(X)   return X ``` Making any given layer's op asynchronous doesn't do anything. What I need is more like: ```python def forward(self, X):   for i, layer enumerate(self.layers):     wait_prefetch_complete(layer)     prefetch_layer(self.layers[i + 1])   kick off communication     X = layer(X)   local computation     free_layer(layer)   return X ``` And if we want to get fancy, I might want to:  coalesce prefetches for various tensors,  take into account the weird traversal order induced by gradient checkpointing,  handle edge cases where a user wants to measure tensor norms (in fwd, bwd, or elsewhere!),  and more. The second problem is that I don't want a custom op. I _just_ want my tensors prefetched + freed after use. I don't want to manually add prefetching logic for every op in my program. Anyway, hope that's helpful!","> One example is to run inference or training on both CPU and GPU and apply various parallelism schemes just as how it is support on either CPU or GPU via DTensor. With distributed tensor abstraction, users can run such heterogeneous compute without the need of changing the original models.  are you referring to the case when the whole layer can't feed into GPU memory and we need to do checkpoint offloading to CPU for some DTensors in the middle? If so, I think this can be achieved by composing activation checkpointing/offloading with DTensor, just like it current works with torch.Tensor","> One nice feature (if not already planned) would be to have a zerocopy API that allows data tiles/fragments loaded into separate training processes to be fused into a single, larger DTensor. This could be useful for large images (e.g., medical domains), long sequence lengths in NLP, etc.  Yeah I think we should be able to do this by using things like `DTensor.from_local`, from local does not do any copy of local torch.Tensors, it actually just create a DTensor wrapper that holds all those actual data tiles/fragments that are loaded into separate training processes.","> ... >  > * coalesce prefetches for various tensors, > * take into account the weird traversal order induced by gradient checkpointing, > * handle edge cases where a user wants to measure tensor norms (in fwd, bwd, or elsewhere!), > * and more. >  > The second problem is that I don't want a custom op. I _just_ want my tensors prefetched + freed after use. I don't want to manually add prefetching logic for every op in my program. >  > Anyway, hope that's helpful!  Thanks for clarifying the detailed use case! It looks like you want to use DTensor to prefetch whole layer with sharded DTensor parameters and achieve behaviors similar to FSDP. This is definitely sth DTensor want to do, and we should be able to support this with async `redistribute` ability, then pretch the sharded DTensor on a separate stream. coalescing directly with DTensor needs some careful design, but it could be achievable with sth like fully shard on dim 0 and then cat all the sharded DTensor on other dims, then async redistribute the big DTensor (some rough thoughts on it, we can discuss further about the exact API you want to see)",">  are you referring to the case when the whole layer can't feed into GPU memory and we need to do checkpoint offloading to CPU for some DTensors in the middle? If so, I think this can be achieved by composing activation checkpointing/offloading with DTensor, just like it current works with torch.Tensor Oh, I was referring to a case where both CPU and GPU are involved in computation.","> Oh, I was referring to a case where both CPU and GPU are involved in computation.  could you share a small code example about how both CPU and GPU are involved in computation and what roughly you would like DTensor to help here? Thanks!","> > Oh, I was referring to a case where both CPU and GPU are involved in computation. >  >  could you share a small code example about how both CPU and GPU are involved in computation and what roughly you would like DTensor to help here? Thanks! Sure. The rough idea would be something like below based on the proposed API in this RFC. ```python  construct a device mesh on cpu and cuda:0 device_mesh = DeviceMesh([""cpu"", ""cuda:0""])    shard on the first dim with a ratio (1:3) on cpu and cuda placement=[Shard(0, [1, 2])]   distributed tensor returned will be sharded across the dimension specified in placements   input = distributed.rand((128, 3, 224, 224), device_mesh=device_mesh, placements=placement)   output = resnet50(input) ```","just curious, is there a stable version of pytorch support these DTensor and Shard modules?  I tried to `from torch.distributed import DeviceMesh, Shard, distribute_tensor ` it reports ModuleNotFoundError. I use `torch1.13.1+cu117`"," I think 1.13.1 does not have DTensor implementation yet (just checked the release source code), it lives in nightly build and master if you build from source, see this folder. DTensor would be in prototype release with the upcoming PyTorch 2.0 release.",hi  lots of people use databricks and run a cluster. does DTensor also work with databricks + pyspark running a cluster with CPUs? is there even a way to run it in that way?,"I have a question？Now， Could DTensor supports treating CPU memory and GPU memory as a unified Memory pool, and then creating a Tensor on this Memory pool? It has  ""tensor.data/tensor.grad/tensor.grad_fn/tensor.backward()"". Based on the following technologies： 1. communications technology： a. Intranode use NVLink/NVSwitch b. Internode use IB c: CPU mem to GPU mem use PCIE I expect a model which runs on a single node can autoimplement model parallel(Tensor parallel/ Pipeline parallel) and data parallel on top of  DTensor + Dist nn.Module.   ",> hi  lots of people use databricks and run a cluster. does DTensor also work with databricks + pyspark running a cluster with CPUs? is there even a way to run it in that way?  Sorry for the late reply. It could possibly run in a cluster with CPUs but probably not together with databricks or pyspark.,> Could DTensor supports treating CPU memory and GPU memory as a unified Memory pool  This might be something we want to explore in long term but not the current focus. The current focus is still around homogenous hardware. > I expect a model which runs on a single node can autoimplement model parallel(Tensor parallel/ Pipeline parallel) and data parallel on top of DTensor + Dist nn.Module.  Yeah this is something we are exploring,"Were there any updates to the API? I'm trying to run the very first example from the README, which appears not to be working (anymore). My full code is as follows: ``` import os import torch from torch.distributed._tensor import DeviceMesh from torch.distributed._tensor import Shard from torch.distributed._tensor import distribute_tensor  Read rank and world size from MPI environment variables rank = int(os.environ[""OMPI_COMM_WORLD_RANK""]) world_size = int(os.environ[""OMPI_COMM_WORLD_SIZE""])  Initiate the process group torch.distributed.init_process_group(backend=""nccl"", world_size=world_size, rank=rank) print(""Hello from rank: {} of {}"".format(rank, world_size))  Device mesh mesh = DeviceMesh(""cuda"", list(range(world_size)))  Allocate some large tensor big_tensor = torch.randn(100000, 88)  Shard this tensor over the mesh by sharding `big_tensor`'s 0th dimension over the 0th dimension of `mesh`. my_dtensor = distribute_tensor(big_tensor, mesh, [Shard(dim=0)]) print(""Tensor shapes: "", big_tensor.shape, `my_dtensor.shape) ``` I'm running this script via `mpirun n 4 python3 hello_dtensor.py`. This results in multiple ranks trying to use the same GPU: ``` Duplicate GPU detected : rank 3 and rank 0 both on CUDA device 100000 ``` If I change the line that creates the tensor to `big_tensor = torch.randn(1000000, 88, device='cuda:{}'.format(rank))`, the code runs but doesn't appear to be doing anything. I.e., the shapes of the big and distributed tensors are the same. Is there something obvious I'm not doing correctly?","> Were there any updates to the API? I'm trying to run the very first example from the README, which appears not to be working (anymore).  Hey sorry for the late reply! We prototype released the APIs and are currently working on enhancing it to push to beta release.  The issue you observed is because when you first do `init_process_group` then construct `DeviceMesh`, we don't automatically set the cuda device for you, because as part of `init_process_group(backend=""nccl"")`, user need to be responsible to set up the cuda device for each process.  However If you just construct DeviceMesh alone without initializing process group, DeviceMesh will automatically set up the devices on each process. I also updated the README to make everything runnable ","How do we handle fused layers with DTensor? For example, in SwiGLU, there are frequently two input matrices in the FF layer. These two matrices are fused into one big matrix. If we apply DTensor Tensor Parallel to shard this big matrix in the output dimension, the sharding will cross the wrong dimensions. I notice that gptfast and torchtitan both avoid using fused layers because of this problem, but that comes with a performance penalty. With megatron layers, I handle the TP sharding and unsharding myself.","> How do we handle fused layers with DTensor? >  Hey , thanks for bringing this up! For fused layers (i.e. fused SwiGLU or even fused QKV), DTensor should be able to represent its sharding layout by implementing a more complicated sharding layout like stride aware sharding, we are currently working on this but haven't enabled it yet, planning to make this work in the upcoming weeks! > I notice that gptfast and torchtitan both avoid using fused layers because of this problem, but that comes with a performance penalty. The fused layers could be more performant, but I feel it might not give too much e2e perf gains to the Transformer model training. Horizontal fusions would give good wins when the CUDA computations are not saturated but usually the compute are saturated already for LLMs. Either way I think it's better if we could support this out of box, but this does involves some work for a more complicated layouts, so stay tuned :) ","On a transformer, with TP + Sequence Parallel (i.e. what torchtitan is doing), we should theoretically be able to hide all the TP comms inside the computation. Is this planned for DTensor? TP is very slow without it.","> On a transformer, with TP + Sequence Parallel (i.e. what torchtitan is doing), we should theoretically be able to hide all the TP comms inside the computation. Is this planned for DTensor? TP is very slow without it.  TP is expected to be slower due to the oncritical path communications happened in each TransformerBlock, that's why TP always need to happen intrahost (where NVLink enable). TP's value is to enable larger model training (i.e. 70B) with hundreds to thousands of GPUs (where FSDP alone would OOM, please take a look at the tutorial we recently added). We are also working on fusing the compute and communication, but NCCL itself aren't performing well with p2p ops so we are working on some alternative solutions (i.e. we found that with NCCL fusing comm and comp is even slower than just exposing the TP comm).  is making a cuda p2p backend that would make comm/computation overlap be performant, and we expect to make no model code change performance improvement with torch.compile, and provide a simple eager API for users who don't want to enable torch.compile.  Wondering how slow you are experiencing? For performance comparison, one common thing I want to mention is that when enabling FSDP + TP and compare the perf with FSDP only, one should bump their local batch size to multiply it by the `model_parallel_degree/tp_degree` so that both setups have the same global batch size."
yi,Cannot install VSIXTorch," 🐛 Describe the bug I am trying to install VSIXTorch and I am getting this error message  ``` 20221110 12:58:47 PM  Microsoft VSIX Installer 20221110 12:58:47 PM   20221110 12:58:47 PM  vsixinstaller.exe version: 20221110 12:58:47 PM  17.4.2118174P4Insege4c88902 20221110 12:58:47 PM   20221110 12:58:47 PM  Command line parameters: 20221110 12:58:47 PM  C:\Program Files (x86)\Microsoft Visual Studio\Installer\resources\app\ServiceHub\Services\Microsoft.VisualStudio.Setup.Service\VSIXInstaller.exe,D:\Downloads\VSIXTorch.vsix 20221110 12:58:47 PM   20221110 12:58:47 PM  Microsoft VSIX Installer 20221110 12:58:47 PM   20221110 12:58:47 PM  Initializing Install... 20221110 12:58:47 PM  Extension Details... 20221110 12:58:47 PM  	Identifier         : VSIXTorch.589304365e144760a3343ab7650104aa 20221110 12:58:47 PM  	Name               : LibTorch Project 20221110 12:58:47 PM  	Author             : Yi Zhang 20221110 12:58:47 PM  	Version            : 0.6.0 20221110 12:58:47 PM  	Description        : LibTorch C++ Project Template 20221110 12:58:47 PM  	Locale             : enUS 20221110 12:58:47 PM  	MoreInfoURL        :  20221110 12:58:47 PM  	InstalledByMSI     : False 20221110 12:58:47 PM  	SupportedFrameworkVersionRange : [4.5,) 20221110 12:58:47 PM   20221110 12:58:47 PM  	SignatureState     : Unsigned 20221110 12:58:47 PM  	Supported Products :  20221110 12:58:47 PM  		Microsoft.VisualStudio.Community 20221110 12:58:47 PM  			Version : [16.0,17.0) 20221110 12:58:47 PM  			ProductArchitecture : x86 20221110 12:58:47 PM   20221110 12:58:47 PM  	References         :  20221110 12:58:47 PM  	Prerequisites      :  20221110 12:58:47 PM  		 20221110 12:58:47 PM  		Identifier   : Microsoft.VisualStudio.Component.CoreEditor 20221110 12:58:47 PM  		Name         : Visual Studio core editor 20221110 12:58:47 PM  		Version      : [16.0,17.0) 20221110 12:58:47 PM   20221110 12:58:47 PM  Signature Details... 20221110 12:58:47 PM  	Extension is not signed. 20221110 12:58:47 PM   20221110 12:58:47 PM  Searching for applicable products... 20221110 12:58:47 PM  Found installed product  Global Location 20221110 12:58:47 PM  Found installed product  Visual Studio Community 2022 20221110 12:58:47 PM  VSIXInstaller.NoApplicableSKUsException: This extension is not installable on any currently installed products.    at VSIXInstaller.ExtensionService.GetInstallableDataImpl(IInstallableExtension extension, String extensionPackParentName, Boolean isRepairSupported, IStateData stateData, IEnumerable`1& skuData)    at VSIXInstaller.ExtensionService.GetInstallableData(String vsixPath, String extensionPackParentName, Boolean isRepairSupported, IStateData stateData, IEnumerable`1& skuData)    at VSIXInstaller.ExtensionPackService.IsExtensionPack(IStateData stateData, Boolean isRepairSupported)    at VSIXInstaller.ExtensionPackService.ExpandExtensionPackToInstall(IStateData stateData, Boolean isRepairSupported)    at VSIXInstaller.App.Initialize(Boolean isRepairSupported)    at VSIXInstaller.App.Initialize()    at System.Threading.Tasks.Task`1.InnerInvoke()    at System.Threading.Tasks.Task.Execute() ``` I have a Visual Studio 2022 installed at the moment  Versions This is for the Torch 1.13 ",2022-11-10T18:01:32Z,module: windows module: cpp triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/88824,cc: aubrecht ," So far, VS 2022 isn't supported for this extension. Please try using VS 2019",   Do you have a plan for when it will be available for VS 2022?,  Can you please give me some context what is the relation of VSIXTorch project to the PyTorch repository? I am mostly currious why this issue is tracked here and not at https://github.com/mszhanyi/VSIXTorch/issues?,VSIXTorch now supports VS2022 https://marketplace.visualstudio.com/items?itemName=YiZhang.LibTorch001
yi,Update lr_scheduler.pyi to match lr_scheduler.py,"Following CC(Publicly expose _LRScheduler to LRScheduler), we should also update the pyi file  ",2022-11-10T16:16:37Z,Merged ciflow/trunk release notes: nn topic: improvements,closed,0,4,https://github.com/pytorch/pytorch/issues/88818, Merge started **The `l` land checks flag is deprecated and no longer needed.** Instead we now automatically add the `ciflow\trunk` label to your PR once it's approved Your change will be merged once all checks on your PR pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Return the attention weights using the Transformer Encoder class. ," 🚀 The feature, motivation and pitch Hey, when using the Transformer Encoder class it would be nice to have ```need_weights``` as an additional variable that can be passed down to ```F.multi_head_attention_forward ```. Right now the feature of returning the attention weights is as far as my understanding goes not accessible from the Transformer class since ```need_weights``` is manually set to false. It would be great if this was possible since one could very easily compute the attention rollout.   Alternatives _No response_  Additional context _No response_ ",2022-11-10T14:13:50Z,module: nn triaged,open,1,6,https://github.com/pytorch/pytorch/issues/88810,"Hi Paul! Thanks for bringing up the issue. I think we have attn_weights for TransformerEncoder planned at some point, but not sure on the eta... ","Hi, im interested in picking up this issue for a class along with a partner. Is there anyway we can get more on information on this  ? "," This is a reasonably substantial change. I'd estimate at least a couple of weeks for back and forth? And since it's an API change there will be some level of scrutiny from the PyTorch team you'll have to wait on. If you're ok with that, then yes a PR would be nice. Relevant bit of code is here: https://github.com/pytorch/pytorch/blob/a6d72f44a4e8b6e9d2e878f30fd8b1d3e1197f0e/torch/nn/modules/transformer.pyL541. Rough steps: 1. Add a need_weights arg to TransformerEncoder 2. Pass it into sa_block, return your attn_weights along with regular output only if needs_weights == True. 3. Make sure to disable fast path when need_weights==True by adding a condition in why_not_sparsity_fast_path. PyTorch transformer has a fast inference path. You could actually make need_weights work for the fast path by hooking up a couple of things, but let's skip it for now. Just default to slow path when we have this arg.  4. Add a test (and probably expand existing tests) for this new arg in https://github.com/pytorch/pytorch/blob/master/test/test_transformers.py","   On a site note: it would also be super nice, if the attention matrix could be hooked such that one also obtains the gradient of the Loss w.r.t the attentionmatrix. I guess if ```need_weights```is set to true, one would still not be able to actually hook the gradient. I now went the long way around by creating a custom layer that inherits from ```TransformerEncoderLayer``` and changed ```_sa_block```. However this is not very clean ","I found that it will take the first component when calculating selfattention. Does it have any specific meaning? According to my understanding, shouldn't it be to keep all the components? In addition, assuming that this operation has practical significance, when `batch_first` is `True` and `False`, the meanings of the components obtained by `[0]` should be different (indicating `batch` and `seq` respectively), so is there any ambiguity? https://github.com/pytorch/pytorch/blob/a6d72f44a4e8b6e9d2e878f30fd8b1d3e1197f0e/torch/nn/modules/transformer.pyL544","  > 4. Make sure to disable fast path when need_weights==True by adding a condition in why_not_sparsity_fast_path. PyTorch transformer has a fast inference path. You could actually make need_weights work for the fast path by hooking up a couple of things, but let's skip it for now. Just default to slow path when we have this arg. The condition should probably also include whether any hooks are attached to submodules that are affected by fast path execution. The fact that hooks seem to be ignored in this case is not really documented and cost me half a night of debugging :sweat_smile:   See exemplary code below. ```python import torch import torch.nn as nn transformer = nn.TransformerEncoder(     nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True),     num_layers=1 ).cuda() def hook(module, module_in, module_out):     print('Hook was called')     return module_out transformer.layers[0].self_attn.register_forward_hook(hook)  Training mode. Fast path is not used; hook is called x = torch.randn(4, 32, 512).cuda() transformer(x)  Evaluation / inference mode. Fast path is used (sometimes?); hook is not called transformer.eval() with torch.no_grad():     transformer(x) ``` "
rag,torch.CharStorage cause abort when called with torch.save," 🐛 Describe the bug `torch.CharStorage` causes abort when called with `torch.save`. ``` import torch input_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  storage = torch.CharStorage(input_data) torch.save(input_data, storage) ``` ``` ...... AttributeError: 'torch.CharStorage' object has no attribute 'flush' terminate called after throwing an instance of 'pybind11::error_already_set'   what():  AttributeError: 'torch.CharStorage' object has no attribute 'write' abort ``` This issue is reproducible on torch 1.11.0+cu113 and 1.14.0.dev20221031, but not reproducible on 1.12.1+cu113 (will raise an exception instead of crashes). Therefore this seems like a regression bug.  Versions torch 1.14.0.dev20221031 ",2022-11-10T02:25:17Z,module: serialization triaged,closed,0,4,https://github.com/pytorch/pytorch/issues/88793, , ,"I found there is another similar crash bug related to numpy array serialization: ``` import torch import numpy as np x = torch.randint(10, (3, 3), dtype=torch.float) torch.save(x.cpu().numpy(), x.cpu().numpy()) ``` Outputs on pytorch 1.12.1: ``` AttributeError: 'numpy.ndarray' object has no attribute 'flush' terminate called after throwing an instance of 'pybind11::error_already_set'   what():  AttributeError: 'numpy.ndarray' object has no attribute 'write' abort (core dumped) ``` Dumping numpy arrays will result in abort crash on pytorch 1.11.0 and 1.12.1, but will raise an normal `AttributeError` on nightly (1.14.0.dev20221031). I think such serialization bugs should be resolved in a unified way.","Per the PyTorch docs, the second argument of `torch.save` is supposed to be a filelike object (or string or pathlike): https://pytorch.org/docs/stable/generated/torch.save.html?highlight=torch+savetorch.save Are storages and ndarrays really meant to be filelike objects? Maybe the answer is yes, I just wanted to confirm before investigating further EDIT: Ok I see, the expected behavior is to raise a graceful error instead of abort. I'll fix that"
transformer,[FSDP+dynamo]: forward treats parameter-views as params,"  CC(Enable DDPOptimizer by default in dynamo)  CC([FSDP+dynamo]: forward treats parameterviews as params) Dynamo+AotAutograd needs a way to wrap all tensors (whether inputs or params/buffers) in FakeTensor wrappers, and FSDP's mangling of parameters hides them from this wrapping. This PR unblocks running hf_bert and hf_T5 with FSDP under dynamo, whether using recursive wrapping around transformer layers or only applying FSDP around the whole model.  Perf/memory validation and possibly optimization is the next step. `python benchmarks/dynamo/distributed.py torchbench_model hf_Bert fsdp dynamo aot_eager` `python benchmarks/dynamo/distributed.py torchbench_model hf_Bert fsdp dynamo aot_eager fsdp_wrap` `python benchmarks/dynamo/distributed.py torchbench_model hf_T5 fsdp dynamo aot_eager` `python benchmarks/dynamo/distributed.py torchbench_model hf_T5 fsdp dynamo aot_eager fsdp_wrap` The problem: Dynamo (Actually aot_autograd) trips up with FSDP becuase it must wrap all input tensors in FakeTensor wrappers, and it only knows to wrap graph inputs or named_(parameters, buffers).  FSDP's pre_forward hook sets views (which are not nn.param) into the flatparam as attrs on the module with the same name as the original param, but they will not show up in named_parameters.  in use_orig_params mode, FSDP still deregisters   params during preforward hook, then reregisters them   postforward  during forward (between the hooks), the params are setattr'd   on the module as regular view tensors, not nn.Parameters  note: use_orig_params is the recommended way to use FSDP,   and use_orig_params=False is being deprecated.  So i only consider   use_orig_params=True for this enablement The solution:  adding them to named_buffers is not possible because it interferes   with how FSDP's `_apply` works  since they are not actual nn.parameters, register_parameter will   complain about registering them  simply seting `module._parameters[name] = view` seems to be a viable   workaround, despite being hacky, and FSDP code does modify _parameters   directly already. Note: Manual checkpointing still isn't working with FSDP+dynamo, so that will have to be addressed in a follow up. ",2022-11-09T23:54:14Z,Merged ciflow/trunk release notes: distributed (fsdp) module: dynamo ciflow/inductor,closed,1,4,https://github.com/pytorch/pytorch/issues/88781, merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,I think it would be better to desugar them as inputs because there are multpile assumptions throughout inductor about parameter data_ptrs being static,"> I think it would be better to desugar them as inputs because there are multpile assumptions throughout inductor about parameter data_ptrs being static Commenting for my own learning: Could you explain more what ""desugar them as inputs"" entails? Also, to clarify, FSDP _will_ change the data pointers across iterations. If inductor has those assumptions, then what happens when they are violated?"
yi,Bug fix: make sure `copy_impl` doesn't read out of bounds,Stack from ghstack:  CC(Bug fix: make sure `copy_impl` doesn't read out of bounds) Fixes CC(Out of bounds read in `copy_impl` due to unsafe `fbgemm` APIs).,2022-11-05T16:54:16Z,module: crash open source Merged ciflow/trunk topic: bug fixes topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/88544, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Out of bounds read in `copy_impl` due to unsafe `fbgemm` APIs," 🐛 Describe the bug  Repro ```py !/usr/bin/env python3 import torch src = torch.empty((0, 2, 3), dtype=torch.float16)   0 elems self = torch.empty(1, dtype=torch.complex64)   1 elem print(src.numel()) print(self.numel()) self.real = src  XXX: trigger bug ```  Details This repro will call `copy_impl`, which has a number of calls to `fbgemm::Float16ToFloat_simd`. On my machine, it goes to: ```cpp       } else {         auto in_data = reinterpret_cast(             src.data_ptr());         auto* output_ptr = self.data_ptr();         if (self.numel() < at::internal::GRAIN_SIZE) {           fbgemm::Float16ToFloat_simd(in_data, output_ptr, self.numel());  // XXX: fails ``` Which dispatches to: ```cpp void Float16ToFloat_simd(const float16* src, float* dst, size_t size) {   // Run time CPU detection   if (cpuinfo_initialize()) {     if (fbgemmHasAvx512Support()) {       Float16ToFloat_avx512(src, dst, size);     } else if (fbgemmHasAvx2Support()) {       Float16ToFloat_avx2(src, dst, size);     } else {       Float16ToFloat_ref(src, dst, size);  // XXX: goes here on my machine       return;     }   } else {     throw std::runtime_error(""Failed to initialize cpuinfo!"");   } } ``` And finally: ```cpp void Float16ToFloat_ref(const float16* src, float* dst, size_t size) {   for (size_t i = 0; i < size; i++) {     dst[i] = cpu_half2float(src[i]);  // XXX: bug: assumes that src has enough elems to read from   } } ```  Mitigation Pass src size to fbgemm functions and check that dst has enough space before calling them.  Notes I'm currently _not planning_ to verify other `fbgemm` APIs or different code paths in `copy_impl`.  Versions master (dc00bb51b8d370bf3891f0edb2c6e0c2914e329a) ",2022-11-05T16:31:37Z,high priority module: crash module: cpu triaged module: complex bug,closed,0,4,https://github.com/pytorch/pytorch/issues/88543, to investigate why we hit this code path in the first place.,"Well, this feels like classic nullpointer deref: ``` $ lldb  python c ""import torch; src = torch.empty((0, 2, 3), dtype=torch.float16); self=torch.empty(1, dtype=torch.complex64); self.real = src"" (lldb) target create ""python"" Current executable set to 'python' (x86_64). (lldb) settings set  target.runargs  ""c"" ""import torch; src = torch.empty((0, 2, 3), dtype=torch.float16); self=torch.empty(1, dtype=torch.complex64); self.real = src"" (lldb) r Process 66194 launched: '/home/nshulga/miniconda3/bin/python' (x86_64) Process 66194 stopped * thread CC(Matrix multiplication operator), name = 'python', stop reason = signal SIGSEGV: invalid address (fault address: 0x0)     frame CC(未找到相关数据): 0x00007fffbf324660 libtorch_cpu.so`fbgemm::Float16ToFloat_ref(unsigned short const*, float*, unsigned long) + 16 libtorch_cpu.so`fbgemm::Float16ToFloat_ref: >  0x7fffbf324660 : movzwl (%rdi,%r8,2), %eax     0x7fffbf324665 : movl   %eax, %ecx     0x7fffbf324667 : shrw   $0xf, %cx     0x7fffbf32466b : movzwl %cx, %r9d ```"," I think it's more general: an out of bounds read, see the snippet above. in your example, it fails when trying to access address zero because memory is not mapped, but maybe with some other configuration it could just read out of bounds and not fail at all. i haven't looked very deep.","Note: I'm working on a fix for this segfault now. I'll just fix the crash and we can have another discussion later about how backends should handle stuff like broadcasting because it seems inconsistent (e.g., this fbgemm code and tensor iterator behave differently, which seems like a bug to me). UPD: Since this fbgemm code doesn't support broadcasting, we dispatch to TensorIterator unless the shapes are the same. Otherwise, it leads to inconsistencies (see https://github.com/pytorch/pytorch/pull/88544discussion_r1022299567)."
transformer,PyTorch is not compatible with Python 3.11.0," 🐛 Describe the bug import argparse import yaml import pandas as pd import torch from TorchCRF import CRF import transformers from data import Dataset from engines import train_fn import warnings warnings.filterwarnings(""ignore"") parser = argparse.ArgumentParser() parser.add_argument(""data_file"", type=str) parser.add_argument(""hyps_file"", type=str) args = parser.parse_args() data_file = yaml.load(open(args.data_file), Loader=yaml.FullLoader) hyps_file = yaml.load(open(args.hyps_file), Loader=yaml.FullLoader) train_loader = torch.utils.data.DataLoader(     Dataset(         df=pd.read_csv(data_file[""train_df_path""]),         tag_names=data_file[""tag_names""],         tokenizer=transformers.AutoTokenizer.from_pretrained(hyps_file[""encoder""], use_fast=False),     ),     num_workers=hyps_file[""num_workers""],     batch_size=hyps_file[""batch_size""],     shuffle=True, ) val_loader = torch.utils.data.DataLoader(     Dataset(         df=pd.read_csv(data_file[""val_df_path""]),         tag_names=data_file[""tag_names""],         tokenizer=transformers.AutoTokenizer.from_pretrained(hyps_file[""encoder""], use_fast=False),     ),     num_workers=hyps_file[""num_workers""],     batch_size=hyps_file[""batch_size""] * 2, ) loaders = {     ""train"": train_loader,     ""val"": val_loader, } model = transformers.RobertaForTokenClassification.from_pretrained(hyps_file[""encoder""],                                                                    num_labels=data_file[""num_tags""]) if hyps_file[""use_crf""]:     criterion = CRF(num_tags=data_file[""num_tags""], batch_first=True) else:     criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=float(hyps_file[""lr""])) train_fn(     loaders, model, torch.device(hyps_file[""device""]), hyps_file[""device_ids""],     criterion,     optimizer,     epochs=hyps_file[""epochs""],     ckp_path=""../ckps/{}.pt"".format(hyps_file[""encoder""].split(""/"")[1]), )  Versions D:\>python collect_env.py Collecting environment information... PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Pro GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: N/A CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: N/A GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 SUPER Nvidia driver version: 512.77 cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7\bin\cudnn_ops_train64_8.dll HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A Versions of relevant libraries: [pip3] numpy==1.23.4 [conda] Could not collect D:\>",2022-11-04T20:27:12Z,,closed,1,2,https://github.com/pytorch/pytorch/issues/88518,Sample project https://github.com/donhuvy/vy_thesis . Related   https://stackoverflow.com/questions/74322227/packagerequirementstensorflow2100transformers4221torchcrf1  https://stackoverflow.com/questions/74322491/ispytorchversion1121compatiblewithpython3110,There's already an open issue tracking this  CC(Support Python 3.11). Closing this one in favour of that other one.
llm,Generalize gesvdjBatched to run whith full_matrices==false,"  CC(Generalize gesvdjBatched to run whith full_matrices==false) As brought up in  CC(SVD with `full_matrices=False` slower than `full_matrices=True` & `full_matrices=False` on GPU 500x slower than CPU)issuecomment1268296036, our heuristic for which SVD backend to choose was not great in some cases. The case in which there could be some improvements is when we have a large batch of very small nonsquare matrices. This PR, adapts the calling code to gesvdj by creating two temporary square buffers to allow to call gesvdjBatched, and then copies back the result into the output buffers. We then modify the heuristic that chooses between gesvdj and gesvdjBatched. ://github.com/pytorch/pytorch/issues/86234",2022-11-04T16:41:33Z,module: performance open source module: linear algebra Merged ciflow/trunk release notes: performance_as_product topic: performance,closed,2,7,https://github.com/pytorch/pytorch/issues/88502,The benchmarks show that the batched version is always faster: These are run on CUDA 11.7.1 on an RTX 2060 ``` [ SVD ]                               34260 Times are in microseconds (us). ```, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `Linear Algebra`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," I deleted a test that was testing the behaviour of the `svd` function when given `NaN`s. This behaviour is not defined in the docs, and it's not consistent between backends or drivers, so I don't think we should test it. For example, this new backend does not raise an error when an input with nans is provided, it simply returns `S = [nan, nan, ...]`.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,Support src_mask and src_key_padding_mask for Better Transformer,"Fixes T135842750 (followup for CC(Enable `src_mask` in fast path of `TransformerEncoderLayer `))  Description At present, having both `src_key_padding_mask` and `src_mask` at the same time is not supported on the fastpath in Transformer and MultiHead Attention. This PR enables using both masks on the fastpath on CPU and GPU: if both masks are passed, we merge them into a 4D mask in Python and change mask type to 2 before passing downstream. Downstream processing in native code is not changed, as it already supports 4D mask. Indeed, it is done depending on the device:  on CUDA, by `SoftMax.cu::masked_softmax_cuda`. When mask type is 2, it calls either `dispatch_softmax_forward` > `softmax_warp_forward` or `at::softmax` (depending on the input size). In both cases 4D mask is supported.  on CPU, by `SoftMax.cpp::masked_softmax_cpp`. It calls `hosted_softmax` which supports 4D mask.  Tests  Extended `test_mask_check_fastpath` to check that fast path is indeed taken in Transformer when two masks are passed  Added `test_multihead_self_attn_two_masks_fast_path_mock` to check that fast path is taken in MHA when two masks are passed  Added `test_multihead_self_attn_two_masks_fast_path` to check that fast and slow paths give the same result when two masks are passed in MHA  `test_masked_softmax_mask_types` now covers mask type 2  `test_transformerencoderlayer_fast_path` (CPU smoke test) is expanded to the case of both masks provided simultaneously  `test_masked_softmax_devices_parity` checks that mask type 2 is accepted by CPU and CUDA paths ",2022-11-04T12:47:27Z,Merged ciflow/trunk release notes: nn,closed,0,14,https://github.com/pytorch/pytorch/issues/88488," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / cuda11.6py3.10gcc7sm86 / test (default, 2, 4, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," The merge above failed, but the errors seem to be unrelated to the PR:  ""ERROR ENCOUNTERED WHEN UPLOADING TO SCRIBE"",  ""KeyError: 'jobs'"" in ""Get workflow job id""   NVIDIA kernel loading error. Could you have a look and say if those are indeed known infra failures?", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `supporttwomasksbettertransformer` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout supporttwomasksbettertransformer && git pull rebase`)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Add use_lazy_shape flag to GenLazyIr class,"Add use_lazy_shape flag to GenLazyIr class to allow XLA to use its custom shape class. The default value is kept to use lazy shape, so this PR does not introduce any new behaviors.  PyTorch/XLA companion PR: https://github.com/pytorch/xla/pull/4111",2022-11-03T19:42:29Z,open source Merged ciflow/trunk topic: not user facing,closed,1,6,https://github.com/pytorch/pytorch/issues/88444,", this is a small followup to https://github.com/pytorch/pytorch/pull/87823. Adds a new flag `use_lazy_shape` to LazyIr codgen, so PyTorch/XLA codegen doesn't have to override the entire `GenLazyIr.gen()` function. ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 additional jobs have failed, first few of them are: trunk Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,TorchDynamo: Add convolution binary(inplace) fusion for cpu in inference mode,  CC(Take input striding for conv fusion op  based on eager output)  CC(TorchDynamo: skip convolution fusion when convolution's padding is string)  CC(Fake Tensor For (ConvFusion) Propagation)  CC(TorchDynamo: Add convolution binary+unary fusion for cpu in inference mode)  CC(TorchDynamo: Add convolution binary(inplace) fusion for cpu in inference mode) ,2022-11-03T04:51:50Z,open source Merged ciflow/trunk module: inductor ciflow/inductor release notes: dynamo,closed,0,3,https://github.com/pytorch/pytorch/issues/88403," , please help review this PR. Thanks! ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Test opinfo's aten op coverage,  CC(Test opinfo's aten op coverage),2022-11-02T20:33:17Z,Stale topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/88358,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,Modifying lazy backend code for SPMD,Fixes ISSUE_NUMBER,2022-11-02T17:54:55Z,open source,closed,0,1,https://github.com/pytorch/pytorch/issues/88340," :x:  login: g . The commit (d599b470c1e6f931e6eeb80f420c32e18b585d62) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket."
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-02T15:55:13Z,Merged ciflow/trunk release notes: vulkan,closed,0,9,https://github.com/pytorch/pytorch/issues/88324, merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge f," merge f ""unrelated cuda job failures"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ", Merge failed **Reason**: This PR has internal changes and must be landed via Phabricator Details for Dev Infra team Raised by workflow job ," merge f 'Landed internally' (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
llm,[dynamo] `VariableTracker.call_method` requires a name,"Summary: as title Test Plan: Before: N2743445, After: N2748186.  Note there's a new error, but at least we got past the easy one. Differential Revision: D40938415 ",2022-11-02T14:29:00Z,fb-exported Merged ciflow/trunk release notes: dataloader module: dynamo ciflow/inductor,closed,0,16,https://github.com/pytorch/pytorch/issues/88311,This pull request was **exported** from Phabricator. Differential Revision: D40938415,This pull request was **exported** from Phabricator. Differential Revision: D40938415, rebase, successfully started a rebase job. Check the current status here,"Tried to rebase and push PR CC([dynamo] `VariableTracker.call_method` requires a name), but it was already up to date", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Move the OpInfo same-storage error to the autograd test,"  CC(Move the OpInfo samestorage error to the autograd test) This check was previously located at the `non_contiguous` test (quite and odd location). Even more, at https://github.com/pytorch/pytorch/pull/86378discussion_r993658395, Kshiteej found that this assert was not doing anything really. We move it to the autograd test and make it a proper `self.assert`. We also disallow returning 1tuples from sample_input functions, as they were breaking this assert.",2022-11-02T13:02:11Z,open source Merged module: testing ciflow/trunk topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/88306,">We also disallow returning 1tuples from sample_input functions, as they were breaking this assert. No objection to the change, but can you explain how it breaks this check?","That's a good point.  I tried reproducing this issue and I was not able to, so I'm not sure now what was going there really. I'm leaving them all as generators for simplicity and consistency of the code though.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. You can rebase by leaving the following comment on this PR: ` rebase` Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / cuda11.6py3.10gcc7sm86 / test (default, 1, 4, linux.g5.4xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job "," merge f ""flaky CI tests"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-02T03:29:13Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88285
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-02T00:12:36Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88274
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-01T23:49:46Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88270
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-01T22:00:28Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88256
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-01T19:20:30Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88240
rag,[codegen] using TORCH_LIBRARY_FRAGMENT for some namespaces,"  CC([codegen] using TORCH_LIBRARY_FRAGMENT for some namespaces) Summary: Sometimes we want to extend an existing custom namespace library, instead of creating a new one, but we don't have a namespace config right now, so we hardcode some custom libraries defined in pytorch today, i.e. quantized and quantized_decomposed Test Plan: ci Reviewers: Subscribers: Tasks: Tags:",2022-11-01T18:58:48Z,Merged ciflow/trunk topic: not user facing,closed,0,12,https://github.com/pytorch/pytorch/issues/88229,Can you fix the failing test?,"> Can you fix the failing test? done, please take a look again","IMO, this should be configurable on a per namespace basis. If custom namespace lives entitely in yaml it is reasonable to want it to be closed.","> IMO, this should be configurable on a per namespace basis. If custom namespace lives entitely in yaml it is reasonable to want it to be closed. I think one way to support this is to add a tag for a function. For example, ```  func: quantized::add.out(Tensor a, ..., *, Tensor(a!) out) > Tensor(a!)   variants: function   dispatch:     CPU: quantized_add_out   tags: fragment ``` This means that we are expecting a `quantized` library already exist in PyTorch (`quantized/library.cpp`) and just extending it by adding a new `add.out` operator.","> > IMO, this should be configurable on a per namespace basis. If custom namespace lives entitely in yaml it is reasonable to want it to be closed. >  > I think one way to support this is to add a tag for a function. For example, >  > ``` >  func: quantized::add.out(Tensor a, ..., *, Tensor(a!) out) > Tensor(a!) >   variants: function >   dispatch: >     CPU: quantized_add_out >   tags: fragment > ``` >  > This means that we are expecting a `quantized` library already exist in PyTorch (`quantized/library.cpp`) and just extending it by adding a new `add.out` operator. should this config be library specific instead of operator specific?",The config should not live on an op. Should be somewhere library info., merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / linuxbioniccuda11.7py3.10gcc7 / test (distributed, 1, 3, linux.8xlarge.nvidia.gpu) Details for Dev Infra team Raised by workflow job ", merge f," merge f ""failing tests unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-11-01T18:25:05Z,release notes: vulkan,closed,0,0,https://github.com/pytorch/pytorch/issues/88225
yi,Add missing args to DDP constructor in distributed.pyi,Summary: As title. And remove all unnecessary `pyrefixme` for the unknown arg in callsite. Test Plan: CI Differential Revision: D40874013,2022-11-01T16:30:54Z,fb-exported Merged ciflow/trunk,closed,0,6,https://github.com/pytorch/pytorch/issues/88209,This pull request was **exported** from Phabricator. Differential Revision: D40874013,This pull request was **exported** from Phabricator. Differential Revision: D40874013,This pull request was **exported** from Phabricator. Differential Revision: D40874013,This pull request was **exported** from Phabricator. Differential Revision: D40874013, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,Update functionalization metadata more eagerly. *_scatter ops should preserve input stride/storage_offset,"Two major changes in this PR: (1) outputs of `*_scatter` ops will now reflect the same storage size, stride and storage_offset of their inputs. See more details at https://github.com/pytorch/pytorch/pull/87610/filesr1007264456. This fixes some silent correctness issues with functionalization advertising incorrect strides/storage_offsets on some tensors. (2) That actually isn't enough: We need to ensure that any time someone calls `.stride()` or `.storage_offset()` on a tensor, its metadata is not stale. To fix this, I updated all of the custom metadata calls on `FunctionalTensorWrapper` to perform a sync first if metadata is out of date. As a motivating example, consider this code: ``` a = torch.ones(2, 2) a_diag = torch.diagonal(a, 0, 1) a_diag.add_(1) ``` Functionalization will temporarily turn this code into: ``` a = torch.ones(2, 2) a_diag = torch.diagonal(a, 0, 1)  a_diag_updated has incorrect metadata! The output of add() is always a contiguous, densely packed tensor.  But a_diag_updated's metadata should advertise as (properly) not being contiguous! it has different strides a_diag_updated = a_diag.add(1) ``` As mentioned in the above comment, this isn't 100% correct  `a_diag_updated` has different metadata than `a_diag`. If user code queries the metadata on `a_diag` at this point, we'll return incorrect metadata. The fix: By eagerly running syncing logic before any metadata on `a_diag` is accessed, we'll do the following: ```  reapply the mutation to the base, a a_updated = torch.diagonal_scatter(a, a_diag.add(1))  regenerate a_diag with proper strides, from a_updated a_diag_updated = torch.diagonal(a_updated, 0, 1) ``` This ensures that any user code that calls `a_diag.stride()` or `a_diag.storage_offset()` sees accurate metadata   CC(Update functionalization metadata more eagerly. *_scatter ops should preserve input stride/storage_offset)  CC(dont clone symints, dont clobber symint proxies)",2022-11-01T15:30:05Z,,closed,0,7,https://github.com/pytorch/pytorch/issues/88198," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.","Darn, I'm getting the cursed error: ``` FAILED test/functorch/test_aotdispatch.py::TestEagerFusionOpInfoCPU::test_aot_autograd_symbolic_exhaustive_nn_functional_feature_alpha_dropout_with_train_cpu_float32 FAILED test/functorch/test_aotdispatch.py::TestEagerFusionOpInfoCPU::test_aot_autograd_symbolic_exhaustive_nn_functional_poisson_nll_loss_cpu_float32 ... RuntimeError: s0 is not tracked with proxy for  ``` Maybe, somehow when we regenerate the views for a mutated view, that regenerate tensor isn't getting proxies attached to its sizes/strides","So I am actually not sure why you need to sync_ the metadata on each pass now. Don't you just need to update the inplace tensor metadata; the other tensors not changing their sizes and strides is fine, because whatever they used to have, should still be valid!","Yeah, I think you're right. At first I was worried about other views of the base that also came from slice/select calls, but their metadata should be accurate to begin with  the only time the metadata becomes ""wrong"" is when we convert an inplace op into an outofplace.","There are a bunch of things going on here, but when I ablate everything except the clone preserving strides changes (https://github.com/pytorch/pytorch/pull/89474) it still fails `python test/test_ops_fwd_gradients.py k test_forward_mode_AD_as_strided_scatter_cpu_complex128`","So, I'm a little confused about whether or not this PR is still necessary. The stated justification was to make ""do not use unsafe restriding for subclasses"" PR be able to go in, c.f. https://github.com/pytorch/pytorch/pull/87610/files/401ddeda1d769bedc88a12de332c7357b60e51a4diffc4e35e76279419d9edda979a995412ee494fad3a97d3fb59486a1f0e326d48be But as best as I can tell, this already has gone in to master without trouble. So is this just for a hypothetical situation where functionalization can produce incorrect strides?",we think this is obsolete
yi,misc symintifying fixes,"  CC(Update functionalization metadata more eagerly. *_scatter ops should preserve input stride/storage_offset)  CC(misc symintifying fixes)  CC(dont clone symints, dont clobber symint proxies)",2022-11-01T15:30:00Z,Stale release notes: composability topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/88197,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Add ability to freeze storages inside functionalization,  CC(Add ability to freeze storages inside functionalization)  CC(Inline Alias into FunctionalStorageImpl) Signedoffby: Edward Z. Yang ,2022-10-31T21:42:18Z,Merged ciflow/trunk release notes: composability,closed,0,6,https://github.com/pytorch/pytorch/issues/88141,"> To be clear though  is it important for this to be exposed to python? (doesn't matter too much since it's a private API). In the prototype at https://gist.github.com/ezyang/eace99f43628422139d476bf6919782e the POC is done entirely in Python so yes I need an access point, but I don't intend to make it public.", merge , Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""labels have been added"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here "
rag,Inline Alias into FunctionalStorageImpl,  CC(Add ability to freeze storages inside functionalization)  CC(Inline Alias into FunctionalStorageImpl) Signedoffby: Edward Z. Yang ,2022-10-31T21:42:14Z,ciflow/trunk release notes: composability topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/88140," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork."
chat,Formalize instructions for customers to make request to the DevX team,"**What** Create wiki page that explains to customers how they should file an issue to the DevX team The instructions should explain: 1. Where to file the ticket 2. How to get it added to the DevX board for grooming The idea is to have a link that we'll share with customers whenever they file a support request **Why** We currently get customer requests on multiple different channels (Workplace, Slack, Chat, and more). It's easy for requests to get missed/dropped on there. By ensuring all requests have a ticket attached that we can view from one central location, we can be more responsive about triaging, prioritizing, and implementing customer requests",2022-10-31T18:42:24Z,triaged module: infra,closed,0,1,https://github.com/pytorch/pytorch/issues/88111,"Added general instructions on how to get help, and specific instructions on how to file issues, in https://github.com/pytorch/pytorch/wiki/Gettinghelpasacontributorfileanissue"
yi,Update _distributed_c10d.pyi,Summary: `_distributed_c10d.pyi` is out of sync with the C++ binding. This change updates it. Test Plan: TBD Differential Revision: D40840836,2022-10-31T13:55:43Z,fb-exported Merged ciflow/trunk release notes: distributed (c10d),closed,0,6,https://github.com/pytorch/pytorch/issues/88088,The committers listed above are authorized under a signed CLA.:white_check_mark: login: yhcharles / name: Charlie Yan  (daa002644c5bfbc75eb02ccf7445a89509b5a5a2),This pull request was **exported** from Phabricator. Differential Revision: D40840836,This pull request was **exported** from Phabricator. Differential Revision: D40840836,/easycla, merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,AttributeError: 'BertModel' object has no attribute 'save'," 🐛 Describe the bug Tried to save the model using jit after dynamic quantization using the following code ``` import torch from transformers import AutoConfig,AutoModel model = AutoModel.from_pretrained(""bertbaseuncased"") model_quant = torch.quantization.quantize_dynamic(model,{torch.nn.Linear}, dtype=torch.qint8) torch.jit.save(model_quant, 'quantized.pt') ``` But it shows the below error ``` AttributeError: 'BertModel' object has no attribute 'save' ``` Even the code  ``` torch.jit.save(model, 'quantized.pt') ``` also shows the same error  Versions torch @ https://download.pytorch.org/whl/cu113/torch1.12.1%2Bcu113cp37cp37mlinux_x86_64.whl transformers==4.23.1 Python 3.7.15 ",2022-10-31T11:11:12Z,oncall: quantization,closed,0,8,https://github.com/pytorch/pytorch/issues/88077,"looks like this is an issue with the AutoModel object, the actual quantization didn't have any errors, its just that this custom object doesn't have a save function, did you try the state dict method?","  as you suggested I tried with ```state_dict``` method like this ``` import torch from transformers import AutoConfig,AutoModel model = AutoModel.from_pretrained(""bertbaseuncased"") model_quant = torch.quantization.quantize_dynamic(model,{torch.nn.Linear}, dtype=torch.qint8) quantized_state_dict = model_quant.state_dict() torch.jit.save(quantized_state_dict, 'scriptmodule.pt') ``` but it shows that ```AttributeError: 'collections.OrderedDict' object has no attribute 'save'```","  Seems it is not an issue with Automodel class. Because the Bertmodel also shows the same error  ``` import torch from transformers import BertConfig, BertModel model = BertModel.from_pretrained(""bertbaseuncased"") model_quant = torch.quantization.quantize_dynamic(model,{torch.nn.Linear}, dtype=torch.qint8) torch.jit.save(model_quant, 'quantized.pt') ``` Shows ```AttributeError: 'BertModel' object has no attribute 'save'```",we have a tutorial for Bert: https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html can you try the serialization code there and see if addresses this issue?,">  as you suggested I tried with `state_dict` method like this >  > ``` > import torch > from transformers import AutoConfig,AutoModel > model = AutoModel.from_pretrained(""bertbaseuncased"") > model_quant = torch.quantization.quantize_dynamic(model,{torch.nn.Linear}, dtype=torch.qint8) > quantized_state_dict = model_quant.state_dict() > torch.jit.save(quantized_state_dict, 'scriptmodule.pt') > ``` >  > but it shows that `AttributeError: 'collections.OrderedDict' object has no attribute 'save'` also i think you are mixing up serialization (jit) with saving (state_dict)","  okay, could you please share the code snippet here for dynamic quantization of bert model and its save procedure using jit? ","  Also how can I define the second argument(example input) in torch.trace method ``` import torch import io from transformers import AutoConfig,AutoModel model = AutoModel.from_pretrained(""bertbaseuncased"") model_quant = torch.quantization.quantize_dynamic(model,{torch.nn.Linear}, dtype=torch.qint8) torch.jit.trace(model_quant,) ``` What should be the **values**, **dimension** and **type** of the example input argument that ```torch.trace``` method excepts?","the serialization+save is here: https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.htmlserializethequantizedmodel the example input is a tensor input or tuple of inputs, can find more info here: https://pytorch.org/docs/stable/generated/torch.jit.trace.html?highlight=torch+jit+tracetorch.jit.trace"
llm,Inductor accuracy failure from PegasusForCausalLM (maybe bmm related)," 🐛 Describe the bug Applying https://github.com/pytorch/pytorch/pull/87943 plus some fixes perturbs the output of AOTAutograd sufficiently to cause inductor to generate an accuracy failure. Don't worry, I have a repro >:)  Error logs ``` [20221029 19:06:11,801] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs Traceback (most recent call last):                                                                             File ""/data/users/ezyang/pytorchtmp2/repro.py"", line 71, in                                           assert same_two_models(mod, compiled, args, only_fwd=True), ""Accuracy failed""                            AssertionError: Accuracy failed ```  Minified repro https://gist.github.com/ezyang/23655080188e0d31ae12525ec5c8762c ``` import torch._inductor.overrides import torch from torch import tensor, device import torch.fx as fx from torch._dynamo.testing import rand_strided from math import inf from torch.fx.experimental.proxy_tensor import make_fx import sys  torch version: 1.13.0a0+git3eb2722  torch cuda version: 11.4  torch git version: 3eb27229dd74dd0bea434326c471f16c50e558a4  CUDA Info:   nvcc: NVIDIA (R) Cuda compiler driver   Copyright (c) 20052021 NVIDIA Corporation   Built on Sun_Aug_15_21:14:11_PDT_2021   Cuda compilation tools, release 11.4, V11.4.120   Build cuda_11.4.r11.4/compiler.30300941_0   GPU Hardware Info:   NVIDIA A100PG509200 : 8  from torch.nn import * class Repro(torch.nn.Module):     def __init__(self):         super().__init__()     def forward(self, arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, add, getitem_1, reciprocal):         sub = torch.ops.aten.sub.Tensor(add, getitem_1);  add = getitem_1 = None         mul = torch.ops.aten.mul.Tensor(sub, reciprocal);  sub = reciprocal = None         mul_1 = torch.ops.aten.mul.Tensor(mul, arg0_1);  mul = arg0_1 = None         add_2 = torch.ops.aten.add.Tensor(mul_1, arg1_1);  mul_1 = arg1_1 = None         convert_element_type = torch.ops.prims.convert_element_type.default(add_2, torch.float32);  add_2 = None         permute = torch.ops.aten.permute.default(arg2_1, [1, 0]);  arg2_1 = None         view = torch.ops.aten.view.default(convert_element_type, [128, 1024]);  convert_element_type = None         addmm = torch.ops.aten.addmm.default(arg3_1, view, permute);  arg3_1 = permute = None         view_1 = torch.ops.aten.view.default(addmm, [1, 128, 1024]);  addmm = None         mul_2 = torch.ops.aten.mul.Tensor(view_1, 0.125);  view_1 = None         permute_1 = torch.ops.aten.permute.default(arg4_1, [1, 0]);  arg4_1 = None         addmm_1 = torch.ops.aten.addmm.default(arg5_1, view, permute_1);  arg5_1 = view = permute_1 = None         view_2 = torch.ops.aten.view.default(addmm_1, [1, 128, 1024]);  addmm_1 = None         view_3 = torch.ops.aten.view.default(view_2, [1, 1, 16, 64]);  view_2 = None         permute_2 = torch.ops.aten.permute.default(view_3, [0, 2, 1, 3]);  view_3 = None         clone = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None         view_6 = torch.ops.aten.view.default(mul_2, [1, 128, 16, 64]);  mul_2 = None         permute_5 = torch.ops.aten.permute.default(view_6, [0, 2, 1, 3]);  view_6 = None         clone_2 = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None         view_7 = torch.ops.aten.view.default(clone_2, [16, 1, 64]);  clone_2 = None         view_8 = torch.ops.aten.view.default(clone, [16, 1, 64]);  clone = None         permute_6 = torch.ops.aten.permute.default(view_8, [0, 2, 1]);  view_8 = None         bmm = torch.ops.aten.bmm.default(view_7, permute_6);  view_7 = permute_6 = None         return (bmm,) args = [((1024,), (1,), torch.float32, 'cuda'), ((1024,), (1,), torch.float32, 'cuda'), ((1024, 1024), (1024, 1), torch.float32, 'cuda'), ((1024,), (1,), torch.float32, 'cuda'), ((1024, 1024), (1024, 1), torch.float32, 'cuda'), ((1024,), (1,), torch.float32, 'cuda'), ((1, 128, 1024), (131072, 1024, 1), torch.float32, 'cuda'), ((1, 128, 1), (128, 1, 1), torch.float32, 'cuda'), ((1, 128, 1), (128, 1, 1), torch.float32, 'cuda')] args = [rand_strided(sh, st, dt, dev) for (sh, st, dt, dev) in args] try:    mod = make_fx(Repro().to(device=""cuda""))(*args) except:    sys.exit(0) from torch._inductor.compile_fx import compile_fx_inner from torch._dynamo.debug_utils import same_two_models try:     compiled = compile_fx_inner(mod, args) except:     sys.exit(0) assert same_two_models(mod, compiled, args, only_fwd=True), ""Accuracy failed"" ```",2022-10-30T02:13:59Z,triaged bug,closed,0,6,https://github.com/pytorch/pytorch/issues/93595,"With one change, I triggered DebertaForQuestionAnswering and PegasusForCausalLM, so DebertaForQuestionAnswering might be failing similarly. I'll try minifying it as well. UPDATE: Probably not related","This seems like fp accuracy issue, I'm getting ``` (Pdb) torch.testing.assert_close(ref, res) *** AssertionError: Tensorlikes are not close! Mismatched elements: 23559 / 262144 (9.0%) Greatest absolute difference: 0.009765625 at index (2, 0, 9) (up to 1e05 allowed) Greatest relative difference: 0.029152313645492717 at index (2, 34, 127) (up to 1.3e06 allowed) ``` and for some reason we cannot generate fp64 ref, because otherwise we would compare ref and res with fp64 and see who is further away (which could have cured this). ","Here is another repro with a similar problem, ``` import torch._inductor.overrides import torch from torch import tensor, device import torch.fx as fx from torch._dynamo.testing import rand_strided from math import inf from torch.fx.experimental.proxy_tensor import make_fx  torch version: 1.14.0a0+fb  torch cuda version: 11.4.0  CUDA Info:   nvcc not found  GPU Hardware Info:   NVIDIA A100PG509200 : 8  from torch.nn import * class Repro(torch.nn.Module):     def __init__(self):         super().__init__()     def forward(self, arg12_1, arg13_1, arg158_1, arg159_1, add_28, mul_20, mm_36):         mul_21 = torch.ops.aten.mul.Tensor(mul_20, arg12_1);  mul_20 = arg12_1 = None         add_5 = torch.ops.aten.add.Tensor(mul_21, arg13_1);  mul_21 = arg13_1 = None         convert_element_type = torch.ops.prims.convert_element_type.default(add_5, torch.float32);  add_5 = None         mm_38 = torch.ops.aten.mm.default(mm_36, arg158_1);  mm_36 = arg158_1 = None         add_30 = torch.ops.aten.add.Tensor(add_28, mm_38);  add_28 = mm_38 = None         mul_50 = torch.ops.aten.mul.Tensor(add_30, convert_element_type);  add_30 = None         mm_40 = torch.ops.aten.mm.default(mul_50, arg159_1);  mul_50 = arg159_1 = None         permute_65 = torch.ops.aten.permute.default(mm_40, [1, 0]);  mm_40 = None         mm_43 = torch.ops.aten.mm.default(permute_65, convert_element_type);  permute_65 = convert_element_type = None         return (mm_43,) args = [((7616,), (1,), torch.float32, 'cuda'), ((7616,), (1,), torch.float32, 'cuda'), ((256, 7616), (7616, 1), torch.float32, 'cuda'), ((7616, 256), (256, 1), torch.float32, 'cuda'), ((8, 7616), (7616, 1), torch.float32, 'cuda'), ((8, 7616), (7616, 1), torch.float32, 'cuda'), ((8, 256), (256, 1), torch.float32, 'cuda')] args = [rand_strided(sh, st, dt, dev) for (sh, st, dt, dev) in args] mod = make_fx(Repro().to(device=""cuda""))(*args) from torch._inductor.compile_fx import compile_fx_inner ```","So the problem, per above, is not comparing with fp64 (because `convert_element_type` hardcodes fp32 dtype, and we cannot generate fp64 reference), fixing that in the repro makes the repro pass (ref and res are the same distance from correct fp64 answer). "," so there might be a bug somewhere, but this minified repro doesn't uncover it.   how do we handle comparisons when fp64 computation fails? There might be quite a few cases, due to hardcoded fp32 conversions. Currently we just compare `ref` and `res`, but it might result in false failures, like here. "," , is there any reason that we didn't turn on `cos_similarity` for `same_two_models`? Turning that on does make these two minified tests we are discussing here pass."
llm,Minifier transforms accuracy failure into runtime failure (on PegasusForCausalLM)," 🐛 Describe the bug Checkout and build PyTorch at e4a8661ab84022c1bff622c6d2f6e679180b1df5 Here is a minifier: https://gist.github.com/ccca1bf4703b4ed8cf1ac7c3b0ab7b9e When I hack up the minifier to run the test, it fails with an accuracy error. https://gist.github.com/b69c1f74caf6b1f7419dff7a37191c14 ``` [20221029 18:17:17,541] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs    Traceback (most recent call last):                                                                             File ""/data/users/ezyang/pytorchtmp2/torchdynamo_debug/run_2022_10_29_18_00_02_673867/minifier/minifier_launcher.py"", line 1569, in      assert same_two_models(mod, compiled, args, only_fwd=True), ""Accuracy failed""                            AssertionError: Accuracy failed  ``` However, when I finished minifying, I got the repro at  CC(Inductor gives obscure error when FX graph to be compiled returns tuple) which no longer fails with an accuracy error, it just hits an assert. Generating asserts is useful. But I would prefer not to be blocked on fixing the assert failure before I can minify the accuracy failure.  Error logs _No response_  Minified repro _No response_",2022-10-30T01:25:13Z,triaged bug,closed,0,3,https://github.com/pytorch/pytorch/issues/93594,"According to  the minifier replaced the list output with a tuple, triggering inductor compile failure, and then happily minified the program down to nothing.","Here is a shitty patch that forces minifier to only consider failures at the end failures ``` diff git a/torch/_dynamo/debug_utils.py b/torch/_dynamo/debug_utils.py index a89f71eac4e..5915ca8b2c2 100644  a/torch/_dynamo/debug_utils.py +++ b/torch/_dynamo/debug_utils.py @@ 154,6 +154,7 @@ def generate_compiler_repro_string(gm, args):          from {config.dynamo_import}.testing import rand_strided          from math import inf          from torch.fx.experimental.proxy_tensor import make_fx +        import sys          """"""      ) @@ 170,7 +171,10 @@ def generate_compiler_repro_string(gm, args):      model_str += (          ""args = [rand_strided(sh, st, dt, dev) for (sh, st, dt, dev) in args]\n""      )     model_str += 'mod = make_fx(Repro().to(device=""cuda""))(*args)\n' +    model_str += 'try:' +    model_str += '    mod = make_fx(Repro().to(device=""cuda""))(*args)\n' +    model_str += 'except:' +    model_str += '    sys.exit(0)'      return model_str @@ 221,7 +225,10 @@ def save_graph_repro(fd, gm, args, compiler_name):          fd.write(              textwrap.dedent(                  f""""""                 compiled = {COMPILER_REPRO_OPTIONS[compiler_name][1]}(mod, args) +                try: +                    compiled = {COMPILER_REPRO_OPTIONS[compiler_name][1]}(mod, args) +                except: +                    sys.exit(0)                  assert same_two_models(mod, compiled, args, only_fwd=True), ""Accuracy failed""                  """"""              ) ``` It doesn't work correctly; at the end of minifying the minifier screwed the pooch ``` Strategy: Truncate suffix (G: 1) (33 nodes, 9 inputs)                                                        FAIL: Truncate suffix                                                                                        Strategy: Delta Debugging (G: 1) (33 nodes, 9 inputs)                                                        >>  [20221029 19:03:19,915] torch._dynamo.debug_utils: [WARNING] Could not generate fp64 outputs           SUCCESS: Went from 33 to 32 nodes                                                                            WARNING: Something went wrong, not applying this minification                                                Strategy: Remove outputs (G: 1) (33 nodes, 9 inputs) FAIL: Remove outputs Traceback (most recent call last):   File ""/data/users/ezyang/pytorchtmp2/torchdynamo_debug/run_2022_10_29_18_00_02_673867/minifier/minifier_l auncher.py"", line 1575, in      minifier(   File ""/data/users/ezyang/pytorchtmp2/functorch/_src/fx_minifier.py"", line 300, in minifier     raise RuntimeError(""Uh oh, something went wrong :( Final graph is not failing"") RuntimeError: Uh oh, something went wrong :( Final graph is not failing ``` but I did get a useful checkpoint that does still accuracy fail","Closing, minifier is propably broken in other ways now"
rag,[pytorch][vulkan] Add bias storage type to template,"  CC([pytorch][vulkan] Add bias storage type to template)  CC([PyTorch][Vulkan] Add template based codegen for shader generation)  CC([Pytorch][vulkan] Generate shader with parameters)  CC([Pytorch][Vulkan] Update spv generation script to embed shader parameters) To enable buffer based use for bias as well, this diff adds storage type for bias to template Differential Revision: D40689003",2022-10-28T14:29:26Z,release notes: vulkan,closed,0,1,https://github.com/pytorch/pytorch/issues/87985," :x:  login:  / name: Kimish Patel . The commit (0db76c7e205d20832fee738057500dc8bd2e5830) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket."
yi,[PT 1.13] resize bilinear yields different results in newer versions," 🐛 Describe the bug First, I tried to generate the inputs with the following code with PyTorch 1.13. ```python >>> import torch >>> import torch.nn.functional as F >>> t = torch.randn(1, 3, 128, 128) >>> torch.save(t, 'bilinear_inputs.pt') >>> ot = F.interpolate(t, scale_factor=2.0, mode='bilinear', align_corners=True) >>> torch.save(ot, 'bilinear_outputs_1_13.pt') ``` Then I tried to do the comparison with the Python environment with PyTorch 1.10. ```python >>> import torch >>> import torch.nn.functional as F >>> t = torch.load('bilinear_inputs.pt') >>> ot = torch.load('bilinear_outputs_1_13.pt') >>> y = F.interpolate(t, scale_factor=2.0, mode='bilinear', align_corners=True) >>> torch.allclose(ot, y) False >>> torch.testing.assert_close(ot, y) Traceback (most recent call last):   File """", line 1, in    File ""/usr/lib/python3.8/sitepackages/torch/testing/_asserts.py"", line 971, in assert_close     raise error_meta.to_error() AssertionError: Tensorlikes are not close! Mismatched elements: 609 / 196608 (0.3%) Greatest absolute difference: 2.562999725341797e05 at index (0, 2, 131, 145) (up to 1e05 allowed) Greatest relative difference: 0.07112191124437972 at index (0, 1, 135, 191) (up to 1.3e06 allowed) ``` As can be seen, the results are not within numerical differences caused by floating point representation. Attachments: bilinear_data.zip  Versions PyTorch version: 1.10.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.6 (x86_64) GCC version: Could not collect Clang version: 12.0.0 (clang1200.0.32.28) CMake version: version 3.18.0 Libc version: N/A Python version: 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang1200.0.32.2)] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.4 [pip3] onnx2pytorch==0.4.1 [pip3] onnx2torch==1.3.0 [pip3] pytorchlightning==1.6.3 [pip3] torch==1.10.0 [pip3] torchmetrics==0.8.2 [pip3] torchvision==0.11.1 [conda] Could not collect ",2022-10-28T07:05:46Z,module: cpu triaged module: regression module: interpolation,closed,1,5,https://github.com/pytorch/pytorch/issues/87968,"Fun fact, `python c ""import torch;x=torch.rand(10);torch.save('foo', x)""` crashes the runtime, we should fix it","5 does this sounds like an acceptable change of accuracy to you? But in general, 2e5 difference for small issues seems fine.","Checking in details the repro code on different versions: 1.10, 1.12, 1.13 IMO, precision issue is related to https://github.com/pytorch/pytorch/commit/658f958bc4bb314d9c6030eeaf3e1784792b5d15  no issue between 1.10 and 1.12 , 1.10 and 1.13.0.dev20220802+cpu, 1.13.0.dev20220902+cpu and 1.13.0  reported issue between 1.13.0.dev20220802+cpu and 1.13 There is no issue between 1.10, 1.12 and 1.13 if `align_corners=False`.   any ideas about the loss of precision here ?  Shouldn't we also static_cast `scale` here ? https://github.com/XiaobingSuper/pytorch/blob/f3ad87eae4a0dccb2e8aa01e379e99e67194dc49/aten/src/ATen/native/UpSample.hL289 ","Ok, reverting CC(fix upsample bf16 issue for channels last path by using high pricsion to compute index) brings down divergence to 0. Modified the test to compute upsample from an torch.arange, to avoid dealing with any denorms, i.e. collecting data using following script: ``` python c ""import torch;x=torch.arange(100, dtype=torch.float32).reshape(1, 1, 10, 10); y=torch.nn.functional.interpolate(x, scale_factor=2.0, mode='bilinear', align_corners=False);torch.save(y, f'y{torch.__version__}.pt')"" ``` and the difference between 1.12 and 1.13 remains `1.5e5`: ```  python c ""import torch;x=torch.arange(100, dtype=torch.float32).reshape(1, 1, 10, 10); y=torch.nn.functional.interpolate(x, scale_factor=2.0, mode='bilinear', align_corners=True);y_old=torch.load('y1.12.pt');print((yy_old).abs().max())"" tensor(1.5259e05) ```","Somehow following fixes it: ```c++ diff git a/aten/src/ATen/native/UpSample.h b/aten/src/ATen/native/UpSample.h index f3dd836444..aca639cc14 100644  a/aten/src/ATen/native/UpSample.h +++ b/aten/src/ATen/native/UpSample.h @@ 447,9 +447,8 @@ static inline void compute_source_index_and_lambda(      lambda0 = static_cast(1);      lambda1 = static_cast(0);    } else {     using accscalar_t = at::acc_type;     const accscalar_t real_input_index =         area_pixel_compute_source_index( +    const scalar_t real_input_index = +        area_pixel_compute_source_index(              ratio, output_index, align_corners, /*cubic=*/false);      input_index0 = static_cast(real_input_index);      int64_t offset = (input_index0 < input_size  1) ? 1 : 0; ``` I..e results from 1.13 are technically not wrong (as `accscalar_t` for float is double)"
yi,[FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`,Stack from ghstack:  CC([FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`) [FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`  CC([FSDP()] Have `fully_shard()` abide by ``!) [FSDP()] Have `fully_shard()` abide by ``!  CC([FSDP()][Easy] Rename `_State` to `_FSDPState`) [FSDP()][Easy] Rename `_State` to `_FSDPState`  CC([FSDP()] Rename to `fully_shard()` and move to `_composable/`) [FSDP()] Rename to `fully_shard()` and move to `_composable/`  CC([FSDP][Easy] Remove unneeded `TrainingState` transition) [FSDP][Easy] Remove unneeded `TrainingState` transition  CC([FSDP] Rename `unflat_param_name` > `fqn` for consistency) [FSDP] Rename `unflat_param_name` > `fqn` for consistency  CC([FSDP] Simplify `_get_buffer_names()`) [FSDP] Simplify `_get_buffer_names()`  CC([FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU) [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU  CC([FSDP][Docs] Add note mentioning rate limiter for backward prefetch) [FSDP][Docs] Add note mentioning rate limiter for backward prefetch  CC([FSDP()][27/N] Add forward hook registration) [FSDP()][27/N] Add forward hook registration * * CC([FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`) [FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`**  CC([FSDP()][25/N] Add `_post_forward_reshard()`) [FSDP()][25/N] Add `_post_forward_reshard()`  CC([FSDP()][24/N] Refactor `_lazy_init()`) [FSDP()][24/N] Refactor `_lazy_init()`  CC([FSDP()][23/N] Refactor handle attr initialization) [FSDP()][23/N] Refactor handle attr initialization  CC([FSDP()][21/N] Refactor and fix `_cast_buffers()`) [FSDP()][21/N] Refactor and fix `_cast_buffers()`  CC([FSDP] Rename `dtype` to `buffer_name_to_dtype`) [FSDP] Rename `dtype` to `buffer_name_to_dtype`  CC([FSDP] Remove `device` arg from `_cast_buffers()`) [FSDP] Remove `device` arg from `_cast_buffers()`  CC([FSDP()][20/N][Easy] Move functions in file) [FSDP()][20/N][Easy] Move functions in file  CC([FSDP()][18/N] Refactor `pre_forward_unshard()`) [FSDP()][18/N] Refactor `pre_forward_unshard()`  CC([FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`) [FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`  CC([FSDP()][16/N] Refactor postforward/prebackward) [FSDP()][16/N] Refactor postforward/prebackward  CC([FSDP()][15/N] Refactor `_init_streams()`) [FSDP()][15/N] Refactor `_init_streams()`  CC([FSDP()][14/N] Refactor preforward/postbackward) [FSDP()][14/N] Refactor preforward/postbackward,2022-10-27T22:22:57Z,Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/87941, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: Command `git C /home/runner/work/pytorch/pytorch cherrypick x 33b85d292b06dac62af023ffb2f6af509e99828d` returned nonzero exit code 1 ``` Automerging torch/distributed/fsdp/fully_sharded_data_parallel.py The previous cherrypick is now empty, possibly due to conflict resolution. If you wish to commit it anyway, use:     git commit allowempty Otherwise, please use 'git cherrypick skip' On branch master Your branch is ahead of 'origin/master' by 1 commit.   (use ""git push"" to publish your local commits) You are currently cherrypicking commit 33b85d292b.   (all conflicts fixed: run ""git cherrypick continue"")   (use ""git cherrypick skip"" to skip this patch)   (use ""git cherrypick abort"" to cancel the cherrypick operation) nothing to commit, working tree clean ``` Details for Dev Infra team Raised by workflow job ", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[FSDP()][24/N] Refactor `_lazy_init()`,Stack from ghstack:  CC([FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`) [FSDP()][Easy] Make `fully_shard()` only `FULL_SHARD`  CC([FSDP()] Have `fully_shard()` abide by ``!) [FSDP()] Have `fully_shard()` abide by ``!  CC([FSDP()][Easy] Rename `_State` to `_FSDPState`) [FSDP()][Easy] Rename `_State` to `_FSDPState`  CC([FSDP()] Rename to `fully_shard()` and move to `_composable/`) [FSDP()] Rename to `fully_shard()` and move to `_composable/`  CC([FSDP][Easy] Remove unneeded `TrainingState` transition) [FSDP][Easy] Remove unneeded `TrainingState` transition  CC([FSDP] Rename `unflat_param_name` > `fqn` for consistency) [FSDP] Rename `unflat_param_name` > `fqn` for consistency  CC([FSDP] Simplify `_get_buffer_names()`) [FSDP] Simplify `_get_buffer_names()`  CC([FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU) [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU  CC([FSDP][Docs] Add note mentioning rate limiter for backward prefetch) [FSDP][Docs] Add note mentioning rate limiter for backward prefetch  CC([FSDP()][27/N] Add forward hook registration) [FSDP()][27/N] Add forward hook registration  CC([FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`) [FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`  CC([FSDP()][25/N] Add `_post_forward_reshard()`) [FSDP()][25/N] Add `_post_forward_reshard()` * * CC([FSDP()][24/N] Refactor `_lazy_init()`) [FSDP()][24/N] Refactor `_lazy_init()`**  CC([FSDP()][23/N] Refactor handle attr initialization) [FSDP()][23/N] Refactor handle attr initialization  CC([FSDP()][21/N] Refactor and fix `_cast_buffers()`) [FSDP()][21/N] Refactor and fix `_cast_buffers()`  CC([FSDP] Rename `dtype` to `buffer_name_to_dtype`) [FSDP] Rename `dtype` to `buffer_name_to_dtype`  CC([FSDP] Remove `device` arg from `_cast_buffers()`) [FSDP] Remove `device` arg from `_cast_buffers()`  CC([FSDP()][20/N][Easy] Move functions in file) [FSDP()][20/N][Easy] Move functions in file  CC([FSDP()][18/N] Refactor `pre_forward_unshard()`) [FSDP()][18/N] Refactor `pre_forward_unshard()`  CC([FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`) [FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`  CC([FSDP()][16/N] Refactor postforward/prebackward) [FSDP()][16/N] Refactor postforward/prebackward  CC([FSDP()][15/N] Refactor `_init_streams()`) [FSDP()][15/N] Refactor `_init_streams()`  CC([FSDP()][14/N] Refactor preforward/postbackward) [FSDP()][14/N] Refactor preforward/postbackward,2022-10-27T22:22:41Z,Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/87939, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,[FSDP] Simplify `_reset_lazy_init()`,"Stack from ghstack:  CC([FSDP()] Have `fully_shard()` abide by ``!) [FSDP()] Have `fully_shard()` abide by `contract`!  CC([FSDP()][Easy] Rename `_State` to `_FSDPState`) [FSDP()][Easy] Rename `_State` to `_FSDPState`  CC([FSDP()] Rename to `fully_shard()` and move to `_composable/`) [FSDP()] Rename to `fully_shard()` and move to `_composable/`  CC([FSDP][Easy] Remove unneeded `TrainingState` transition) [FSDP][Easy] Remove unneeded `TrainingState` transition  CC([FSDP] Rename `unflat_param_name` > `fqn` for consistency) [FSDP] Rename `unflat_param_name` > `fqn` for consistency  CC([FSDP] Simplify `_get_buffer_names()`) [FSDP] Simplify `_get_buffer_names()`  CC([FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU) [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU  CC([FSDP][Docs] Add note mentioning rate limiter for backward prefetch) [FSDP][Docs] Add note mentioning rate limiter for backward prefetch  CC([FSDP()][27/N] Add forward hook registration) [FSDP()][27/N] Add forward hook registration  CC([FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`) [FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`  CC([FSDP()][25/N] Add `_post_forward_reshard()`) [FSDP()][25/N] Add `_post_forward_reshard()`  CC([FSDP()][24/N] Refactor `_lazy_init()`) [FSDP()][24/N] Refactor `_lazy_init()`  CC([FSDP()][23/N] Refactor handle attr initialization) [FSDP()][23/N] Refactor handle attr initialization * * CC([FSDP] Simplify `_reset_lazy_init()`) [FSDP] Simplify `_reset_lazy_init()`**  CC([FSDP()][21/N] Refactor and fix `_cast_buffers()`) [FSDP()][21/N] Refactor and fix `_cast_buffers()`  CC([FSDP] Rename `dtype` to `buffer_name_to_dtype`) [FSDP] Rename `dtype` to `buffer_name_to_dtype`  CC([FSDP] Remove `device` arg from `_cast_buffers()`) [FSDP] Remove `device` arg from `_cast_buffers()`  CC([FSDP()][20/N][Easy] Move functions in file) [FSDP()][20/N][Easy] Move functions in file  CC([FSDP()][18/N] Refactor `pre_forward_unshard()`) [FSDP()][18/N] Refactor `pre_forward_unshard()`  CC([FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`) [FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`  CC([FSDP()][16/N] Refactor postforward/prebackward) [FSDP()][16/N] Refactor postforward/prebackward  CC([FSDP()][15/N] Refactor `_init_streams()`) [FSDP()][15/N] Refactor `_init_streams()`  CC([FSDP()][14/N] Refactor preforward/postbackward) [FSDP()][14/N] Refactor preforward/postbackward  CC([FSDP()][13/N] Refactor unshard/reshard/grads) [FSDP()][13/N] Refactor unshard/reshard/grads  CC([FSDP()][12/N] Easy cleanup) [FSDP()][12/N] Easy cleanup  CC([FSDP()][10/N][11/N] Introduce composable (ctor only)) [FSDP()][10/N][11/N] Introduce composable (ctor only)  CC([FSDP()][9/N] Refactor ctor (continued)) [FSDP()][9/N] Refactor ctor (continued) From previous PRs, I have refactored `_lazy_init()` such that the actual logic only runs once and only for the root FSDP instance gated precisely by `self._is_root is None` or not (`None` means no root is set, not`None` means root is set). Furthermore, `_init_param_attributes()` (now renamed to `_init_flat_param_attributes()`) is only ever called from that actual `_lazy_init()` logic. Therefore, there is no need to additionally delete `p._local_shard` in `_lazy_init()`, which would only be meaningful if we called `_init_param_attributes()` separately outside `_lazy_init()` and it needed to be additionally gated. If we reset `_lazy_init()` by setting `self._is_root = None` and call `_lazy_init()` again, it will simply override `_local_shard`.",2022-10-27T22:22:25Z,ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/87937,I coalesced this with https://github.com/pytorch/pytorch/pull/87938 because I realized they should be landed together as an atomic change.
yi,[FSDP()][22/N] Refactor `_cast_buffers()` in `_lazy_init()`,Stack from ghstack:  CC([FSDP()] Have `fully_shard()` abide by ``!) [FSDP()] Have `fully_shard()` abide by `contract`!  CC([FSDP()][Easy] Rename `_State` to `_FSDPState`) [FSDP()][Easy] Rename `_State` to `_FSDPState`  CC([FSDP()] Rename to `fully_shard()` and move to `_composable/`) [FSDP()] Rename to `fully_shard()` and move to `_composable/`  CC([FSDP][Easy] Remove unneeded `TrainingState` transition) [FSDP][Easy] Remove unneeded `TrainingState` transition  CC([FSDP] Rename `unflat_param_name` > `fqn` for consistency) [FSDP] Rename `unflat_param_name` > `fqn` for consistency  CC([FSDP] Simplify `_get_buffer_names()`) [FSDP] Simplify `_get_buffer_names()`  CC([FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU) [FSDP] Remove unneeded `torch.no_grad()` context when offloading to CPU  CC([FSDP][Docs] Add note mentioning rate limiter for backward prefetch) [FSDP][Docs] Add note mentioning rate limiter for backward prefetch  CC([FSDP()][27/N] Add forward hook registration) [FSDP()][27/N] Add forward hook registration  CC([FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`) [FSDP()][26/N] Move `_lazy_init()` into `_fsdp_root_pre_forward()`  CC([FSDP()][25/N] Add `_post_forward_reshard()`) [FSDP()][25/N] Add `_post_forward_reshard()`  CC([FSDP()][24/N] Refactor `_lazy_init()`) [FSDP()][24/N] Refactor `_lazy_init()`  CC([FSDP()][23/N] Refactor handle attr initialization) [FSDP()][23/N] Refactor handle attr initialization  CC([FSDP] Simplify `_reset_lazy_init()`) [FSDP] Simplify `_reset_lazy_init()` * * CC([FSDP()][22/N] Refactor `_cast_buffers()` in `_lazy_init()`) [FSDP()][22/N] Refactor `_cast_buffers()` in `_lazy_init()`**  CC([FSDP()][21/N] Refactor and fix `_cast_buffers()`) [FSDP()][21/N] Refactor `_buffer_name_to_orig_dtype` computation  CC([FSDP] Rename `dtype` to `buffer_name_to_dtype`) [FSDP] Rename `dtype` to `buffer_name_to_dtype`  CC([FSDP] Remove `device` arg from `_cast_buffers()`) [FSDP] Remove `device` arg from `_cast_buffers()`  CC([FSDP()][20/N][Easy] Move functions in file) [FSDP()][20/N][Easy] Move functions in file  CC([FSDP()][18/N] Refactor `pre_forward_unshard()`) [FSDP()][18/N] Refactor `pre_forward_unshard()`  CC([FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`) [FSDP()][17/N] Refactor `_fsdp_root_pre_forward()`  CC([FSDP()][16/N] Refactor postforward/prebackward) [FSDP()][16/N] Refactor postforward/prebackward  CC([FSDP()][15/N] Refactor `_init_streams()`) [FSDP()][15/N] Refactor `_init_streams()`  CC([FSDP()][14/N] Refactor preforward/postbackward) [FSDP()][14/N] Refactor preforward/postbackward  CC([FSDP()][13/N] Refactor unshard/reshard/grads) [FSDP()][13/N] Refactor unshard/reshard/grads  CC([FSDP()][12/N] Easy cleanup) [FSDP()][12/N] Easy cleanup  CC([FSDP()][10/N][11/N] Introduce composable (ctor only)) [FSDP()][10/N][11/N] Introduce composable (ctor only)  CC([FSDP()][9/N] Refactor ctor (continued)) [FSDP()][9/N] Refactor ctor (continued),2022-10-27T22:22:17Z,ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/87936,I coalesced this PR with the preceding one (https://github.com/pytorch/pytorch/pull/87935) since I needed to fix `_cast_buffers()` altogether. I will close this one soon.
transformer,AttributeError: 'torch.dtype' object has no attribute 'numel'," 🐛 Describe the bug Tried the following code for dynamic quantization ``` import torch from transformers import AutoConfig, AutoModel model = AutoModel.from_pretrained(""bertbaseuncased"") model_quantized = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8) model_quantized.save_pretrained(""quantized/"")   ``` And it shows the following error while executing ``` AttributeError: 'torch.dtype' object has no attribute 'numel' ```  Versions Python 3.7.15 transformers==4.23.1 torch==1.12.1 ",2022-10-27T04:41:52Z,oncall: quantization,closed,2,3,https://github.com/pytorch/pytorch/issues/87843,I'm also facing the same issue **Versions** Python 3.7.15 transformers==4.23.1 torch==1.14.0.dev20221027+cpu,  Because the quantized model contains the ```dtype``` object which is not handle by the  ```save_pretrained``` method  now,this is all related to save_pretrained not being made to work with quantized models. quantized models have specific supported methods of saving/serialization so errors are expected when other methods are used. in general you need to use state_dict to save https://pytorch.org/docs/stable/quantization.htmlsavingandloadingquantizedmodels we have a tutorial for Bert which has code for serialization: https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html There's also this other thread that is dealing with the same issue:  CC(AttributeError: 'BertModel' object has no attribute 'save')
transformer,Any ideas on how we can convert a model from huggingface (transformers library )to tensorflow lite?," 🐛 Describe the bug I want to convert CamembertQuestionAnsewring model to tensoflow lite, i download it from huggingface platform, because when i want to save the model locally it gives me the model with 'bin' format.    i'm asking here because huggingface use pytorch pretrained models.  when i try to convert the model it gives me this error : AttributeError: 'CamembertForQuestionAnswering' object has no attribute 'call' by using tf_model.h5 file.   Also i can't load it using : tf.keras.models.load_model() it gives me : ValueError: No model config found in the file at .  when i want to save the transformers model locally it gives me the model with 'bin' format, so i download it from the platform.  Versions https://huggingface.co/etalabia/camembertbasesquadFRfquadpiaf?context=Etalab+est+une+administration+publique+fran%C3%A7aise+qui+fait+notamment+office+de+Chief+Data+Officer+de+l%27%C3%89tat+et+coordonne+la+conception+et+la+mise+en+%C5%93uvre+de+sa+strat%C3%A9gie+dans+le+domaine+de+la+donn%C3%A9e+%28ouverture+et+partage+des+donn%C3%A9es+publiques+ou+open+data%2C+exploitation+des+donn%C3%A9es+et+intelligence+artificielle...%29.+Ainsi%2C+Etalab+d%C3%A9veloppe+et+maintient+le+portail+des+donn%C3%A9es+ouvertes+du+gouvernement+fran%C3%A7ais+data.gouv.fr.+Etalab+promeut+%C3%A9galement+une+plus+grande+ouverture+l%27administration+sur+la+soci%C3%A9t%C3%A9+%28gouvernement+ouvert%29+%3A+transparence+de+l%27action+publique%2C+innovation+ouverte%2C+participation+citoyenne...+elle+promeut+l%E2%80%99innovation%2C+l%E2%80%99exp%C3%A9rimentation%2C+les+m%C3%A9thodes+de+travail+ouvertes%2C+agiles+et+it%C3%A9ratives%2C+ainsi+que+les+synergies+avec+la+soci%C3%A9t%C3%A9+civile+pour+d%C3%A9cloisonner+l%E2%80%99administration+et+favoriser+l%E2%80%99adoption+des+meilleures+pratiques+professionnelles+dans+le+domaine+du+num%C3%A9rique.+%C3%80+ce+titre+elle+%C3%A9tudie+notamment+l%E2%80%99opportunit%C3%A9+de+recourir+%C3%A0+des+technologies+en+voie+de+maturation+issues+du+monde+de+la+recherche.+Cette+entit%C3%A9+charg%C3%A9e+de+l%27innovation+au+sein+de+l%27administration+doit+contribuer+%C3%A0+l%27am%C3%A9lioration+du+service+public+gr%C3%A2ce+au+num%C3%A9rique.+Elle+est+rattach%C3%A9e+%C3%A0+la+Direction+interminist%C3%A9rielle+du+num%C3%A9rique%2C+dont+les+missions+et+l%E2%80%99organisation+ont+%C3%A9t%C3%A9+fix%C3%A9es+par+le+d%C3%A9cret+du+30+octobre+2019.%E2%80%89+Dirig%C3%A9+par+Laure+Lucchesi+depuis+2016%2C+elle+rassemble+une+%C3%A9quipe+pluridisciplinaire+d%27une+trentaine+de+personnes.&question=Comment+s%27appelle+le+portail+open+data+du+gouvernement+%3F",2022-10-26T16:00:34Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/87789,this is not a question answering forum
transformer,Unable to see the weight files after quantization," 🐛 Describe the bug I have tried the following code for dynamic quantization ``` import torch import os from transformers import AutoConfig, AutoModel model = AutoModel.from_pretrained(""bertbaseuncased"") model_quantized = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)   quantized_output_dir = ""quantized/"" if not os.path.exists(quantized_output_dir):     os.makedirs(quantized_output_dir)     model_quantized.save_pretrained(quantized_output_dir) ``` After the execution, I could see that there is a new folder named **quantized** created in the directory which contains only the ```config.json``` file. contents are as follows ``` {   ""_name_or_path"": ""bertbaseuncased"",   ""architectures"": [     ""BertModel""   ],   ""attention_probs_dropout_prob"": 0.1,   ""classifier_dropout"": null,   ""gradient_checkpointing"": false,   ""hidden_act"": ""gelu"",   ""hidden_dropout_prob"": 0.1,   ""hidden_size"": 768,   ""initializer_range"": 0.02,   ""intermediate_size"": 3072,   ""layer_norm_eps"": 1e12,   ""max_position_embeddings"": 512,   ""model_type"": ""bert"",   ""num_attention_heads"": 12,   ""num_hidden_layers"": 12,   ""pad_token_id"": 0,   ""position_embedding_type"": ""absolute"",   ""torch_dtype"": ""float32"",   ""transformers_version"": ""4.23.1"",   ""type_vocab_size"": 2,   ""use_cache"": true,   ""vocab_size"": 30522 } ``` I can't see any other **.bin** or **.wt** files after quantization. Why it is so?  Versions Python 3.7.15 transformers==4.23.1 torch==1.12.1 ",2022-10-26T14:53:43Z,oncall: quantization,closed,0,2,https://github.com/pytorch/pytorch/issues/87784,"hi 1995 , `save_pretrained` is not a PyTorch API.  Are you asking about HuggingFace's `save_pretrained` function?  In that case, maybe post on their discussion forum?","are you following the supported method for saving/loading quantized models? https://pytorch.org/docs/stable/quantization.htmlsavingandloadingquantizedmodels my guess is whatever save_pretrained does, isn't aligned with quantization out of the box since  for example  there are no longer weight tensors because they have been packed into the quantized module"
transformer,[NVFuser] Upstream push 1026,"  CC([NVFuser] Upstream push 1026) Syncing nvfuser devel branch to upstream master. https://github.com/csarofeen/pytorch/ Codegen changes include: * codegen improvement:     i. allow nonroot trivial reductions, allow empty/noop fusion     ii. fixes vectorization checks and size calculation     iii. bank conflict handle improvement     iv. enables transpose scheduler * misc:     i. CI tests failure fixes     ii. cpp tests file clean up     iii. trivial forwarding supports added in codegen runtime     iv. added factory methods support in codegen Commits that's in this PR from the devel branch: ``` 7117a7e37ebec372d9e802fdfb8abb7786960f4a patching nvfuser conv cudnn test numerics mismatch ( CC([READY] torch.nn.Threshold: `less or equal` vs `less` problem)) 65af1a4e7013f070df1ba33701f2d524de79d096 Inserting sync for redundant parallel types is already done at the ( CC(Update readme of installation docs for OSX)) 6ac74d181689c8f135f60bfc1ec139d88941c98c Fix sync map ( CC(Add __repr__ to Avgpool and maxunpool layers)) f5bca333355e2c0033523f3402de5b8aac602c00 Bank conflict checker improvements ( CC(Possible bug in RNN module)) d2ca7e3fd203537946be3f7b435303c60fa7f51e Minor update on cp.async code generation. ( CC(How to select GPU programmatically in code)) d36cf61f5570c9c992a748126287c4e7432228e0 Test file cleanup ( CC(add atan2 function to autograd)) 0b8e83f49c2ea9f04a4aad5061c1e7f4268474c6 Allow nonroot trivial reductions ( CC(support dictionary return types in nn.Module's __call__)) a2dfe40b27cd3f5c04207596f0a1818fbd5e5439 Fix vectorize size calculation ( CC(Installing from source C issue)) e040676a317fe34ea5875276270c7be88f6eaa56 Use withPredicate to replace setPredicate to maintain Exprs immutable ( CC(Small mistake in nn.Threshold documentation)) 197221b847ad5eb347d7ec1cf2706733aacbf97c removing ci workflow ( CC(Fix typos in docstrings)) 40e2703d00795526e7855860aa00b9ab7160755f Reduction rand like patch ( CC([Feature request] unique operation)) bc772661cbdb3b711d8e9854ae9b8b7052e3e4a3 Add utility for checking bank conflict of shared memory ( CC(MacOSX, CUDA, OS call failed or operation not supported on this OS)) ddd1cf7695f3fb172a0e4bcb8e4004573617a037 Add back FusionReductionWithTrivialReduction_CUDA ( CC(Fix typos in the docstrings of Conv3d, AvgPool3d and MaxPool3d)) fbd97e5ef15fa0f7573800e6fbb5743463fd9e57 Revert ""Cleanup trivial reduction workarounds ( CC(Tensor.sum() over multiple axes))"" ( CC([Feature Request] Add NoisyLinear layer)) bca20c1dfb8aa8d881fc7973e7579ce82bc6a894 Cleanup trivial reduction workarounds ( CC(Tensor.sum() over multiple axes)) e4b65850eee1d70084105bb6e1f290651adde23e Trivial forwarding ( CC(torch.save is saving too much)) 1a0e355b5027ed0df501989194ee8f2be3fdd37a Fix contiguity analysis of predicates to match updated contiguity. ( CC(Why there is a `device_id` parameter in `nn.module.cpu()` call?)) a4effa6a5f7066647519dc56e854f4c8a2efd2a7 Enable output allocation cache ( CC(gpu version of pytorch not working on docker image)) 35440b7953ed8da164a5fb28f87d7fd760ac5e00 Patching bn inference ( CC(Adds Cyclical Learning Rates)) 0f9f0b4060dc8ca18dc65779cfd7e0776b6b38e8 Add matmul benchmark ( CC(Adding Spatial Transformers w/CuDNN support)) 45045cd05ea268f510587321dbcc8d7c2977cdab Enable tests previously disabled due to an aliasing bug ( CC(Add comments for readability)) 967aa77d2c8e360c7c01587522eec1c1d377c87e Contiguous indexing for View operations ( CC([Feature request] Allow exceptions in load_state_dict)) a43cb20f48943595894e345865bc1eabf58a5b48 Make inlining even more modular ( CC(Skip distributed tests if not supported)) dc458358c0ac91dfaf4e6655a9b3fc206fc0c897 Test util cleanup ( CC(Handling of no/zero gradients in cpp Function)) 3ca21ebe4d213f0070ffdfa4ae5d7f6cb0b8e870 More strict validation ( CC(Make grad_output contiguous in cudnn path for batchnorm)) a7a7d573310c4707a9f381831d3114210461af01 Fix build problem ( CC(mac build fail.)) fc235b064e27921fa9d6dbb9dc7055e5bae1c222 Just fixes comments ( CC(Program runs fine on Mac but not on Linux (both CPU and GPU problems))) 482386c0509fee6edb2964c5ae72074791f3e43a cleanup ( CC(Several problems when training a convnet on mnist)) 4cbe0db6558a82c3097d281eec9c85ad2ea0893a Improve divisible split detection ( CC(Correct test random seeding and autograd test flakiness)) 42ccc52bdc18bab0330f4b93ed1399164e2980c9 Minor build fix. ( CC(Use torch.arange instead of torch.range in test_torch.py)) fcf8c091f72d46f3055975a35afd06263324ede6 Cleanup of lower_utils.cpp: Isolate out GpuLower usage ( CC(Reductions returning scalars cause implicit syncpoint)) 15f2f6dba8cbf408ec93c344767c1862c30f7ecc Move ConcretizedBroadcastDomains to shared_ptr in GpuLower. ( CC(Feature request: 3D Adaptive Pooling)) 8f1c7f52679a3ad6acfd419d28a2f4be4a7d89e2 Minor cleanup lower_unroll.cpp ( CC(Broadcasting doesn't match docs for conda package)) 1d9858c80319ca7f0037db7de5f04e47f540d76c Minor cleanup ( CC(Allow arbitrary inputs in DataParallel)) f262d9cab59f41c669f53799c6d4a6b9fc4267eb Add support for uniform RNG ( CC(Inconsistency: can instantiate Tensor from List, but cannot instantiate Variable)) eb1dad10c73f855eb1ecb20a8b1f7b6edb0c9ea3 Remove nonconst functions, remove GpuLower instance on build, pass in ca_map. ( CC(add functional embedding)) 634820c5e3586c0fe44132c51179b3155be18072 Add support for some empty fusion ( CC([Controversial] `pyyaml` is 6 years old, might be best to switch to `ruamel.yaml`?)) eabe8d844ad765ee4973faa4821d451ef71b83c3 Segment self mapping fusions ( CC(install from source ,i get some error!!!)) e96aacfd9cf9b3c6d08f120282762489bdf540c8 Enable Transpose operation ( CC(Add container for recurrent nets)) 425dce2777420248e9f08893765b5402644f4161 Add a null scheduler that helps segmenting away noop schedules ( CC(numerical stability for logSigmoid)) 306d4a68f127dd1b854b749855e48ba23444ba60 Fix canScheduleCompileTime check of transpose scheduler ( CC(Naming inconsistencies)) b1bd32cc1b2ae7bbd44701477bddbcfa6642a9be Minor fix ( CC(version `GLIBC_2.7' not found when import pytorch)) bd93578143c1763c1e00ba613a017f8130a6b989 Enable transpose scheduler ( CC([feature request] timedistributed layers for application of normal layers to sequence data)) b7a206e93b4ac823c791c87f12859cf7af264a4c Move scheduler vectorize utilities into their own file ( CC([Feature Request] Layer Normalization)) d9420e4ca090489bf210e68e9912bb059b895baf View scheduling ( CC(Error refers to retain_variables when it should be retain_graph)) c668e13aea0cf21d40f95b48e0163b812712cdf2 Upstream push ci fixes ( CC(Equivalent of tf.gather , tf.scatter, tf.update in pytorch)) c40202bb40ce955955bb97b12762ef3b6b612997 Fix dump effective bandwidth ( CC(Incremental variance estimation instead of smoothing in batch normalization )) 93505bcbb90a7849bd67090fe5708d867e8909e4 WAR on index mapping when exact and permissive maps differ ( CC([feature request] Add YellowFin optimizer)) 45e95fd1d3c773ee9b2a21d79624c279d269da9f Allow splitting innermost ID to create virtual innermost ID in transpose scheduler ( CC(Different behavior of sum() inplace on CPU vs GPU)) a3ecb339442131f87842eb56955e4f17c544e99f Improve the comments at the beginning of index_compute.h ( CC([docs] Docs missing online for nn.functional.normalize)) f7bc3417cc2923a635042cc6cc361b2f344248d6 Remove unused variables ( CC(Cuda runtime error (8) : invalid device function at THCTensorMathPointwise.cu for latest build)) df3393adbb5cb0309d091f358cfa98706bd4d313 Some cleanup ( CC(cuda.FloatTensor constructor with float32 array)) 7d1d7c8724ab5a226fad0f5a80feeac04975a496 TVDomainGuard factory ( CC(Advanced indexing on the GPU)) 357ba224c0fb41ed3e4e8594d95599c973f4a0ca Fill allocation with nan on tests ( CC(Too many resources requested for launch)) 8eafc54685d406f5ac527bcbacc475fda4492d7a Fix detection of unmappable root domains ( CC([minor] list(empty tensor) fails)) 90a51f282601ba8ebd4c84b9334efd7762a234bc Some indexing cleanups, Add eye support ( CC(Make cudnn warnings clean.)) ddc01e4e16428aec92f9c84d698f959b6436a971 Exclude unsupported data types ( CC([feature request] add tuple support for dim specification of squeeze() and unsqueeze())) 992e17c0688fe690c51b50e81a75803621b7e6aa test the groups the same order as they are merged ( CC(Implement ConstantPad2d double backwards.)) 208262b75d1fed0597a0329d61d57bc8bcd7ff14 Move detection of self mapping IDs to IterDomainGraph from ( CC(Install Issue: torch/_C.so looks for two versions of cuda)) ac4de38c6ee53b366e85fdfe408c3642d32b57df Merge pull request CC(Add weight normalization implementation) from csarofeen/master_merge_0828 631094891a96f715d8c9925fb73d41013ca7f2e3 Add full, full_like, zeros, zeros_like, ones, ones_like ( CC(Handle None in modules list.)) aab10bce4541204c46b91ff0f0ed9878aec1bfc4 Merge remotetracking branch 'upstream/viable/strict' into HEAD 4c254c063bb55887b45677e3812357556a7aa80d Fix arange when step is negative ( CC([pylint] E1101:Module 'torch' has no 'squeeze' member)) 89330aa23aa804340b2406ab58899d816e3dc3d2 Tensor factories must set the output shape as its input ( CC(Different behaviour of BCEWithLogitsLoss and BCELoss + Sigmoid)) ``` RUN_TORCHBENCH: nvfuser Differential Revision: D40869846",2022-10-26T09:34:52Z,open source Merged ciflow/trunk release notes: jit skip-pr-sanity-checks,closed,1,12,https://github.com/pytorch/pytorch/issues/87779,"Quick update on things that's being worked on / tracked: 1. Maxwell failure is coming from synchronization... we relies on thread independent scheduling for reduction, which doesn't work with prevolta device. With transpose scheduler enabled, now we can hit synchronization even for pw kernels. We need to disable nvfuser on prevolta device and update our python tests to skip those. I know that we recently added AXX GPUs on CI, just want to make sure that we are still at least running something to check the functional correctness. 2. There's some codegen failures regarding empty fusion with concrete shapes....  is working on that :stuck_out_tongue_closed_eyes:  3. I also saw a few codegen error on ROCM. Looks like there's some missing synchronization templates in the runtime system.   "," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","Added `RUN_TORCHBENCH: nvfuser` in the commit message, hopefully this will trigger torchbench runs."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",Failure looks strange and seems like there's just no runner picking it up?!?! The upstream commit I have looks clean on PyTorch CI HUD. I'm trying it again.,Strangely I don't see any log from the CI failure..... I'll try rebasing. :crying_cat_face:  ," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.",CI are green now!!! Yay... Turns out there was no real failure but flaky tests :crying_cat_face:  cc'ing  ," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
rag,fix sym_storage conversion and some cleanup,  CC(Fix all references to torchdynamo from the merge)  CC(fix sym_storage conversion and some cleanup) ,2022-10-25T18:58:59Z,Merged ciflow/trunk release notes: fx,closed,0,3,https://github.com/pytorch/pytorch/issues/87718," merge f ""test failures all look fake"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Pass in Storage Offset For Input Tensors,  CC(Pass in Storage Offset For Input Tensors) Fix for  CC([Bug]: TorchInductor Input As_Strided Calls Dont Compose With Offset Inputs) ,2022-10-25T17:23:27Z,Stale ciflow/trunk topic: not user facing module: inductor module: dynamo ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/87709," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.", rebase, successfully started a rebase job. Check the current status here,"Rebase failed due to Command `git C /home/runner/work/pytorch/pytorch rebase refs/remotes/origin/viable/strict gh/eellison/344/orig` returned nonzero exit code 1 ``` Rebasing (1/1) Automerging test/inductor/test_torchinductor.py Automerging torch/_dynamo/testing.py CONFLICT (content): Merge conflict in torch/_dynamo/testing.py Automerging torch/_inductor/codegen/wrapper.py Automerging torch/_inductor/graph.py error: could not apply 598d18867d... Storage offsets of inputs hint: Resolve all conflicts manually, mark them as resolved with hint: ""git add/rm "", then run ""git rebase continue"". hint: You can instead skip this commit: run ""git rebase skip"". hint: To abort and get back to the state before ""git rebase"", run ""git rebase abort"". Could not apply 598d18867d... Storage offsets of inputs ``` Raised by https://github.com/pytorch/pytorch/actions/runs/3517976326","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,"KPI: Average time to first response, average time to respond to any turn over message",Integrate with GitHub API to collect:  average time to first response   average time to turn over message Data will be collected only from GitHub discussions.,2022-10-25T14:56:32Z,module: windows triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/87699,"Grafana plugin we use doesn't have support for fetching comments and related metadata. I investigated few options and tested some stuff.  **Details** I was able to fetch all necessary metadata and parse it through curl + Powershell. Thou GitHub requires paging (max 100 comments at a time), so it's not just single query to GitHub, but some caching + crawling mechanism is needed. **Options** We do have these options for this ticket:  Drop it for now in case we evaluate it does not worth the effort.  Extend Grafana GitHub plugin (implementation wise plugin seems fairly short and simple, my guess would be couple of days of development. What I don't know at this moment is how deployment & merge part would go ...)  Write some REST API in Azure (serverless API?) Implementation wise second and third options are quite similar, they need to crawl and cache. Benefits / Disbenefits:  GitHub plugin  No cost for us, no maintanance needed. Cache is already implemented in plugin (might be needed to extend the API).  REST API  Easier to deliver than GitHub plugin (no need to learn existing code, no problems with delivery). Cost us regularly money and we'll need to maintain it. **Results of internal discussion** At this moment these metrics do not worth investment. Closing ticket as won't fix."
rag,KPI: Average age of issues,Integrate with GitHub API to collect:  average age of issues with `module: windows` tag.,2022-10-25T14:52:42Z,module: windows triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/87698,Average age of the issues are now shown in 'Days' format.
rag,[LTC] Remove tensor.storage_,"Summary: Since LTC now supports functionalization, we don't need to fake a storage to support is_alias_of anymore. Let's remove it. Test Plan:  ./build/bin/test_lazy gtest_filter=LazyOpsTest.IsAliasOf",2022-10-24T22:24:33Z,open source Merged ciflow/trunk,closed,0,27,https://github.com/pytorch/pytorch/issues/87645,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: alanwaketan / name: Jiewen Tan  (a9bad4c9c5e75e014ba96835f63601729f0b2215, 7534405c73e1ab5655f2235b0a32ee77fdc04297, 8965d36a3e239f8076cbf969ba8ce20d0e1a949d)",The iOS failures don't seem to be related.,Thanks Jack and Brian for approving this pull request.,  merge, Merge failed **Reason**: The following mandatory check(s) failed (Rule `Lazy Tensor`):  EasyCLA Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 1 additional jobs have failed, first few of them are: build Details for Dev Infra team Raised by workflow job ", merge f," merge f ""The iOS failures are not related.""", merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `Lazy Tensor`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `Lazy Tensor`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `Lazy Tensor`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," merge f ""The iOS failures are not related.""", merge,"The merge job was canceled. If you believe this is a mistake,then you can re trigger it through pytorchbot.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
agent,Fix use after free in tensorpipe agent,"Stack from ghstack: * * CC(Fix use after free in tensorpipe agent) Fix use after free in tensorpipe agent** Fixes CC(Use after free in TensorPipeAgent), which identifies use after free for reverse device maps. This is only in the dynamic RPC feature and not effecting stable RPC code path. Unfortunately the test `TensorPipeRpcTest.test_dynamic_rpc_existing_rank_can_communicate_with_new_rank_cuda` that is failing is also running into separate issue. I've temporarily disabled some of the test code to investigate the error in asychronously. Testing plan:  tested all the dynamic RPC tests",2022-10-24T19:30:44Z,Merged ciflow/trunk release notes: distributed (rpc) topic: bug fixes,closed,0,3,https://github.com/pytorch/pytorch/issues/87627,, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,Reenable test_fake_crossref_backward_amp_cholesky_inverse_cuda_float32,  CC(Reenable test_fake_crossref_backward_amp_cholesky_inverse_cuda_float32)  CC(Disallow duplicated py_imp registration)  CC(Fix checking if CompositeImplicitAutograd key exists in Library)  CC(Prefer python meta function over c++ meta function),2022-10-22T12:55:49Z,topic: not user facing,closed,0,0,https://github.com/pytorch/pytorch/issues/87548
yi,RAM leak when copying tensor from cpu to cuda," 🐛 Describe the bug If you profile run the following code then you'll see that line 8 grabs ~2GB of RAM, but it doesn't get released: ` import gc import torch from memory_profiler import profile  def test(n):     t1 = torch.ones(n)     t2 = torch.ones(n).to(""cuda"")     del t1     del t2     gc.collect()     torch.cuda.empty_cache() if __name__ == ""__main__"":     test(3000000) `  Versions PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.12 (default, Sep 10 2021, 00:16:05)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.214120.368.amzn2.x86_64x86_64withglibc2.27 Is CUDA available: True CUDA runtime version: Could not collect CUDA_MODULE_LOADING set to: GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 510.47.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] botorch==0.2.1 [pip3] gpytorch==1.4.2 [pip3] mypy==0.982 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.4 [pip3] pytorchlightning==1.7.7 [pip3] torch==1.12.0 [pip3] torchmetrics==0.10.0 [pip3] torchvision==0.13.1 ",2022-10-22T02:17:55Z,module: cuda module: memory usage triaged,open,0,4,https://github.com/pytorch/pytorch/issues/87539,"This is caused by lazy loading cuda libraries at this point, the memory is not expected to get released. "," Thanks for replying! Could you elaborate, please?", any updates? ,"It's not a leak, it's RAM required by cuda libraries"
yi,Disallow duplicated py_imp registration,"  CC(Consistent meta reg across dispatcher, pydispatcher, active_meta_table)  CC(Fix stride for prims.where)  CC(Fix _refs for aten.zeros/ones/empty/randn)  CC(Disallow duplicated py_imp registration)  CC(Fix checking if CompositeImplicitAutograd key exists in Library)  CC(Prefer python meta function over c++ meta function)",2022-10-21T21:03:11Z,ciflow/trunk topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/87503," This PR needs a label If your changes are user facing and intended to be a part of release notes, please use a label starting with `release notes:`. If not, please add the `topic: not user facing` label. For more information, see https://github.com/pytorch/pytorch/wiki/PyTorchAutoLabelBotwhycategorizeforreleasenotesandhowdoesitwork.",Folded into https://github.com/pytorch/pytorch/pull/87426
rag,as_strided_scatter storage offset defaults to None not 0,  CC(as_strided_scatter storage offset defaults to None not 0) Signedoffby: Edward Z. Yang ,2022-10-21T16:57:55Z,Merged ciflow/trunk,closed,0,9,https://github.com/pytorch/pytorch/issues/87481, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,This is true for as_strided as well btw:  CC(as_strided_ doc is missing),I checked `as_strided` but it seems to be None,"Ho nice, I guess it was updated recently!", Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""ci sev problem"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Some operations do not keep `channels_last` memory format which yields accuracy drop," 🐛 Describe the bug I tried to switch to `torch.channels_last` in order to reduce the training time but I noticed that the new training shows quite a performance drop. While looking deeper into the cause, I noticed that there are some modules which do not keep the `torch.channels_last` memory format: ```python from functools import partial import itertools import torch import torch.nn as nn devices = ['cpu', 'cuda'] modules = [partial(nn.ConvTranspose2d, in_channels=3, out_channels=3, kernel_size=(2, 2), stride=(2, 2), bias=False),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(2, 2), stride=(2, 2), bias=False),            partial(nn.Linear, in_features=5, out_features=5, bias=False),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(1, 1)),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(1, 2), stride=1, bias=False),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(1, 2), stride=1, padding=(0, 1)),            nn.GELU,            partial(nn.BatchNorm2d, num_features=3)] for device, module in itertools.product(devices, modules):     m = module()      create random input     inp = torch.randn(2, 3, 4, 5)      cast input and module to device/memory format     inp = inp.to(memory_format=torch.channels_last).to(device=device)     m = m.to(memory_format=torch.channels_last).to(device=device)     out = m(inp)     if not out.is_contiguous(memory_format=torch.channels_last):         print(f'Operation does not keep channels_last for '               f'device = {device}, module = {module} but has now stride {out.stride()}') ``` which yields ```bash Operation does not keep channels_last for device = cpu, module = functools.partial(, in_channels=3, out_channels=3, kernel_size=(2, 2), stride=(2, 2), bias=False) but has now stride (240, 80, 10, 1) Operation does not keep channels_last for device = cpu, module = functools.partial(, in_features=5, out_features=5, bias=False) but has now stride (60, 20, 5, 1) Operation does not keep channels_last for device = cuda, module = functools.partial(, in_features=5, out_features=5, bias=False) but has now stride (60, 20, 5, 1) ``` As `nn.Linear` does not keep the channels_last memory layout on the GPU, this then causes a problem, as the following modules are now in `torch.channels_last` but the input is not anymore: ```python from functools import partial import itertools import torch import torch.nn as nn devices = ['cpu', 'cuda'] memory_formats = [torch.contiguous_format, torch.channels_last] modules = [partial(nn.ConvTranspose2d, in_channels=3, out_channels=3, kernel_size=(2, 2), stride=(2, 2), bias=False),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(2, 2), stride=(2, 2), bias=False),            partial(nn.Linear, in_features=5, out_features=5, bias=False),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(1, 1)),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(1, 2), stride=1, bias=False),            partial(nn.Conv2d, in_channels=3, out_channels=3, kernel_size=(1, 2), stride=1, padding=(0, 1)),            nn.GELU,            partial(nn.BatchNorm2d, num_features=3)] for device, memory_format, module in itertools.product(devices, memory_formats, modules):     m1 = module()     m2 = module()      use identical weights for both modules     m2.load_state_dict(m1.state_dict())      create random input     inp = torch.randn(2, 3, 4, 5)      cast module to device/memory format     m1 = m1.to(device=device)     m2 = m2.to(memory_format=memory_format).to(device=device)      cast input to device (keeping `contiguous` memory format)     inp = inp.to(device=device)     out1 = m1(inp)     out2 = m2(inp)     if not torch.allclose(out1, out2):         print(f'Results differ for device = {device}, memory_format = {memory_format}, module = {module} '               f'with max difference {torch.max(torch.abs(out1  out2)).item()}') ``` which is a problem for the `Conv2d`: ```bash Results differ for device = cuda, memory_format = torch.channels_last, module = functools.partial(, in_channels=3, out_channels=3, kernel_size=(2, 2), stride=(2, 2), bias=False) with max difference 0.0002925097942352295 Results differ for device = cuda, memory_format = torch.channels_last, module = functools.partial(, in_channels=3, out_channels=3, kernel_size=(1, 1)) with max difference 0.0007414817810058594 Results differ for device = cuda, memory_format = torch.channels_last, module = functools.partial(, in_channels=3, out_channels=3, kernel_size=(1, 2), stride=1, bias=False) with max difference 0.000747382640838623 Results differ for device = cuda, memory_format = torch.channels_last, module = functools.partial(, in_channels=3, out_channels=3, kernel_size=(1, 2), stride=1, padding=(0, 1)) with max difference 0.0005008578300476074 ``` So I think the `nn.Conv2d` silently assumes that the input has the correct memory layout which is not the case due to a previous module that brought it back to be contiguous.  Versions PyTorch version: 1.12.1+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.3 (default, Jul  2 2020, 16:21:59)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.4.0107genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.2.152 CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: NVIDIA RTX A6000 GPU 1: NVIDIA RTX A6000 GPU 2: NVIDIA RTX A6000 GPU 3: NVIDIA RTX A6000 GPU 4: NVIDIA RTX A6000 GPU 5: NVIDIA RTX A6000 GPU 6: NVIDIA RTX A6000 GPU 7: NVIDIA RTX A6000 Nvidia driver version: 470.103.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.4.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.4.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.20.3 [pip3] torch==1.12.1+cu113 [pip3] torchaudio==0.12.1+cu113 [pip3] torchinfo==1.7.1 [pip3] torchvision==0.13.1+cu113 [pip3] torchviz==0.0.2 [conda] Could not collect ",2022-10-21T13:09:03Z,triaged module: memory format,open,0,1,https://github.com/pytorch/pytorch/issues/87451,"I had another look and noticed that the error that I can see seems to be due to cuDNN using TF32 memory format. By adding ```python torch.backends.cuda.matmul.allow_tf32 = False torch.backends.cudnn.allow_tf32 = False ``` the difference for the outputs of `Conv2d` w/ and w/o `torch.channels_last` vanishes. Probably, cuDNN is using a different algorithm for the convolution if the module is put into `channels_last` mode and can make use TF32 such that we loose precision (but the computation itself is faster). So in summary, there seems to be another problem with my training, which I need to find  for `Conv2d` everything seems fine if the input is not in `torch.channels_last`. The only thing to look into would be the operations that do not keep `channels_last` (`ConvTranspose2d` on `cpu` and `Linear` on `cpu` and `cuda`)."
yi,consider numel args when identifying aligned args,Fixes ISSUE_NUMBER https://github.com/pytorch/torchdynamo/issues/1527  ,2022-10-20T19:46:19Z,Merged ciflow/trunk module: inductor ciflow/inductor,closed,0,5,https://github.com/pytorch/pytorch/issues/87394,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: stumpOS  (18a7cad753b0d69e5d9b564f382505aeac270d8d, 01ef16373d4e0cd3599e1e02a299c09aa42a3751, 0914bbab7d214fc962a2c0946871e8a4865a638a, ffcdf3779de10fdc1aa5a6204da645d64dab8102)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[functorch][docs] Downgrade the warning about forward-mode AD coverag…,"…e ( CC([functorch][docs] Downgrade the warning about forwardmode AD coverage)) Previously we claimed that ""forwardmode AD coverage is not that good"". We've since improved it so I clarified the statement in our docs and downgraded the warning to a note. Test Plan:  view docs Pull Request resolved: https://github.com/pytorch/pytorch/pull/87383 Approved by: https://github.com/samdow Fixes ISSUE_NUMBER",2022-10-20T18:55:26Z,ciflow/trunk,closed,0,0,https://github.com/pytorch/pytorch/issues/87386
rag,[functorch][docs] Downgrade the warning about forward-mode AD coverage,"Stack from ghstack:  CC([functorch][docs] Downgrade the warning about forwardmode AD coverage) Previously we claimed that ""forwardmode AD coverage is not that good"". We've since improved it so I clarified the statement in our docs and downgraded the warning to a note. Test Plan:  view docs",2022-10-20T18:06:24Z,Merged ciflow/trunk,closed,0,3,https://github.com/pytorch/pytorch/issues/87383," merge f ""docsonly change; docs builds have passed"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Enable `src_mask` in fast path of `TransformerEncoderLayer `," Issues Fixes  CC(Transformer and CPU path with `src_mask` raises error with torch 1.12)issuecomment1179435674  Description Passing a 2D attention mask `src_mask` into the fast path of `TransformerEncoderLayer` in CPU was causing an error and so was disabled in https://github.com/pytorch/pytorch/pull/81277. This PR unrolls this fix, enabling `src_mask` on the fast path:  Either attention mask `src_mask` of shape `(L, L)` or padding mask `src_key_padding_mask` of shape `(B, L)` are now allowed on the CPU fast path. If softmax is applied along the last dimension (as in multihead attention), these masks are processed without expanding them to 4D. Instead, when iterating through the input, `Softmax.cpp::host_softmax` converts the index to match the mask dimensions, depending on the type.  If softmax is applied along the dimension other than the last, `Softmax.cpp::masked_softmax_cpu` expands masks to 4D, converting them to `mask_type=2`. Theoretically one could also add special optimized cases for `dim=0, 1, 2` and process them without mask expansion, but I don't know how often is that used  Tests:  `test_transformerencoderlayer_fast_path` is extended to cover both attention mask and padding mask  `test_masked_softmax_mask_types_0_1` is added to ensure results from CPU softmax with attention and padding masks match the explicit slow calculation  `test_masked_softmax_devices_parity` is added to ensure results from masked softmax on CPU and CUDA match  Note I had to replace `float` with `torch.get_default_dtype()` in a couple of tests for the following reason:  `test_nn.py` sets the default type to `torch.double`  If I execute `test_nn.py` and `test_transformers.py` in one `pytest` run, this default still holds for transformer tests  Some tests in `test_transformers.py` which were previously following the slow path now switched to fast path, and hardcoded `float` started clashing with default `double` Let me know if there is a better way around it  or maybe I'm not supposed to run tests with `pytest` like this",2022-10-20T17:39:24Z,Merged ciflow/trunk release notes: nn,closed,0,19,https://github.com/pytorch/pytorch/issues/87377,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: sgrigory / name: Grigory Sizov  (f897a73e3db09ed47586e32fb12ab20bd68bf0ff, a4e9b1e1a69dce87c829dc43aafdd0a3ab80df8f, 7b3e282d2e3e2304727ffab5676f3d22240b39fe)",,/easycla, rebase," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge," Merge failed **Reason**: Approval needed from one of the following (Rule 'superuser'): EscapeZero, weiwangmeta, suphoff, ymao1993, xcheng16, ... Details for Dev Infra team Raised by workflow job "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `enablesrcmaskbettertransformer` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout enablesrcmaskbettertransformer && git pull rebase`)"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), merge (Initiating merge automatically since Phabricator Diff has merged), Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
llm,[Bug]: OPTForCausalLM failing with TORCHDYNAMO_DYNAMIC_SHAPES=1: UNPACK_SEQUENCE AssertionError: assert len(seq.items) == inst.argval," 🐛 Describe the bug ```   File ""/raid/ezyang/pytorchscratch2/torch/_dynamo/symbolic_convert.py"", line 349, in run     and self.step()   File ""/raid/ezyang/pytorchscratch2/torch/_dynamo/symbolic_convert.py"", line 322, in step     getattr(self, inst.opname)(inst)   File ""/raid/ezyang/pytorchscratch2/torch/_dynamo/symbolic_convert.py"", line 1000, in UNPACK_SEQUENCE     assert len(seq.items) == inst.argval AssertionError ```    Error logs https://gist.github.com/db994c33e9020d15e4c2ae8dbcf91d2a  Did Dynamo succeed?  [ ] Does dynamo.optimize(""eager"") succeed?  Did AOT succeed?  [ ] Did dynamo.optimize(""aot_eager"") succeed?  Did Inductor succeed?  [ ] Does dynamo.optimize(""inductor"") succeed?  Minified repro ``` TORCHDYNAMO_DYNAMIC_SHAPES=1 AOT_DYNAMIC_SHAPES=0 time python benchmarks/dynamo/huggingface.py  accuracy backend eager training only OPTForCausalLM ``` minifier did not work (minifier ran, but resulting program did not actually fail)",2022-10-20T15:58:53Z,triaged bug oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/93561,"Close as issue seems fixed. Used the same repro command ``` TORCHDYNAMO_DYNAMIC_SHAPES=1 AOT_DYNAMIC_SHAPES=0 time python benchmarks/dynamo/huggingface.py  accuracy backend eager training only OPTForCausalLM ``` Log ``` loading model: 0it [00:06, ?it/s] cuda train OPTForCausalLM                      WARNING:common:fp64 golden ref were not generated for OPTForCausalLM. Setting accuracy check to cosine pass 159.80user 9.26system 0:26.58elapsed 635%CPU (0avgtext+0avgdata 4741528maxresident)k ```"
transformer,"[Bug]: speech_transformer failing with TORCHDYNAMO_DYNAMIC_SHAPES=1: RuntimeError: expand(CUDABoolType{[10, 1, 204, 320]}, size=[-1, 204, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"," 🐛 Describe the bug ``` RuntimeError: expand(CUDABoolType{[10, 1, 204, 320]}, size=[1, 204, 1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4) ``` Cannot minify as the exception occurs outside of dynamo compiled code  Error logs https://gist.github.com/110f0d3706cee14947dd8c4eed9a4c72  Did Dynamo succeed?  [ ] Does dynamo.optimize(""eager"") succeed?  Did AOT succeed?  [ ] Did dynamo.optimize(""aot_eager"") succeed?  Did Inductor succeed?  [ ] Does dynamo.optimize(""inductor"") succeed?  Minified repro ``` TORCHDYNAMO_DYNAMIC_SHAPES=1 python benchmarks/dynamo/torchbench.py  accuracy backend eager training only speech_transformer ``` minifier doesn't work on this example ",2022-10-20T15:45:55Z,triaged bug oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/93560, ,"Close as issue seems fixed. Used the same repro command Download data: ``` wget https://osscidatasets.s3.amazonaws.com/torchbench/data/speech_transformer_inputs.tar.gz ``` Run the command ```  TORCHDYNAMO_DYNAMIC_SHAPES=1 python pytorch/benchmarks/dynamo/torchbench.py  accuracy backend eager training only speech_transformer ``` Log: ``` loading model: 0it [00:05, ?it/s] cuda train speech_transformer                  pass ```"
transformer,TorchScript bug: torch.nn.transformer gives inconsistent results after conversion to TorchScript," 🐛 Describe the bug torch.nn.transformer gives inconsistent results after conversion to TorchScript ```python m = nn.TransformerEncoderLayer(3, 1, 8, batch_first=True)   or batch_first=False  s = torch.jit.script(m) input_1d = torch.randn(1, 5, 3) assert torch.allclose(m(input_1d), s(input_1d)) ```  Versions PyTorch version: 1.12.1 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.6 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.102) CMake version: Could not collect Libc version: N/A Python version: 3.9.12 (main, Apr  5 2022, 01:52:34)  [Clang 12.0.0 ] (64bit runtime) Python platform: macOS12.6arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA CUDA_MODULE_LOADING set to: N/A GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.12.1 [pip3] torchliter==0.3.3 [pip3] torchtext==0.13.1 [pip3] torchvision==0.13.1 [conda] numpy                     1.23.3                   pypi_0    pypi [conda] torch                     1.12.1                   pypi_0    pypi [conda] torchliter                0.3.3                    pypi_0    pypi [conda] torchtext                 0.13.1                   pypi_0    pypi [conda] torchvision               0.13.1                   pypi_0    pypi",2022-10-20T15:35:20Z,oncall: jit,closed,1,2,https://github.com/pytorch/pytorch/issues/87368,Same issue on Linux using GPU PyTorch version: 1.12.1 Cuda 11.3 Python 3.9.13,"There are dropout layers in transformer modules. If set module and scripted module to eval, then the outputs are consistent."
agent,Use after free in TensorPipeAgent," 🐛 Describe the bug CC([Dynamic RPC] Add graceful shutdown for dynamic RPC members) introduced a useafterfree bug: ```     for (const auto& it : reverseDeviceMaps_) {       if (reverseDeviceMaps.find(it.first) == reverseDeviceMaps.end()) {         reverseDeviceMaps_.erase(it.first);       }     } ``` In this loop it deletes the iterator it is currently using, in other words the `erase` call invalidates the current iterator which is then wrongly used to get the next iterator, i.e. there is a (hidden) call to `++it` on the invalidated iterator. This leads to crashes e.g. in TensorPipeRpcTest.test_without_world_size_existing_rank_can_communicate_with_new_rank_cuda The correct code would look similar to the below code for devices: ``` auto iter = devices_.begin();     while (iter != devices_.end()) {       if (std::find(devices.begin(), devices.end(), *iter) == devices.end()) {         iter = devices_.erase(iter);       } else {         iter++;       }     } ```  Versions Started at 1.12.0, still there in 1.13 and master ",2022-10-20T12:38:11Z,high priority triage review oncall: distributed,closed,0,3,https://github.com/pytorch/pytorch/issues/87359,Huang could you please follow up on this,"Hey , thanks for catching this. This definitely looks like an issue. Luckily, this code path is only executed for a prototype feature (when `world_size` is not specified in `init_rpc`) so the stable RPC features / framework do not encounter this bug. I will send out a fix for this shortly.",BTW: I suggest to use a forloop for this to properly scope the `iter` variable. In the patch I'm using I had problems due to `iter` being used for the deviceloop already and polutes the scope. I.e. `for(auto iter = devices_.begin(); iter != devices_.end(); /*empty*/) {`
transformer,[torch.float16] Tensor product @ produces different output when batch size is changed," 🐛 Describe the bug **Issue:** With torch.float16 precision, tensor product @ from several released pytorch versions produce different output results when batch size is changed. (nn.Linear is also affected) This bug may be serious in networks with multiple stacked linear layers like Transformers, since errors could be accumlated through layers. It seems like this bug existed from version 1.10 to 1.14, and as far as I know many new models from various work were trained on these releases.  **How to reproduce?** ```python import torch import random import torch.nn as nn from torch.cuda.amp import autocast seed = 1234 dtype = torch.float16 random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.cuda.set_device(0) torch.backends.cuda.matmul.allow_tf32 = False net = nn.Linear(768, 768).to('cuda:0', dtype=dtype) net.eval() K = 10 N = 10 x = torch.randn(K, 768).to('cuda:0', dtype=dtype) y = torch.randn(N, 768).to('cuda:0', dtype=dtype) z = torch.cat([x, y], dim=0) print('') print('nn.Linear: ') with torch.no_grad():     a = net(x)     b = net(z) print(torch.abs(a[:K]b[:K]).sum().item()) print('') print('Product @: ') with torch.no_grad():     c = x @ net.weight.data.t() + net.bias.data.view(1, 1)     d = z @ net.weight.data.t() + net.bias.data.view(1, 1) print(torch.abs(c[:K]d[:K]).sum().item()) ``` **Expected behaviour:** Zeros should be printed. **Result:** ```bash >>> torch.__version__ '1.14.0.dev20221020+cu117' Env: pytorchnightly from pip Device: Nvidia A6000 Driver verison: 520.56.06 nn.Linear: 0.96044921875 Product @: 0.84619140625 ``` ```bash >>> torch.__version__ '1.14.0.dev20221019' Env: pytorchnightly from conda Device: Nvidia Titan RTX Driver verison: 515.76 nn.Linear: 0.0 Product @: 0.0 ``` ```bash >>> torch.__version__ '1.13.0a0+08820cb' Env: NGC 22.07 Device: Nvidia A6000 Driver verison: 520.56.06 nn.Linear: 0.96044921875 Product @: 0.84619140625 ``` ```bash >>> torch.__version__ '1.10.0a0+ecc3718' Env: NGC 21.07 Device: Nvidia A6000 Driver verison: 520.56.06 nn.Linear: 0.82568359375 Product @: 0.84619140625 ``` ```bash >>> torch.__version__ '1.9.0a0+df837d0' Env: NGC 21.03 Device: Nvidia A6000 Driver verison: 520.56.06 nn.Linear: 0.0 Product @: 0.0 ``` ```bash >>> torch.__version__ '1.9.0+cu102' Env: pytorch installed from pip Device: Nvidia Titan RTX Driver verison: 515.76 nn.Linear: 0.0 Product @: 0.0 ```",2022-10-20T09:51:21Z,module: numerical-stability,closed,0,5,https://github.com/pytorch/pytorch/issues/87355,"This is expected, https://pytorch.org/docs/stable/notes/numerical_accuracy.html, cublas uses different kernels for different size inputs under the hood, pytorch calls directly into cublas kernels for these operations. ","> This is expected, https://pytorch.org/docs/stable/notes/numerical_accuracy.html, cublas uses different kernels for different size inputs under the hood, pytorch calls directly into cublas kernels for these operations. Hi, thanks for your reply.  Even though it's expected, inconsistancy across releases is still problematic, I guess I have to do more investigation about this.",Inconsistency across releases is most likely due to different cuda versions. ,"> Inconsistency across releases is most likely due to different cuda versions. Yes, you're probably correct. Seems like cuda 11.3 has some changes on matrix production ops. But I am still wondering why nn.Linear has different behaviour with ops ""@""? And more, torch.mm, torch.bmm, torch ""@"" are not the same. Guess they decide to use cuda kernel depends on context in different ways as well.  Anyway, thanks for your notice and your reply saved my day.","`nn.Linear` calls `addmm` that fuses addition operation to matmul, that results in a different and more accurate result than doing addition separately. With `@` you are doing addition separately. `mm` calls into cublasGemm, `bmm` calls into `cublasBatchedGemm` (even if batch size is 1), so whatever cublas decides to do in these cases. "
transformer,dynamo/aot fails when run with autograd.detect_anomaly context manager,"I was trying to debug https://github.com/pytorch/torchdynamo/issues/1712 and more specifically why it was doing ""double"" backward() call. We have a context manager called autograd.detect_anomaly which allows you to see which part of ""forward"" made the ""backward"" error out. Enabling that context manager errors out. ``` $ python main.py cuda model Transformer /home/soumith/code/examples/word_language_model/main.py:232: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.   with torch.autograd.detect_anomaly(): /home/soumith/code/pytorch/torch/autograd/__init__.py:300: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error: Module stack: {}   File ""/home/soumith/code/examples/word_language_model/model.py"", line 151, in forward     return F.log_softmax(output, dim=1)  (Triggered internally at /home/soumith/code/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:114.)   return Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass Traceback (most recent call last):   File ""/home/soumith/code/examples/word_language_model/main.py"", line 233, in      train()   File ""/home/soumith/code/examples/word_language_model/main.py"", line 188, in train     output = model(data)   File ""/home/soumith/code/pytorch/torch/_dynamo/eval_frame.py"", line 137, in __call__     return self.forward(*args, **kwargs)   File ""/home/soumith/code/pytorch/torch/_dynamo/eval_frame.py"", line 134, in forward     return optimized_forward(*args, **kwargs)   File ""/home/soumith/code/pytorch/torch/_dynamo/eval_frame.py"", line 157, in _fn     return fn(*args, **kwargs)   File ""/home/soumith/code/examples/word_language_model/model.py"", line 138, in forward     def forward(self, src, has_mask=True):   File ""/home/soumith/code/pytorch/torch/_dynamo/eval_frame.py"", line 157, in _fn     return fn(*args, **kwargs)   File ""/home/soumith/code/pytorch/functorch/_src/aot_autograd.py"", line 858, in forward     return compiled_f(   File ""/home/soumith/code/pytorch/functorch/_src/aot_autograd.py"", line 844, in new_func     compiled_fn = create_aot_dispatcher_function(   File ""/home/soumith/code/pytorch/functorch/_src/aot_autograd.py"", line 564, in create_aot_dispatcher_function     aot_dispatch_autograd(flat_fn, fake_flat_tensor_args, aot_config)   File ""/home/soumith/code/pytorch/functorch/_src/aot_autograd.py"", line 390, in aot_dispatch_autograd     fx_g = make_fx(joint_forward_backward)(*joint_inputs)   File ""/home/soumith/code/pytorch/torch/fx/experimental/proxy_tensor.py"", line 663, in wrapped     t = dispatch_trace(wrap_key(func, args, fx_tracer), tracer=fx_tracer, concrete_args=tuple(phs))   File ""/home/soumith/code/pytorch/torch/_dynamo/eval_frame.py"", line 157, in _fn     return fn(*args, **kwargs)   File ""/home/soumith/code/pytorch/torch/fx/experimental/proxy_tensor.py"", line 413, in dispatch_trace     graph = tracer.trace(root, concrete_args)   File ""/home/soumith/code/pytorch/torch/_dynamo/eval_frame.py"", line 157, in _fn     return fn(*args, **kwargs)   File ""/home/soumith/code/pytorch/torch/fx/_symbolic_trace.py"", line 739, in trace     (self.create_arg(fn(*args)),),   File ""/home/soumith/code/pytorch/torch/fx/_symbolic_trace.py"", line 614, in flatten_fn     tree_out = root_fn(*tree_args)   File ""/home/soumith/code/pytorch/torch/fx/experimental/proxy_tensor.py"", line 427, in wrapped     out = f(*tensors)   File ""/home/soumith/code/pytorch/functorch/_src/aot_autograd.py"", line 166, in joint_forward_backward     backward_out = torch.autograd.grad(   File ""/home/soumith/code/pytorch/torch/autograd/__init__.py"", line 300, in grad     return Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass   File ""/home/soumith/code/pytorch/torch/utils/_python_dispatch.py"", line 101, in __torch_dispatch__     return old.__torch_dispatch__(func, types, args, kwargs)   File ""/home/soumith/code/pytorch/torch/fx/experimental/proxy_tensor.py"", line 453, in __torch_dispatch__     return self.inner_torch_dispatch(func, types, args, kwargs)   File ""/home/soumith/code/pytorch/torch/fx/experimental/proxy_tensor.py"", line 478, in inner_torch_dispatch     out = proxy_call(self, func, args, kwargs)   File ""/home/soumith/code/pytorch/torch/fx/experimental/proxy_tensor.py"", line 256, in proxy_call     raise RuntimeError( RuntimeError: It appears that you're trying to get value out of a tracing tensor with aten._local_scalar_dense.default  erroring out! It's likely that this is caused by datadependent control flow or similar. ``` ",2022-10-19T06:20:27Z,triaged oncall: pt2,closed,0,2,https://github.com/pytorch/pytorch/issues/93547,it only fails with `nan` checking on. `torch.autograd.detect_anomaly(check_nan=False)` passes,"It seems torch.autograd.detect_anomaly compose fairly with torch.compile but could be wrong since there's no anomaly in the body.  Repro internally D51598964. Need to download the dataset and rename the data. ``` wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext2v1.zip  need to rename data set to train.txt, valid.txt and test.txt python main.py cuda model Transformer ``` Will close it for now but feel free to reopen it if it's still an issue."
transformer,Quantization Aware Training not supported for nn.Embedding layers.," 🐛 Describe the bug It seems like Quantization Aware Training with Torch>=10.2 does not support nn.Embedding layers. The following example code: ```python class Model(nn.Module):     def __init__(self):         super().__init__()         self.embed = nn.Embedding(100,100)     def forward(self,x):         return self.embed(x) model = Model() model.train() model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') model = torch.quantization.prepare_qat(model) torch.quantization.convert(model) ``` Runs into the following error: ```python torch/nn/quantized/modules/embedding_ops.py:162, in Embedding.from_float(cls, mod)     160 dtype = weight_observer.dtype     161 is_float_qparams_qconfig = weight_observer.qscheme == torch.per_channel_affine_float_qparams > 162 assert is_float_qparams_qconfig, \     163     'Embedding quantization is only supported with float_qparams_weight_only_qconfig.'     165 assert dtype == torch.quint8 or dtype == torch.quint4x2, \     166     f'The only supported dtype for nnq.Embedding is torch.quint8 and torch.quint4x2, got {dtype}'     168  Run the observer to calculate qparams. AssertionError: Embedding quantization is only supported with float_qparams_weight_only_qconfig. ``` Issues  CC(Is it planning to support nn.Embeddings quantization?)) and [ CC([quant] Add support for Embedding/EmbeddingBag quantization via dynamic quant APIs)]( CC([quant] Add support for Embedding/EmbeddingBag quantization via dynamic quant APIs)issue998785028) offer solutions for static and dynamic quantization but not for quantization aware training. Is there any solution to this? If not, fixing this issue would be interesting for applications in transformers among others.  Thanks for the help!  Versions PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 8.3.1 20190311 (Red Hat 8.3.13) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.17 Python version: 3.9.12 (main, Sep 14 2022, 07:55:59)  [GCC 4.8.5 20150623 (Red Hat 4.8.544)] (64bit runtime) Python platform: Linux5.4.861.el7.x86_64x86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 10.2.89 CUDA_MODULE_LOADING set to:  GPU models and configuration:  GPU 0: Tesla V100PCIE16GB GPU 1: Tesla V100PCIE16GB Nvidia driver version: 440.64.00 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] gpytorch==1.9.0 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.3 [pip3] torch==1.12.1 [pip3] torchtbprofiler==0.4.0 [conda] Could not collect ",2022-10-18T08:58:45Z,oncall: quantization,closed,0,4,https://github.com/pytorch/pytorch/issues/87187,"to unblock, can you try setting the correct qconfig to embeddings manually?","Hi , thanks for getting back to me. I guess this could be done manually but it seems very heavy.  All the examples I find prepare QAT using: ```python model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') model = torch.quantization.prepare_qat(model) ``` If I understand what you're suggesting, I would need to pass a qconfig_dict dictionary that maps modules to their respective qconfig. But `torch.quantization.prepare_qat` sets this qconfig_dict to None without the option to pass one: ```python propagate_qconfig_(model, qconfig_dict=None)  In torch.ao.quantization.prepare_qat ``` Shouldn't this be fixed so that Embedding layers can be quantized in default configurations like it was the case on previous versions to torch?","Hi Git ,  Your example above uses the Eager mode quantization workflow.  In this workflow, the user is responsible for manually specifying how to quantize everything, so the user is expected to specify how they would like to quantize each layer.  The default config to quantize embeddings is different compared to the default config to quantize conv/linear, which is why the default config is not working for you.  You can use something like this to easily do this: ``` for _, mod in model.named_modules():     if isinstance(mod, torch.nn.Embedding):         mod.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig ``` We also have a graph based workflow, FX graph mode quantization.  In that workflow, we have default per layer quantization configurations.  Currently they don't include embeddings  that is a bug we will fix, thanks for the report.",looks like this one is fixed
yi,fsdp lazy_init typo,"Minor typo, changed with > without",2022-10-18T08:12:21Z,open source Merged ciflow/trunk release notes: distributed (fsdp) topic: not user facing,closed,0,9,https://github.com/pytorch/pytorch/issues/87184,The committers listed above are authorized under a signed CLA.:white_check_mark: login: chiaolun / name: Chiao  (bfec297c23bca497115f1861b4079dabc71ebc49), merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , rebase s, successfully started a rebase job. Check the current status here,"Successfully rebased `patch1` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout patch1 && git pull rebase`)", Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
transformer,DDPOptimizer+inductor OOMs with hf_GPT2_large and timm_vision_transformer_large,"Repro: `gpui oversubscribe python run_benchmark.py distributed ngpus 1 model torchbenchmark.models.hf_GPT2_large.Model trainer torchbenchmark.util.distributed.core_model.trainer.Trainer job_dir /fsx/users/dberard/scratchlocal/benchfast/benchmark/logs cluster local torchdynamo inductor optimize_dynamo_ddp` Error: ``` Traceback (most recent call last):   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/submitit/core/submission.py"", line 54, in process_job     result = delayed.result()   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/submitit/core/utils.py"", line 133, in result     self._result = self.function(*self.args, **self.kwargs)   File ""/fsx/users/dberard/scratchlocal/benchfast/benchmark/torchbenchmark/util/distributed/submit.py"", line 123, in __call__     return trainer_class(self.args, model_class, model_args=self.model_args).measure()   File ""/fsx/users/dberard/scratchlocal/benchfast/benchmark/torchbenchmark/util/distributed/core_model/trainer.py"", line 90, in measure     self.benchmark.invoke()   File ""/fsx/users/dberard/scratchlocal/benchfast/benchmark/torchbenchmark/util/model.py"", line 243, in invoke     self.train()   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_dynamo/eval_frame.py"", line 157, in _fn     return fn(*args, **kwargs)   File ""/fsx/users/dberard/scratchlocal/benchfast/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py"", line 123, in train     def train(self):   File ""/fsx/users/dberard/scratchlocal/benchfast/benchmark/torchbenchmark/util/framework/huggingface/model_factory.py"", line 124, in      outputs = self.model(**self.example_inputs)   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_tensor.py"", line 488, in backward     torch.autograd.backward(   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/autograd/__init__.py"", line 197, in backward     Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/autograd/function.py"", line 270, in apply     return user_fn(self, *args)   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/functorch/_src/aot_autograd.py"", line 464, in backward     CompiledFunction.compiled_bw = aot_config.bw_compiler(   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_dynamo/optimizations/backends.py"", line 555, in _wrapped_bw_compiler     return disable(bw_compiler(*args, **kwargs))   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_dynamo/utils.py"", line 85, in time_wrapper     r = func(*args, **kwargs)   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_inductor/compile_fx.py"", line 353, in bw_compiler     return compile_fx_inner(   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_dynamo/debug_utils.py"", line 450, in debug_wrapper     compiled_fn = compiler_fn(gm, example_inputs, **kwargs)   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_inductor/debug.py"", line 178, in inner     return fn(*args, **kwargs)   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/contextlib.py"", line 75, in inner     return func(*args, **kwds)   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_inductor/compile_fx.py"", line 120, in compile_fx_inner     compiled_fn = cudagraphify(   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_dynamo/utils.py"", line 85, in time_wrapper     r = func(*args, **kwargs)   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_inductor/compile_fx.py"", line 183, in cudagraphify     return cudagraphify_impl(model, inputs, static_input_idxs)   File ""/data/home/dberard/miniconda/envs/benchfast/lib/python3.8/sitepackages/torch/_inductor/compile_fx.py"", line 249, in cudagraphify_impl     model(list(static_inputs))   File ""/tmp/torchinductor_dberard/wm/cwmx6z66cmrszqbfpvgyixq7nrq23aagw7qu7or5ijuzlvwfucn4.py"", line 470, in call     buf9 = empty_strided((1280, 1280), (1280, 1), device='cuda', dtype=torch.float32) torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.41 GiB total capacity; 31.47 GiB already allocated; 7.50 MiB free; 38.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try sett ing max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF ``` ",2022-10-18T03:26:24Z,triaged oncall: pt2,closed,0,3,https://github.com/pytorch/pytorch/issues/93543,  is this still valid?,Can we have an updated cmd for the test?,Closing it as it seems not interesting any more but feel free to reopen.
transformer,Add tensor.to()/tensor._to_copy() support to nested_tensor," 🚀 The feature, motivation and pitch This is quite a common operation so could be used for many things, but a major thing this enables is a module using nested_tensor as a parameter. Many PyTorch frameworks, here huggingface transformers' `Trainer` class will attempt to load a model on the CPU before copying it over to a CUDA device (or this is what it looks like at least). Say we have a module somewhere in the module tree which has ```     class MyRaggedModule(nn.Module):         def __init__(self, sizes, device=None):             super().__init__()             self.weights = torch.nn.Parameter(torch.nested.nested_tensor([                 torch.empty(size, device=device, dtype=torch.float)                 for size in sizes             ]))             self.reset_parameters()         def reset_parameters(self):             with torch.no_grad():                 bits = self.weights.unbind()                 for bit in bits:                     torch.nn.init.normal_(bit) ``` Then when we attempt to train, the framework will try to copy the tensor, which will fail  ``` Traceback (most recent call last):   File ""train.py"", line 176, in      main()   File ""train.py"", line 164, in main     trainer = Trainer(   File ""/users/frrobert/.local/lib/python3.10/sitepackages/transformers/trainer.py"", line 437, in __init__     self._move_model_to_device(model, args.device)   File ""/users/frrobert/.local/lib/python3.10/sitepackages/transformers/trainer.py"", line 703, in _move_model_to_device     model = model.to(device)   File ""/CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1112, in to     return self._apply(convert)   File ""/CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 764, in _apply     module._apply(fn)   File ""/CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 787, in _apply     param_applied = fn(param)   File ""/CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1110, in convert     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking) NotImplementedError: Could not run 'aten::_to_copy' with arguments from the 'NestedTensorCUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_to_copy' is only available for these backends: [CPU, CUDA, HIP, XLA, MPS, IPU, XPU, HPU, VE, Lazy, Meta, PrivateUse1, PrivateUse2, PrivateUse3, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedXLA, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedLazy, QuantizedMeta, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, CustomRNGKeyId, MkldnnCPU, SparseCPU, SparseCUDA, SparseHIP, SparseXLA, SparseMPS, SparseIPU, SparseXPU, SparseHPU, SparseVE, SparseLazy, SparseMeta, SparsePrivateUse1, SparsePrivateUse2, SparsePrivateUse3, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher]. ```  Alternatives ModuleList or arrange for model to never be copied  Additional context _No response_ ",2022-10-16T07:24:28Z,triaged module: nestedtensor,closed,0,3,https://github.com/pytorch/pytorch/issues/87034,"Thanks for opening up this issue, we are actually working on implementing this function and should have a PR soon. The author of that PR is . Will post updates on this issue.",The PR is CC(Add support for .to() for NestedTensor backends),Looks like it's merged now so closing. Thanks  !
rag,Saving and loading from physical storage," 🚀 The feature, motivation and pitch Physical storage allows us to work with very large tensors. Ideally, saving a tensor from physical storage into a file would not create a copy of the full tensor in memory. I believe that a call to `torch.save` will already do that. However, when loading the tensor, it will be directly loaded on memory, not on physical storage. This is a problem if the tensor is too big. A nice feature would be that a tensor that was saved on physical storage would be loaded on physical storage again, without creating a full copy in memory: ```python x = torch.zeros(10).share_memory_() torch.save(x, ""x.pt"") x_loaded = torch.load(""x.pt"") x.is_shared()   the tensor is on physical storage again ```   Alternatives TorchSnapshot offers an alternative where we can directly save a tensor from physical to physical storage, then load a tensor in place on another one in any memory location without creating a full copy in memory (hence solving this problem). I don't think PyTorch intends to do inplace tensor loading but still, I think that keeping the memory location (as we keep the device) would be a nicetohave feature.  Additional context We have implemented in TorchRL a buffer API where big tensors that would not fit in RAM are stored on disk (say > 1Tb). Through the use of memmap arrays, we can quickly access any given set of rows of these tensors during sampling from the buffer. This is much faster than accessing shared tensors stored in a list for instance. When checkpointing, we'd like to save directly those tensors from disk storage to disk storage, without loading the full tensor on RAM. We would also like to load tensors that were saved on physical storage on the same storage location, without loading the full tensor in memory.",2022-10-16T05:38:58Z,module: memory usage module: serialization triaged,open,0,2,https://github.com/pytorch/pytorch/issues/87033,"Supporting outparameter tensorloading would also be a nice thing! also some relevant discussion in  CC(large model, low memory: need `torch.load` that loads one submodule at a time) and  CC(`torch.load(..., weights_only=True)` currently raises a Deprecation warning + [proposal] `weights_only=True` should become default for safe legacy-loading pickles) /  CC([feature request] LazyTensor that provides/loads/computes its contents only upon request to be returned from torch.load) (maybe does hdf5 already support slice loading/saving directly to the disk file via mmap?)  ",and  CC(RFC: multi-part `torch.load`/`torch.save` to support huge models and/or low CPU memory)
chat,AttributeError: Can't get attribute 'ChatDataset' on <module '__mp_main__' from 'c:\\Users\\hp\\Desktop\\Pytorch\\train.py'> Traceback (most recent call last):," 🐛 Describe the bug import json from nltk_utlis import tokenize, stem, bag_of_words import numpy as np import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader from model import NeuralNet if __name__ == '__main__':     with open('intents.json', 'r') as f:         intents = json.load(f)     all_words = []     tags =[]     xy = []     for intent in intents['intents']:         tag = intent['tag']         tags.append(tag)         for pattern in intent['patterns']:             w = tokenize(pattern)             all_words.extend(w)             xy.append((w, tag))     ignore_words = ['?', '!', '.', ',']     all_words = [stem(w) for w in all_words if w not in ignore_words]     all_words = sorted(set(all_words))     tags = sorted(set(tags))     x_train = []     y_train = []     for (pattern_sentence, tag) in xy:         bag = bag_of_words(pattern_sentence, all_words)         x_train.append(bag)         label = tags.index(tag)         y_train.append(label)      x_train = np.array(x_train)     y_train = np.array(y_train)     class ChatDataset(Dataset):         def __init__(self):             self.n_samples = len(x_train)             self.x_data = x_train             self.y_data = y_train         def __getitem__(self, index):             return self.x_data[index], self.y_data[index]         def __len__(self):             return self.n_samples     Hyperparameters     batch_size = 8     hidden_size = 8     output_size = len(tags)     input_size = len(x_train[0])     learning_rate = 0.001     num_epochs = 1000     dataset = ChatDataset()     train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')     model = NeuralNet(input_size, hidden_size, output_size).to(device)      loss and optimizer      criterion = nn.CrossEntropyLoss()     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)      Train the model     for epoch in range(num_epochs):         for (words, labels) in train_loader:             words = words.to(device)             labels = labels.to(device)              Forward pass             outputs = model(words)              if y would be onehot, we must apply              labels = torch.max(labels, 1)[1]             loss = criterion(outputs, labels)              Backward and optimize             optimizer.zero_grad()             loss.backward()             optimizer.step()         if (epoch+1) % 100 == 0:             print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')     print(f'final loss: {loss.item():.4f}')  Versions [nltk_data] Downloading package punkt to [nltk_data]     C:\Users\hp\AppData\Roaming\nltk_data... [nltk_data]   Package punkt is already uptodate! [nltk_data] Downloading package punkt to [nltk_data]     C:\Users\hp\AppData\Roaming\nltk_data... [nltk_data]   Package punkt is already uptodate! Traceback (most recent call last):   File """", line 1, in    File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py"", line 116, in spawn_main     exitcode = _main(fd, parent_sentinel)   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\multiprocessing\spawn.py"", line 126, in _main     self = reduction.pickle.load(from_parent) AttributeError: Can't get attribute 'ChatDataset' on  Traceback (most recent call last):   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\utils\data\dataloader.py"", line 1163, in _try_get_data     data = self._data_queue.get(timeout=timeout)   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\multiprocessing\queues.py"", line 114, in get     raise Empty _queue.Empty The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""c:\Users\hp\Desktop\Pytorch\train.py"", line 82, in      for (words, labels) in train_loader:   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\utils\data\dataloader.py"", line 681, in __next__     data = self._next_data()   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\utils\data\dataloader.py"", line 1359, in _next_data     idx, data = self._get_data()   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\utils\data\dataloader.py"", line 1325, in _get_data     success, data = self._try_get_data()   File ""C:\Users\hp\AppData\Local\Programs\Python\Python310\lib\sitepackages\torch\utils\data\dataloader.py"", line 1176, in _try_get_data     raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e RuntimeError: DataLoader worker (pid(s) 8052) exited unexpectedly **Everytime I try to run this code I gets the same error even tough the guy who taught me this code in his pc he is not getting any error can anyone please help me? Thank you.**  ",2022-10-15T16:30:49Z,module: dataloader triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/87016,"Please move you class definition out of 'if __name__ == ""__main__"".","> Please move you class definition out of 'if **name** == ""**main**"". Even after removing if **name** == ""**main**"". it is receiving the same error","I mean ```py class ChatDataset:     ... if __name__ == ""__main__"":     dataset = ChatDataset()     ... ``` Please take a reference from https://pytorch.org/docs/stable/data.htmlplatformspecificbehaviors","> I mean >  > ```python > class ChatDataset: >     ... >  > if __name__ == ""__main__"": >     dataset = ChatDataset() >     ... > ``` >  > Please take a reference from https://pytorch.org/docs/stable/data.htmlplatformspecificbehaviors Thank you that thing worked but now a new error has arisen. **ValueError: optimizer got an empty parameter list** can you please help me out with this","> Thank you that thing worked but now a new error has arisen. >  > **ValueError: optimizer got an empty parameter list** >  > can you please help me out with this Based on this Error, I would guess it comes from your model `NeuralNet` where `parameter` are not properly attached. Please check https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.htmlpytorchcustomnnmodules If the Error still persists, please share a minimum reproducible script to us. I am closing this Issue since it's not datarelated."
rag,Fixed partitioner issue with getitem and made metadata a storage more consistent,  CC(Fixed partitioner issue with getitem and made metadata a storage more consistent),2022-10-15T02:49:10Z,Merged ciflow/trunk release notes: fx,closed,0,3,https://github.com/pytorch/pytorch/issues/87012, merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[MPS] Copy from CPU always add storageOffset,Because why wouldn't it? Fixes  CC([MPS] tensors loaded via torch.load() -- when transferred to GPU -- do not honour indices/views),2022-10-14T04:29:30Z,Merged ciflow/trunk release notes: mps ciflow/mps,closed,0,3,https://github.com/pytorch/pytorch/issues/86958," merge f ""MPS tests passing"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[DONOTLAND] Skip py_imp if meta function is already registered to C++ dispatcher #86825,  CC([DONOTLAND] Skip py_imp if meta function is already registered to C++ dispatcher 86825),2022-10-13T21:33:41Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/86942
rag,Fix incorrect tensor storage check ,"Fix incorrect tensor storage check  This change contains an incorrect check for storage: https://github.com/pytorch/pytorch/pull/86557 **self.storage is not None** should have been: **not torch._C._has_storage(self)** These fixes were run through the DirectML test suite, and confirm the check is now working correctly.",2022-10-12T22:47:37Z,triaged open source Merged ciflow/trunk,closed,0,7,https://github.com/pytorch/pytorch/issues/86845, If you get a moment to review. This is a fix to the previous commit around private use. , merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: 2 additional jobs have failed, first few of them are: trunk ,trunk / linuxbioniccuda11.7py3.10gcc7 / test (nogpu_NO_AVX2, 1, 1, linux.2xlarge) Details for Dev Infra team Raised by workflow job "," merge f ""flaky cuda build failure: Stream content length mismatch. Received 619773476 of 626228992 bytes."""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,DISABLED test_self_remote_rref_as_rpc_arg_sparse (__main__.TensorPipeTensorPipeAgentRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_self_remote_rref_as_rpc_arg_sparse` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-10-12T21:42:47Z,oncall: distributed module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/86840,This occurs before CC(Revert distributed test parallelization) was landed. Closing this issue to see if CC(Revert distributed test parallelization) fixed it. 
yi,"[DONOTLAND, Experiment] Skip py_imp if meta function is already registered to C++ dispatcher","  CC([DONOTLAND, Experiment] Skip py_imp if meta function is already registered to C++ dispatcher)",2022-10-12T20:44:40Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/86825
rag,Allow PrivateUse1 backends to not have Storage (#86557),"This is cherrypick of https://github.com/pytorch/pytorch/pull/86557 to enable the DirectML outoftree plugin in 1.13. Link to landed master PR (if applicable): https://github.com/pytorch/pytorch/pull/86557, https://github.com/pytorch/pytorch/pull/86845 Criteria Category: Critical fixes for enabling the DirectML outoftree backend plugin in 1.13.",2022-10-12T17:08:41Z,open source ciflow/trunk,closed,0,1,https://github.com/pytorch/pytorch/issues/86803,Will reopen with this change also CPed: https://github.com/pytorch/pytorch/pull/86845
yi,Make stateless.functional_call support weight tying," 🚀 The feature, motivation and pitch A tied weight is a model that looks like ```python class Foo(nn.Module):   def __init__(self):     self.weight = nn.Parameter(torch.randn(3))     self.tied_weight = self.weight   def forward(self, inp):     ... ``` Currently functorch's `make_functional` supports weight tying by only returning one copy of it to the user from. In other words, if `model` is an instance of `Foo` from above and a user calls `model_fn, params = make_functional(model)`, then `params` will only have one element and any updates done to that tensor will reflect in both usages of the parameter This is not true in `stateless.functional_call`. If you pass a value for `weight`, `tied_weight` will not use that value. Similarly, you could pass a separate value for `weight` and `tied_weights`. This has caused issues for AOTAutograd) and will cause issues for functorch as we deprecate `make_functional` for rationalization Pitch is that `stateless.functional_call` should support weight tying (i.e. passing a value for `weight` and having it be used for `tied_weight`).  Alternatives We would likely need to error if this supports weight tying and a user passed a value for both `weight` and `tied_weight` (since it's ambiguous which to use). That's BCbreaking on a now public API so if we don't want to do this some alternatives include:  We introduce this as a flag (`stateless.functional_call(..., tie_weights=True)` which ties when only a value for `weight` or `tied_weights` is passed, but errors when both passes)  We introduce a wrapper function around `stateless.functional_call` that ties weights correctly (and errors as we expected when both weight and tied_weight are passed)  Additional context _No response_ ",2022-10-11T18:34:08Z,module: nn triaged module: functorch,closed,0,0,https://github.com/pytorch/pytorch/issues/86708
transformer,DISABLED test_transformer_offload_true_no_shard_norm_type_None (__main__.TestParityWithDDP),"Platforms: rocm This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_transformer_offload_true_no_shard_norm_type_None` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-10-11T06:53:45Z,oncall: distributed module: flaky-tests skipped module: fsdp,closed,0,3,https://github.com/pytorch/pytorch/issues/86675,"varma  I saw unit test flakiness for `NO_SHARD` and CPU offloading when using subtesting when writing unit tests for `use_orig_params=True`. As a result, I separated CPU offloading tests into their own test. Either `NO_SHARD` has an actual stream synchronization issue, or there is something else at play. Unfortunately, I was not able to see any stream synchronization issue. Also, we cannot directly say that only the `NO_SHARD` code path has an issue; it is possible that calling allgather in the preforward _masks_ the underlying issue, which may be common to sharded and nonsharded cases.","https://github.com/pytorch/pytorch/pull/87930 The PR sets `non_blocking=False` for the gradient offload in the postbackward for `NO_SHARD` and `CPUOffload(True)`. In local testing, this seems to have eliminated the test flakiness. I will close this for now to reenable the unit test.",> Another case of trunk flakiness has been found here. Reopening the issue to disable. Please verify that the platforms list includes all of [linux]. Closing this again because the trunk flakiness mentioned here was from a commit that landed before the commit to fix (https://github.com/pytorch/pytorch/pull/87930).
rag,out of bounds for storage of size," 🐛 Describe the bug Hello, I recently used MQbench for multiGPU QAT quantization training and reported a bug that was out of storage. when I changed the code to single GPU training it worked. The detailed error message is as follows. ``` Traceback (most recent call last):                                                                                                                                                                                            File ""main.py"", line 546, in                                                                                                                                                                                          main()                                                                                                                                                                                                                    File ""main.py"", line 301, in main                                                                                                                                                                                             train(train_loader, model, criterion, optimizer, scheduler, epoch, args)                                                                                                                                                  File ""main.py"", line 415, in train                                                                                                                                                                                            optimizer.zero_grad()                                                                                                                                                                                                     File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/_tensor.py"", line 307, in backward                   torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)                          File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/autograd/__init__.py"", line 156, in backward                                                                                                                       allow_unreachable=True, accumulate_grad=True)   allow_unreachable flag                                   RuntimeError: setStorage: sizes [1000], strides [1], storage offset 2874348, and itemsize 4 requiring a storage size of 11501392 are out of bounds for storage of size 11497648 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 67503) of binary: /usr/bin/python3                                                                                             Traceback (most recent call last):                                                                                                                                                                                            File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main                                                                                                                                                          ""__main__"", mod_spec)                                                                                                                                                                                                     File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code                                                                                                                                                                     exec(code, run_globals)                                                                                                                                                                                                   File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/distributed/launch.py"", line 193, in                                                                                                                       main()                                                                                                                                                                                                                    File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/distributed/launch.py"", line 189, in main                                                                                                                          launch(args)                                                                                                                                                                                                              File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/distributed/launch.py"", line 174, in launch                                                                                                                        run(args)                                            File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/distributed/run.py"", line 713, in run                                                                                                                              )(*cmd_args)                                         File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))                                                                                                                                                           File ""/home/tiger/.local/lib/python3.7/sitepackages/torch/distributed/launcher/api.py"", line 261, in launch_agent                                                                                                            failures=result.failures,                                                                                                                                                                                               torch.distributed.elastic.multiprocessing.errors.ChildFailedError:                                            ============================================================                                                  main.py FAILED                                                                                           Failures:                                                NO_OTHER_FAILURES  ``` mutiGPU training main function code is as follows. ``` def main():     args = parser.parse_args()     args.quant = not args.not_quant     args.backend = BackendMap[args.backend]     if args.seed is not None:         random.seed(args.seed)         torch.manual_seed(args.seed)         cudnn.deterministic = True         warnings.warn('You have chosen to seed training. '                       'This will turn on the CUDNN deterministic setting, '                       'which can slow down your training considerably! '                       'You may see unexpected behavior when restarting '                       'from checkpoints.')     if args.gpu is not None:         warnings.warn('You have chosen a specific GPU. This will completely '                       'disable data parallelism.')      distributed setting     torch.cuda.set_device(args.local_rank)     torch.distributed.init_process_group(backend='nccl',                                          init_method='env://')     args.world_size = torch.distributed.get_world_size()     args.distributed = True      create model     if args.pretrained:         print(""=> using pretrained model '{}'"".format(args.arch))         model = models.__dict__args.arch     else:         print(""=> creating model '{}'"".format(args.arch))         model = models.__dict__[args.arch]()      for internal cluster     if args.model_path:         new_state_dict = OrderedDict()         pretrained_dict = torch.load(args.model_path)         model_dict = model.state_dict()         keys = []         for k, v in model_dict.items():             keys.append(k)         for k1,k2 in zip(keys, pretrained_dict):             new_state_dict[k1] = pretrained_dict[k2]         print(f'load pretrained checkpoint from: {args.model_path}')         model.load_state_dict(new_state_dict)      quantize model     if args.quant:         model = prepare_by_platform(model, args.backend)      replace_bn_to_syncbn(model)     if not torch.cuda.is_available():         print('using CPU, this will be slow')     elif args.distributed:          For multiprocessing distributed, DistributedDataParallel constructor          should always set the single device scope, otherwise,          DistributedDataParallel will use all available devices.         if args.gpu is not None:             torch.cuda.set_device(args.gpu)             model.cuda(args.gpu)              When using a single GPU per process and per              DistributedDataParallel, we need to divide the batch size              ourselves based on the total number of GPUs we have             args.batch_size = int(args.batch_size / ngpus_per_node)             args.workers = int((args.workers + ngpus_per_node  1) / ngpus_per_node)             model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])         else:             model.cuda()              DistributedDataParallel will divide and allocate batch_size to all              available GPUs if device_ids are not set             model = torch.nn.parallel.DistributedDataParallel(model)     elif args.gpu is not None:         torch.cuda.set_device(args.gpu)         model = model.cuda(args.gpu)     else:          DataParallel will divide and allocate batch_size to all available GPUs         if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):             model.features = torch.nn.DataParallel(model.features)             model.cuda()         else:             model = torch.nn.DataParallel(model).cuda()      define loss function (criterion) and optimizer     criterion = nn.CrossEntropyLoss().cuda()     if args.optim == 'sgd':         optimizer = torch.optim.SGD(model.parameters(), args.lr,                                     momentum=args.momentum,                                     weight_decay=args.weight_decay)     elif args.optim == 'adam':         optimizer = torch.optim.Adam(model.parameters(), args.lr,                                      betas=(0.9, 0.999), eps=1e08,                                      weight_decay=args.weight_decay,                                      amsgrad=False)      prepare dataset     train_loader, train_sampler, val_loader, cali_loader = prepare_dataloader(args)     if args.lr_scheduler == 'cosine':         if args.schedule_per_iter:             scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * args.epochs)         else:             scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)     elif args.lr_scheduler == 'step':         if args.decay_schedule is not None:             milestones = list(map(lambda x: int(x), args.decay_schedule.split('')))             for i in range(len(milestones)):                 milestones[i] = milestones[i] * len(train_loader)         else:             milestones = [(args.epochs+1) * len(train_loader)]         scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=args.gamma)      optionally resume from a checkpoint     if args.resume:         if os.path.isfile(args.resume):             print(""=> loading checkpoint '{}'"".format(args.resume))             if args.gpu is None:                 checkpoint = torch.load(args.resume)             else:                  Map model to be loaded to specified single gpu.                 loc = 'cuda:{}'.format(args.gpu)                 checkpoint = torch.load(args.resume, map_location=loc)             args.start_epoch = checkpoint['epoch']             best_acc1 = checkpoint['best_acc1']             if args.gpu is not None:                  best_acc1 may be from a checkpoint from a different GPU                 best_acc1 = best_acc1.to(args.gpu)             state_dict = checkpoint['state_dict']             model_dict = model.state_dict()             if 'module.' in list(state_dict.keys())[0] and 'module.' not in list(model_dict.keys())[0]:                 for k in list(state_dict.keys()):                     state_dict[k[7:]] = state_dict.pop(k)             model.load_state_dict(checkpoint['state_dict'])             optimizer.load_state_dict(checkpoint['optimizer'])             print(""=> loaded checkpoint '{}' (epoch {}), acc = {}""                   .format(args.resume, checkpoint['epoch'], best_acc1))         else:             print(""=> no checkpoint found at '{}'"".format(args.resume))     elif args.quant:         enable_calibration(model)         calibrate(cali_loader, model, args)     cudnn.benchmark = True     if args.quant:         enable_quantization(model)     if args.quant and args.deploy:         convert_deploy(model.eval(), args.backend, input_shape_dict={'data': [10, 3, 224, 224]})         return     if args.evaluate:         if args.quant:             from mqbench.convert_deploy import convert_merge_bn             convert_merge_bn(model.eval())         validate(val_loader, model, criterion, args)         return     for epoch in range(args.start_epoch, args.epochs):         if args.distributed:             train_loader.sampler.set_epoch(epoch)          adjust_learning_rate(optimizer, epoch, args)          train for one epoch         train(train_loader, model, criterion, optimizer, scheduler, epoch, args)          evaluate on validation set         acc1 = validate(val_loader, model, criterion, args)          remember best acc and save checkpoint         is_best = acc1 > best_acc1         best_acc1 = max(acc1, best_acc1)         if dist.get_rank() == 0:             save_checkpoint({                 'epoch': epoch + 1,                 'arch': args.arch,                 'state_dict': model.module.state_dict(),                 'best_acc1': best_acc1,                 'optimizer' : optimizer.state_dict(),             }, is_best) ```  Versions PyTorch version: 1.10.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Debian GNU/Linux 10 (buster) (x86_64) GCC version: (Debian 8.3.06) 8.3.0 Clang version: Could not collect CMake version: version 3.18.4 Libc version: glibc2.28 Python version: 3.7.3 (default, Jan 22 2021, 20:04:44)  [GCC 8.3.0] (64bit runtime) Python platform: Linux5.4.56.bsk.10amd64x86_64withdebian10.12 Is CUDA available: True CUDA runtime version: 11.3.109 GPU models and configuration:  GPU 0: A100SXM80GB GPU 1: A100SXM80GB GPU 2: A100SXM80GB GPU 3: A100SXM80GB GPU 4: A100SXM80GB GPU 5: A100SXM80GB GPU 6: A100SXM80GB GPU 7: A100SXM80GB Nvidia driver version: 450.142.00 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.5.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.5.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] bytedtorch==1.10.0.post11 [pip3] numpy==1.21.5 [pip3] torch==1.10.0 [pip3] torchaudio==0.9.0 [pip3] torchvision==0.11.1+cu113 [conda] Could not collect ",2022-10-11T06:41:48Z,oncall: distributed oncall: quantization triaged,closed,0,5,https://github.com/pytorch/pytorch/issues/86674," Can you confirm this script works with a singleGPU and also works on multiGPU without QAT?  I think we need some tutorials on DDP and FSDPbased QAT, wdyt?",">  Can you confirm this script works with a singleGPU and also works on multiGPU without QAT? >  >  I think we need some tutorials on DDP and FSDPbased QAT, wdyt? Yes, I confirm that this code works on singleGPU with QAT or on multiGPU without QAT.",">  I think we need some tutorials on DDP and FSDPbased QAT, wdyt? I don't see how DDP/FSDP will fail because of quantization specific reasons. in this case can we tune the parameters for multiGPU training to see whether it is actually caused by OOM? does multGPU training require more memories than singleGPU training?"," Hi, have you solved this problem? I also met this problem when QAT with multi GPUs using DDP, but it works when training with only one GPU","could you open the issue in MQBench? https://github.com/ModelTC/MQBench, closing for now"
rag,DISABLED test_periodic_model_averager_param_group (__main__.TestDistBackendWithSpawn),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_periodic_model_averager_param_group` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-10-11T04:20:09Z,oncall: distributed module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/86669,Fixed by https://github.com/pytorch/pytorch/pull/86756
yi,[ONNX] Avoid copying constant in scalar_type_analysis pass,"  CC([ONNX] Avoid copying constant in scalar_type_analysis pass)  CC([ONNX] Fix scalar_type_analysis metadata for copied constant) To prevent unnecessarily increasing model size due to duplicating constants in graph. Follow up work is needed to apply CSE on graph, in case the cast nodes are constant folded, and reintroducing duplicated constants. This issue was discovered in CC([ONNX] CSE pass in export pollutes Scope information). ~~This PR also adds unit test coverage for scope information of nodes when they are altered by CSE and related passes.~~ Edit: Convert to draft now that it appears some legacy caffe2 tests are breaking, which might depend on the behavior of copying constants.",2022-10-10T23:39:02Z,module: onnx open source Stale release notes: onnx topic: bug fixes,closed,0,2,https://github.com/pytorch/pytorch/issues/86648,The committers listed above are authorized under a signed CLA.:white_check_mark: login: BowenBao / name: Bowen Bao  (c5b5cf5058cc9f7cebb9d2cf55557b7772e6137e),"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Allow PrivateUse1 backends to not have Storage,"Allow PrivateUse1 backends to not have Storage To unblock the DirectML backend, this change would be needed for 1.13 as well. The DirectML backend creates tensors using the open registration pattern documented here: https://pytorch.org/tutorials/advanced/extend_dispatcher.html registration example However, DirectML tensors are opaque, and do not have Storage. The DirectML Tensor Impl derives from OpaqueTensorImpl, which does not have a storage. Because of this various places in the code fail that expect storage to be present. We had made various changes intree to accommodate this: a.	def __deepcopy__(self, memo): https://github.com/pytorch/pytorch/blob/b5acba88959698d35cb548c78dd3fb151f85f28b/torch/_tensor.pyL119 or self.device.type in [""lazy"", ""xla"", ""mps"", ""ort"", ""meta"", ""hpu"", 'dml'] b.	def _reduce_ex_internal(self, proto): https://github.com/pytorch/pytorch/blob/b5acba88959698d35cb548c78dd3fb151f85f28b/torch/_tensor.pyL275 if self.device.type in [""xla"", ""ort"", ""hpu"", ""dml""]: c.	TensorIteratorBase::build has an unsupported list for tensors without storage. https://github.com/pytorch/pytorch/blob/b5acba88959698d35cb548c78dd3fb151f85f28b/aten/src/ATen/TensorIterator.cppL1497 Using the PrivateUse1 backend, similar exemptions need to be made in order to relax requirements on Storage so that the DirectML backend tensors can work.",2022-10-09T16:34:26Z,triaged open source Merged ciflow/trunk,closed,0,19,https://github.com/pytorch/pytorch/issues/86557,"The committers listed above are authorized under a signed CLA.:white_check_mark: login: smk2007 / name: Sheil Kumar  (4411a1a72f56232e1ca28ae7bfd78cbcb8b84003, a89d4bb4af9c122a98a57cd7f4bf1042ba529ada, d1bf7d864d0a591d43f53b903dcc6180c495e72e)","Can you sign the CLA? That's needed to land the PR. > To unblock the DirectML backend, this change would be needed for 1.13 as well. The branch cut for the 1.13 release was a few days ago. There's an option for cherrypicking commits into the branch, but there are some criteria for what's allowed to be cherrypicked. You can see the details for the process here:  CC([v.1.13.0] Release Tracker)","> Can you sign the CLA? That's needed to land the PR. >  > > To unblock the DirectML backend, this change would be needed for 1.13 as well. >  > The branch cut for the 1.13 release was a few days ago. There's an option for cherrypicking commits into the branch, but there are some criteria for what's allowed to be cherrypicked. You can see the details for the process here: CC([v.1.13.0] Release Tracker) Thanks bdhirsh!  I see some of the CI checks failed. They dont seem related to the changes. Is there a way to rerun these failing CI tests?",> Thanks for identifying issues with using the `PrivateUse1` key! These changes look reasonable to me. I believe i need a maintainer to approve. Is there someone I need to ping to make this happen? Thanks!, merge g, Merge started Your change will be merged once all checks on your PR pass since you used the green (g) flag (ETA: 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , merge l, Merge started **The `l` land checks flag is deprecated and no longer needed.** Instead we now automatically add the `ciflow\trunk` label to your PR once it's approved Your change will be merged once all checks on your PR pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job , rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `user/sheilk/privateuse1` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout user/sheilk/privateuse1 && git pull rebase`)", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","> Can you sign the CLA? That's needed to land the PR. >  > > To unblock the DirectML backend, this change would be needed for 1.13 as well. >  > The branch cut for the 1.13 release was a few days ago. There's an option for cherrypicking commits into the branch, but there are some criteria for what's allowed to be cherrypicked. You can see the details for the process here: CC([v.1.13.0] Release Tracker)  Created this PR for CPing into release/1,13: https://github.com/pytorch/pytorch/pull/86803", sounds good!  you'll also have to put in a request to the infra team on the release tracker issue:  CC([v.1.13.0] Release Tracker),">  sounds good!  you'll also have to put in a request to the infra team on the release tracker issue: CC([v.1.13.0] Release Tracker) Thanks, I also opened this PR into master, which contains a fix to this same change. The storage check was done incorrectly. This should be done correctly now! https://github.com/pytorch/pytorch/pull/86845"
rag,DISABLED test_periodic_model_averager (__main__.TestDistBackendWithSpawn),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_periodic_model_averager` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-10-07T18:48:56Z,oncall: distributed module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/86477,Fixed by https://github.com/pytorch/pytorch/pull/86756
transformer,Allow casting (instead of converting) tensors to/from channels_last to enable mixing Conv2d & Attention," 🚀 The feature, motivation and pitch Transformers are all the rage, and those networks tend to use `BTC` tensor format: batch size, tokens, channels. Regular CNNs however use `NCHW`, and this leads to code mixing the formats looking like this (from the MixTransformer backbone of SegFormer):  ```python x_ = x.permute(0, 2, 1).reshape(B, C, H, W) x_ = self.sr(x_).reshape(B, C, 1).permute(0, 2, 1) x_ = self.norm(x_) ``` i.e. lots of permute memory churn.  Pytorch has experimental support for `channels_last` for NCHW ops: https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html That beta offers an API whereby you can convert a channels_first NCHW tensor to channels_last format. That's great.  However, when mixing BTC modules with NCHW modules, it would be really useful to ""castwithoutconverting"" tensors from one format to the other.  Then the attention modules could be essentially operating on `BHWC`reshapedas`BTC` tensors, while the CNN modules would operate on channels_last`NCHW` tensors (= same effective memory layout, just slightly different tensor meta info).  You'd still have to deal with the `reshape` and castto`channels_last` line noise, but at least you're not shuffling memory around.   Alternatives Create a supertensor that appears like it's `BTC` in Transformercontexts, and channels_last`NCHW` in CNN contexts. (No idea how that would work and it would probably break everything everywhere...)  Additional context This is related to  CC(torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application) but kind of goes in the other direction  shift CNNs to effectively use the Transformer memory layout instead of having Transformer ops support the CNN memory layout. ",2022-10-07T04:54:58Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/86447,Reading  CC(torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application)issuecomment1176482782 it sounds like this might kindofsortof already be possible?  I guess using as_strided would enable the cast I'm looking for. 
transformer,[PT1.13 cherry pick]Fix Transformer's issue when padding is long type & unit test,Summary: Fix the issue described in  CC(TransformerEncoder src_key_padding_mask does not work in eval()) Test Plan: buck test mode/opt caffe2/test:test_transformers  test_train_with_long_type_pad Differential Revision: D40129968,2022-10-06T05:53:35Z,fb-exported cla signed topic: bc breaking,closed,0,8,https://github.com/pytorch/pytorch/issues/86353,The committers listed above are authorized under a signed CLA.:white_check_mark: login: zrphercule / name: Rui Zhu  (17adaf8962f21a2e62e945f565eae12487ea6bc5),This pull request was **exported** from Phabricator. Differential Revision: D40129968,This pull request was **exported** from Phabricator. Differential Revision: D40129968,This pull request was **exported** from Phabricator. Differential Revision: D40129968,"Removing milestone for now, as PR is not yet passing the CI.  any plans to fix the signal and land it?","Seems phabricator's linking has some issue, will export a new one","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," if this is BCbreaking and was cherry picked into 1.13 release could you please write a description for it, see  here for instructions. The release is soon so it would be great if you could do this ASAP. Thank you!"
yi,Error when applying CUDA Graph to a Recommendation System called DLRM," 🐛 Describe the bug When I apply CUDA Graph to DLRM, a TypeError occured: Module.apply() takes 2 positional arguments but 16 were given  Versions CUDA 10.4 pytorch ",2022-10-05T16:35:26Z,triaged module: cuda graphs,closed,0,7,https://github.com/pytorch/pytorch/issues/86281,"Hi , Could you please share a little more details about your use case? A minimal code snippet would be the most helpful in understanding the issue and debugging it.","> Hi , >  > Could you please share a little more details about your use case? A minimal code snippet would be the most helpful in understanding the issue and debugging it. A  Hi, Aidyn I want to apply cuda graph to dlrm. There exist an function to apply cuda graph after capture graph in your code:         def functionalized(*user_args):              Runs the autograd function with inputs == all inputs to the graph that might require grad              (explicit user args + module parameters)              Assumes module params didn't change since capture.             out = Graphed.apply(*(user_args + module_params))             return out[0] if output_was_tensor else out However, when I tried to apply cuda graph to dlrm: a TypeError occured: Module.apply() takes 2 positional arguments but 16 were given. user_args and module_params are 2 positional arguments, there are more input when building DLRM for user_args, it is no longer 1 positional arguments and I don't know why DLRM has more positional arguments on module_params than other models. ",", Are you using https://github.com/facebookresearch/dlrm?","A  Yes, that is it!",", I am not sure what is happening in facebookresearch/dlrm. However, this DLRM implementation was designed to work with CUDA Graphs.","A Ok, thanks a lot!","Closing for now, feel free to reopen this issue if you think there's a bug."
transformer,TransformerEncoder/TransformerDecoder has same initial parameters for all layers," 🐛 Describe the bug Example code (based on documentation): ```python encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6) ``` Then you have: ```python (transformer_encoder.layers[0].linear1.weight == transformer_encoder.layers[1].linear1.weight).all() == True ``` Etc. This is because: ```python self.layers = _get_clones(encoder_layer, num_layers) ``` Where `_get_clones` uses `deepcopy`. The same problem is for `TransformerDecoder`. This is unexpected behavior to me but maybe it is expected and just not well documented? Note that `Transformer` itself does not have this problem because it calls `_reset_parameters` after all the layers are assigned.  Versions (I have an outdated version but I checked the source code of the current version.) ",2022-10-05T15:14:54Z,module: nn triaged,open,0,4,https://github.com/pytorch/pytorch/issues/86274,I think this is expected and not well documented. But it could definitely be changed. Is the desire to make sure the weights are initialized differently between layers? ,"I assume that users usually do not want this. Users of `nn.Transformer` anyway would not have the problem though, and other users might already have overwritten the param init *if they know about this*.","I see. Would you be open to making a PR fixing this? We have a couple of options 1. Add a reset_parameters() function to both TransformerEncoder and TransformerDecoder but do not automatically call it. Let people call it themselves.  2. Do option 1 but also add a call to reset_parameters() in the init function. 3. No code changes and add documentation that all layers will be initialized the same.  I was thinking to just do option 1. As a user, what do you think? ","> Would you be open to making a PR fixing this? Yes, if I find the time. Not sure exactly when. > As a user, what do you think? My main issue here is about unexpected behavior  or rather, it would be nice if a normal use of those modules would yield expected behavior. The problem is introduced due to the use of `deepcopy`. So maybe it makes sense to fix the param init right after the `deepcopy`, i.e. in the `TransformerEncoder` and `TransformerDecoder`, so basically your option 2. Then the `_reset_parameters` in the `Transformer` itself might not be needed anymore? I'm not sure if people might pass `custom_encoder` or `custom_decoder` thought which depend on the `Transformer._reset_parameters` though."
transformer,Autocast with BF16 on CPU slows down model more than 2X," 🐛 Describe the bug Trying to run inference with autocast and bf16 slows down a Hugging Face BERT model immensely. I would expect inference acceleration. This makes it hard to just include autocast in a script for inference and have it run bfloat16 or fp16 depending on the availability of CUDA, since the model might be slowed on. The benchmark is very simple. Code to replicate:  ``` import torch from transformers import AutoModelForSequenceClassification bert_model = AutoModelForSequenceClassification.from_pretrained('bertbaseuncased').eval() tokenizer = AutoTokenizer.from_pretrained('bertbaseuncased', do_lower_case=True) inp_single = ""Bloomberg has decided to publish a new report on global economic situation."" tensor_single = tokenizer(         inp_single,         max_length=128,         pad_to_max_length=True,         add_special_tokens=True,         return_tensors=""pt"",     ) with torch.no_grad():     start = time.time()     for i in range(30):         bert_model(tensor_single['input_ids']) duration =time.time()  start print(f'Normal time: {duration}') with torch.autocast(device_type='cpu',enabled=True,dtype=torch.bfloat16):     with torch.no_grad():          Warmup         for i in range(5):             bert_model(tensor_single['input_ids'])         start = time.time()             for i in range(30):             bert_model(tensor_single['input_ids']) duration =time.time()  start print(f'Autocast time: {duration}') ``` Output:  ``` Normal time: 2.8249216079711914 Autocast time: 7.403575658798218 ``` System Info:  Torch Version: 1.10.0 Transformers Version: 4.9.1 CPU Info: ``` /bin/bash /proc/cpuinfo processor       : 0 vendor_id       : GenuineIntel cpu family      : 6 model           : 85 model name      : Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz stepping        : 7 microcode       : 0x500320a cpu MHz         : 3202.860 cache size      : 36608 KB physical id     : 0 siblings        : 8 core id         : 0 cpu cores       : 4 apicid          : 0 initial apicid  : 0 fpu             : yes fpu_exception   : yes cpuid level     : 13 wp              : yes :         : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer a/proc/cpuinfo ```  Versions Collecting environment information... PyTorch version: 1.10.0 Is debug build: False CUDA used to build PyTorch: 11.1 ROCM used to build PyTorch: N/A OS: Amazon Linux 2 (x86_64) GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.115) Clang version: Could not collect CMake version: version 3.22.3 Libc version: glibc2.26 Python version: 3.8.12  (default, Oct 12 2021, 21:59:51)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.10299.473.amzn2.x86_64x86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.1.105 GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 510.47.03 cuDNN version: Probably one of the following: /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_adv_train.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.0.5 /usr/local/cuda11.0/targets/x86_64linux/lib/libcudnn_ops_train.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_adv_train.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.0.5 /usr/local/cuda11.1/targets/x86_64linux/lib/libcudnn_ops_train.so.8.0.5 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn.so.8.1.1 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.1.1 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_adv_train.so.8.1.1 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.1.1 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.1.1 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.1.1 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_ops_train.so.8.1.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.2 [pip3] numpydoc==1.1.0 [pip3] torch==1.10.0 [pip3] torchmodelarchiver==0.5.0b20211117 [pip3] torchworkflowarchiver==0.2.0b20211118 [pip3] torchaudio==0.10.0 [pip3] torchserve==0.5.0b20211117 [pip3] torchtext==0.11.0 [pip3] torchvision==0.11.1 [conda] blas                      1.0                         mkl   [conda] captum                    0.4.1                         0    pytorch [conda] cudatoolkit               11.1.1               h6406543_9    condaforge [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] libblas                   3.9.0            12_linux64_mkl    condaforge [conda] libcblas                  3.9.0            12_linux64_mkl    condaforge [conda] liblapack                 3.9.0            12_linux64_mkl    condaforge [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0            py38h95df7f1_0    condaforge [conda] mkl_fft                   1.3.1            py38h8666266_1    condaforge [conda] mkl_random                1.2.2            py38h1abd341_0    condaforge [conda] numpy                     1.21.2           py38h20f2e39_0   [conda] numpybase                1.21.2           py38h79a1101_0   [conda] numpydoc                  1.1.0                      py_1    condaforge [conda] pytorch                   1.10.0          py3.8_cuda11.1_cudnn8.0.5_0    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torchmodelarchiver      0.5.0                    py38_0    pytorch [conda] torchworkflowarchiver   0.2.0                    py38_0    pytorch [conda] torchaudio                0.10.0               py38_cu111    pytorch [conda] torchserve                0.5.0                    py38_0    pytorch [conda] torchtext                 0.11.0                     py38    pytorch [conda] torchvision               0.11.1               py38_cu111    pytorch ",2022-10-05T13:01:39Z,module: performance triaged module: bfloat16 module: amp (automated mixed precision),open,0,4,https://github.com/pytorch/pytorch/issues/86270, could this thing be somehow tested automatically? in HF there're a lot of models that could be relatively easy to automate, so do you think that this is a specific model issue and not a hardware issue? ,"i don't know, but if there are some published popular model results with specified hardware, at least you know that on this/that hardware the speedup is indeed there (i think, nvidia gpu's support natively bf16 starting from A100 / Ampere  but the memory savings + more speed from less memory usage should already be there if memoryaccesstimebound)",Agreed! 
yi,PixelShuffle check that output is not null before applying kernel (#85155),* Checks that output tensor is not null before applying kernel in `pixel_shuffle` op * Checks that output tensor is not null before applying kernel in `pixel_unshuffle` op * Add test case testing `pixel_shuffle` with shapes producing empty output * Add test case testing `pixel_unshuffle` with shapes producing empty output Fixes CC(torch.nn.PixelShuffle crash with floating point exception when input has 0 size in the last three dimensions) FYI  ,2022-10-05T06:50:14Z,open source Merged cla signed ciflow/trunk,closed,0,9,https://github.com/pytorch/pytorch/issues/86262,"FWIW the CI error seems unrelated. To avoid these errors in the future, you may want to base your PRs off `viable/strict` rather than `master`. If the error is still there when you want to merge the PR, you'll need to merge via ` merge f ""the CI error is unrelated""` or smth"," merge f ""CI error unrelated to the content of the PR""", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!", Merge failed **Reason**: The following mandatory check(s) failed (Rule `Core Reviewers`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ," merge f ""CI error unrelated""",">  merge f ""CI error unrelated"" Thanks"," successfully started a merge job. Check the current status here. The merge job was triggered with the force (f) flag. This means your change will be merged **immediately**, bypassing any CI checks (ETA: 15 minutes). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,SVD with `full_matrices=False` slower than `full_matrices=True` & `full_matrices=False` on GPU 500x slower than CPU," 🐛 Describe the bug ``` import torch from torch.utils.benchmark import Timer, Compare results = [] def test(_sizes):     x = torch.randn(*_sizes, dtype=torch.float)     xcpu = x.cpu()     xcuda = x.cuda()     def _subtest(stmt, desc, xcpu=xcpu, xcuda=xcuda):         t1 = Timer(             stmt=stmt,             label='svd',             sub_label=str(x.size()),             description=desc,             globals=dict(globals(), **locals())             )         results.append(t1.blocked_autorange())     _subtest('torch.linalg.svd(xcpu,  full_matrices=False)', 'cpu')     _subtest(""torch.linalg.svd(xcuda, full_matrices=False)"", 'cuda') test((100, 10, 10)) test((300, 900, 100)) test((1000, 60, 3)) test((8716, 3, 2)) Compare(results).print() ``` ``` [ svd ]                                      6578951.4 Times are in microseconds (us). ``` If you run the above test, you should see that the SVD for CUDA will be extremely slow, especially for `Nx3x2` and `Nx60x3`. Originally I was going to attach this to CC(SVD is slow on GPU vs CPU for skinny matrices), but didn't want to reopen the closed issue and suspect it may be something different since it only happens when `full_matrices=False`. I'm also not sure if setting `full_matrices=False` is expected to be slower than running with `full_matrices=True`, but it should at least be documented, since I'd expect it to have to do less work and intuitively it should be faster.    Environment Collecting environment information... PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.16.3microsoftstandardWSL2x86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA TITAN Xp GPU 1: NVIDIA TITAN Xp Nvidia driver version: 516.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] pytorchmsssim==0.2.1 [pip3] pytorch3d==0.6.2 [pip3] torch==1.11.0+cu113 [pip3] torchdiffeq==0.2.3 [pip3] torchvision==0.12.0+cu113 [conda] Could not collect  ",2022-10-04T21:12:33Z,module: performance triaged module: linear algebra,closed,0,10,https://github.com/pytorch/pytorch/issues/86234,"For SVD on CUDA, you can try a cuda 11.7 nightly build with `driver=` option.   doc: https://pytorch.org/docs/master/generated/torch.linalg.svd.htmltorch.linalg.svd  nightly build: https://pytorch.org/getstarted/locally/ For nonsquared small matrices (m, n <=32), setting `full_matrices=False` may cause a slower driver being selected. For tall and thin matrices (Nx60x3), the `gesvda` driver may give a better performance.","I'll try out the other drivers, but for my use in practice I would want to stick to stable for now, just so other features remain reliable. I imagine the gesvda driver is probably the one that would work best, as per the benchmark from a previous issue. my specific usecase is 3x3, 3x2, or 2x2 so the matrices I'd need to factor are just tiny and not tall. I haven't written an SVD algorithm (or at least can't remember doing so), but is the cost of specialcasing extremely tiny matrices worth doing? I also made this post in specific because it seems that this issue is hit only when `full_matrices=False`, but when `full_matrices=True`, the performance is fine.","Implementing a reliable, stable and fast SVD algorithm is *very* difficult. Even for small matrices (see e.g. here). I agree with  that playing around with the different algorithms that we expose through the `driver` kwarg may be what you want. Now, I modified a bit the test to compare with/without the `full_matrices` flag, and I got some very surprising resuts: ``` [ svd ]                                                54770.2  ``` other than that, I think that the CUDA results are always comparable with those from CPU even in these notsocommon cases. Note how in the only case when CUDA was much slower than CPU, that is `shape=[1000, 60, 3]`, `gesvda` is able to run at a comparable speed. At any rate, you may want to consider benchmarking CUDA vs casting to CPU, run the op on CPU and then casting back to CUDA for your given inputs, as you may be able to squeeze some efficiency that way."," That's not a cusolver issue but likely our heuristics here https://github.com/pytorch/pytorch/blob/63d8d4f6ec5c973ad7b8669cd39ee9b550e5f55b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cppL848L853 We may need to revisit our u ,s, vt settings or cusolver calls on `full_matrices=False` here.",That is indeed annoying. We don't have a fast and accurate way to compute the SVD for large batches of rectangular matrices :(,"Actually, what we could do in that case is to call the batched path and then copy the relevant slices of the output to `U` and `V`. I'll put up a PR later this week for this case.","That sounds like a good plan, but note that it's not always faster in this way.",I'll throw in some benchmarks on the house :D, do you mind tagging me in the PR? Not that I have much to comment but am curious,"For reference, after https://github.com/pytorch/pytorch/pull/88502issuecomment1305477400 is merged, the new results of the benchmarks are as follows: ``` [ SVD ]                                    7  ```"
yi,Stop modifying the global logger on `import functorch`,Stack from ghstack:  CC([easy] Add spaces to vmap over as_strided error message)  CC(Stop modifying the global logger on `import functorch`)  CC(Reintroduce the functorch docs build (85838)) Fixes  CC(functorch import messed up python logging module) `logging.basicConfig` modifies the global logger which affects other programs. importing a package should generally be sideeffect free so this PR gets rid of that call. Test Plan:  tested locally,2022-10-03T19:22:42Z,Merged cla signed ciflow/trunk release notes: fx fx,closed,0,5,https://github.com/pytorch/pytorch/issues/86147,`format_str` is dead too now right,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," merge f ""preexisting failures"""," successfully started a merge job. Check the current status here. The merge job was triggered with the force (f) flag. This means your change will be merged **immediately**, bypassing any CI checks (ETA: 15 minutes). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,TransformerEncoder src_key_padding_mask does not work in eval()," 🐛 Describe the bug Described here: https://discuss.pytorch.org/t/modelevalpredictsallthesameornearlysameoutputs/155025/12 Starting with version 1.12, TransformerEncoder uses two different code paths, which perform differently between training and evaluation mode. In training, the `why_not_sparsity_fast_path` will be set, and the sparsity fast path will not be used. During eval(), if using a `src_key_padding_mask`, output will be modified: https://github.com/pytorch/pytorch/blob/b04b2fa9aa52cacbdc9aaaf477d55b0af845ce81/torch/nn/modules/transformer.pyL271 ```             if (not why_not_sparsity_fast_path) and (src_key_padding_mask is not None):                 convert_to_nested = True                 output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)                 src_key_padding_mask_for_layers = None ``` The output is usually some constant value. It seems like their might be some misalignment with the expected `src_key_padding_mask` and the `logical_not()` operation. Perhaps the mask inversion should not be performed.  Versions Python version: 3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)] (64bit runtime) Python platform: Windows1010.0.22621SP0 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Nvidia driver version: 516.94 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] efficientnetpytorch==0.7.1 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.3 [pip3] segmentationmodelspytorch==0.3.0 [pip3] torch==1.12.1+cu116 [pip3] torchaudio==0.12.1+cu116 [pip3] torchmetrics==0.9.3 [pip3] torchvision==0.13.1+cu116 [conda] efficientnetpytorch      0.7.1                    pypi_0    pypi [conda] numpy                     1.23.3                   pypi_0    pypi [conda] segmentationmodelspytorch 0.3.0                    pypi_0    pypi [conda] torch                     1.12.1+cu116             pypi_0    pypi [conda] torchaudio                0.12.1+cu116             pypi_0    pypi [conda] torchmetrics              0.9.3                    pypi_0    pypi [conda] torchvision               0.13.1+cu116             pypi_0    pypi ",2022-10-03T15:46:50Z,high priority module: nn triaged,open,0,9,https://github.com/pytorch/pytorch/issues/86120,Shared internally. Thank you for calling this out! ,"Hi  , thanks for bringing this! For better understanding of this issue, could you please give a simple reproduce of this issue? I tried to repro it but I dont think I reproed this issue (Probably due to I misunderstood it.) Thanks!","``` import torch from torch import nn import torch.optim as optim criterion = nn.MSELoss() pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.long) layer = nn.TransformerEncoderLayer(             d_model=2,             dim_feedforward=4,             nhead=2,             batch_first=True,             activation=""gelu"",             dropout=0,         ) encoder = nn.TransformerEncoder(layer, 2) optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9) encoder.train() for i in range(100):     encoder.train()     optimizer.zero_grad()     inputs = torch.cat([torch.randn(1,2,2), torch.zeros(1,2,2)], dim=1)     outputs = encoder(inputs, src_key_padding_mask=pad_mask)     loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])     loss.backward()     optimizer.step()     with torch.no_grad():         test = torch.cat([torch.randn(1,2,2), torch.zeros(1,2,2)], dim=1)         test_train_long = encoder(test, src_key_padding_mask=pad_mask.long())         test_train_bool = encoder(test, src_key_padding_mask=pad_mask.bool())         encoder.eval()         test_eval_long = encoder(test, src_key_padding_mask=pad_mask.long())         test_eval_bool = encoder(test, src_key_padding_mask=pad_mask.bool())         l1_train_train = nn.L1Loss()(test_train_bool[:, 0:2, :], test_train_long[:, 0:2, :]).item()         l1_long = nn.L1Loss()(test_train_long[:, 0:2, :], test_eval_long[:, 0:2, :]).item()         l1_bool = nn.L1Loss()(test_train_bool[:, 0:2, :], test_eval_bool[:, 0:2, :]).item()         if l1_bool > 1e4:             print(f""Eval/Train difference in pad_mask BOOL({i:02d}): {l1_bool:.4f}"")         if l1_long > 1e4:             print(f""Eval/Train difference in pad_mask LONG({i:02d}): {l1_long:.4f}"")         if l1_train_train > 1e4:             print(f""Train/Train difference in pad_mask BOOL/LONG({i:02d}): {l1_long:.4f}"") ``` Sorry it took so long, it was a bit difficult to pin down and the encoder shows the issue intermittently. The issue seems to be related to type conversion of the pad mask. If the pad mask is boolean, then there is no issue; however, when inputting the pad mask as long, the result is different between train and eval (probably due to bool conversion at the `logical_not()`). I found you can also see differences in the results in train() as well using different types. However, the differences are not apparent at most steps. Here is the output from a run I did: ``` Eval/Train difference in pad_mask LONG(34): 0.8490 Train/Train difference in pad_mask BOOL/LONG(34): 0.8490 Eval/Train difference in pad_mask LONG(39): 0.5622 Train/Train difference in pad_mask BOOL/LONG(39): 0.5622 Eval/Train difference in pad_mask LONG(40): 0.5577 Train/Train difference in pad_mask BOOL/LONG(40): 0.5577 Eval/Train difference in pad_mask LONG(41): 0.5929 Train/Train difference in pad_mask BOOL/LONG(41): 0.5929 Eval/Train difference in pad_mask LONG(46): 0.2728 Train/Train difference in pad_mask BOOL/LONG(46): 0.2728 Eval/Train difference in pad_mask LONG(62): 0.4571 Train/Train difference in pad_mask BOOL/LONG(62): 0.4571 Eval/Train difference in pad_mask LONG(68): 0.4367 Train/Train difference in pad_mask BOOL/LONG(68): 0.4367 Eval/Train difference in pad_mask LONG(80): 0.7433 Train/Train difference in pad_mask BOOL/LONG(80): 0.7433 ``` Only a handful of steps in the loop will produce a difference. Most of the time, this code will print differences, but a few times I ran it, it did not."," Thank you very much, we found the reason is mainly because the old version of PyTorch does not support Long type mask. Fix and your unit test: https://github.com/pytorch/pytorch/pull/86353/files","Hi   Can you please explain if both mask = None and src_key_padding_mask = None in the encoder part of the transformer, will I experience this issue or not?","Thanks !  no, this issue is only seen when using src_key_padding_mask.","> Hi  Can you please explain if both mask = None and src_key_padding_mask = None in the encoder part of the transformer, will I experience this issue or not? Hi  No, this bug is actually not related to BetterTransformer; instead it is the old version of PyTorch Transformer did not handle nonbool src_key_padding_mask properly. If you runs the model in inference mode (which will lead you to BetterTransformer), or you are not using src_key_padding_mask with a nonbool dtype, you will not encounter this bug.",Hi  and  thank you for your answers!,We should allow only bool dtype of `src_key_padding_mask`
yi,Add PyInterpreterTLS,"  CC(Add PyInterpreterTLS)  CC(Add some documentation to deploy.h) This is set by torchdeploy to indicate the current Interpreter that you are interacting with.  This will allow Python op registrations to contextually select the correct interpreter to attempt to handle an operator. There is some perf concern as we have to set the TLS on every InterpreterSession entry point.  Hopefully you are not going over these in a tight loop. There is some encapsulation concern.  I sprayed this over every method in InterpreterSession, but I am not sure if this gets every possible way to end up ""in"" libtorch in a context where you could trigger an operator that was overridden by a Python interpreter, and it makes sense to ask the Python interpreter to figure it out (clearly, noninterpreter associated operations should never get the Python treatment). Signedoffby: Edward Z. Yang ",2022-10-03T04:41:15Z,cla signed ciflow/trunk,closed,0,10,https://github.com/pytorch/pytorch/issues/86095," and I discussed this and we're going to try an alternate strategy, where libc10 maintains a vector of ALL known PyInterpreters, and we use TLS to speed up finding the 'active' interpreter for a thread (but in the slow path, we will iterate through the list ""looking"" for the one that thinks it owns a thread)"," the strategy we discussed doesn't fully work. Although each libpython instance ""knows"" if it has relevant state on a thread, the TLS used by each libpython instance is disjoint, so if you touch multiple Python interpreters from a single OS thread, they will all report that they have nonempty thread state, and you are in the same situation of not knowing which one to use. This strategy *does* help if a thread is never used by more than one interpreter, which would be the situation with threading module. So I could use this to solve that problem. But I wouldn't be able to get rid of the TLS on torchdeploy API entry points, as that would be needed to disambiguate if multiple interpreters were used from one thread. Maybe we should reconsider putting the TLS setter on InterpreterSession? We could just have the last session locked ""win""; you should still get a reasonable error message if it's inconsistent, and I don't have to spray the function call everywhere. WDYT?","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," and I discussed this some more and we have a new strategy. First, the easy case is when there is some Tensor argument that belongs to a Python interpreter. In that case, we just use that Python interpreter. But what if there is no argument that belongs to a Python interpreter? In this case, it doesn't matter which interpreter we choose to run the operation on: the Python operator registration is supposed to be equivalent to some C++ code, so we simply need to make sure that whatever Python objects we temporarily allocate to actually run the interpreter get disposed of when we come back. There are two cases to consider: 1. Input tensors. It is possible, if an invocation comes entirely from C++, for none of the Tensors to have PyObjects. We must create PyObjects for them, but we must also not permanently associate the PyObject with the Tensor because the caller may pass the tensor on to its own interpreter. To handle this, we simply create a PyObject that points to Tensor, *without* setting the pyobj on Tensor. In essence, this is what we used to do before we implement PyObject preservation. Furthermore, if the tensor is passed back into Python (e.g., via some inner Python to C++ call), we must reallocate a new PyObject for it in the same way (without setting pyobj). So we will set TLS which *disables PyObject preservation* to ensure that we get pristine C++ objects that can be reused by the other interpreter. 2. Output tensors. Given the TLS above, any allocated tensor will not actually have a PyObject set on them, so once again there is nothing to do.", what about the case when you have a tensor that's shared between multiple python interpreters i.e. we created a model one interpreter and then transferred that model to a different interpreter with the same underlying storage. Is the PyObject only set on the at::tensor and not the at::Storage? https://www.internalfb.com/code/fbsource/[e5194e7ea445]/fbcode/multipy/runtime/interpreter/interpreter_impl.h?lines=2027,"According to Zach when I last asked him about this, we are supposed to be detaching all the tensors when you do this cross interpreter transfer, so this never shows up in reality.",And yes we don't do PyObject preservation on storage, new approach sounds reasonable to me There's a third case where there's two input objects on different interpreters but it seems reasonable to me to discard the PyObjects regardless, what's the performance hit do you expect of not caching PyObjects? Presumably there was one otherwise you wouldn't be caching them,"The caching actually is nothing to do with perf, and for making sure attributes stored in `__dict__` get preserved. Which there aren't any in this case."
transformer,[ONNX] Add transformer tests,Scripted `nn.Transformer` used to be exportable but no longer ( CC([ONNX][bug] `nn.Transformer` contains unsupported tensor scalar type)). We need tests for it to prevent regression.,2022-10-02T16:05:59Z,module: onnx triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/86066
transformer,PyTorch don't have an op for aten::constant_pad_nd but it isn't a special case.," 🐛 Describe the bug When using `torch.onnx.export` for compiling a model from `transformers`, an error indicated the op `aten::constant_pad_nd` isn't supported but it isn't a special case.  Code to reproduce: ```python import torch from transformers import LEDForConditionalGeneration, LEDConfig, LEDTokenizer tokenizer = LEDTokenizer.from_pretrained(""allenai/ledlarge16384arxiv"") model = LEDForConditionalGeneration.from_pretrained(     ""allenai/ledlarge16384arxiv"", ) input_sample = ""This is an example for summarization."" input_sample = tokenizer(     input_sample,     return_tensors=""pt"", ) filepath = ""ledlarge163840arxiv.onnx"" torch.onnx.export(     model,      tuple(input_sample.values()),     f=filepath,     input_names=['input_ids', 'attention_mask'],      output_names=['logits'],      dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},                    'attention_mask': {0: 'batch_size', 1: 'sequence'},                    'logits': {0: 'batch_size', 1: 'sequence'}},      do_constant_folding=True,      opset_version=13,  ) ```  Error Message: ```bash Traceback (most recent call last):   File """", line 1, in    File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/onnx/__init__.py"", line 305, in export     return utils.export(model, args, f, export_params, verbose, training,   File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 118, in export     _export(model, args, f, export_params, verbose, training, input_names, output_names,   File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 719, in _export     _model_to_graph(model, args, verbose, input_names,   File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 499, in _model_to_graph     graph, params, torch_out, module = _create_jit_graph(model, args)   File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 440, in _create_jit_graph     graph, torch_out = _trace_and_get_graph_from_model(model, args)   File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 391, in _trace_and_get_graph_from_model     torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)   File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/jit/_trace.py"", line 1166, in _get_trace_graph     outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)   File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1110, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/pwchi/fine_tuning/env/lib/python3.8/sitepackages/torch/jit/_trace.py"", line 127, in forward     graph, out = torch._C._create_graph_by_tracing( RuntimeError: 0INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":607, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool,  Candidates:         aten::constant_pad_nd(Tensor self, int[] pad, Scalar value=0) > (Tensor) ```  Versions ``` PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 8.4.03ubuntu2) 8.4.0 Clang version: Could not collect CMake version: version 3.23.1 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.13.041genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration:  GPU 0: Quadro RTX 8000 GPU 1: Quadro RTX 8000 Nvidia driver version: 510.60.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.3.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.961 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] pytorchlightning==1.6.4 [pip3] torch==1.11.0 [pip3] torchtbprofiler==0.4.0 [pip3] torchaudio==0.11.0 [pip3] torchmetrics==0.9.1 [pip3] torchvision==0.12.0 [conda] Could not collect ``` ``` Version of transformers: transformers==4.18.0 ```",2022-10-02T04:15:16Z,module: onnx onnx-needs-info,closed,0,3,https://github.com/pytorch/pytorch/issues/86063,Can you test with the latest pytorch nightly build and report back? Thanks!,"Hi  , thanks for the advice! I upgraded PyTorch to `1.12.1+cu102` and changed `optset_version` to 14 for `torch.onnx.export`, then everything worked fine and smoothly. I reported the updated code and environment here for future reference:  Code ```python import torch from transformers import LEDForConditionalGeneration, LEDConfig, LEDTokenizer tokenizer = LEDTokenizer.from_pretrained(""allenai/ledlarge16384arxiv"") model = LEDForConditionalGeneration.from_pretrained(     ""allenai/ledlarge16384arxiv"", ) input_sample = ""This is an example for summarization."" input_sample = tokenizer(     input_sample,     return_tensors=""pt"", ) filepath = ""ledlarge16384arxiv.onnx"" torch.onnx.export(     model,      tuple(input_sample.values()),     f=filepath,     input_names=['input_ids', 'attention_mask'],      output_names=['logits'],      dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},                    'attention_mask': {0: 'batch_size', 1: 'sequence'},                    'logits': {0: 'batch_size', 1: 'sequence'}},      do_constant_folding=True,      opset_version=14,  ) ```  Versions ```bash Collecting environment information... PyTorch version: 1.12.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 8.4.03ubuntu2) 8.4.0 Clang version: Could not collect CMake version: version 3.23.1 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.13.041genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration:  GPU 0: Quadro RTX 8000 GPU 1: Quadro RTX 8000 Nvidia driver version: 510.60.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.3.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.961 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] pytorchlightning==1.6.4 [pip3] torch==1.12.1 [pip3] torchtbprofiler==0.4.0 [pip3] torchaudio==0.11.0 [pip3] torchmetrics==0.9.1 [pip3] torchvision==0.12.0 [conda] Could not collect ``` ``` Version of transformers: transformers==4.22.2 ```  Validation Code  Onnx Checker Didn't raise errors ```python import onnx onnx_path = ""ledlarge16384arxiv.onnx"" onnx_model = onnx.load(onnx_path) onnx.checker.check_model(onnx_model) ```  Dummy Input ```python import onnxruntime as ort ort_session = ort.InferenceSession(onnx_path) inputs = tokenizer(100 * ""This is a testing sentence."", return_tensors=""pt"") input_ids = inputs[""input_ids""].cpu().numpy() attention_mask = inputs[""attention_mask""].cpu().numpy() outputs = ort_session.run(     None,     {         ""input_ids"": input_ids,         ""attention_mask"": attention_mask,          ""decoder_input_ids"": None,          ""decoder_attention_mask"": None,     }, ) ```",Problem solved so close the issue.
rag,[MPS] Pin_memory on MPS backend raises exception of setting wrong device storage," 🐛 Describe the bug ```python import torch torch.zeros([2,2]).pin_memory('mps') ``` ``` RuntimeError: Attempted to set the storage of a tensor on device ""cpu"" to a storage on different device ""mps:0"".  This is no longer allowed; the devices must match. ```  Versions Collecting environment information... PyTorch version: 1.13.0a0+git614d6f1 Is debug build: True CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.6 (arm64) GCC version: Could not collect Clang version: 14.0.0 (clang1400.0.29.102) CMake version: version 3.23.2 Libc version: N/A Python version: 3.10.6 (main, Aug 30 2022, 04:58:14) [Clang 13.1.6 (clang1316.0.21.2.5)] (64bit runtime) Python platform: macOS12.6arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.13.0a0+git614d6f1 [conda] Could not collect ",2022-10-02T01:51:20Z,triaged module: mps,closed,0,1,https://github.com/pytorch/pytorch/issues/86060,"Hi . Thank you for reporting the issue. Currently, there's no support for pinned memory on the MPS backend.    "
rag,"A bunch of coverage improvements (re for models in inference snext50, BERT_pytorch, mobilenet_v3_large, pytorch_CycleGAN_and_pix2pix, dcgan, resnet18, mnasnet1_0)","  CC(Symintified factory functions)  CC(A bunch of coverage improvements (re for models in inference snext50, BERT_pytorch, mobilenet_v3_large, pytorch_CycleGAN_and_pix2pix, dcgan, resnet18, mnasnet1_0))  CC(Ported linear to symints)  CC(Ported reshape to symints and added a shim for BC)",2022-10-01T09:36:02Z,Merged cla signed ciflow/trunk release notes: fx fx,closed,0,3,https://github.com/pytorch/pytorch/issues/86050," merge f ""ci failures are irrelevant"""," successfully started a merge job. Check the current status here. The merge job was triggered with the force (f) flag. This means your change will be merged **immediately**, bypassing any CI checks (ETA: 15 minutes). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,"[Distributed] Loading distributed checkpoint with FSDP fails with varying key errors (pos.embedding, shared.weight)"," 🐛 Describe the bug Using 1.13.0.dev20220928+cu116 and FSDP, save out a distributed checkpoint.  Saving proceeds smoothly, and distributed checkpoint is created. However, in attempting to load the just saved checkpoint, receive varying key errors depending on the model that was saved. DeepVit: ~~~ Traceback (most recent call last): (RANK 3)   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/utils.py"", line 140, in reduce_scatter     local_data = map_fun()   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/state_dict_loader.py"", line 89, in local_step     local_plan = planner.create_local_plan()   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/default_planner.py"", line 85, in create_local_plan     return create_default_local_load_plan(self.state_dict, self.metadata)   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/default_planner.py"", line 131, in create_default_local_load_plan     md = metadata.state_dict_metadata[fqn] KeyError: 'pos_embedding' ~~~ T5: ~~~ Traceback (most recent call last): (RANK 3)   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/utils.py"", line 140, in reduce_scatter     local_data = map_fun()   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/state_dict_loader.py"", line 89, in local_step     local_plan = planner.create_local_plan()   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/default_planner.py"", line 85, in create_local_plan     return create_default_local_load_plan(self.state_dict, self.metadata)   File ""/opt/conda/envs/pytorch/lib/python3.9/sitepackages/torch/distributed/_shard/checkpoint/default_planner.py"", line 131, in create_default_local_load_plan     md = metadata.state_dict_metadata[fqn] KeyError: 'shared.weight' ~~~ I'm unclear if we need to make changes to properly load, but this code used to work fine two months ago.  Saving works nicely as before, just the loading is erring. here's the code used to load: ~~~ def load_distributed_model_checkpoint(model, rank, cfg):     if cfg.checkpoint_type == StateDictType.LOCAL_STATE_DICT:         print(f""loading distributed checkpoint, rank {rank}..."")         folder_name = (             cfg.dist_checkpoint_root_folder             + ""/""             + cfg.dist_checkpoint_folder             + """"             + cfg.model_name         )         checkdir = Path.cwd() / folder_name         if not checkdir.exists():             if rank == 0:                 print(f""No checkpoint directory found...skipping"")             return         if rank == 0:             load_timer = Timer()             load_timer.start()         reader = FileSystemReader(checkdir)         with FSDP.state_dict_type(             model,             StateDictType.LOCAL_STATE_DICT,         ):             state_dict = model.state_dict()             load_state_dict(state_dict, reader)             if rank == 0:                 load_timer.interval(name=""load of state dict "")             model.load_state_dict(state_dict)         print(f""> local state loaded on rank {rank}"")         if rank == 0:             load_timer.stop()         return ~~~  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220928+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.22.3 Libc version: glibc2.31 Python version: 3.9.13  (main, May 27 2022, 16:56:21)  [GCC 10.3.0] (64bit runtime) Python platform: Linux5.15.01020awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration:  GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G Nvidia driver version: 510.73.08 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.3 [pip3] torch==1.13.0.dev20220928+cu116 [pip3] torchmodelarchiver==0.5.3b20220226 [pip3] torchworkflowarchiver==0.2.4b20220513 [pip3] torchserve==0.6.0b20220513 [pip3] torchtext==0.13.1 [pip3] torchvision==0.14.0.dev20220928+cu116 [pip3] vitpytorch==0.35.8 [conda] blas                      2.116                       mkl    condaforge [conda] blasdevel                3.9.0            16_linux64_mkl    condaforge [conda] captum                    0.5.0                         0    pytorch [conda] cudatoolkit               11.6.0              hecad31d_10    condaforge [conda] libblas                   3.9.0            16_linux64_mkl    condaforge [conda] libcblas                  3.9.0            16_linux64_mkl    condaforge [conda] liblapack                 3.9.0            16_linux64_mkl    condaforge [conda] liblapacke                3.9.0            16_linux64_mkl    condaforge [conda] magmacuda116             2.6.1                         1    pytorch [conda] mkl                       2022.1.0           h84fe81f_915    condaforge [conda] mkldevel                 2022.1.0           ha770c72_916    condaforge [conda] mklinclude               2022.1.0           h84fe81f_915    condaforge [conda] numpy                     1.23.3           py39hba7629e_0    condaforge [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torch                     1.13.0.dev20220928+cu116          pypi_0    pypi [conda] torchmodelarchiver      0.5.3                    py39_0    pytorch [conda] torchworkflowarchiver   0.2.4                    py39_0    pytorch [conda] torchserve                0.6.0                    py39_0    pytorch [conda] torchtext                 0.13.1                     py39    pytorch [conda] torchvision               0.14.0.dev20220928+cu116          pypi_0    pypi ",2022-09-30T03:00:50Z,oncall: distributed,open,0,1,https://github.com/pytorch/pytorch/issues/85949,  Is this a distributed checkpointing issue or an FSDP issue or both?
rag,[ONNX] Upload code coverage to codecov,TODO: Limit the report scope to torch.onnx,2022-09-29T16:32:17Z,triaged open source better-engineering cla signed Stale topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/85900,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Justin Chu . The commit (47581dadc6d6f0794f051ff0709ba2613640503f, 4c2277486c71bae4c5761c7546abefddd870b25f) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `justinchu/codecov` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout justinchu/codecov && git pull rebase`)","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,Collect Operator Coverage,Aten decomposes operators at the FX level (torch/_decomp/decompositions.py). To collect the decompose operator coverage both for FX and TorchInductor. ,2022-09-29T05:49:04Z,triaged oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/93703,Is this what you're looking for? Also discussed in https://devdiscuss.pytorch.org/t/setofopsforabackendtoregister/1024. Closed for now but feel free to reopen it if anyone feels necessary.
rag,Slightly beefed up dynamic shapes tests for storage_offset,  CC(Slightly beefed up dynamic shapes tests for storage_offset) Signedoffby: Edward Z. Yang ,2022-09-28T14:09:30Z,Merged cla signed topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/85806, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!"
transformer,RuntimeError: CUDA error: device not ready," Issue description **when I use torch.cuda.Event for Record the time that my model used, an error has occurred.**  Code example here is my test code ```python from transformers import Wav2Vec2Processor, Wav2Vec2Model import torch from datasets import load_dataset dataset = load_dataset(""hfinternaltesting/librispeech_asr_demo"", ""clean"", split=""validation"") dataset = dataset.sort(""id"") sampling_rate = dataset.features[""audio""].sampling_rate processor = Wav2Vec2Processor.from_pretrained(""facebook/wav2vec2base960h"") model = Wav2Vec2Model.from_pretrained(""facebook/wav2vec2base960h"") model = model.cuda() inputs = processor(dataset[0][""audio""][""array""], sampling_rate=sampling_rate, return_tensors=""pt"") input_values = inputs[""input_values""].cuda() start = torch.cuda.Event(enable_timing=True) end = torch.cuda.Event(enable_timing=True) with torch.no_grad():     start.record()      outputs = model(**inputs)     outputs = model(input_values)     end.record() ``` Traceback (most recent call last):   File ""/home/crj/PycharmProjects/huggincface/test.py"", line 23, in      elapsed_time = start.elapsed_time(end)   File ""/home/crj/anaconda3/envs/wav2vec2/lib/python3.7/sitepackages/torch/cuda/streams.py"", line 204, in elapsed_time     return super(Event, self).elapsed_time(end_event) RuntimeError: CUDA error: device not ready CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1.  System Info Collecting environment information... PyTorch version: 1.12.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.16.3 Libc version: glibc2.10 Python version: 3.7.12  (default, Oct 26 2021, 06:08:21)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.13.041genericx86_64withdebianbullseyesid Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Nvidia driver version: 515.65.01 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.20.0 [pip3] torch==1.12.0+cu116 [pip3] torchaudio==0.12.0+rocm5.1.1 [pip3] torchvision==0.13.0+cu116 [conda] numpy                     1.20.0                   pypi_0    pypi [conda] torch                     1.12.0+cu116             pypi_0    pypi [conda] torchaudio                0.12.0+rocm5.1.1          pypi_0    pypi [conda] torchvision               0.13.0+cu116             pypi_0    pypi Process finished with exit code 0 ```",2022-09-28T09:43:55Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/85796,"I had the same issue and this code worked for me : import gc gc.collect() torch.cuda.empty_cache() If is still doesn't work for you then please check this link, there are multiple ways people have discussed this issue. It might be helpful : https://stackoverflow.com/questions/54374935/howtofixthisstrangeerrorruntimeerrorcudaerroroutofmemory","> I had the same issue and this code worked for me : import gc gc.collect() torch.cuda.empty_cache() >  > If is still doesn't work for you then please check this link, there are multiple ways people have discussed this issue. It might be helpful : https://stackoverflow.com/questions/54374935/howtofixthisstrangeerrorruntimeerrorcudaerroroutofmemory thanks,when I add torch.cuda.synchronize() after end.record(),It word well! people discussed this issue here too: https://stackoverflow.com/questions/6551121/cudacudaeventelapsedtimereturnsdevicenotreadyerror"
yi,"Revert ""Revert ""Symintifying slice ops (#85196)""""","  CC(Revert ""Revert ""Symintifying slice ops (85196)"""") This reverts commit 3a171dfb0c08956d55f341039cf35e3a18269c34.",2022-09-27T19:15:31Z,Merged cla signed release notes: vulkan,closed,1,4,https://github.com/pytorch/pytorch/issues/85746,"There are some new torchgen changes here, mostly just making it possible to turn off symint codegen for executorch.", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,create swin-block in torchvision," 🚀 The feature, motivation and pitch I'm working on swin transformer, and noticed the excellent performance of swin transformer, now there is a swin_transformer in torchvision, but I hope to have a swinblock to help me quickly build a network.  Alternatives _No response_  Additional context _No response_ ",2022-09-27T10:55:16Z,feature triaged module: vision,closed,0,2,https://github.com/pytorch/pytorch/issues/85697,66666 it might also be helpful to post this in the torchvision repo so that they're sure to see it,"66666 closing this since you have opened pytorch/vision CC(``bincount`` feature implementation).  You can ""Transfer issue"" (bottom of the right column on the issue) to `torchvision`. That avoids extra work for the OP as well as automatically removes the issue from this tracker. Plus, it keeps all the comments from the original thread, which admittedly are not existent here."
rag,Update hierarchical_model_averager.py,Fixes ISSUE_NUMBER,2022-09-26T19:13:00Z,oncall: distributed Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/85648," merge f ""string change"""," successfully started a merge job. Check the current status here. The merge job was triggered with the force (f) flag. This means your change will be merged **immediately**, bypassing any CI checks (ETA: 15 minutes). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey varma. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,[Distributed: RPC] Failed to initialize RPC with >18 workers," 🐛 Describe the bug Failed to initialize RPC with >18 workers. Here is a minimal script: ```python  rpc_test.py import os import random import numpy as np import torch import torch.distributed.rpc as rpc def worker_init():     rank = int(os.environ['RANK'])     random.seed(rank)     np.random.seed(rank)     torch.manual_seed(rank)     print(f'Rank {rank}') def main():     rank = int(os.environ['RANK'])     world_size = int(os.environ['WORLD_SIZE'])     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)     worker_init()      noop     rpc.shutdown() if __name__ == '__main__':     main() ``` Run in command line: ```console $ torchrun nnode 1 nproc_per_node 18 rpc_test.py WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  ***************************************** Rank 0 Rank 1 Rank 2 Rank 3 Rank 4 Rank 5 Rank 6 Rank 7 Rank 8 Rank 9 Rank 10 Rank 11 Rank 12 Rank 13 Rank 14 Rank 15 Rank 17 Rank 16 ``` It works for me for 18 workers but raises errors when I change the proc number larger than 18 (e.g. 19) Output: ```console $ torchrun nnode 1 nproc_per_node 19 rpc_test.py WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  ***************************************** [W tensorpipe_agent.cpp:916] RPC agent for worker10 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for worker2 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) Traceback (most recent call last):   File ""rpc_test.py"", line 33, in  [W tensorpipe_agent.cpp:916] RPC agent for worker16 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     main()   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend Traceback (most recent call last):   File ""rpc_test.py"", line 33, in      return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler Traceback (most recent call last):   File ""rpc_test.py"", line 33, in      main()   File ""rpc_test.py"", line 24, in main         api._all_gather(None, timeout=rpc_backend_options.rpc_timeout) rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     main()   File ""rpc_test.py"", line 24, in main             rpc_agent = backend_registry.init_backend(rpc_sync(rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc         return backend.value.init_backend_handler(*args, **kwargs)return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync [W tensorpipe_agent.cpp:916] RPC agent for worker14 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)         _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_agent = backend_registry.init_backend(       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend rpc_sync(       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper return fut.wait()         return backend.value.init_backend_handler(*args, **kwargs)return func(*args, **kwargs) RuntimeError   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync : connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)     return fut.wait()   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync Traceback (most recent call last):     return fut.wait()   File ""rpc_test.py"", line 33, in  RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     main()   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc [W tensorpipe_agent.cpp:916] RPC agent for worker4 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend [W tensorpipe_agent.cpp:916] RPC agent for worker8 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)Traceback (most recent call last):   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync   File ""rpc_test.py"", line 33, in  Traceback (most recent call last):   File ""rpc_test.py"", line 33, in      return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     main()   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     main()   File ""rpc_test.py"", line 24, in main         _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc         _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend         rpc_agent = backend_registry.init_backend(return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler         api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         return func(*args, **kwargs)return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather         rpc_sync(rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)     return func(*args, **kwargs)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:530] RPC agent for worker0 encountered error when accepting incoming pipe: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker14: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker10: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker15: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker9: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker8: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker1: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker17: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker6: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker11: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker13: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker7: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker3: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker5: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker4: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker2: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker16: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker12: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker18: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker18 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker5 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker17 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker3 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker9 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker15 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker1 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker13 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker7 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker12 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker11 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""rpc_test.py"", line 33, in  Traceback (most recent call last):   File ""rpc_test.py"", line 33, in  Traceback (most recent call last): Traceback (most recent call last):   File ""rpc_test.py"", line 33, in    File ""rpc_test.py"", line 33, in  Traceback (most recent call last):   File ""rpc_test.py"", line 33, in          main()main()   File ""rpc_test.py"", line 24, in main   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     main()    main()   File ""rpc_test.py"", line 24, in main   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)           File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)      File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     main()     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     rpc_agent = backend_registry.init_backend(  File ""rpc_test.py"", line 24, in main           File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend return backend.value.init_backend_handler(*args, **kwargs)rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)            return func(*args, **kwargs)return func(*args, **kwargs) return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     rpc_sync(     rpc_sync(  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         return func(*args, **kwargs)return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     return fut.wait()     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)RuntimeError : async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout) Traceback (most recent call last):   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""rpc_test.py"", line 33, in      main()   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     rpc_sync(       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather [W tensorpipe_agent.cpp:940] RPC agent for worker6 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return func(*args, **kwargs)    return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync Traceback (most recent call last):   File ""rpc_test.py"", line 33, in      main()   File ""rpc_test.py"", line 24, in main     return fut.wait()     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""rpc_test.py"", line 33, in      main()   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper Traceback (most recent call last):   File ""rpc_test.py"", line 33, in      return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     main()   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last): Traceback (most recent call last):   File ""rpc_test.py"", line 33, in    File ""rpc_test.py"", line 33, in      main()   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     main()    rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""rpc_test.py"", line 24, in main     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""rpc_test.py"", line 33, in      main()   File ""rpc_test.py"", line 24, in main     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17244 closing signal SIGTERM ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 17245) of binary: /home/PanXuehai/Miniconda3/envs/torchopt/bin/python Traceback (most recent call last):   File ""/home/PanXuehai/Miniconda3/envs/torchopt/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==1.12.1', 'console_scripts', 'torchrun')())   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/run.py"", line 761, in main     run(args)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/run.py"", line 752, in run     elastic_launch(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 245, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ============================================================ rpc_test.py FAILED  Failures: [1]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 2 (local_rank: 2)   exitcode  : 1 (pid: 17246)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [2]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 3 (local_rank: 3)   exitcode  : 1 (pid: 17247)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [3]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 4 (local_rank: 4)   exitcode  : 1 (pid: 17248)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [4]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 5 (local_rank: 5)   exitcode  : 1 (pid: 17249)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [5]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 6 (local_rank: 6)   exitcode  : 1 (pid: 17250)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [6]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 7 (local_rank: 7)   exitcode  : 1 (pid: 17251)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [7]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 8 (local_rank: 8)   exitcode  : 1 (pid: 17252)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [8]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 9 (local_rank: 9)   exitcode  : 1 (pid: 17253)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [9]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 10 (local_rank: 10)   exitcode  : 1 (pid: 17254)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [10]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 11 (local_rank: 11)   exitcode  : 1 (pid: 17256)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [11]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 12 (local_rank: 12)   exitcode  : 1 (pid: 17259)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [12]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 13 (local_rank: 13)   exitcode  : 1 (pid: 17264)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [13]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 14 (local_rank: 14)   exitcode  : 1 (pid: 17267)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [14]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 15 (local_rank: 15)   exitcode  : 1 (pid: 17268)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [15]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 16 (local_rank: 16)   exitcode  : 1 (pid: 17269)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [16]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 17 (local_rank: 17)   exitcode  : 1 (pid: 17271)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [17]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 18 (local_rank: 18)   exitcode  : 1 (pid: 17273)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html  Root Cause (first observed failure): [0]:   time      : 20220925_22:37:55   host      : BIGAIPanXuehai.localdomain   rank      : 1 (local_rank: 1)   exitcode  : 1 (pid: 17245)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ============================================================ ``` Different machines report different maximum worker numbers on my side.  singlesocket 16C32T CPU: raise error 19+ procs  singlesocket 6C12T CPU: raises error 9+ procs  dualsocket 22C44T CPU (44C88T in total): raise error 9+ procs  Versions ``` PyTorch version: 1.12.1 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.5 LTS (x86_64) GCC version: (condaforge gcc 10.4.016) 10.4.0 Clang version: 10.0.1  CMake version: version 3.22.1 Libc version: glibc2.31 Python version: 3.8.12  (default, Jan 30 2022, 23:42:07)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.102.1microsoftstandardWSL2x86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.6.124 GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Nvidia driver version: 516.94 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] functorch==0.2.1 [pip3] mypy==0.910 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.1 [pip3] torch==1.12.1 [pip3] torchopt==0.5.1.dev66+gd738101 [pip3] torchvision==0.13.1 [pip3] torchviz==0.0.2 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.6.0               habf752d_9   [conda] functorch                 0.2.1                    pypi_0   [conda] libblas                   3.9.0            12_linux64_mkl   [conda] libcblas                  3.9.0            12_linux64_mkl   [conda] liblapack                 3.9.0            12_linux64_mkl   [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0            py38h7f8727e_0   [conda] mkl_fft                   1.3.1            py38hd3c417c_0   [conda] mkl_random                1.2.2            py38h51133e4_0   [conda] numpy                     1.23.1           py38h6c91a56_0   [conda] numpybase                1.23.1           py38ha15fc14_0   [conda] pytorch                   1.12.1          py3.8_cuda11.6_cudnn8.3.2_0   [conda] pytorchmutex             1.0                        cuda   [conda] torchopt                  0.5.1.dev66+gd738101          pypi_0   [conda] torchvision               0.13.1               py38_cu116   [conda] torchviz                  0.0.2                    pypi_0 ``` ",2022-09-25T14:39:54Z,oncall: distributed triaged module: rpc,open,0,4,https://github.com/pytorch/pytorch/issues/85607,Huang  Do you mind taking a look?,"Hi , this is likely due to the threadpool getting exhausted. Can you retry your code with this: ```python  rpc_test.py import os import random import numpy as np import torch import torch.distributed.rpc as rpc def worker_init():     rank = int(os.environ['RANK'])     random.seed(rank)     np.random.seed(rank)     torch.manual_seed(rank)     print(f'Rank {rank}') def main():     rank = int(os.environ['RANK'])     world_size = int(os.environ['WORLD_SIZE'])     rpc.init_rpc(name=f'worker{rank}', rank=rank, world_size=world_size,          rpc_backend_options=rpc.TensorPipeRpcBackendOptions(num_worker_threads=64)  ADD THIS     )     worker_init()      noop     rpc.shutdown() if __name__ == '__main__':     main() ``` I've filed this as an issue before ( CC(Provide error message when thread pool is exhausted in RPC)), but will mark it as highpri since its been hit twice now so we can provide a better error message","Huang Adding `num_worker_threads` reports the same error (connection reset by peer) for 19 workers:  Script ```python  rpc_test.py import os import random import numpy as np import torch import torch.distributed.rpc as rpc def worker_init():     rank = int(os.environ['RANK'])     random.seed(rank)     np.random.seed(rank)     torch.manual_seed(rank)     print(f'Rank {rank}') def main():     rank = int(os.environ['RANK'])     world_size = int(os.environ['WORLD_SIZE'])     rpc.init_rpc(         name=f'worker{rank}',         rank=rank,         world_size=world_size,         rpc_backend_options=rpc.TensorPipeRpcBackendOptions(num_worker_threads=64),     )     worker_init()      noop     rpc.shutdown() if __name__ == '__main__':     main() ```  ```console $ torchrun standalone nnode 1 nproc_per_node 19 rpc_test.py     WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  ***************************************** [W tensorpipe_agent.cpp:916] RPC agent for worker8 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for worker9 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for worker18 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:530] RPC agent for worker0 encountered error when accepting incoming pipe: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker16: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker17: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker5 encountered error when sending outgoing request CC(未找到相关数据) to worker0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker12: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker5: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker18: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker1: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker9: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker4: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker14: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker11: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker10: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker8: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker15: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker2: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker3: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker6: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker7: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:726] RPC agent for worker0 encountered error when reading incoming request from worker13: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker15 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker4 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker2 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker3 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker6 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker1 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker7 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker10 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker13 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker12 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker14 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker17 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for worker11 encountered error when reading incoming response from worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for worker16 encountered error when sending outgoing request CC(未找到相关数据) to worker0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""rpc_test.py"", line 37, in  Traceback (most recent call last):   File ""rpc_test.py"", line 37, in  Traceback (most recent call last):   File ""rpc_test.py"", line 37, in  Traceback (most recent call last): Traceback (most recent call last):   File ""rpc_test.py"", line 37, in    File ""rpc_test.py"", line 37, in  Traceback (most recent call last): Traceback (most recent call last): Traceback (most recent call last): Traceback (most recent call last): Traceback (most recent call last): Traceback (most recent call last): Traceback (most recent call last):   File ""rpc_test.py"", line 37, in    File ""rpc_test.py"", line 37, in    File ""rpc_test.py"", line 37, in    File ""rpc_test.py"", line 37, in    File ""rpc_test.py"", line 37, in    File ""rpc_test.py"", line 37, in    File ""rpc_test.py"", line 37, in              main()main()main()   File ""rpc_test.py"", line 23, in main   File ""rpc_test.py"", line 23, in main   File ""rpc_test.py"", line 23, in main                 main()rpc.init_rpc(rpc.init_rpc( rpc.init_rpc(   File ""rpc_test.py"", line 23, in main   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc         rpc.init_rpc(main()   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""rpc_test.py"", line 23, in main                         main()main()main()main()    main()main()        _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)         _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""rpc_test.py"", line 23, in main     rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)_init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend main()   File ""rpc_test.py"", line 23, in main     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""rpc_test.py"", line 23, in main   File ""rpc_test.py"", line 23, in main _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc.init_rpc(     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     rpc_sync(    _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""rpc_test.py"", line 23, in main rpc.init_rpc(  File ""rpc_test.py"", line 23, in main   File ""rpc_test.py"", line 23, in main   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc.init_rpc(         return backend.value.init_backend_handler(*args, **kwargs)return backend.value.init_backend_handler(*args, **kwargs)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper rpc_agent = backend_registry.init_backend(  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         rpc_agent = backend_registry.init_backend(rpc_agent = backend_registry.init_backend(     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather         rpc_agent = backend_registry.init_backend(     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)    rpc.init_rpc(  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         rpc_sync(return backend.value.init_backend_handler(*args, **kwargs) return func(*args, **kwargs)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     return fut.wait() RuntimeError    return backend.value.init_backend_handler(*args, **kwargs):  async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler         api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)return func(*args, **kwargs)               File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)rpc_sync( return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper           File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         return func(*args, **kwargs)     rpc_agent = backend_registry.init_backend(return fut.wait()  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather             api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend return func(*args, **kwargs)return func(*args, **kwargs)return func(*args, **kwargs)RuntimeError :    File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync return func(*args, **kwargs)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     return backend.value.init_backend_handler(*args, **kwargs)       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return fut.wait()     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper RuntimeError:     async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)rpc_sync( return func(*args, **kwargs)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper Traceback (most recent call last):       File ""rpc_test.py"", line 37, in  return func(*args, **kwargs)return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync Traceback (most recent call last):   File ""rpc_test.py"", line 37, in      main()   File ""rpc_test.py"", line 23, in main     main()   File ""rpc_test.py"", line 23, in main     return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)         rpc.init_rpc(rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)         return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather Traceback (most recent call last):   File ""rpc_test.py"", line 37, in      rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     main()   File ""rpc_test.py"", line 23, in main     rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc         rpc.init_rpc( api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper             _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)return func(*args, **kwargs) _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend         rpc_sync(rpc_agent = backend_registry.init_backend(     rpc_agent = backend_registry.init_backend(  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend         return backend.value.init_backend_handler(*args, **kwargs)return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)     rpc_sync(  File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         return func(*args, **kwargs)return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return fut.wait() RuntimeError    return func(*args, **kwargs):  async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last): Traceback (most recent call last):   File ""rpc_test.py"", line 37, in    File ""rpc_test.py"", line 37, in          main()main()   File ""rpc_test.py"", line 23, in main   File ""rpc_test.py"", line 23, in main     rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper         return func(*args, **kwargs)return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)       File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync         return fut.wait()return fut.wait() RuntimeErrorRuntimeError: : async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     Traceback (most recent call last): return fut.wait()   File ""rpc_test.py"", line 37, in  RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)     main()   File ""rpc_test.py"", line 23, in main     rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 27273 closing signal SIGTERM ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 27274) of binary: /home/PanXuehai/Miniconda3/envs/torchopt/bin/python Traceback (most recent call last):   File ""/home/PanXuehai/Miniconda3/envs/torchopt/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==1.12.1', 'console_scripts', 'torchrun')())   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/run.py"", line 761, in main     run(args)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/run.py"", line 752, in run     elastic_launch(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 245, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ============================================================ rpc_test.py FAILED  Failures: [1]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 2 (local_rank: 2)   exitcode  : 1 (pid: 27275)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [2]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 3 (local_rank: 3)   exitcode  : 1 (pid: 27276)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [3]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 4 (local_rank: 4)   exitcode  : 1 (pid: 27277)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [4]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 5 (local_rank: 5)   exitcode  : 1 (pid: 27278)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [5]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 6 (local_rank: 6)   exitcode  : 1 (pid: 27279)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [6]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 7 (local_rank: 7)   exitcode  : 1 (pid: 27280)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [7]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 8 (local_rank: 8)   exitcode  : 1 (pid: 27281)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [8]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 9 (local_rank: 9)   exitcode  : 1 (pid: 27282)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [9]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 10 (local_rank: 10)   exitcode  : 1 (pid: 27283)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [10]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 11 (local_rank: 11)   exitcode  : 1 (pid: 27284)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [11]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 12 (local_rank: 12)   exitcode  : 1 (pid: 27285)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [12]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 13 (local_rank: 13)   exitcode  : 1 (pid: 27286)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [13]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 14 (local_rank: 14)   exitcode  : 1 (pid: 27289)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [14]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 15 (local_rank: 15)   exitcode  : 1 (pid: 27291)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [15]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 16 (local_rank: 16)   exitcode  : 1 (pid: 27294)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [16]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 17 (local_rank: 17)   exitcode  : 1 (pid: 27297)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [17]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 18 (local_rank: 18)   exitcode  : 1 (pid: 27300)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html  Root Cause (first observed failure): [0]:   time      : 20220927_14:07:12   host      : BIGAIPanXuehai.localdomain   rank      : 1 (local_rank: 1)   exitcode  : 1 (pid: 27274)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ============================================================ ``` Also, it's not quite stable for 18 workers: ```console $ torchrun standalone nnode 1 nproc_per_node 18 rpc_test.py WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  ***************************************** Rank 0 Rank 1 Rank 2 Rank 3 Rank 4 Rank 5 Rank 6 Rank 7 Rank 8 Rank 9 Rank 10 Rank 11 Rank 12Rank 13 Rank 14 Rank 16 Rank 15 Rank 17 $ torchrun standalone nnode 1 nproc_per_node 18 rpc_test.py WARNING:torch.distributed.run: ***************************************** Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.  ***************************************** Traceback (most recent call last):   File ""rpc_test.py"", line 37, in      main()   File ""rpc_test.py"", line 23, in main     rpc.init_rpc(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/backend_registry.py"", line 353, in _tensorpipe_init_backend_handler     api._init_rpc_states(agent)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/rpc/api.py"", line 120, in _init_rpc_states     _set_and_start_rpc_agent(agent) RuntimeError: In create at tensorpipe/common/nvml_lib.h:141 ""lib.init_v2()(0) GPU access blocked by the operating system"" WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31082 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31083 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31084 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31085 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31086 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31087 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31088 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31090 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31091 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31092 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31093 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31094 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31095 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31096 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31099 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31102 closing signal SIGTERM WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 31106 closing signal SIGTERM ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 7 (pid: 31089) of binary: /home/PanXuehai/Miniconda3/envs/torchopt/bin/python Traceback (most recent call last):   File ""/home/PanXuehai/Miniconda3/envs/torchopt/bin/torchrun"", line 33, in      sys.exit(load_entry_point('torch==1.12.1', 'console_scripts', 'torchrun')())   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/run.py"", line 761, in main     run(args)   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/run.py"", line 752, in run     elastic_launch(   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/home/PanXuehai/Miniconda3/envs/torchopt/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 245, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ============================================================ rpc_test.py FAILED  Failures:     Root Cause (first observed failure): [0]:   time      : 20220927_14:09:17   host      : BIGAIPanXuehai.localdomain   rank      : 7 (local_rank: 7)   exitcode  : 1 (pid: 31089)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ```","I also encounter this issues, I could launch 30+ rpc. But it doesn't work after 40 RPCs initializing. Here is error log. [W tensorpipe_agent.cpp:916] RPC agent for 21 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 15 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 31 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 19 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 23 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 22 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 6 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 2 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 12 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 11 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 26 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 30 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 20 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 5 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 1 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 32 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 34 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 27 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 4 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 28 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 37 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:916] RPC agent for 39 encountered error when sending outgoing request CC(未找到相关数据) to 0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:530] RPC agent for 0 encountered error when accepting incoming pipe: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 37: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 7: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 28: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 4: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 39: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 34: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 27: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 32: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 1: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 5: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 10: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 20: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 30: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 26: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 3: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 15: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 25: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 21: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 16: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 36: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 35: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 31: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 24: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 19: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 22: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 23: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 6: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 8: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 2: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 12: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 14: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:726] RPC agent for 0 encountered error when reading incoming request from 11: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:916] RPC agent for 25 encountered error when sending outgoing request CC(未找到相关数据) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 35 encountered error when sending outgoing request CC(未找到相关数据) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 24 encountered error when sending outgoing request CC(未找到相关数据) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 8 encountered error when sending outgoing request CC(未找到相关数据) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 14 encountered error when sending outgoing request CC(未找到相关数据) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 3 encountered error when sending outgoing request CC(未找到相关数据) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 10 encountered error when sending outgoing request CC(未找到相关数据) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for 36 encountered error when reading incoming response from 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:916] RPC agent for 7 encountered error when sending outgoing request CC(未找到相关数据) to 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:940] RPC agent for 16 encountered error when reading incoming response from 0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""/data00/home/guyi.1016/workspace/torzilla/tztest/rpc/main.py"", line 52, in      main()   File ""/data00/home/guyi.1016/workspace/torzilla/tztest/rpc/main.py"", line 38, in main     torzilla.lanuch(   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/lanucher.py"", line 12, in lanuch     raise e   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/lanucher.py"", line 10, in lanuch     proc.start()   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 19, in start     self._on_start()   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 54, in _on_start     self._spawn(num_process, spw_kwargs)   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 62, in _spawn     return mp.spawn(   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/multiprocessing/spawn.py"", line 240, in spawn     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/multiprocessing/spawn.py"", line 198, in start_processes     while not context.join():   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/multiprocessing/spawn.py"", line 160, in join     raise ProcessRaisedException(msg, error_index, failed_process.pid) torch.multiprocessing.spawn.ProcessRaisedException:  Process 2 terminated with the following error: Traceback (most recent call last):   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/multiprocessing/spawn.py"", line 69, in _wrap     fn(i, *args)   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 88, in _on_process_entry     raise e   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 86, in _on_process_entry     p.start()   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 19, in start     self._on_start()   File ""/data00/home/guyi.1016/workspace/torzilla/tztest/rpc/main.py"", line 24, in _on_start     self._try_init_rpc()   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 110, in _try_init_rpc     self._init_rpc(**rpc_kwargs)   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/launcher/process.py"", line 31, in _init_rpc     return rpc.init_rpc(**rpc_args)   File ""/data00/home/guyi.1016/workspace/torzilla/torzilla/rpc/rpc.py"", line 47, in init_rpc     return _rpc.init_rpc(   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/__init__.py"", line 196, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/__init__.py"", line 231, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/backend_registry.py"", line 101, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/backend_registry.py"", line 359, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/api.py"", line 222, in _all_gather     rpc_sync(   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/api.py"", line 83, in wrapper     return func(*args, **kwargs)   File ""/home/guyi.1016/.conda/envs/py39/lib/python3.9/sitepackages/torch/distributed/rpc/api.py"", line 800, in rpc_sync     return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114)"
transformer,Enabling Transformer fast path for not batch_first,Summary: The fast path for the `forward()` method in `MultiheadAttention` only accepted `batch_first = True`. This diff enables fast path for `batch_first=False` as well. Test Plan: Added unit test for fast path for both values of `batch_first` producing identical outputs. `buck test mode/devnosan //caffe2/torch/nn/modules/tests:test_activation` passed Reviewed By: mikekgfb Differential Revision: D39669982,2022-09-23T22:27:51Z,fb-exported cla signed Stale ciflow/trunk,closed,0,12,https://github.com/pytorch/pytorch/issues/85576,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.",The committers listed above are authorized under a signed CLA.:white_check_mark: login: soumyodipto / name: Soumyodipto Mukherjee  (f49d97d95b232937697ec1a21a17d68e4ea5ed80),This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This pull request was **exported** from Phabricator. Differential Revision: D39669982,This is the updated diff with  and 's suggested changes,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Execute smoke test for Better Transformer feature ," 🚀 The feature, motivation and pitch Run the smoke test with the RC binary (available on September 31) on A100 machines.  Alternatives _No response_  Additional context _No response_ /pytorchdevinfra",2022-09-22T20:19:44Z,module: ci triaged,open,0,2,https://github.com/pytorch/pytorch/issues/85499,Are we still doing Better Transformer for release/1.13? Removing milestone for now, is this still needed?
transformer,Issue with converting Comet model to ONNX. Split-node error.,"I'm trying to convert the Comet model to ONNX. In particular I'm working with the referenceless version of the metric (""wmt21cometqemqm""). It is based on BERT and contains a regression head. The export to ONNX succeeds. And the resulting model passes the checks. However, performing inference fails with an error  Cannot split using values in 'split' attribute. Below I provide details of the error. I wonder if the ONNX supports such configuration. I also prepared a short script that may be used to reproduce this issue. ```python import os from transformers import AutoTokenizer import torch from comet import download_model, load_from_checkpoint import onnxruntime as ort import onnx model = load_from_checkpoint(download_model(""wmt21cometqemqm"")) CONVERTED_COMET_DIR = 'CONVERTED_COMET' CONVERTED_COMET_MODEL_PATH = os.path.join(CONVERTED_COMET_DIR, 'model.onnx') tokenizer = AutoTokenizer.from_pretrained('xlmrobertalarge') MODEL_MAX_LENGTH = tokenizer.model_max_length try:      os.makedirs(CONVERTED_COMET_DIR) except FileExistsError:     pass input_names = [""src_input_ids"", ""src_attention_mask"", ""mt_input_ids"", ""mt_attention_mask""] inputs = {key: torch.ones(1, MODEL_MAX_LENGTH, dtype=torch.int64) for key in input_names} symbolic_names = {0: ""batch_size""} torch.onnx.export(     model,     (*[inputs[key] for key in input_names],),     CONVERTED_COMET_MODEL_PATH,     opset_version=15,     do_constant_folding=True,     input_names=input_names,     output_names=[""score""],     dynamic_axes={key: symbolic_names for key in input_names}, ) onnx.checker.check_model(CONVERTED_COMET_MODEL_PATH) ort_sess = ort.InferenceSession(CONVERTED_COMET_MODEL_PATH) input_src = tokenizer(""Algo está mal aquí..."", return_tensors=""np"", max_length=MODEL_MAX_LENGTH, padding='max_length') input_mt = tokenizer(""Coś tu nie gra..."", return_tensors=""np"", max_length=MODEL_MAX_LENGTH, padding='max_length') inp = {""src_input_ids"": input_src['input_ids'],        ""src_attention_mask"": input_src['attention_mask'],        ""mt_input_ids"": input_mt['input_ids'],        ""mt_attention_mask"": input_mt['attention_mask']} outputs = ort_sess.run(None, inp) ``` The error details ``` 20220922 12:34:13.957212579 [E:onnxruntime:, sequential_executor.cc:368 Execute] Nonzero status code returned while running Split node. Name:'Split_5573' Status Message: Cannot split using values in 'split' attribute. Axis=0 Input shape={1,512} NumOutputs=1 Num entries in 'split' (must equal number of outputs) was 1 Sum of sizes in 'split' (must equal size of selected axis) was 8  Fail                                      Traceback (most recent call last) /tmp/ipykernel_7700/1124372680.py in       60 inp = prepare_pair(""Algo está mal aquí..."", ""Coś tu nie gra..."")      61  > 62 outputs = ort_sess.run(None, inp) /opt/conda/lib/python3.7/sitepackages/onnxruntime/capi/onnxruntime_inference_collection.py in run(self, output_names, input_feed, run_options)     198             output_names = [output.name for output in self._outputs_meta]     199         try: > 200             return self._sess.run(output_names, input_feed, run_options)     201         except C.EPFail as err:     202             if self._enable_fallback: Fail: [ONNXRuntimeError] : 1 : FAIL : Nonzero status code returned while running Split node. Name:'Split_5573' Status Message: Cannot split using values in 'split' attribute. Axis=0 Input shape={1,512} NumOutputs=1 Num entries in 'split' (must equal number of outputs) was 1 Sum of sizes in 'split' (must equal size of selected axis) was 8 ```  Versions Collecting environment information... PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Debian GNU/Linux 10 (buster) (x86_64) GCC version: (Debian 8.3.06) 8.3.0 Clang version: Could not collect CMake version: version 3.13.4 Libc version: glibc2.10 Python version: 3.7.12  (default, Oct 26 2021, 06:08:53)  [GCC 9.4.0] (64bit runtime) Python platform: Linux4.19.021cloudamd64x86_64withdebian10.12 Is CUDA available: False CUDA runtime version: 11.3.109 GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.6 [pip3] pytorchlightning==1.6.4 [pip3] torch==1.11.0 [pip3] torchxla==1.11 [pip3] torchmetrics==0.8.2 [pip3] torchvision==0.12.0+cu113 [conda] blas                      2.115                       mkl    condaforge [conda] blasdevel                3.9.0            15_linux64_mkl    condaforge [conda] cudatoolkit               11.3.1               ha36c431_9    nvidia [conda] dlenvpytorch111gpu    1.0.20220630     py37hc1c1d6d_0    file:///tmp/condapkgs [conda] libblas                   3.9.0            15_linux64_mkl    condaforge [conda] libcblas                  3.9.0            15_linux64_mkl    condaforge [conda] liblapack                 3.9.0            15_linux64_mkl    condaforge [conda] liblapacke                3.9.0            15_linux64_mkl    condaforge [conda] mkl                       2022.1.0           h84fe81f_915    condaforge [conda] mkldevel                 2022.1.0           ha770c72_916    condaforge [conda] mklinclude               2022.1.0           h84fe81f_915    condaforge [conda] numpy                     1.21.6                   pypi_0    pypi [conda] pytorch                   1.11.0          py3.7_cuda11.3_cudnn8.2.0_0    pytorch [conda] pytorchlightning         1.6.4                    pypi_0    pypi [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torchmetrics              0.8.2                    pypi_0    pypi [conda] torchvision               0.12.0+cu113             pypi_0    pypi Additional libraries: unbabelcomet==1.1.2 transformers==4.17.0",2022-09-22T14:30:12Z,module: onnx triaged,closed,0,6,https://github.com/pytorch/pytorch/issues/85475,"Hi, Could you 1. try with nightly PyTorch 2. validate with the following strict mode checker if you want to run it on ONNXRUNTIME: ```python onnx.checker.check_model(CONVERTED_COMET_MODEL_PATH, full_check=True) ``` NOTE: if this still passed, please raise an issue ONNXRUNTIME","Hi, I gave a try to both of your suggestions. Anyway the result is more or less the same. Only the number of the split node in the error message has changed, when I used the nightly build. Will post it on ONNXRUNTIME if nothing else helps. Thank you.","FWIW, ONNXRUNTIME has a folder to demonstrate the exporting and running of huggingface/transformers model: https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/python/tools/transformers/models",chojnowski  any news ?,Unfortunately I couldn't spend more time on it. And this idea was put on hold for now.,`torch.onnx.export` is in maintenance mode and we don't plan to add new operators/features or fix complex issues. Please try the new ONNX exporter and reopen this issue with a full repro if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial
yi,"[PolishTypo] Alisa->Alias, indivually->individually, jEverything->Everything","Polish comment typo, `Alisa>Alias`, `indivually>individually`, `jEverything>Everything`",2022-09-22T14:07:24Z,triaged open source cla signed Stale,closed,0,6,https://github.com/pytorch/pytorch/issues/85472,   Typo change should have no influence on CircleCIbuild. Can you help me rerun `CircleCI Checks / build`? ,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: HongyuJia . The commit (bde2d949e0d2a281a1c140ccb540ae46642281fc) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.", Please sign the CLA.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",/easycla
transformer,CyclicLR memory leak fix,"Hi, we noticed in our team that by using CyclicLR, there is a problem with memory clearance on GPU (probably it will be the case without the GPU as well, but that was our use case) After initializing CyclicLR, GPU memory is not cleared even after the model, optimizer and scheduler are out of scope (e.g. reference count is zero). This is because `__init__` method inside `CyclicLR` creates reference to its own methods and it will not get removed until `gc.collect()` is called manually. This is a problem if people want to test multiple models in one run of a script, after testing the first model, second one will fail on `CUDA out of memory error` because the first one is not cleared from the memory. I propose a simple fix by using `weakref`, similarly as in `_LRScheduler` base class, but if you have any comments I am happy to change it.  Here is the code to reproduce the bug: ``` import torch import weakref from transformers import DetrForObjectDetection class X:     def __init__(self, optimizer):         self.optimizer = optimizer          Will cause cyclic reference.         self.func = self.dummy          Will work as expected, memory cleared after instance count is zero.          self.func = weakref.WeakMethod(self.dummy)     def dummy(self, x):         return 1. def test():     model = DetrForObjectDetection.from_pretrained('facebook/detrresnet50')     model.to('cuda')     optimizer = torch.optim.Adam(model.parameters())     x = X(optimizer) test() print(f'{torch.cuda.memory_reserved()}, {torch.cuda.memory_allocated()}')   Should print (, 0), but with cyclic reference, it will print (, ). ```",2022-09-22T06:56:49Z,triaged open source Merged cla signed release notes: python_frontend topic: improvements,closed,0,14,https://github.com/pytorch/pytorch/issues/85462,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!," Sorry for the late reply, both test and renaming to private variables are done. One can easily verify that the test will fail if you include something like this inside CyclicLR's __init__ method: ```python self.test = self._triangular_scale_fn ```", merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!", Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  Lint Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ,Could you please fix the lint issues?,"> Could you please fix the lint issues?  Sorry! When I execute `lintrunner` locally I have no errors, probably a bad setup. Fixed.", merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.",Seems I do not have permission to add any label?,"Added! In the future, you can ask the bot to do it for you as needed:  help", h
yi,[Quant] Make x86 backend default when querying qconfig,This PR is a followup of CC([Quant] Add unified x86 quant backend) [[Quant] Add unified x86 quant backend](https://github.com/pytorch/pytorch/pull/84329) It makes `x86` backend default when querying `qconfig`. Users get x86's qconfig/qconfig_mappings if backend is not specified.,2022-09-22T06:32:44Z,triaged open source Merged cla signed ciflow/trunk,closed,1,6,https://github.com/pytorch/pytorch/issues/85461,"> Looks reasonable, but just to be safe can we rebase this on top of https://github.com/pytorch/pytorch/pull/84329/files and then put for review? Currently this doesn't include the base revision, so review is a bit confusing and it could be easy to miss a detail. Thanks. Maybe we rebase it after https://github.com/pytorch/pytorch/pull/84329 is merged.",Hi  Do you have more comments?, Is it good to land? We hope to land it before 1.13 branch cut. Thanks., merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey Weiwen. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[RFC] Separate CPU offload activation to its own wrapper,"  CC([FSDP][BE][Docs] Expose helper classes)  CC([FSDP] Doc to explain running submodules)  CC([FSDP] Fix clip_grad_norm for CPU offload)  CC([FSDP] assert to runtime error)  CC(CheckpointSequential support nonreentrant)  CC([RFC] Separate CPU offload activation to its own wrapper) Passing in `offload_to_cpu=True` to checkpoint_wrapper is a bit confusing, because this causes the activation checkpoint args to be ignored and we do CPU offloading. This isn't ideal from API design perspective, so proposing to make `offload_wrapper` its own concept. Now, offload to CPU + checkpoint can be composed together, such as ```  apply AC to transformer layers apply_ac_wrapper(model, checkpoint_wrapper, check_fn=lambda mod: isinstance(mod, TransformerLayer))  offload the rest of activations to CPU model = offload_wrapper(model) ``` Will polish / add tests if this proposal sounds good. Differential Revision: D39719854",2022-09-22T06:12:40Z,oncall: distributed Merged cla signed ciflow/trunk release notes: distributed (fsdp),closed,2,8,https://github.com/pytorch/pytorch/issues/85459,"Related:  CC([feature request] Autocast module and function wrappers) Also, if autocast also existed as explicit wrapper, these problems would be easier to understand:  CC(Simultaneously using `torch.no_grad` and `autocast` causes `RuntimeError: expected scalar type Half but found Float` for some operations.)  CC(with torch.cuda.amp.autocast() get out of memory error when using with torch.no_grad() during validation)","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.",discussed with  to punt the renaming to the future when API is more finalized and functional approach is being taken.,"varma has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.","upload test status  Blocked is the failure, unrelated. Landing"," merge f ""CI failure unrelated"""," Merge started Your change will be merged immediately since you used the force (f) flag, **bypassing any CI checks** (ETA: 15 minutes). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ","Hey varma. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Fix thread-allocation in `_vec_log_softmax_lastdim`," Problem history There seems to always have been a bug in `_vec_log_softmax_lastdim `. In particular, there were two issues with it   Bug 1  Before AVX512 support was added, `CHUNK_SIZE` had been heuristically chosen in `_vec_log_softmax_lastdim`:  `CHUNK_SIZE = (128 / sizeof(scalar_t)) * Vec::size();` It was  `256` for float32, bfloat16, and float16. When AVX512 support was added, `CHUNK_SIZE` became `512`. The rationale behind determining `CHUNK_SIZE` has not been described, and seems flawed, since the number of OpenMP threads used currently depends upon it.  Bug 2 `grain_size` had been defined as `internal::GRAIN_SIZE / (16 * dim_size * CHUNK_SIZE)` So, `grain_size` was usually 0, as it was `8 / (dim_size)`, so, it's always replaced by `CHUNK_SIZE`, viz. 256. Since `256` was always the `grain_size` for `at::parallel_for`, few threads were used in certain cases.  Problem caused by bugs With `outer_size` of say, 700, only 3 threads would have been used with AVX2, irrespective of the value of `dim_size`! When AVX512 support was added, since `CHUNK_SIZE` became `512`, only 2 threads were used if `outer_dim` was 700. In the Transformers training example, `log_softmax` was computed on the last dim of a tensor of shape `(700, 23258)`. AVX512 thus appeared to be quite slower, cloaking the actual issue that even AVX2 performance for the kernel was quite poor due to inefficient work distribution amongst OpenMP threads.  Solution Distribute work more efficiently, which would result in higher performance for both AVX2 & AVX512 than now, and fixes the regression observed with AVX512 (AVX512 kernel would now be faster than its AVX2 counterpart).  Benchmarks  Machineconfig: Intel(R) Xeon(R) Platinum 8371HC CPU (Cooper Lake) One socket of 26 physical cores was used. Intel OpenMP & tcmalloc were preloaded. Example of a command to run benchmark: `ATEN_CPU_CAPABILITY=avx512 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 KMP_BLOCKTIME=1 KMP_SETTINGS=1 MKL_NUM_THREADS=26 OMP_NUM_THREADS=26 numactl membind=0 cpunodebind=0 python3.8 m pt.softmax_test test_name LogSoftmax_N1024_seq_len23258_dim1_cpu` Benchmark  7.08x ",2022-09-21T09:58:06Z,module: cpu triaged open source Merged cla signed ciflow/trunk intel,closed,3,16,https://github.com/pytorch/pytorch/issues/85398, label ciflow/trunk,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: sanchitintel  (2aea9ec811f184664e8ed2d3200c36bc088c3117, d65a38889c1d98db2d50fd8981e669136c9e4a2c)","Unrelated CI failure  `RuntimeError: Failed running call_function (*(FakeTensor(FakeTensor(..., device='meta', size=(2, 2)), cpu), 0.1, 10, torch.quint8), **{})`", merge," Merge failed **Reason**: Approval needed from one of the following (Rule 'superuser'): kkosik20, jg2912, swolchok, qihqi, shajrawi, ... Details for Dev Infra team Raised by workflow job ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Hi  , just wanted to run this change by you. Thanks!",Would this PR fix  CC(AVX512 CPU kernels result in 5+% slower Transformer training on Cascade Lake CPU)?,"And if so, please throw in some relevant benchmarks once 's concerns are addressed ","Hi ,  yes, this PR fixes CC(AVX512 CPU kernels result in 5+% slower Transformer training on Cascade Lake CPU). I've updated the benchmark data in the PR description. Thanks!", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ," Merge failed **Reason**: GraphQL query  fragment PRReviews on PullRequestReviewConnection {   nodes {     author {       login     }     state   }   pageInfo {     startCursor     hasPreviousPage   } } fragment PRCheckSuites on CheckSuiteConnection {   edges {     node {       app {         name         databaseId       }       workflowRun {         workflow {           name         }         url       }       checkRuns(first: 50) {         nodes {           name           conclusion           detailsUrl         }         pageInfo {           endCursor           hasNextPage         }       }       conclusion     }     cursor   }   pageInfo {     hasNextPage   } } fragment CommitAuthors on PullRequestCommitConnection {   nodes {     commit {       author {         user {           login         }         email         name       }       oid     }   }   pageInfo {     endCursor     hasNextPage   } } query ($owner: String!, $name: String!, $number: Int!) {   repository(owner: $owner, name: $name) {     pullRequest(number: $number) {       closed       isCrossRepository       author {         login       }       title       body       headRefName       headRepository {         nameWithOwner       }       baseRefName       baseRepository {         nameWithOwner         isPrivate         defaultBranchRef {           name         }       }       mergeCommit {         oid       }       commits_with_authors: commits(first: 100) {         ...CommitAuthors         totalCount       }       commits(last: 1) {         nodes {           commit {             checkSuites(first: 10) {               ...PRCheckSuites             }             status {               contexts {                 context                 state                 targetUrl               }             }             pushedDate             oid           }         }       }       changedFiles       files(first: 100) {         nodes {           path         }         pageInfo {           endCursor           hasNextPage         }       }       reviews(last: 100) {         ...PRReviews       }       comments(last: 5) {         nodes {           bodyText           createdAt           author {             login           }           authorAssociation           editor {             login           }           databaseId         }         pageInfo {           startCursor           hasPreviousPage         }       }       labels(first: 100) {         edges {           node {             name           }         }       }     }   } } , args {'name': 'pytorch', 'owner': 'pytorch', 'number': 85398} failed: [{'message': 'Something went wrong while executing your query. Please include `040C:7DAB:194D17A:33EF84A:63E2542E` when reporting this issue.'}] Details for Dev Infra team Raised by workflow job ", merge that's a new one..., Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here 
yi,PixelShuffle check that output is not null before applying kernel (#85155),Checks that tensor is not null before applying kernel in `pixel_shuffle` op Signedoffby: Thytu  Fixes CC(torch.nn.PixelShuffle crash with floating point exception when input has 0 size in the last three dimensions),2022-09-20T17:00:59Z,triaged open source cla signed,closed,0,23,https://github.com/pytorch/pytorch/issues/85347,"Yes, add a test please. `math_pixed_shuffle` is used in any dispatch that's not CPU. For example, CUDA, XLA, META, LTC (LazyTensor Core)...",>  Roger that,Test added but not sure if I it's the right place to put it as a `FunctionalTest` (it's rather a UT IMO) but didn't find another place to test in the C++ API. Would you prefer the test to be in test/test_nn.py or in another place ?,"It crashs here when running test_ops_gradients.py : https://github.com/pytorch/pytorch/blob/8bad17f6b97d719a7dcece99ac91126a3029a10b/torch/autograd/__init__.pyL300L302 I currently don't know why (I don't know this part of the stack), I'm still inspecting.","As per  CC(torch.nn.PixelShuffle crash with floating point exception when input has 0 size in the last three dimensions)issuecomment1250791994, you added the check to just one of the two functions and not the other, so when the other one is being executed it segfaults :D","> As per  CC(torch.nn.PixelShuffle crash with floating point exception when input has 0 size in the last three dimensions) (comment)issuecomment1250791994), you added the check to just one of the two functions and not the other, so when the other one is being executed it segfaults :D I thought the same but I'm not sure as:  I'm testing by running `python m pytest test/test_ops_gradients.py k test_fn_fwgrad_bwgrad_nn_functional_pixel_shuffle_cpu_float64` (so it runs on CPU) and it fail.  I force check that it's not caused by `math_pixel_shuffle` by changing it to : ```cpp Tensor math_pixel_shuffle(const Tensor& self, int64_t upscale_factor) {   return self; } ```  The following code works : ```py torch.nn.functional.pixel_shuffle(     input=torch.ones((1, 1, 1, 0), dtype=torch.float64).to(torch.device('cuda')),     upscale_factor=1, ) ```",The new sample inputs look like the right coverage but I think the crashes just need more debugging to understand. ,It may be that the backward for this function also needs a similar treatment. The equivalent of `native_functions.yaml` is called `derivatives.yaml`.,"> It may be that the backward for this function also needs a similar treatment. The equivalent of `native_functions.yaml` is called `derivatives.yaml`. Alright I will take a look at it, thanks",> It may be that the backward for this function also needs a similar treatment. The equivalent of `native_functions.yaml` is called `derivatives.yaml`. As it's `auto_linear` it's autogenerated. Quick question :what would the grad supposed to be as the output is empty? `1`? Three options: 1. I missinterpreted the debug and the issue come from elsewhere (don't think so) 2. The `derivative` as to be handwritten (no longer autogenerated) 3. `pixel_shuffle` shouldn't return an empty output What are your thought on it?,`auto_linear` means that the forward AD is autogenerated. You should look into the formula for the first parameters., any update here?,>  any update here? No progress Tell me if I'm wrong but to fix the fw/bw grad it will require replacing the autogenerated `auto_linear` by a hand defined derivative fn. Is it?,"As mentioned in https://github.com/pytorch/pytorch/pull/85347issuecomment1258387365, `auto_linear` is just for forward AD and forward AD just works. You are having issues with backward AD. The backward AD is the formula after the `self:` in the entry for this function in `derivatives.yaml`. It seems that it's not handling well empty inputs. Have a go at it, and if you get stuck, ping me and I'll try to give you a hand.",>  Giving it a try,"So if I understand well, the backward of pixel_shuffle, is just pixel_unshuffle. Right?",yep! It looks like pixel_unshuffle may need a similar fix,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: Thytu / name: Valentin De Matos  (95e1481a4e26b49f14db9196ae5ccdf43c4a0902, cab68d7126fa1600e4a8cc3138ed76dd9b9d83bd, bdda9b6f8f1e2ac09749d9fea08c109ce72555aa, af9efc6c384884d8001f9d5a25b5ab4a7c8210e4, 3cdf429c75e394a0b9cabbd3463836eb326f74b4, 8bad17f6b97d719a7dcece99ac91126a3029a10b, b786597aa296561197e2f3a0965ec8af29122735)","Sorry for the waiting time, I locked a time slot to work on it this evening :) ","> As mentioned in  CC(PixelShuffle check that output is not null before applying kernel (85155)) (comment), `auto_linear` is just for forward AD and forward AD just works. You are having issues with backward AD. The backward AD is the formula after the `self:` in the entry for this function in `derivatives.yaml`. It seems that it's not handling well empty inputs. >  > Have a go at it, and if you get stuck, ping me and I'll try to give you a hand. I admit to be stuck on this one... * Test failing : `test_fn_fwgrad_bwgrad_nn_functional_pixel_shuffle_cpu_float64` * To reproduce : `python bb test_ops_gradients.py v importslowtests importdisabledtests k test_fn_fwgrad_bwgrad_nn_functional_pixel_shuffle_cpu_float64` * Test exec `pixel_unshuffle_cpu`",Oh shit 🤡,you may want to close this PR and push a new one :)
yi,"[PolishTypo] inherentely->inherently, intentially->intentionally","Polish comment typo, `inherentely>inherently`, `intentially>intentionally`",2022-09-20T06:34:29Z,open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/85325, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[PyTorch] StorageImpl: cache size_bytes.is_symbolic(),"  CC([PyTorch] StorageImpl: cache size_bytes.is_symbolic()) We've got 6 bools' worth of extra space, so let's try caching this. Differential Revision: D39636570 **NOTE FOR REVIEWERS**: This PR has internal Metaspecific changes or comments, please review them on Phabricator!",2022-09-19T22:41:32Z,Merged cla signed topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/85309, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,"Deprecate TypedStorage, its derived classes, and all of their public methods","Part of CC(Remove `TypedStorage` and use only `UntypedStorage`)   BCbreaking note  Deprecate `torch.Tensor.storage()` in favor of `torch.Tensor.untyped_storage()` Version 1.13 ```python tensor.storage() ``` Version 2.0 ```python tensor.untyped_storage() ```  Deprecate `torch.TypedStorage` and all its methods in favor of `torch.UntypedStorage` Version 1.13 ```python torch.TypedStorage(...) ``` Version 2.0 ```python torch.UntypedStorage(...) ``` If you need to access individual elements in a storage as a particular dtype, you can simply create a tensor to view it: ```python torch.tensor(storage, dtype=...) ```",2022-09-19T21:02:03Z,open source Merged cla signed with-ssh ciflow/trunk release notes: distributed (c10d) module: python frontend ciflow/periodic module: dynamo ciflow/inductor,closed,0,42,https://github.com/pytorch/pytorch/issues/85303,", I've added a test to check that it only gets raised once unless warnings are cleared. I didn't add all of the functions to the test, but I can if we want to be that thorough",nah that's good enough,A good follow up would be to make sure pytorch proper doesn't raise these deprecations, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!", Merge failed **Reason**: The following mandatory check(s) failed (Rule `superuser`):  pull Dig deeper by viewing the failures on hud Details for Dev Infra team Raised by workflow job ,"> A good follow up would be to make sure pytorch proper doesn't raise these deprecations By this, do you mean because a `DeprecationWarning` doesn't raise by default? Actually, I forgot to use DeprecationWarning, I will fix that","> By this, do you mean because a DeprecationWarning doesn't raise by default? Actually, I forgot to use DeprecationWarning, I will fix that Different: what I mean is that we shouldn't trigger the deprecation warning if the user didn't explicitly use typed storage. If pytorch internally is hitting the DeprecationWarning, we should fix it (because otherwise it will spam users)","I see, good point"," , there are some places where internal calls are raising the warning. For instance, when serializing tensors. I'm not sure what would be the best way to avoid internally generated warnings, but I thought of two options. The first one is to use the `warnings` module filter to suppress the warning at every internal call site. But I don't think that's very good, since the filter has to do a string search and it will probably affect performance significantly. The other is to add an underscored version of each function which does everything except for raising the warning. The public function would just raise the warning and then call the underscored function. We would change all the internal call sites to use the underscored version to avoid raising the warning. Something like this: ```python def _func(...):      do stuff def func(...):     _warn_typed_storage_removal()     return _func(...) ``` I think I like this solution, but what do you think? Is there a better way? I tried googling a common solution for this and haven't found anything yet","No warn variants sgtm, esp if you only need a few / we have a strategy for getting rid of them","Actually, I just realized that I can't do this for dunder functions. So instead, I think I'll have to add a kwarg `_internal=False` to all the functions. If it's False, the warning will get raised","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: kurtamohler  (895f95c6488196de139dd96b339d09cba3390688, 9f654a8ce3ebab4cfbb3b0aa7606b9d6c8d5b7ee)","I've fixed a lot of the places where internal storage method calls were raising the warning. There may be more though, because I don't really have a great way to find all of them. I found some call sites with grep and some by running a bunch of the unit tests that don't deal with storages directly and raise an error if `_warn_typed_storage_removal` is called. Do you think this is alright,  , or should I keep looking for more internal call sites? (I also renamed a couple nonpublic things, like `TypedStorage._storage` > `TypedStorage._untyped_storage` to make them a bit clearer)",I'm happy to ship this as is. Do you want another look over?,> I'm happy to ship this as is. Do you want another look over? Sounds good,looks like you've still got some tests to fix,"Well, I think I need to suppress more of the warnings before we can merge this. In the linuxbionicpy3.7clang9 / test (crossref, 2, 2, linux.2xlarge) job, 48% of the lines in the log are the warning message, which is way too many. I'm not sure why it's getting raised so many times in that job though, it doesn't happen locally for me. One thing that would reduce the number of warnings is to use the internal TypedStorage functions in the unit tests instead of the public ones. I'm not sure if that's a good idea though. Or I could add a warning filter to tests that use the public functions","> One thing that would reduce the number of warnings is to use the internal TypedStorage functions in the unit tests instead of the public ones. I'm not sure if that's a good idea though. Or I could add a warning filter to tests that use the public functions For the tests, it's fine to suppress them entirely, since you're testing deprecated apis, the deprecation doesn't apply.",For some reason I'm not able to ssh into CI jobs anymore. Looking into it,"I can't ssh into CI jobs because Quansight's `pytorchjumphost.quansight.dev` server is down. While that's being fixed, I decided to put a short stack trace into the warning messages so the logs will show more information about where the duplicate warnings are coming from","There are two errors that keeps coming up in the linuxbionicpy3.7clang9 / test (crossref, 1, 2, linux.2xlarge) CI job (and others), which appear to be part of the command `/opt/conda/bin/python bb test_serialization.py v importslowtests importdisabledtests`:  click to expand ``` ====================================================================== FAIL [0.004s]: test_serialization_save_warnings (__main__.TestOldSerialization)  Traceback (most recent call last):   File ""test_serialization.py"", line 417, in test_serialization_save_warnings     self.assertEqual(len(warns), 0)   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 2510, in assertEqual     msg=(lambda generated_msg: f""{generated_msg} : {msg}"") if isinstance(msg, str) and self.longMessage else msg,   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_comparison.py"", line 1118, in assert_equal     raise error_metas[0].to_error(msg) AssertionError: Scalars are not equal! Absolute difference: 1 Relative difference: inf ====================================================================== FAIL [0.002s]: test_serialization_save_warnings (__main__.TestSerialization)  Traceback (most recent call last):   File ""test_serialization.py"", line 417, in test_serialization_save_warnings     self.assertEqual(len(warns), 0)   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 2510, in assertEqual     msg=(lambda generated_msg: f""{generated_msg} : {msg}"") if isinstance(msg, str) and self.longMessage else msg,   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_comparison.py"", line 1118, in assert_equal     raise error_metas[0].to_error(msg) AssertionError: Scalars are not equal! Absolute difference: 1 Relative difference: inf ```  But for some reason when I log into that CI machine and run `/opt/conda/bin/python bb test_serialization.py v importslowtests importdisabledtests`, I don't get these errors. Maybe there's some environment state that the CI job sets up before running the tests that I'm not aware of. The tests do fail if I run `.jenkins/pytorch/test.sh` though. EDIT: Here's the warning that's popping up: click to expand ``` TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   File ""test_serialization.py"", line 991, in      run_tests()   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 788, in run_tests     resultclass=XMLTestResultVerbose))   File ""/opt/conda/lib/python3.7/unittest/main.py"", line 101, in __init__     self.runTests()   File ""/opt/conda/lib/python3.7/unittest/main.py"", line 271, in runTests     self.result = testRunner.run(self.test)   File ""/opt/conda/lib/python3.7/sitepackages/xmlrunner/runner.py"", line 67, in run     test(result)   File ""/opt/conda/lib/python3.7/unittest/suite.py"", line 84, in __call__     return self.run(*args, **kwds)   File ""/opt/conda/lib/python3.7/unittest/suite.py"", line 122, in run     test(result)   File ""/opt/conda/lib/python3.7/unittest/suite.py"", line 84, in __call__     return self.run(*args, **kwds)   File ""/opt/conda/lib/python3.7/unittest/suite.py"", line 122, in run     test(result)   File ""/opt/conda/lib/python3.7/unittest/case.py"", line 676, in __call__     return self.run(*args, **kwds)   File ""test_serialization.py"", line 859, in run     return super(TestSerialization, self).run(*args, **kwargs)   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 2107, in run     num_green=0)   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 2076, in _run_with_retry     num_red=num_red + 1, num_green=num_green)   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 2076, in _run_with_retry     num_red=num_red + 1, num_green=num_green)   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 2076, in _run_with_retry     num_red=num_red + 1, num_green=num_green)   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 2044, in _run_with_retry     super().run(result=result)   File ""/opt/conda/lib/python3.7/unittest/case.py"", line 628, in run     testMethod()   File ""test_serialization.py"", line 417, in test_serialization_save_warnings     x = torch.save(torch.nn.Linear(2, 3), checkpoint)   File ""test_serialization.py"", line 679, in wrapper     return self.torch_save(*args, **kwargs)   File ""/opt/conda/lib/python3.7/sitepackages/torch/serialization.py"", line 422, in save     _save(obj, opened_zipfile, pickle_module, pickle_protocol)   File ""/opt/conda/lib/python3.7/sitepackages/torch/serialization.py"", line 634, in _save     pickler.dump(obj)   File ""/opt/conda/lib/python3.7/sitepackages/torch/_tensor.py"", line 227, in __reduce_ex__     return self._reduce_ex_internal(proto)   File ""/opt/conda/lib/python3.7/sitepackages/torch/_tensor.py"", line 406, in _reduce_ex_internal     wrap_storage=self._typed_storage()._untyped_storage,   File ""/opt/conda/lib/python3.7/sitepackages/torch/_tensor.py"", line 267, in _typed_storage     _storage = self._storage()   File ""/opt/conda/lib/python3.7/sitepackages/torch/overrides.py"", line 1863, in __torch_function__     return old.__torch_function__(func, types, args, kwargs)   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 930, in __torch_function__     r = func(*args, **kwargs)   File ""/opt/conda/lib/python3.7/sitepackages/torch/_tensor.py"", line 262, in storage     torch.storage._warn_typed_storage_removal() ```  So I guess somewhere in `.jenkins/pytorch/test.sh`, this CI job must be overriding the `Tensor._storage()` method with `__torch_function__`, and whatever it's being overridden with ends up calling `Tensor.storage()`, which produces the error. I'm not very familiar with `__torch_function__`, so I'm not yet sure how to avoid this `Tensor.storage()` call. EDIT 2: I found that if I do this, I get the errors, without having to run all of `.jenkins/pytorch/test.sh`: ``` PYTORCH_TEST_WITH_CROSSREF=1 python test/test_serialization.py k test_serialization_save_warnings ``` And this reproduces locally, thankfully. I'm not really sure what crossref is, but I'll find out. Also, I found that I can now reproduce a lot of the duplicate warnings locally with `PYTORCH_TEST_WITH_CROSSREF=1 python test/test_proxy_tensor.py`. So once I understand this crossref thing, I should be able to fix those too","One of the CI jobs is failing because MultiPy depends on `TypedStorage._storage`, which I renamed to `TypedStorage._untyped_storage` in an attempt to make things more clear. I could just restore the old name. Although I think the better way is to change MultiPy to use the public method `TypedStorage.untyped()` instead","My last update fixed most of the duplicate TypedStorage warnings, at least for the `linuxbionicpy3.7clang9 / test (crossref, 1, 2, linux.2xlarge)` CI job. There are a handful left that I need to look into, and I'll also look at all the other CI logs as well","Some of the duplicate warnings happen because `gradcheck` evidently resets the warnings. For instance: ```python import torch import warnings from torch.testing._internal.common_utils import gradcheck for _ in range(3):     torch.FloatStorage()     warnings.warn('other warning')     gradcheck(lambda x: x, [torch.zeros(10, dtype=torch.double, requires_grad=True)]) ``` Output: ``` /work2/kurtamohler/development/pytorch0/torch/storage.py:322: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   warnings.warn(message, UserWarning) /work2/kurtamohler/development/pytorch0/../tmp/tmp.py:7: UserWarning: other warning   warnings.warn('other warning') /work2/kurtamohler/development/pytorch0/torch/storage.py:322: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   warnings.warn(message, UserWarning) /work2/kurtamohler/development/pytorch0/../tmp/tmp.py:7: UserWarning: other warning   warnings.warn('other warning') /work2/kurtamohler/development/pytorch0/torch/storage.py:322: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   warnings.warn(message, UserWarning) /work2/kurtamohler/development/pytorch0/../tmp/tmp.py:7: UserWarning: other warning   warnings.warn('other warning') ``` I'm not sure yet where in `gradcheck` warnings are getting reset. But this only happens in a handful of places in the CI tests, so I think it can looked into later. Also, I'm still fixing up a few other internally generated warnings. Very close to being finished, I think","Weird, gradcheck shouldnt reset warnings.   do either of you know?","Turns out `gradcheck` has this behavior just because of the `warnings.catch_warnings()` context manager: https://github.com/pytorch/pytorch/blob/7f88934a8fb9b376b32c722ac2f05959da34c147/torch/autograd/gradcheck.pyL835 For instance: ```python import torch import warnings for _ in range(3):     with warnings.catch_warnings() as w:         pass     warnings.warn('other warning')     torch.FloatStorage() ``` Output: ``` /work2/kurtamohler/development/pytorch0/torch/storage.py:321: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   warnings.warn(message, UserWarning) /work2/kurtamohler/development/pytorch0/../tmp1.py:7: UserWarning: other warning   warnings.warn('other warning') /work2/kurtamohler/development/pytorch0/torch/storage.py:321: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   warnings.warn(message, UserWarning) /work2/kurtamohler/development/pytorch0/../tmp1.py:7: UserWarning: other warning   warnings.warn('other warning') /work2/kurtamohler/development/pytorch0/torch/storage.py:321: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.   warnings.warn(message, UserWarning) /work2/kurtamohler/development/pytorch0/../tmp1.py:7: UserWarning: other warning   warnings.warn('other warning') ``` There is an old issue open for this on the `warnings` repo: https://github.com/python/cpython/issues/73858","Looking through the logs, the warnings aren't very noisy anymore. Most of the test logs have less than 30 of them, and most of these are just from separate test file executions (separate processes). Only two of the jobs have more than 100 warnings. I am sifting through all the logs now and soon I will post an overview of all the duplicate counts for each job and what I know about them. I think there's only a handful of root causes left now, and I think almost all of them can probably be investigated/fixed after this PR is merged (for instance, the duplicates from inbetween `warnings.catch_warnings()` calls). But I am sifting through the logs that generate the most duplicates now, and I'll post a summary soon"
rag,Remove `TypedStorage` and use only `UntypedStorage`,"Now that typed storages have been removed from the C++ side and `torch.UntypedStorage` is in place, we can remove `torch.TypedStorage` and all of its subclasses, and just use `UntypedStorage` instead. First, we should add warnings to all of the methods of `TypedStorage` and its subclasses for at least one whole release.",2022-09-19T21:00:55Z,triaged module: python frontend,open,0,0,https://github.com/pytorch/pytorch/issues/85302
rag,Templatize checkInBoundsForStorage and setStrided for SymInt,  CC(Add braces around single line conditional)  CC(Remove improper asserts.)  CC(Templatize checkInBoundsForStorage and setStrided for SymInt)  CC(Fix bug in computeStorageNbytes) Signedoffby: Edward Z. Yang ,2022-09-17T16:00:27Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/85205
rag,Fix bug in computeStorageNbytes,  CC(Add braces around single line conditional)  CC(Remove improper asserts.)  CC(Templatize checkInBoundsForStorage and setStrided for SymInt)  CC(Fix bug in computeStorageNbytes) Signedoffby: Edward Z. Yang ,2022-09-17T15:57:39Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/85204
yi,Symintifying slice ops,  CC(Symintifying slice ops)  CC(OpInfo for Slice),2022-09-16T23:44:36Z,Merged cla signed Reverted release notes: vulkan,closed,0,8,https://github.com/pytorch/pytorch/issues/85196,Testing?,commandeering this PR, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""Break internal build Exutorch"" c ghfirst", successfully started a revert job. Check the current status here. Please reach out to the PyTorch DevX Team with feedback or questions!, your PR has been successfully reverted.
transformer,[ONNX][bug] `nn.Transformer` contains unsupported tensor scalar type," 🐛 Describe the bug PyTorch fails to export a model containing an `nn.Transformer` module. It fails with `RuntimeError: unexpected tensor scalar type`. Here's a minimal repro script: ```python import torch import torch.nn as nn class M(nn.Module):   def __init__(self):     super().__init__()     self.transformer = nn.Transformer(d_model=128)   def forward(self, x, y):     return self.transformer(x, y) module = M() module = torch.jit.script(module) x = torch.randn([10, 1, 128]) y = torch.randn([10, 1, 128]) dummy_input = (x, y) torch.onnx.export(module, dummy_input, 'test.onnx', verbose=True, opset_version=14) ```  Versions ``` Collecting environment information... PyTorch version: 1.12.1+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.19.6 Libc version: glibc2.27 Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.15.0118genericx86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.3.58 GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 3090 GPU 1: NVIDIA GeForce GTX 1080 Ti GPU 2: NVIDIA GeForce RTX 3090 Nvidia driver version: 465.19.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.5.1 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn.so.8.1.0 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_adv_infer.so.8.1.0 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_adv_train.so.8.1.0 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_cnn_infer.so.8.1.0 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_cnn_train.so.8.1.0 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_ops_infer.so.8.1.0 /usr/local/cuda11.2/targets/x86_64linux/lib/libcudnn_ops_train.so.8.1.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.942 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.4 [pip3] pytestmypy==0.9.1 [pip3] torch==1.12.1+cu113 [pip3] torchtbprofiler==0.3.1 [pip3] torchtensorrt==1.1.0 [pip3] torchtext==0.11.0a0+d697db5 [conda] libblas                   3.9.0            12_linux64_mkl    condaforge [conda] libcblas                  3.9.0            12_linux64_mkl    condaforge [conda] liblapack                 3.9.0            12_linux64_mkl    condaforge [conda] magmacuda112             2.5.2                         1    pytorch [conda] mkl                       2021.4.0           h8d4b97c_729    condaforge [conda] mklinclude               2021.3.0           h06a4308_520   [conda] numpy                     1.22.4                   pypi_0    pypi [conda] torch                     1.12.1+cu113             pypi_0    pypi [conda] torchtbprofiler         0.3.1                    pypi_0    pypi [conda] torchtensorrt            1.1.0                    pypi_0    pypi [conda] torchtext                 0.11.0a0+d697db5          pypi_0    pypi ```",2022-09-15T22:00:46Z,module: onnx triaged onnx-triaged bug,closed,0,12,https://github.com/pytorch/pytorch/issues/85116," There are three places in our code base that will raise ""unexpected tensor scalar type"". CC([ONNX] Improve torch.onnx diagnostics)",", to clarify: I expect that the sample code I provided executes without any errors. This is not a diagnostic issue; rather, it's a bug in either the exporter such that it can't handle `nn.Transformer` or a bug in `nn.Transformer` where it's written in a way that it cannot be exported.","FYI, this bug is a regression from earlier releases. The same code snippet I provided above exports successfully when PyTorch is installed with `pip install torch==1.11 extraindexurl https://download.pytorch.org/whl/cu113`. The regression was introduced somewhere between the `1.11.0` and `1.12.0` releases.", Understood. Thanks for the info! We are in the process of improving diagnostics so this is relevant. Please feel free to share thoughts if you have any suggestions!,"Notes: is_cuda, is_grad_enabled",I am able to export the nonscripted version of the module with the latest pytorchnightly. Does that work for you?,"Yes, that works. The nonscripted version works on 1.12.1 as well, though, so this bug is kinda different. FWIW, I'm attempting to script because tracing my actual network (not the minified version I pasted here) takes ~45 mins to export on a trivial sequence of 3 elements. And when I try to run that network, I get random results. The network is a basic autoregressive loop with a Transformer core and a decode loop of 32 steps.",Thanks for the info. I was not able to reproduce yet because apparently nn.Transformer introduced new ops it uses internally the onnx exporter doesn't yet support. So I haven't been able to get to the type error.  any suggestions? I don't think it's possible to trace only the Transformer and script the rest of the decoder?," what you mentioned is probably achievable, but I'm more curious as how it regress from 1.11. It could be though that `nn.Transformer` has changed. ",Apparently there is a `_nested_tensor_from_mask` op being used since 4 months ago  https://github.com/pytorch/pytorch/blame/4cfd09d7bc51f700373a3ee633776bdc903f9cde/torch/nn/modules/transformer.py,Now I observe `torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'prim::is_cuda' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues`,We are not planning adding new ops to torch.onnx.export API. Please try `torch.onnx.dynamo_export` and reopen this issue with an updated repro if the issue persists
finetuning,[MPS] division-by-zero returns 0.0 instead of Inf," 🐛 Describe the bug Divisionbyzero correctly returns Inf onCPU: ```python (torch.tensor([1], device='cpu')/0).item()  inf torch.isinf(torch.tensor([1], device='cpu')/0).any().item()  True ``` MPS returns 0.0 instead: ```python (torch.tensor([1], device='mps')/0).item()  0.0 torch.isinf(torch.tensor([1], device='mps')/0).any().item()  False ``` noticed this whilst we were trying to find the source of NaN and Inf during stablediffusion finetuning:   https://github.com/lstein/stablediffusion/issues/517issuecomment1248573390  Versions ``` Collecting environment information... PyTorch version: 1.13.0.dev20220826 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.5 (arm64) GCC version: Could not collect Clang version: 13.0.0 (clang1300.0.29.30) CMake version: version 3.22.1 Libc version: N/A Python version: 3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ] (64bit runtime) Python platform: macOS12.5arm6464bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] pytorchlightning==1.4.2 [pip3] torch==1.13.0.dev20220826 [pip3] torchfidelity==0.3.0 [pip3] torchdiffeq==0.2.3 [pip3] torchmetrics==0.6.0 [pip3] torchtyping==0.1.4 [pip3] torchvision==0.14.0.dev20220826 [conda] numpy                     1.22.4                   pypi_0    pypi [conda] pytorch                   1.13.0.dev20220826        py3.10_0    pytorchnightly [conda] pytorchlightning         1.4.2                    pypi_0    pypi [conda] torchfidelity            0.3.0                    pypi_0    pypi [conda] torchdiffeq               0.2.3                    pypi_0    pypi [conda] torchmetrics              0.6.0                    pypi_0    pypi [conda] torchtyping               0.1.4                    pypi_0    pypi [conda] torchvision               0.14.0.dev20220826       py310_cpu    pytorchnightly ``` ",2022-09-15T20:31:59Z,triaged module: mps,closed,2,3,https://github.com/pytorch/pytorch/issues/85106,"```python (torch.tensor([1], device='mps')/0.).item()  inf (torch.ones(1, device='mps')/0).item()  inf ``` `torch.ones()`, or making the 0 into a `0.`, seem to be ways to make it return inf.",Thanks san for the report. I've tried to reproduce this issue with latest nightly but without success. Could you please give a try on latest nightly build (torch1.13.0.dev20221004) and let me know if you still see the issue? Thanks.,"yup, confirmed working on latest nightly (`1.14.0.dev20221013`). thanks!"
rag,change the type of storage_offset to SymInt,Fixes ISSUE_NUMBER,2022-09-15T19:54:45Z,Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/85102, merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
chat,torch.atan2 crash with segmentation fault in the nightly version," 🐛 Describe the bug `torch.atan2` crash with segmentation fault ``` import torch torch.atan2(input=torch.ones([2]), other=torch.ones([1]), out=torch.ones([2,2,2])) ``` Output: ``` Segmentation fault (core dumped) ``` Also reproduced in the gist  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220913+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.4 LTS (x86_64) GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.10 Python version: 3.7.6 (default, Jan 8 2020, 19:59:22) [GCC 7.3.0] (64bit runtime) Python platform: Linux4.15.0176genericx86_64withdebianbustersid Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.19.2 [pip3] torch==1.13.0.dev20220913+cpu [pip3] torchaudio==0.13.0.dev20220913+cpu [pip3] torchvision==0.14.0.dev20220913+cpu [conda] blas 1.0 mkl [conda] mkl 2020.2 256 [conda] mklservice 2.3.0 py37he8ac12f_0 [conda] mkl_fft 1.3.0 py37h54f3939_0 [conda] mkl_random 1.1.1 py37h0573a6f_0 [conda] numpy 1.19.2 py37h54aff64_0 [conda] numpybase 1.19.2 py37hfa32c7d_0 [conda] torch 1.13.0.dev20220913+cpu pypi_0 pypi [conda] torchaudio 0.13.0.dev20220913+cpu pypi_0 pypi [conda] torchvision 0.14.0.dev20220913+cpu pypi_0 pypi",2022-09-15T00:14:21Z,module: error checking triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/85059,Should be fixed by https://github.com/pytorch/pytorch/pull/85294
rag,[reland 2] Call jit decomp in VariableType to improve forward AD coverage,Reland of https://github.com/pytorch/pytorch/pull/84675,2022-09-14T01:03:06Z,oncall: jit Merged cla signed ciflow/trunk release notes: autograd topic: improvements ciflow/periodic fx,closed,0,13,https://github.com/pytorch/pytorch/issues/84976," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hopefully internal builds should passing now. The issue was that certain builds (e.g. lite_trainer) depended on the VariableType files, but do not depend on the jit. Including all of those additional dependencies to get jit to work on lite trainer will blow up the binary size so I don't think we want that. Instead, what I'm doing in this updated PR (see latest commit for new changes) is the same thing that is done with VariableHooksInterface, i.e. provide the apis conditionally depending on whether the library depends on jit.","I've confirmed that internal tests are passing now. I've cleaned up this PR a little bit and addressed the comments, so it should be ready for another look! > There's a question of what we want to do in the long term. Should we take the TorchScript dependency, or should we just invoke the decomposition in Python? Since the TorchScript route is easy to do and proven to work, we should go with this PR for the shorttomedium term. This route makes sense to me as well"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,SymInt support for multiply_integers,These helpers are used in symintifying `reshape`. Ported from symbolicshapes branch    CC(SymInt support for computeStride)  CC(SymInt support for multiply_integers)  CC(StmInt support for InferSize),2022-09-12T23:56:47Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/84904
yi,multiply_integers,  CC(computeStride)  CC(computeStride)  CC(multiply_integers)  CC(Add SymInt support for infer_size_dv),2022-09-12T23:49:00Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/84900
rag,[TensorImpl] Make set_storage_keep_dtype virtual,"Summary: For lazy tensor impl, set_storage_keep_dtype needs to be virtual to take care of dummy storage created for frontend tensors. For lazy tensor impl, simple move of storage doesn't work because it may not have been evaluated and for other book keeping that TensorImpl need to do, this function should be virtual.",2022-09-12T08:48:21Z,triaged open source cla signed Stale,closed,0,11,https://github.com/pytorch/pytorch/issues/84855," could you please elaborate maybe with some examples why a dummy storage option won't work?  This PR only makes `LTCTensorImpl::set_storage_keep_dtype` virtual, if it's called via a base class pointer, it would still be using the base class implementation? For example, ```cpp include  struct A {     void foo() {std::cout foo(); // this still prints A     aa>foo();     return 0; } ``` This could be inconsistent and confusing.","  I have added my comments below  >  could you please elaborate maybe with some examples why a dummy storage option won't work? >  > This PR only makes `LTCTensorImpl::set_storage_keep_dtype` virtual, if it's called via a base class pointer, it would still be using the base class implementation? This PR is making the base tensor TensorImpl::set_storage_keep_dtype` virtual, so that it can be overridden by other derived classes of TensorImpl. If there is no overloading, then the base class TensorImpl::set_storage_keep_dtype` would be called, the purpose of making this virtual is to handle derived class specific functionality. This would be similar to existing virtual methods in TensorImpl like has_storage(), storage() etc. >  > For example, >  > ```c++ > include  >  >  > struct A { >     void foo() {std::cout  }; >  > struct AA: public A { >     virtual void foo() {std::cout  }; >  > struct BB: public AA { >     void foo() override {std::cout  }; >  > int main() { >      >     A* a = new BB(); >     AA* aa = (AA*)a; >     a>foo(); // this still prints A >     aa>foo(); >     return 0; > } > ``` >  > This could be inconsistent and confusing."," Any thoughts on what  mentioned. Basically, in your example we are doing  struct A {     **virtual** void foo() {std::cout << ""A\n""; } };",">  Any thoughts on what  mentioned. Basically, in your example we are doing struct A { virtual void foo() {std::cout << ""A\n""; } }; I think i agree that the proposed change is making the base TensorImpl method virtual and thus doesn't apply to your example . But I also don't understand the implications of making this change well enough or whether there is a better way to achieve the goals.  Maybe  can help.",Could you give us some more context on the lazy tensor implementation you're working on?,"Hi , we are facing issue with jit.load using Habana backend. In rebuildTensor() function in unpickler.cpp, empty tensor is created and then some storage from some other tensor is attached to it. storage() of tensor in lazy mode may be just dummy storage, if ""impl>set_storage_keep_dtype(storage_tensor.storage());"" is called, we want control for our impl to evaluate the ops accumulated till that point, so that storage is actual storage. I see set_storage_offset is already virtual, that's why proposed that if set_storage_keep_dtype can also be made virtual. Other option is to create a new function that encapsulates set_storage_keep_dtype, set_storage_offset, set_sizes_and_strides into one function and make it as virtual and call only from rebuildTensor.",What is stopping Habana from having a real storage for its serialization format?,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.",The committers listed above are authorized under a signed CLA.:white_check_mark: login: amitchawla1  (d7b175570216ee703dbf6c60b4922a3dacd2f935),", problem is set_storage_keep_dtype is setting storage directly and we can't assume it to be actual storage, since this function is used at many places.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
yi,MPS: allow selecting specific MTLDevice by registryID via environment variable," 🚀 The feature, motivation and pitch Hello :wave: _very_ new to pytorch and running it on both M1 macs and on a mac with a couple Radeons. I would like to be able to use more than one of the Radeons at times (in different processes), or to select one specifically so I can use the other GPU for other tasks while I work on other things, etc. I think this should (hopefully?) be pretty trivial  I'm not familiar enough with the build system and intricacies of conda/pip etc to have run a build just yet, but I have a draft PR on my fork here: https://github.com/jaeschliman/pytorch/pull/1 Open to any changes, pointers, etc. Thanks!  Alternatives At first considered selecting by peerGroup and peerIndex, but Apple's docs state that registryID is stable across processes, so it looks like the correct choice.  Additional context https://developer.apple.com/documentation/metal/mtldevice/2915737registryid?language=objc ",2022-09-10T13:32:52Z,triaged enhancement module: mps,open,0,2,https://github.com/pytorch/pytorch/issues/84813,"Just curious: why wouldn't follow a pretty standard approach of  `{device}:{ordinal}`, i.e. `x=torch.empty((3, 3), device='mps:1')`?","  The use case I hope to enable is pinning a script to a chosen device without modifying the source of the script. In other words, naming the device by an ordinal in the script source seems orthogonal to what I hope to accomplish here. Does that make sense? Again, I am very new to pytorch, so apologies if I missed something that seems obvious. Thanks!"
yi,macOS Pyinstaller: libc++abi: terminating with uncaught exception of type c10::Error: Type c10::intrusive_ptr<ConvPackedParamsBase<2>> could not be converted to any of the known types," 🐛 Describe the bug I'm trying to package a python app using pyinstaller.  Torch: 1.12.1 Pyinstaller: 5.3 OS: macOS 12.5 (M1) [UPDATE] This appears to be a problem with `torchvision`. If I remove the dependency, no crash. When I try to run my UNIX executable, I get the following:  ``` [41110] WARNING: file already exists but should not: /var/folders/9l/0lk2nvvs7j39q893l1k0qvcc0000gn/T/_MEI9VPQ38/pyarrow/lib.cpython310darwin.so [41110] WARNING: file already exists but should not: /var/folders/9l/0lk2nvvs7j39q893l1k0qvcc0000gn/T/_MEI9VPQ38/torch/_C.cpython310darwin.so [41110] WARNING: file already exists but should not: /var/folders/9l/0lk2nvvs7j39q893l1k0qvcc0000gn/T/_MEI9VPQ38/torch/_C_flatbuffer.cpython310darwin.so [41110] WARNING: file already exists but should not: /var/folders/9l/0lk2nvvs7j39q893l1k0qvcc0000gn/T/_MEI9VPQ38/torch/_dl.cpython310darwin.so libc++abi: terminating with uncaught exception of type c10::Error: Type c10::intrusive_ptr> could not be converted to any of the known types. Exception raised from operator() at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/jit_type.h:1735 (most recent call first): frame CC(未找到相关数据): c10::Error::Error(c10::SourceLocation, std::__1::basic_string, std::__1::allocator >) + 81 (0x18a356951 in libc10.dylib) frame CC(Matrix multiplication operator): c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__1::basic_string, std::__1::allocator > const&) + 98 (0x18a355022 in libc10.dylib) frame CC(Don't support legacy Python): c10::detail::getTypePtr_, c10::detail::intrusive_target_default_null_type > > >::call()::'lambda'()::operator()() const + 276 (0x1d894caa4 in libtorch_cpu.dylib) frame CC(PEP8): c10::Type::SingletonOrSharedTypePtr c10::getTypePtrCopy, c10::detail::intrusive_target_default_null_type > > >() + 27 (0x1d894c82b in libtorch_cpu.dylib) frame CC(PEP8): c10::detail::infer_schema::(anonymous namespace)::createArgumentVector(c10::ArrayRef) + 210 (0x1d82fb0a2 in libtorch_cpu.dylib) frame CC(Checklist for Release): c10::detail::infer_schema::make_function_schema(std::__1::basic_string, std::__1::allocator >&&, std::__1::basic_string, std::__1::allocator >&&, c10::ArrayRef, c10::ArrayRef) + 86 (0x1d82fae56 in libtorch_cpu.dylib) frame CC(Remove dampening from SGD): c10::detail::infer_schema::make_function_schema(c10::ArrayRef, c10::ArrayRef) + 67 (0x1d82fb893 in libtorch_cpu.dylib) frame CC(ImportError: No module named _C): std::__1::unique_ptr > c10::detail::inferFunctionSchemaFromFunctor, c10::detail::intrusive_target_default_null_type > > const&, double, long long)>() + 85 (0x1d89e5945 in libtorch_cpu.dylib) frame CC(fake commit): torch::CppFunction::CppFunction, c10::detail::intrusive_target_default_null_type > > const&, double, long long)>(at::Tensor (*)(at::Tensor, c10::intrusive_ptr, c10::detail::intrusive_target_default_null_type > > const&, double, long long), std::__1::enable_if, c10::detail::intrusive_target_default_null_type > > const&, double, long long)>::value, std::nullptr_t>::type) + 122 (0x1d89e586a in libtorch_cpu.dylib) frame CC(Add Storage.from_buffer): at::native::(anonymous namespace)::TORCH_LIBRARY_IMPL_init_quantized_QuantizedCPU_4(torch::Library&) + 35 (0x1d89e3b93 in libtorch_cpu.dylib) frame CC(Tensors don't print sometimes): torch::detail::TorchLibraryInit::TorchLibraryInit(torch::Library::Kind, void (*)(torch::Library&), char const*, c10::optional, char const*, unsigned int) + 203 (0x1d81a762b in libtorch_cpu.dylib) frame CC(add multiprocessing unit tests, working filedescriptor based solution and OSX): _GLOBAL__sub_I_qconv.cpp + 178 (0x1d89e8cb2 in libtorch_cpu.dylib) frame CC(Initial utils implementation + bug fixes): invocation function for block in dyld4::Loader::findAndRunAllInitializers(dyld4::RuntimeState&) const + 182 (0x220f2ee4f in dyld) frame CC(Infinite recursion when indexing Variable): invocation function for block in dyld3::MachOAnalyzer::forEachInitializer(Diagnostics&, dyld3::MachOAnalyzer::VMAddrConverter const&, void (unsigned int) block_pointer, void const*) const + 129 (0x220f55911 in dyld) frame CC(Clean up Module forward and __call__): invocation function for block in dyld3::MachOFile::forEachSection(void (dyld3::MachOFile::SectionInfo const&, bool, bool&) block_pointer) const + 557 (0x220f4ce26 in dyld) frame CC(Use chainerstyle constructor for Conv2d): dyld3::MachOFile::forEachLoadCommand(Diagnostics&, void (load_command const*, bool&) block_pointer) const + 129 (0x220f1bdb3 in dyld) frame CC(Error on legacy.nn serialization): dyld3::MachOFile::forEachSection(void (dyld3::MachOFile::SectionInfo const&, bool, bool&) block_pointer) const + 179 (0x220f4cbb7 in dyld) frame CC(Error on printing nn.ConcatTable): dyld3::MachOAnalyzer::forEachInitializerPointerSection(Diagnostics&, void (unsigned int, unsigned int, unsigned char const*, bool&) block_pointer) const + 118 (0x220f55342 in dyld) frame CC(OS X build issue in THP_decodeInt64Buffer): dyld3::MachOAnalyzer::forEachInitializer(Diagnostics&, dyld3::MachOAnalyzer::VMAddrConverter const&, void (unsigned int) block_pointer, void const*) const + 386 (0x220f555b4 in dyld) frame CC(Add missing PyBuffer_Release calls): dyld4::Loader::findAndRunAllInitializers(dyld4::RuntimeState&) const + 144 (0x220f2ed82 in dyld) frame CC(Figure out and fix Tensor(Storage) constructor): dyld4::Loader::runInitializersBottomUp(dyld4::RuntimeState&, dyld3::Array&) const + 178 (0x220f2ef0e in dyld) frame CC(import torch works in ipython but not in python (_THRefcountedMapAllocator)): dyld4::Loader::runInitializersBottomUp(dyld4::RuntimeState&, dyld3::Array&) const + 149 (0x220f2eef1 in dyld) frame CC(adding cuda driver check functions for runtime checking): dyld4::Loader::runInitializersBottomUp(dyld4::RuntimeState&, dyld3::Array&) const + 149 (0x220f2eef1 in dyld) frame CC(OSX Multiprocessing errors out): dyld4::Loader::runInitializersBottomUpPlusUpwardLinks(dyld4::RuntimeState&) const + 108 (0x220f2efb2 in dyld) frame CC(More modules for nn + improvements in CUDA tests): dyld4::APIs::dlopen_from(char const*, int, void*) + 592 (0x220f3de00 in dyld) frame CC(Convert between CUDA types): py_dl_open + 135 (0x1457cef77 in _ctypes.cpython310darwin.so)  ```  Versions [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.1 [pip3] pytorchlightning==1.4.2 [pip3] torch==1.12.1 [pip3] torchfidelity==0.3.0 [pip3] torchaudio==0.12.1 [pip3] torchmetrics==0.6.0 [pip3] torchvision==0.13.1 [conda] numpy                     1.23.1                   pypi_0    pypi [conda] pytorchlightning         1.4.2                    pypi_0    pypi [conda] torch                     1.13.0.dev20220907          pypi_0    pypi [conda] torchfidelity            0.3.0                    pypi_0    pypi [conda] torchmetrics              0.6.0                    pypi_0    pypi [conda] torchvision               0.14.0.dev20220907          pypi_0    pypi ",2022-09-09T20:50:33Z,module: cpp-extensions triaged module: third_party needs research module: m1,open,0,6,https://github.com/pytorch/pytorch/issues/84782,"Looks like importing from `pytorch_lightning` causes the same error, but could still be an import of `torchvision` under the hood.","Hmm, `oncall: jit` feels like a wrong tag here (as it has nothing to do with it, likely crash happens because libtorch_cpu somehow getting packaged twice by pyinstaller.... At least I can reproduce the crash while trying to package something as simple as: ``` !/usr/bin/env python3 import torch import torchvision def main():     print(torch.__version__)     print(torchvision.__version__) if __name__ == ""__main__"":     main() ```","I found the workaround from this thread to work for the timebeing. After pyinstaller finishes, I can run: ``` rm libtorch_python.dylib rm libtorch.dylib rm libc10.dylib rm libtorch_cpu.dylib ln s ./torch/lib/libtorch_python.dylib libtorch_python.dylib ln s ./torch/lib/libtorch.dylib libtorch.dylib ln s ./torch/lib/libc10.dylib libc10.dylib ln s ./torch/lib/libtorch_cpu.dylib libtorch_cpu.dylib ``` https://github.com/pyinstaller/pyinstallerhookscontrib/issues/375issuecomment1053572749 This indicates it might be a pyinstaller bug, rather than a torch issue."," thank you for confirming. I.e. it does not look like the bug is in PyTorch or TorchVision, but rather in a way how pyinstaller packages those libraries together. Do you mind opening an issue there and I would gladly coordinate with them if there is a metadata we can provide in the PyTorch dependent packages to avoid this doublepacking issue","Meta question, should we transfer this issue to `pytorch/vision` since this is more of an issue with vision than core pytorch?",I don't think it's specific to torchvision (nor to M1 to be frank)
transformer,add autocast to test transformers,Add additional test for autocast + transformer fastpath. ,2022-09-08T22:35:58Z,cla signed,closed,0,0,https://github.com/pytorch/pytorch/issues/84723
transformer,Disable Transformer/MHA fast path when autocast is enabled,Differential Revision: D39362298,2022-09-08T22:04:48Z,fb-exported Merged cla signed release notes: nn topic: bug fixes,closed,0,11,https://github.com/pytorch/pytorch/issues/84722,This pull request was **exported** from Phabricator. Differential Revision: D39362298, merge g, Merge failed **Reason**: PR CC(Disable Transformer/MHA fast path when autocast is enabled) has not been reviewed yet (Rule superuser) Details for Dev Infra team Raised by workflow job ,This pull request was **exported** from Phabricator. Differential Revision: D39362298, merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!",This pull request was **exported** from Phabricator. Differential Revision: D39362298, Merge failed **Reason**: New commits were pushed while merging. Please rerun the merge command. Details for Dev Infra team Raised by workflow job , merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Error when trying to export MONAI model to ONNX," 🐛 Describe the bug When trying to export the SwinUNETR model from MONAI, I get the error: ``` RuntimeError: Failed to export an ONNX attribute 'onnx::Gather', since it's not constant, please try to make things (e.g., kernel size) static if possible. ``` In a different issue, I read that this issue might get fixed by changing `x_shape = x.size()` to `x_shape = [int(s) for s in x.size()]` in the problematic code  I found out that problem manifests at `proj_out()`. Doing this, though, results in a different error: ``` RuntimeError: Unsupported: ONNX export of instance_norm for unknown channel size. ``` Making this change in all places where I find `x_shape = x.size()` results in a floating point exception! Here is a minimal example demonstrating the issue: ```python from monai.networks.nets import SwinUNETR     import torch                                  if __name__ == '__main__':                        model = SwinUNETR(img_size=(96, 96, 96),                        in_channels=1,                                out_channels=5,                               feature_size=48,                              drop_rate=0.0,                                attn_drop_rate=0.0,                           dropout_path_rate=0.0,                        use_checkpoint=True,                          )                           inputs = [torch.randn([1,1,96,96,96])]     input_names = ['input']                               output_names = ['output']                             torch.onnx.export(                                        model,                                                tuple(inputs), 'model.onnx',                          verbose=False,                                        input_names=input_names,                              output_names=output_names,                            dynamic_axes=None,                                    opset_version=11,                                 )                                                 ```  Versions Collecting environment information... PyTorch version: 1.12.1.post200       Is debug build: False                 CUDA used to build PyTorch: 11.2                                                                               ROCM used to build PyTorch: N/A                                                                                OS: CentOS Linux release 7.9.2009 (Core) (x86_64)                                                              GCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.12)                                                            Clang version: Could not collect                                                                               CMake version: version 3.20.4                                                                                  Libc version: glibc2.17                                                                                       Python version: 3.10.6  (main, Aug 22 2022, 20:36:39) [GCC 10.4.0] (64bit runtime) Python platform: Linux3.10.01160.36.2.el7.x86_64x86_64withglibc2.17                                       Is CUDA available: True                                                                                        CUDA runtime version: 11.1.105                                                                                 GPU models and configuration:                                                                                  GPU 0: Tesla T4                                                                                                GPU 1: Tesla T4                                                                                                GPU 2: Tesla T4                                                                                                GPU 3: Tesla T4                                                                                                GPU 4: Tesla T4                                                                                                Nvidia driver version: 510.39.01                                                                               cuDNN version: Could not collect                                                                               HIP runtime version: N/A                                                                                       MIOpen runtime version: N/A                                                                                    Is XNNPACK available: True                                                                                     Versions of relevant libraries:                                                                                [pip3] numpy==1.23.2                                                                                           [pip3] torch==1.12.1.post200                                                                                   [conda] cudatoolkit               11.7.0              hd8887f6_10                                              [conda] magma                     2.5.4                h6103c52_2                                              [conda] mkl                       2022.1.0           h84fe81f_915                                              [conda] numpy                     1.23.2          py310h53a5b5f_0                                              [conda] pytorch                   1.12.1          cuda112py310h51fe464_200                                    ",2022-09-08T12:41:08Z,module: onnx triaged onnx-needs-info,closed,0,5,https://github.com/pytorch/pytorch/issues/84692,"It seems to me that you are exporting a transformer with static input shape, which is unlikely correct, as transformer is complicated. Please refer to torch.onnx.onnx, and see how you can utilize `dynamic_axes`.","I don't quite understand why it would be a problem to export with static axes. Nonetheless, I tried changing dynamic_axes to: ``` dynamic_axes={                      'input': {0: 'batch_size'},     'output': {0: 'batch_size'} },                              ``` or even ``` dynamic_axes={     'input': [0,1,2,3,4],            'output': [0,1,2,3,4]        },                               ``` With the exact same error message.","The following repro on latest torch master branch with monai/einops ```bash pip install monai einops ``` ```python from monai.networks.nets import SwinUNETR import torch if __name__ == '__main__':     model = SwinUNETR(img_size=(96, 96, 96),                       in_channels=1,                       out_channels=5,                       feature_size=48,                       drop_rate=0.0,                       attn_drop_rate=0.0,                       dropout_path_rate=0.0,                       use_checkpoint=True,                       )     inputs = [torch.randn([1,1,96,96,96])]     input_names = ['input']     output_names = ['output']     torch.onnx.export(         model,         tuple(inputs), 'model.onnx',         verbose=False,         input_names=input_names,         output_names=output_names,         dynamic_axes=None,         opset_version=17,     ) ``` produced this error ```bash /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/monai/networks/blocks/patchembedding.py:175: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   if w % self.patch_size[2] != 0: /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/monai/networks/blocks/patchembedding.py:177: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   if h % self.patch_size[1] != 0: /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/monai/networks/blocks/patchembedding.py:179: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   if d % self.patch_size[0] != 0: /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/monai/networks/nets/swin_unetr.py:393: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   if x_size[i]  0 or pad_r > 0 or pad_b > 0: Traceback (most recent call last):   File ""/home/thiagofc/dev/pytorch_repros/stale_repro.py"", line 19, in      torch.onnx.export(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 504, in export     _export(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 1529, in _export     graph, params_dict, torch_out = _model_to_graph(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 1111, in _model_to_graph     graph, params, torch_out, module = _create_jit_graph(model, args)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 987, in _create_jit_graph     graph, torch_out = _trace_and_get_graph_from_model(model, args)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 891, in _trace_and_get_graph_from_model     trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/jit/_trace.py"", line 1260, in _get_trace_graph     outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1423, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/jit/_trace.py"", line 127, in forward     graph, out = torch._C._create_graph_by_tracing(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/jit/_trace.py"", line 118, in wrapper     outs.append(self.inner(*trace_inputs))   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1423, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1410, in _slow_forward     result = self.forward(*input, **kwargs)   File ""/home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/monai/networks/nets/swin_unetr.py"", line 297, in forward     hidden_states_out = self.swinViT(x_in, self.normalize)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1423, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1410, in _slow_forward     result = self.forward(*input, **kwargs)   File ""/home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/monai/networks/nets/swin_unetr.py"", line 1011, in forward     x1 = self.layers10)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1423, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1410, in _slow_forward     result = self.forward(*input, **kwargs)   File ""/home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/monai/networks/nets/swin_unetr.py"", line 876, in forward     x = blk(x, attn_mask)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1423, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/nn/modules/module.py"", line 1410, in _slow_forward     result = self.forward(*input, **kwargs)   File ""/home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/monai/networks/nets/swin_unetr.py"", line 668, in forward     x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/utils/checkpoint.py"", line 249, in checkpoint     return CheckpointFunction.apply(function, preserve, *args) RuntimeError: _Map_base::a ```   Could you review the provided script to make sure it runs on latest torch?", Tried with pytorch 1.13 and got the same error you posted.,"`torch.onnx.export` is in maintenance mode and we don't plan to add new operators/features or fix complex issues, including checkpoint activation Please try the new ONNX exporter and reopen this issue with a full repro if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial"
yi,[TensorExpr] applying `rfactor` for a Mul Reducer with init value different than 1 results in wrong results," 🐛 Describe the bug `rfactor` initializes the `rfac_init` with the initializer of the original reduce op. If the original Reducer uses Mul as the ReduceInteraction with an init value different than 1, the result after `rfactor` will be wrong (the same issue for an Add reducer with an init value different than 0). https://github.com/pytorch/pytorch/blob/8bd9fe3f493073bf8f4a2e428c3048096fb36052/torch/csrc/jit/tensorexpr/loopnest.cppL3380L3381  CPP UT to reproduce: The tensor of size `2 x 8` has values all set to 1. The reduce axis is the last dim (where `size == 8`). The Reducer has the init value = 2. The ReduceInteraction is Mul. The expected result will be `tensor([ 2, 2])`. With `rfactor`, the result becomes `tensor([ 4, 4])`. ```cpp TEST(Reductions, ReduceCustomProductWithRfactor) {   const int M = 2;   const int N = 8;   BufHandle b(""b"", {M, N}, kFloat);   std::vector in(M * N);   for (const auto i : c10::irange(M)) {     for (const auto j : c10::irange(N)) {       in[i * N + j] = 1;     }   }   std::vector out(M, 1.f);   Reducer product(       ExprHandle(2.f), [](ExprHandle a, ExprHandle b) { return a * b; });   Tensor c = Reduce(""product"", {M}, product, b, {N});   LoopNest nest({c});   // rfactor   auto loops = nest.getLoopStmtsFor(c);   ForPtr mi, mo, tail;   BufPtr rf;     constexpr int kChunkSize = 8;   nest.splitWithTail(loops[1], kChunkSize, &mi, &tail);   TORCH_CHECK(nest.rfactor(nest.getLoopBodyFor(c), loops[1], &rf));   nest.prepareForCodegen();   StmtPtr s = nest.root_stmt();   s = IRSimplifier::simplify(s);   std::cout << ""final stmt: \n"" << *s << ""\n"";   SimpleIREvaluator cg(s, {b, c});   cg.call({in, out});   float expected = 2;   for (const auto i : c10::irange(N)) {     // NOLINTNEXTLINE(bugpronenarrowingconversions,cppcoreguidelinesnarrowingconversions)     expected *= 1;   }   for (const auto i : c10::irange(M)) {     ASSERT_EQ(out[i], expected);   } } ```  Output log: ```bash Expected equality of these values:   out[i]     Which is: 4   expected     Which is: 2 [  FAILED  ] Reductions.ReduceCustomProductWithRfactor (1 ms) [] 1 test from Reductions (1 ms total) [] Global test environment teardown [==========] 1 test from 1 test suite ran. (1 ms total) [  PASSED  ] 0 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] Reductions.ReduceCustomProductWithRfactor  1 FAILED TEST ```  Stmt without and with `rfactor`: The **correct** final stmt without `rfactor`: ``` {   for (int i = 0; i < 2; i++) {     product[i] = 2.f;     for (int i_1 = 0; i_1 < 8; i_1++) {       product[i] = (product[i]) * (b[i_1 + 8 * i]);     }   } } ``` The **wrong** final stmt with `rfactor`: ``` {   for (int i = 0; i < 2; i++) {     product[i] = 2.f;     product_rfac[i] = 2.f;     for (int i_inner = 0; i_inner < 8; i_inner++) {       product_rfac[i] = (product_rfac[i]) * (b[i_inner + 8 * i]);     }     product[i] = (product[i]) * (product_rfac[i]);   } } ```  Versions ``` PyTorch version: 1.13.0a0+git8bd9fe3 ```",2022-09-08T06:04:46Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/84685
rag,[reland] Call jit decomposition in VariableType to increase forward AD coverage (#84151),"Stack from ghstack:  CC([reland] Call jit decomposition in VariableType to increase forward AD coverage (84151)) This reverts commit acb4a09628284201281e262aaee58e3dc6be9c2b. In addition, we also fix a memory leak in layer norm.",2022-09-08T00:05:08Z,oncall: jit Merged cla signed Reverted ciflow/trunk release notes: autograd topic: improvements ciflow/periodic,closed,0,9,https://github.com/pytorch/pytorch/issues/84675,"Made an update to address your comments   (1) completely agree with this point, it has been reverted! (2) I'd rather have this in a different PR since the changes required would probably be mostly unrelated to the ones here",> (2) I'd rather have this in a different PR since the changes required would probably be mostly unrelated to the ones here SGTM, merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This causes internal asan linktime failures:  ``` ld.lld: error: undefined symbol: torch::jit::has_jit_decomposition(c10::FunctionSchema const&) >>> referenced by VariableTypeUtilsDependOnOps.h:22 (buckout/gen/aab7ed39/xplat/caffe2/torch_headersAndroidheadermodesymlinktreewithheadermap,headers/torch/csrc/autograd/VariableTypeUtilsDependOnOps.h:22) ``` I'd recommend ghimporting this PR the next time you plan to reland and ensuring the internal signal is green before merging the PR, given that this is the second revert."," revert m ""causing asan xplat linktime errors like ld.lld: error: undefined symbol: torch::jit::has_jit_decomposition(c10::FunctionSchema const&)"" c ""ghfirst""", successfully started a revert job. Check the current status here. Please reach out to the PyTorch DevX Team with feedback or questions!, your PR has been successfully reverted.
yi,Convert ConcretePyInterpreterVTable into Meyer singleton,  CC(Convert ConcretePyInterpreterVTable into Meyer singleton)  CC(Convert NoopPyInterpreterVTable into a Meyer singleton) Signedoffby: Edward Z. Yang ,2022-09-07T19:47:01Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/84657,"Internal is complaining that LeakSanitizer says this code is leaking, so I wanted to see if a Meyer singleton would fix it.", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Convert NoopPyInterpreterVTable into a Meyer singleton,  CC(Convert ConcretePyInterpreterVTable into Meyer singleton)  CC(Convert NoopPyInterpreterVTable into a Meyer singleton) Signedoffby: Edward Z. Yang ,2022-09-07T19:43:58Z,cla signed Reverted,closed,0,3,https://github.com/pytorch/pytorch/issues/84656," revert m ""this breaks some build configs"" c weird", successfully started a revert job. Check the current status here. Please reach out to the PyTorch DevX Team with feedback or questions!, your PR has been successfully reverted.
yi,Add underlying_store property for PrefixStore,Stack from ghstack: * * CC(Add underlying_store property for PrefixStore) Add underlying_store property for PrefixStore**  CC(Add host and port to TCPStore pyi definition) Add host and port to TCPStore pyi definition Add a property to `PrefixStore` to retrieve the underlying store it is wrapping around. Open for suggestions on property name. This change is based on discussion in D39225101 where we need to read properties of the store that PrefixStore is wrapping around. Differential Revision: D39311151,2022-09-07T15:26:15Z,oncall: distributed Merged cla signed release notes: distributed (c10d),closed,0,5,https://github.com/pytorch/pytorch/issues/84640,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84640**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 16d676377f (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Huang has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge," successfully started a merge job. Check the current status here and land check progress here. The merge job was triggered with the land checks (l) flag. If you did not specify this flag yourself,  you are likely enrolled in the land checks rollout. This means that your change will be merged once all checks on your PR and the land checks have passed (**ETA 4 Hours**). If you need to coordinate lands between different changes and cannot risk a land race, please add the `ciflow/trunk` label to your PR and wait for signal to complete, and then land your changes in proper order. Having `trunk`, `pull`, and `Lint` prerun on a PR will bypass land checks and the ETA should be immediate. If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey Huang. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Add host and port to TCPStore pyi definition,"Stack from ghstack:  CC(Add underlying_store property for PrefixStore) Add underlying_store property for PrefixStore * * CC(Add host and port to TCPStore pyi definition) Add host and port to TCPStore pyi definition** `host` and `port` are already exposed in the `TCPStore` pybind definition, this is a small change adding it in the pyi stub Differential Revision: D39311153",2022-09-07T14:39:21Z,oncall: distributed Merged cla signed release notes: distributed (c10d),closed,0,5,https://github.com/pytorch/pytorch/issues/84636,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84636**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 3c7897e4ef (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Huang has imported this pull request.  If you are a Meta employee, you can view this diff on Phabricator.", merge," successfully started a merge job. Check the current status here and land check progress here. The merge job was triggered with the land checks (l) flag. If you did not specify this flag yourself,  you are likely enrolled in the land checks rollout. This means that your change will be merged once all checks on your PR and the land checks have passed (**ETA 4 Hours**). If you need to coordinate lands between different changes and cannot risk a land race, please add the `ciflow/trunk` label to your PR and wait for signal to complete, and then land your changes in proper order. Having `trunk`, `pull`, and `Lint` prerun on a PR will bypass land checks and the ETA should be immediate. If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey Huang. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,"Fix exception handling, improve overheads and avoid constructing storage for element size for DLPack",These changes were proposed by  in CC(Tensor cuda array interface: avoid constructing storage for element size) and CC(DLPack interface: fix excpetion handling and improve overheads) that fix CC(Improve speed/overhead of CUDA array interface) and CC(Handle python exceptions correctly in DLPack Capsule Destructor) respectively.  The reason I am creating the pull request is CLA check (see original PRs).   ,2022-09-07T00:14:05Z,triaged open source Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/84612,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84612**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (17 Pending) As of commit c9040ecef1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"A : we could also fix CC(Do not call nullptr deleter in at::fromDLPack (dlpack)) by checking whether the source tensor has a `deleter` in `fromDLPack`: Changes these lines: https://github.com/pytorch/pytorch/blob/4dfa6d28a139e8325fe9b255af86e8d1360ae7ee/aten/src/ATen/DLConvertor.cppL254L257 To this: ```C++   auto deleter = src {     if (src>deleter) {       // NOLINTNEXTLINE(cppcoreguidelinesprotypeconstcast)       src>deleter(const_cast(src));     }   }; ``` This would make it conform to the DLPack spec which mentions that the deleter maybe `nullptr`. This isn't really related to the other changes here though, so this could be another PR.", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey A. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"[ONNX] when exporting a PyTorch model, creates an unnecessary variable that messes up inferencing."," 🐛 Describe the bug I have a NLP application I've built on PyTorch w/ transformers RoBERTa. As expected, the inferencing should take 2 parameters, input_ids & attention_mask.  However, if I export the said model and view it on netron, it shows 3 inputs: input_ids,attention_mask, and a onnx::Reshape_2 input that shouldn't be there.  My personal prediction is that because my forward pass is implemented as follows: ```python     def forward(self, input_ids, attention_mask, labels=None):        roberta layer       output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)       pooled_output = torch.mean(output.last_hidden_state, 1)        final logits       pooled_output = self.dropout(pooled_output)       pooled_output = self.hidden(pooled_output)       pooled_output = F.relu(pooled_output)       pooled_output = self.dropout(pooled_output)       logits = self.classifier(pooled_output)        calculate loss       loss = 0       if labels is not None:         loss = self.loss_func(logits.view(1, self.config['n_labels']), labels.view(1, self.config['n_labels']))       return loss, logits ``` Onnx converter thinks that labels, is another input that's required for inferencing. But it's not named as ""labels"" aswell, so that is also confusing.  After renaming this onnx:Reshape_2 as ""labels"" , If, one tries to take this model, and run it with an inferencing script in another python environment, we get the following error: The script: ```python import onnx import torch import numpy as np from transformers import AutoTokenizer onnx_model = onnx.load('ekmanv1torch (1).onnx') text = ""Text from the news article"" tokenizer = AutoTokenizer.from_pretrained(""robertabase"") tokens = tokenizer.encode_plus(text,                     add_special_tokens = True,                     return_tensors = 'np',                     truncation = True,                     max_length = 128,                     padding = 'max_length',                     return_attention_mask = True                     ) input_ids = tokens.input_ids attention_mask = tokens.attention_mask labels = np.zeros(6).astype(np.float32) print(labels) input ={'input_ids' : input_ids,         'attention_mask' : attention_mask,         'labels' : labels}  onnx.checker.check_model(onnx_model) import onnxruntime as ort import numpy as np ort_sess = ort.InferenceSession('ekmanv1torch (1).onnx') outputs = ort_sess.run(None, {'input_ids' : input_ids,'attention_mask' : attention_mask,'labels': labels}) ``` Error Message: ```   File ""/Users/MyUser/Library/Python/3.8/lib/python/sitepackages/onnxruntime/capi/onnxruntime_inference_collection.py"", line 200, in run     return self._sess.run(output_names, input_feed, run_options) onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid rank for input: labels Got: 1 Expected: 2 Please fix either the inputs or the model. ``` This error basicly says, that my input of 6 zeros (a dummy version of my 6 classed labels) is invalid, and needs 2 , if I substitute: labels = np.zeros(6).astype(np.float32) with labels = np.zeros((2,6)).astype(np.float32) then the error says ```   File ""/Users/ugurkoc/Library/Python/3.8/lib/python/sitepackages/onnxruntime/capi/onnxruntime_inference_collection.py"", line 200, in run     return self._sess.run(output_names, input_feed, run_options) onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: labels for the following indices  index: 0 Got: 2 Expected: 1 ``` When I inspect the problem with netron, it shows an input right at the end of the computational graph, that seems useless as predicted to the whole graph.   Finally if I were to remove this ""labels"" third input from my model seeing it's useless, then the error message pops up saying: [ CPULongType{} ]) of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer. Thanks in advance.  Versions Collecting environment information... PyTorch version: 1.12.1+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.6 Libc version: glibc2.26 Python version: 3.7.13 (default, Apr 24 2022, 01:04:09)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.188+x86_64withUbuntu18.04bionic Is CUDA available: True CUDA runtime version: 11.1.105 GPU models and configuration: GPU 0: Tesla P100PCIE16GB Nvidia driver version: 460.32.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.0.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] pytorchlightning==1.7.4 [pip3] torch==1.12.1+cu113 [pip3] torchaudio==0.12.1+cu113 [pip3] torchmetrics==0.9.3 [pip3] torchsummary==1.5.1 [pip3] torchtext==0.13.1 [pip3] torchvision==0.13.1+cu113",2022-09-03T12:16:45Z,module: onnx triaged onnx-needs-info,closed,0,4,https://github.com/pytorch/pytorch/issues/84514,"Here is how I create my export aswell: ```python X = next(iter(dummy_dl)) del X[""labels""] print(X) torch.onnx.export(loaded_model,X,""ekmanv1torch.onnx"",export_params = True, do_constant_folding = True, input_names = ['input_ids','attention_mask','labels'],output_names = []) ```","> Here is how I create my export aswell: >  > ```python > X = next(iter(dummy_dl)) > del X[""labels""] >  > print(X) >  > torch.onnx.export(loaded_model,X,""ekmanv1torch.onnx"",export_params = True, do_constant_folding = True, input_names = ['input_ids','attention_mask','labels'],output_names = []) > ``` Hi  , `torch.onnx.export` provides **dynamic_axes** for user to keep their calculation with dynamic shapes. I think that might be one of the reasons you are getting a weird graph. Please find the usage of it in this link: https://pytorch.org/docs/stable/onnx.htmltorch.onnx.export","I would suggest extracting the loss computation part out from your forward function (so that it is not part of your model), as it is not needed for inference and should not be traced during export.","Thank you very much for comments  : the dynamic_axes implementation solved my issue. 👍   : Noted, it was in my TODO list for a while now. 👍 "
transformer,"For PyTorch Nightly, failure when changing MPS device to CPU after PYTORCH_ENABLE_MPS_FALLBACK occurs."," 🐛 Describe the bug When trying to generate text with a GPT2 from the transformers library, I get this error: NotImplementedError: The operator 'aten::cumsum.out' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on  CC(General MPS op coverage tracking issue). As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS. So I activated the environment variable (I set it in the terminal, because it didn't work with the version in the following code), but afterwards another error occurs (I posted it after the code used). I need to mention that if I only use CPU from the start, the generation works without problems. ```python  Sample code to reproduce the problem import torch from transformers import AutoTokenizer, AutoModelForCausalLM import os os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = ""1"" device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') tokenizer = AutoTokenizer.from_pretrained('readerbench/RoGPT2base') model = AutoModelForCausalLM.from_pretrained('readerbench/RoGPT2base').to(device) inputs = tokenizer('Salut priete', return_tensors='pt').to(device) generation = model.generate(inputs['input_ids'], max_length = len(inputs['input_ids'][0]) + 10, no_repeat_ngram_size=2, num_beams=5, early_stopping=True, num_return_sequences=1) ``` ``` The error message you got, with the full traceback. Traceback (most recent call last):   File ""/Users/alexandrudima/home/Research/test.py"", line 15, in      generation = model.generate(inputs['input_ids'], max_length = len(inputs['input_ids'][0]) + 10, no_repeat_ngram_size=2, num_beams=5, early_stopping=True, num_return_sequences=1)[0][len(inputs['input_ids'][0]):]   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/torch/autograd/grad_mode.py"", line 27, in decorate_context     return func(*args, **kwargs)   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/transformers/generation_utils.py"", line 1386, in generate     return self.beam_search(   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/transformers/generation_utils.py"", line 2232, in beam_search     model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/transformers/models/gpt2/modeling_gpt2.py"", line 1016, in prepare_inputs_for_generation     position_ids = attention_mask.long().cumsum(1)  1 NotImplementedError: The operator 'aten::cumsum.out' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on  CC(General MPS op coverage tracking issue). As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS. (ml) alexandrudimaMacBook Research % PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py Special tokens have been added in the vocabulary, make sure the associated word embeddings are finetuned or trained. Special tokens have been added in the vocabulary, make sure the associated word embeddings are finetuned or trained. tensor([[23640,   344,  3205]], device='mps:0') tensor([23640,   344,  3205], device='mps:0') The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results. Setting `pad_token_id` to `eos_token_id`:0 for openend generation. /opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/transformers/models/gpt2/modeling_gpt2.py:1016: UserWarning: The operator 'aten::cumsum.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/condabld/pytorch_1661929883516/work/aten/src/ATen/mps/MPSFallback.mm:11.)   position_ids = attention_mask.long().cumsum(1)  1 Traceback (most recent call last):   File ""/Users/alexandrudima/home/Research/test.py"", line 15, in      generation = model.generate(inputs['input_ids'], max_length = len(inputs['input_ids'][0]) + 10, no_repeat_ngram_size=2, num_beams=5, early_stopping=True, num_return_sequences=1)[0][len(inputs['input_ids'][0]):]   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/torch/autograd/grad_mode.py"", line 27, in decorate_context     return func(*args, **kwargs)   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/transformers/generation_utils.py"", line 1386, in generate     return self.beam_search(   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/transformers/generation_utils.py"", line 2253, in beam_search     next_token_scores_processed = logits_processor(input_ids, next_token_scores)   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/transformers/generation_logits_process.py"", line 92, in __call__     scores = processor(input_ids, scores)   File ""/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/sitepackages/transformers/generation_logits_process.py"", line 333, in __call__     scores[i, banned_tokens] = float(""inf"") RuntimeError: dst_.nbytes() >= dst_byte_offset INTERNAL ASSERT FAILED at ""/Users/runner/work/_temp/anaconda/condabld/pytorch_1661929883516/work/aten/src/ATen/native/mps/operations/Copy.mm"":184, please report a bug to PyTorch.  ```  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220831 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.5.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.9.13  (main, May 27 2022, 17:00:33)  [Clang 13.0.1 ] (64bit runtime) Python platform: macOS12.5.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] pytorchlightning==1.5.8 [pip3] torch==1.13.0.dev20220831 [pip3] torchaudio==0.13.0.dev20220831 [pip3] torchmetrics==0.9.3 [pip3] torchvision==0.14.0.dev20220831 [conda] numpy                     1.22.4           py39h7df2422_0    condaforge [conda] pytorch                   1.13.0.dev20220831         py3.9_0    pytorchnightly [conda] pytorchlightning         1.5.8              pyhd8ed1ab_0    condaforge [conda] torchaudio                0.13.0.dev20220831        py39_cpu    pytorchnightly [conda] torchmetrics              0.9.3              pyhd8ed1ab_0    condaforge [conda] torchvision               0.14.0.dev20220831        py39_cpu    pytorchnightly ",2022-09-02T14:28:54Z,triaged module: mps,open,5,1,https://github.com/pytorch/pytorch/issues/84489,Hotfix doesn't work anymore. !image
yi,OpInfo: Use yield consistently instead of appending to lists,  CC(OpInfo: Add test that sample_inputs_func returns a generator)  CC(OpInfo: Use yield consistently instead of appending to lists)  CC(OpInfo: Sample input cleanup (4/n))  CC(OpInfo: Sample input cleanup (1/n)) The yield syntax reduces the numbers of tensors held inmemory when running opinfo tests and also avoids constructing all samples when only the first sample is going to be used.,2022-09-02T01:56:49Z,open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/84454,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84454**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (33 Pending) As of commit dd6044c6a5 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Incredible effort! It would be good to add a test to make sure that people keep using this pattern rather than the list pattern.,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: peterbell10  (99a49b36bea6cc914ceb61bf65afc8685326e056, e688d5fbdcdab1398757fe7bc72e69e05268fbc3, 06a3543158cf7e8c780f3669effd5fd941664cd5, 828b9bda3124180432a777bf672145e2b2f00744, a168ebc8545bf9bd97e3981945e3f4db2b55129b, 150dd4d387abe604a62ea9a061af13c7c72537e0, 00956ca297d8afe99ea185a0957086c76a81d327, 51115024502f506f0b9324c7bf7391af80613a5b, 9d8240d0a5de79be40913ddd9f965a04c2f092b5, 1a5a7bbd1538b51e0467786a7dfe518f4f628354, 70d3d34cc82649de6dcc1f82b32845358e56f199, 8b0aedb7578fb51b6058be2abf47f2e8c18be97d, dd6044c6a574e10c9c751422ec5c76ae6e59e337, 4510f66ddb2fc9d00d59af318192db43e9a04f33, 239320fd7b7c476f22adf5ea291a5c8eb6ebf1f2, 62bc7e8a3960e376a56555706d27d9c410d79ea8, 77ebeb8a733c2a373e83392d81d5d9973abcba12, f8c989e24962294f8c57b1467747d3da35195d04, 84ae19ca161ca8084e0e69a265397621171d51ec, edcb2f40f998a88d14fe9e2b8fe258e0f97aa034, 8df6fa13fbbc21a66e5cffa5062c2547090fef21, 7ebdcb32cf68347a5fcfbe053e9b06bc7a9c4b85, 3a291ee94a0dfd1377087bfafe52d1605af9a4c3, 745fafa4ca7da8a07cc7c2f4943188f144adf9b0, f5132f1884a7764c2c91412a999cc63f606f18e4, 49c92c6dbc431b0ede43c36ae41438b7f3893f46)"
transformer,[ONNX] Exporting the operator zero to ONNX opset version 9-15 is not supported.," 🐛 Describe the bug When i convert transformer to onnx, throwing the above error. RuntimeError: Exporting the operator zero to ONNX opset version 9 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub.  Model Structure ```python d_model = 256   Embedding Size d_ff = 1024   FeedForward dimension d_k = d_v = 64   dimension of K(=Q), V n_layers = 6   number of Encoder of Decoder Layer n_heads = 8   number of heads in MultiHead Attention   class PositionalEncoding(nn.Module):     def __init__(self, d_model, dropout=0.1, max_len=5000):         super(PositionalEncoding, self).__init__()         self.dropout = nn.Dropout(p=dropout)         pe = torch.zeros(max_len, d_model)         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model))         pe[:, 0::2] = torch.sin(position * div_term)         pe[:, 1::2] = torch.cos(position * div_term)         pe = pe.unsqueeze(0).transpose(0, 1)         self.register_buffer('pe', pe)     def forward(self, x):         '''         x: [seq_len, batch_size, d_model]         '''         x = x + self.pe[:x.size(0), :]         return self.dropout(x) def get_attn_pad_mask(seq_q, seq_k):     '''     seq_q: [batch_size, seq_len]     seq_k: [batch_size, seq_len]     seq_len could be src_len or it could be tgt_len     seq_len in seq_q and seq_len in seq_k maybe not equal     '''     batch_size, len_q = seq_q.size()     batch_size, len_k = seq_k.size()      eq(zero) is PAD token     pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)   [batch_size, 1, len_k], False is masked     return pad_attn_mask.expand(batch_size, len_q, len_k)   [batch_size, len_q, len_k] def get_attn_subsequence_mask(seq):     '''     seq: [batch_size, tgt_len]     '''     attn_shape = [seq.size(0), seq.size(1), seq.size(1)]     subsequence_mask = np.triu(np.ones(attn_shape), k=1)  Upper triangular matrix     subsequence_mask = torch.from_numpy(subsequence_mask).byte()     return subsequence_mask  [batch_size, tgt_len, tgt_len]   class ScaledDotProductAttention(nn.Module):     def __init__(self):         super(ScaledDotProductAttention, self).__init__()     def forward(self, Q, K, V, attn_mask):         '''         Q: [batch_size, n_heads, len_q, d_k]         K: [batch_size, n_heads, len_k, d_k]         V: [batch_size, n_heads, len_v(=len_k), d_v]         attn_mask: [batch_size, n_heads, seq_len, seq_len]         '''         scores = torch.matmul(Q, K.transpose(1, 2)) / np.sqrt(d_k)  scores : [batch_size, n_heads, len_q, len_k]         scores.masked_fill_(attn_mask, 1e9)  Fills elements of self tensor with value where mask is True.         attn = nn.Softmax(dim=1)(scores)         context = torch.matmul(attn, V)  [batch_size, n_heads, len_q, d_v]         return context, attn class MultiHeadAttention(nn.Module):     def __init__(self):         super(MultiHeadAttention, self).__init__()         self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)         self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)         self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)         self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)     def forward(self, input_Q, input_K, input_V, attn_mask):         '''         input_Q: [batch_size, len_q, d_model]         input_K: [batch_size, len_k, d_model]         input_V: [batch_size, len_v(=len_k), d_model]         attn_mask: [batch_size, seq_len, seq_len]         '''         residual, batch_size = input_Q, input_Q.size(0)          (B, S, D) proj> (B, S, D_new) split> (B, S, H, W) trans> (B, H, S, W)         Q = self.W_Q(input_Q).view(batch_size, 1, n_heads, d_k).transpose(1,2)   Q: [batch_size, n_heads, len_q, d_k]         K = self.W_K(input_K).view(batch_size, 1, n_heads, d_k).transpose(1,2)   K: [batch_size, n_heads, len_k, d_k]         V = self.W_V(input_V).view(batch_size, 1, n_heads, d_v).transpose(1,2)   V: [batch_size, n_heads, len_v(=len_k), d_v]         attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)  attn_mask : [batch_size, n_heads, seq_len, seq_len]          context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]         context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)         context = context.transpose(1, 2).reshape(batch_size, 1, n_heads * d_v)  context: [batch_size, len_q, n_heads * d_v]         output = self.fc(context)  [batch_size, len_q, d_model]         return nn.LayerNorm(d_model).to(device)(output + residual), attn class PoswiseFeedForwardNet(nn.Module):     def __init__(self):         super(PoswiseFeedForwardNet, self).__init__()         self.fc = nn.Sequential(             nn.Linear(d_model, d_ff, bias=False),             nn.ReLU(),             nn.Linear(d_ff, d_model, bias=False)         )     def forward(self, inputs):         '''         inputs: [batch_size, seq_len, d_model]         '''         residual = inputs         output = self.fc(inputs)         return nn.LayerNorm(d_model).to(device)(output + residual)  [batch_size, seq_len, d_model] class EncoderLayer(nn.Module):     def __init__(self):         super(EncoderLayer, self).__init__()         self.enc_self_attn = MultiHeadAttention()         self.pos_ffn = PoswiseFeedForwardNet()     def forward(self, enc_inputs, enc_self_attn_mask):         '''         enc_inputs: [batch_size, src_len, d_model]         enc_self_attn_mask: [batch_size, src_len, src_len]         '''          enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]         enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)  enc_inputs to same Q,K,V         enc_outputs = self.pos_ffn(enc_outputs)  enc_outputs: [batch_size, src_len, d_model]         return enc_outputs, attn class DecoderLayer(nn.Module):     def __init__(self):         super(DecoderLayer, self).__init__()         self.dec_self_attn = MultiHeadAttention()         self.dec_enc_attn = MultiHeadAttention()         self.pos_ffn = PoswiseFeedForwardNet()     def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):         '''         dec_inputs: [batch_size, tgt_len, d_model]         enc_outputs: [batch_size, src_len, d_model]         dec_self_attn_mask: [batch_size, tgt_len, tgt_len]         dec_enc_attn_mask: [batch_size, tgt_len, src_len]         '''          dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]         dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)          dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]         dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)         dec_outputs = self.pos_ffn(dec_outputs)  [batch_size, tgt_len, d_model]         return dec_outputs, dec_self_attn, dec_enc_attn class Encoder(nn.Module):     def __init__(self, src_vocab_size):         super(Encoder, self).__init__()         self.src_emb = nn.Embedding(src_vocab_size, d_model)         self.pos_emb = PositionalEncoding(d_model)         self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])     def forward(self, enc_inputs):         '''         enc_inputs: [batch_size, src_len]         '''         enc_outputs = self.src_emb(enc_inputs)  [batch_size, src_len, d_model]         enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1)  [batch_size, src_len, d_model]         enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  [batch_size, src_len, src_len]         enc_self_attns = []         for layer in self.layers:              enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]             enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)             enc_self_attns.append(enc_self_attn)         return enc_outputs, enc_self_attns class Decoder(nn.Module):     def __init__(self, tgt_vocab_size):         super(Decoder, self).__init__()         self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)         self.pos_emb = PositionalEncoding(d_model)         self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])     def forward(self, dec_inputs, enc_inputs, enc_outputs):         '''         dec_inputs: [batch_size, tgt_len]         enc_intpus: [batch_size, src_len]         enc_outputs: [batsh_size, src_len, d_model]         '''         dec_outputs = self.tgt_emb(dec_inputs)  [batch_size, tgt_len, d_model]         dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).to(device)  [batch_size, tgt_len, d_model]         dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(device) [batch_size, tgt_len, tgt_len]         dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(device)  [batch_size, tgt_len, tgt_len]         dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0).to(device)  [batch_size, tgt_len, tgt_len]         dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)  [batc_size, tgt_len, src_len]         dec_self_attns, dec_enc_attns = [], []         for layer in self.layers:              dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]             dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)             dec_self_attns.append(dec_self_attn)             dec_enc_attns.append(dec_enc_attn)         return dec_outputs, dec_self_attns, dec_enc_attns class Transformer(nn.Module):     def __init__(self, src_vocab_size, tgt_vocab_size):         super(Transformer, self).__init__()         self.encoder = Encoder(src_vocab_size).to(device)         self.decoder = Decoder(tgt_vocab_size).to(device)         self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).to(device)     def forward(self, enc_inputs, dec_inputs):         '''         enc_inputs: [batch_size, src_len]         dec_inputs: [batch_size, tgt_len]         '''          tensor to store decoder outputs          outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)          enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]         enc_outputs, enc_self_attns = self.encoder(enc_inputs)          dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]         dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)         dec_logits = self.projection(dec_outputs)  dec_logits: [batch_size, tgt_len, tgt_vocab_size]         return dec_logits.view(1, dec_logits.size(1)), enc_self_attns, dec_self_attns, dec_enc_attns ```  main code ```python num_examples=100     source,target=preprocess(num_examples)     enc_inputs, dec_inputs, dec_outputs, src_vocab_size, tgt_vocab_size, src_vocab, tgt_vocab = make_data(20, source, target)     model = Transformer(src_vocab_size, tgt_vocab_size).to(device)     model.load_state_dict(torch.load(""application/saved/model0.29741358757019043.pt""))     model.eval()     dummy_enc_input = torch.randint(0, 10,(1,20)).to(device)     dummy_enc_input = torch.randint(0, 10,(1,4)).to(device)     torch.onnx.export(model, (dummy_enc_input, dummy_enc_input) ,""application/transformer.onnx"",        input_names=['enc_input','dec_input'],         output_names=['output'],         opset_version=9     ) ```  Versions ``` PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~18.04) 9.4.0 Clang version: Could not collect CMake version: version 3.13.4 Libc version: glibc2.27 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.15.0169genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.5 [pip3] torch==1.11.0 [pip3] torchfile==0.1.0 [pip3] torchvision==0.12.0 [conda] numpy                     1.21.5                     [conda] torch                     1.11.0                     [conda] torchfile                 0.1.0                      [conda] torchvision               0.12.0                     ```",2022-09-01T08:01:16Z,needs reproduction module: onnx triaged onnx-needs-info,closed,0,3,https://github.com/pytorch/pytorch/issues/84406,Would you be able to break it into a simple reproduction? I can't repro with the provided code.,Like  requested. Also please consider using the latest pytorchnightly version when you create your reproducible example.,"We’ve gone ahead and closed this issue because it is **stale**. If you still believe this issue is relevant, please feel free to reopen the issue and we will triage it as necessary. Please specify in a comment any updated information you may have so that we can address it effectively. We encourage you to try the latest pytorchpreview (nightly) version to see if it has resolved the issue."
yi,Refactor PyInterpreter to use normal vtables,"  CC(Refactor PyInterpreter to use normal vtables) I realized that we can deal with the dead vtable problem by... introducing another indirection!  The resulting code is worse (you have to do one more dereference to get to the vtable), but the reduction in boilerplate is, IMO, worth it. I did this refactor because I'm about to add a lot more methods to PyInterpreter to handle expunging SymInt from TensorImpl. Signedoffby: Edward Z. Yang ",2022-09-01T02:16:06Z,Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/84388,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84388**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 2 New Failures As of commit 359101b96f (more details on the Dr. CI page): Expand to see more  * **2/2** failures introduced in this PR   :female_detective: 2 failures *not* recognized by patterns: The following CI failures may be due to changes from the PR    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.",took a look. we're executing Python anyway; what's one more layer of indirection between friends?
yi,ROCm support for test_lazy_init,"Added ROCm support for the test_lazy_init unit test by including a condition on TEST_WITH_ROCM to switch CUDA_VISIBLE_DEVICES with HIP_VISIBLE_DEVICES.  This is needed because HIP_VISIBLE_DEVICES is set when running the singleGPU tests in CI: https://github.com/pytorch/pytorch/blob/a47bc96fb7176d43752d3e376697971d4ba47317/.jenkins/pytorch/test.shL38, but this test sets CUDA_VISIBLE_DEVICES, which takes lower precedence than HIP_VISIBLE_DEVICES on ROCm. **Testing Logs (to show behavior difference)** 12:40:41 Aug 30 11:40:41 CUDA_VISIBLE_DEVICES='0': 0 12:40:41 Aug 30 11:40:41 1 12:40:41 Aug 30 11:40:41 CUDA_VISIBLE_DEVICES='32': 32 12:40:41 Aug 30 11:40:41 1 12:40:41 Aug 30 11:40:41 HIP_VISIBLE_DEVICES='0': 0 12:40:41 Aug 30 11:40:41 1 12:40:41 Aug 30 11:40:41 HIP_VISIBLE_DEVICES='32': 32 12:40:41 Aug 30 11:40:41 0 **Passing UT** Aug 30 17:03:15 test_lazy_init (main.TestCuda) Aug 30 17:03:17 Validate that no CUDA calls are made during import torch call ... ok (2.471s)",2022-08-31T09:32:18Z,module: rocm triaged open source Merged cla signed ciflow/trunk,closed,0,6,https://github.com/pytorch/pytorch/issues/84333,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84333**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 8eaf38788f (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Rebased on viable/strict as suggested to retest CI,PR now passes the CI after rebase  , merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Profiler] Capture storage data pointer,"  CC([Profiler] Compute unique IDs for Tensors)  CC([Profiler] Clean up Tensor representation)  CC([Profiler] Capture storage data pointer) This is approximately a reland of the storage half of https://github.com/pytorch/pytorch/pull/80266 I've directly represented and exposed storage impl rather than using it as a first guess for an ID. (Mostly for testing, which happened to save me as I was initially recording the wrong thing.) Differential Revision: D39136546",2022-08-30T16:05:37Z,cla signed release notes: profiler,closed,0,1,https://github.com/pytorch/pytorch/issues/84276,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84276**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 9 New Failures As of commit c69af93b2e (more details on the Dr. CI page): Expand to see more  * **9/9** failures introduced in this PR   :detective: 9 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/8119092339?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (dynamo, 1, 2, linux.2xlarge) (1/9) **Step:** ""Test"" (full log  diagnosis details)   20220831T17:29:05.5897688Z RuntimeError: test_proxy_tensor failed!  ``` 20220831T17:29:02.3600939Z Generated XML report: testreports/pythonunittest/test_proxy_tensor/TESTTestGenericProxyTensorFake20220831171732.xml 20220831T17:29:02.3621170Z Generated XML report: testreports/pythonunittest/test_proxy_tensor/TESTTestGenericProxyTensorReal20220831171732.xml 20220831T17:29:02.3644352Z Generated XML report: testreports/pythonunittest/test_proxy_tensor/TESTTestGenericProxyTensorSymbolic20220831171732.xml 20220831T17:29:02.5055962Z Generated XML report: testreports/pythonunittest/test_proxy_tensor/TESTTestProxyTensorOpInfoCPU20220831171732.xml 20220831T17:29:02.5062462Z Generated XML report: testreports/pythonunittest/test_proxy_tensor/TESTTestSymbolicTracing20220831171732.xml 20220831T17:29:05.5893018Z Traceback (most recent call last): 20220831T17:29:05.5893318Z   File ""test/run_test.py"", line 1065, in  20220831T17:29:05.5894864Z     main() 20220831T17:29:05.5895052Z   File ""test/run_test.py"", line 1043, in main 20220831T17:29:05.5897265Z     raise RuntimeError(err_message) 20220831T17:29:05.5897688Z RuntimeError: test_proxy_tensor failed! 20220831T17:29:05.8897981Z  20220831T17:29:05.8898241Z real	11m38.668s 20220831T17:29:05.8898539Z user	15m21.228s 20220831T17:29:05.8900016Z sys	0m12.905s 20220831T17:29:05.8934497Z [error]Process completed with exit code 1. 20220831T17:29:05.8973993Z Prepare all required actions 20220831T17:29:05.8974302Z Getting action download info 20220831T17:29:06.0761593Z [group]Run ./.github/actions/getworkflowjobid 20220831T17:29:06.0761822Z with: 20220831T17:29:06.0762220Z   githubtoken: *** ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "
rag,Tensor cuda array interface: avoid constructing storage for element size,Fixes CC(Improve speed/overhead of CUDA array interface) ,2022-08-30T13:47:30Z,open source,closed,0,6,https://github.com/pytorch/pytorch/issues/84271,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84271**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 2cbcf55fb7 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," Thank you for approving the PR. I'm working for NVIDIA and still waiting on them for some guidance on what type of CLA (individual/company) I should accept. This may take some time, sorry for that!","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Matt Joux . The commit (2cbcf55fb7dae00bfa8152a302cba6d01e424e9b) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.",Closing since this was already part of https://github.com/pytorch/pytorch/pull/84612 (easier due to CLA).
llm,[No CI] Demo: Subgraph Rewriter for call_module,  CC([No CI] Demo: Subgraph Rewriter for call_module)  CC(Pretty print stack trace with gm.print_readable()),2022-08-29T23:21:37Z,cla signed not4land fx,closed,0,1,https://github.com/pytorch/pytorch/issues/84233,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84233**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :white_check_mark: 1 Base Failures As of commit e662892e65 (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 7c7a60c405 on Aug 29 from  3:58pm to  6:19pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * Lint / lintrunner on Aug 29 from  3:58pm to  6:19pm (b5cbc56c48  8aba2535e4)  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "
llm,Inductor misaligned address error in PegasusForCausalLM,"Note this adds a CI skip for PegasusForCausalLM.  I don't think the following error is related to this PR: ``` cuda train PegasusForCausalLM                 ERROR:root:unhandled error Traceback (most recent call last):   File ""/home/circleci/project/benchmarks/common.py"", line 1139, in run_one_model     new_result = optimized_model_iter_fn(model, example_inputs)   File ""/home/circleci/project/torchdynamo/eval_frame.py"", line 154, in _fn     return fn(*args, **kwargs)   File ""benchmarks/huggingface.py"", line 448, in forward_and_backward_pass     def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):   File ""benchmarks/huggingface.py"", line 448, in forward_and_backward_pass     def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):   File ""benchmarks/huggingface.py"", line 454, in forward_and_backward_pass     self.grad_scaler.scale(loss).backward()   File ""/home/circleci/project/env/lib/python3.8/sitepackages/functorch/_src/monkey_patching.py"", line 77, in _backward     return _old_backward(*args, **kwargs)   File ""/home/circleci/project/env/lib/python3.8/sitepackages/torch/_tensor.py"", line 484, in backward     torch.autograd.backward(   File ""/home/circleci/project/env/lib/python3.8/sitepackages/torch/autograd/__init__.py"", line 191, in backward     Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass   File ""/home/circleci/project/env/lib/python3.8/sitepackages/torch/autograd/function.py"", line 267, in apply     return user_fn(self, *args)   File ""/home/circleci/project/env/lib/python3.8/sitepackages/functorch/_src/aot_autograd.py"", line 365, in backward     CompiledFunction.compiled_bw = aot_config.bw_compiler(   File ""/home/circleci/project/torchdynamo/optimizations/backends.py"", line 542, in _wrapped_bw_compiler     return torchdynamo.disable(bw_compiler(*args, **kwargs))   File ""/home/circleci/project/torchinductor/compile_fx.py"", line 176, in bw_compiler     return compile_fx_inner(   File ""/home/circleci/project/torchdynamo/debug_utils.py"", line 253, in debug_wrapper     raise e   File ""/home/circleci/project/torchdynamo/debug_utils.py"", line 241, in debug_wrapper     compiled_fn = compiler(gm, example_inputs, **kwargs)   File ""/home/circleci/project/torchinductor/debug.py"", line 182, in inner     return fn(*args, **kwargs)   File ""/home/circleci/project/torchinductor/compile_fx.py"", line 60, in compile_fx_inner     compiled_fn = cudagraphify(   File ""/home/circleci/project/torchinductor/compile_fx.py"", line 102, in cudagraphify     model(*inputs)   File ""/tmp/torchinductor_circleci/2a/c2amnj4bvgezefiyv4h23vy34iycx3kg377uhapp6obobswi5h5p.py"", line 1051, in call     kernel0grid(128)   File ""/home/circleci/project/env/lib/python3.8/sitepackages/triton/code_gen.py"", line 999, in __call__     return self.kernel(*wargs, **kwargs, grid=self.grid)   File ""/home/circleci/project/env/lib/python3.8/sitepackages/triton/code_gen.py"", line 1073, in __call__     timings = {config: self._bench(*args, config=config, **kwargs)   File ""/home/circleci/project/env/lib/python3.8/sitepackages/triton/code_gen.py"", line 1073, in      timings = {config: self._bench(*args, config=config, **kwargs)   File ""/home/circleci/project/torchinductor/triton_ops/autotune.py"", line 28, in _bench     return super()._bench(*args, config=config, **kwargs)   File ""/home/circleci/project/env/lib/python3.8/sitepackages/triton/code_gen.py"", line 1054, in _bench     return triton.testing.do_bench(kernel_call)   File ""/home/circleci/project/env/lib/python3.8/sitepackages/triton/testing.py"", line 138, in do_bench     torch.cuda.synchronize()   File ""/home/circleci/project/env/lib/python3.8/sitepackages/torch/cuda/__init__.py"", line 501, in synchronize     return torch._C._cuda_synchronize() RuntimeError: CUDA error: misaligned address CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. ERROR CUDA error: misaligned address CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. ERROR ``` Likely just uncovered by different fusions happening. _Originally posted by  in https://github.com/pytorch/torchdynamo/issues/1077issuecomment1229762518_ ",2022-08-29T04:39:28Z,oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/93651,PegasusForCausalLM is no longer skipped
transformer,Support setting strides on quantized weights of Embedding," 🚀 The feature, motivation and pitch For quantized Embedding, deberta in transformers will choose weights from original weights, but I get:  `*** RuntimeError: Setting strides is possible only on uniformly or per channel quantized tensors`. Below are the details of quantized weights of Embedding. torch.per_channel_affine_float_qparams should be a per channel quantization schema, right?  tensor([[ 0.0064, 0.0069, 0.0082,  ..., 0.0215, 0.0056, 0.0003],         [0.0069,  0.0087, 0.0225,  ..., 0.0136, 0.0069,  0.0065],         [0.0071,  0.0225, 0.0071,  ..., 0.0182, 0.0219,  0.0151],         ...,         [0.0037,  0.0060, 0.0203,  ..., 0.0176,  0.0088,  0.0102],         [0.0126, 0.0350, 0.0074,  ..., 0.0284,  0.0032, 0.0218],         [ 0.0065, 0.0123,  0.0018,  ..., 0.0170,  0.0065,  0.0018]],        size=(50265, 768), dtype=torch.quint8,        quantization_scheme=torch.per_channel_affine_float_qparams,        scale=tensor([0.0013, 0.0022, 0.0037,  ..., 0.0014, 0.0013, 0.0047]),        zero_point=tensor([ 87.2115,  50.1056,  93.9225,  ...,  77.6577,  81.5797, 115.6229]),        axis=0)  Alternatives No  Additional context _No response_ ",2022-08-28T09:36:52Z,oncall: quantization low priority triaged,open,0,1,https://github.com/pytorch/pytorch/issues/84178,"per_channel_float_qparams is a different qscheme: https://github.com/pytorch/pytorch/blob/main/c10/core/QScheme.hL19, please feel free to add support for it here: https://www.internalfb.com/code/fbsource/[f4da2ffcd586948ec1fcafced11e99a3b6d934a1]/fbcode/caffe2/aten/src/ATen/native/TensorShape.cpp?lines=1228"
transformer,FSDP Forward order differs from that of first run," 🐛 Describe the bug Hi, I am using fsdp(integrated with hf accelerate) to extend support for the transformer reinforcement learning library to multigpu. This requires me to run multiple .generate calls, forward passes, and then a backwards pass.  I am getting a warning: `UserWarning: Forward order differs from that of the first iteration on rank 0  collectives are unchecked and may give incorrect results or hang`. Some insight would be appreciated. Minimum code to reproduce: ``` import torch import transformers from transformers import (     AutoModelForCausalLM,     AutoTokenizer ) from accelerate import Accelerator from accelerate.logging import get_logger from trl.gpt2 import GPT2HeadWithValueModel from torch.optim import Adam from transformers import DataCollatorForLanguageModeling import torch.nn.functional as F def main():     accelerator = Accelerator()     tokenizer = AutoTokenizer.from_pretrained(""gpt2"")     model = GPT2HeadWithValueModel.from_pretrained('gpt2')     ref_model = GPT2HeadWithValueModel.from_pretrained('gpt2')     optimizer = Adam(model.parameters(), lr=1.41e5)     model.config.pad_token_id = model.config.eos_token_id     model = accelerator.prepare(model)     model.to(accelerator.device)     print(accelerator.state)     optimizer = accelerator.prepare(optimizer)     rank = torch.distributed.get_rank()     if rank == 0:         text_in = ""The purpose of life is ""     elif rank == 1:         text_in = ""Are you human? ""     query_tensors = tokenizer(text_in, return_tensors=""pt"").to(accelerator.device)[""input_ids""]      had to run this 1 time at the start else was giving device mismatch error.      So, before directly using `model.generate` pass a batch with dummy data through the model     outputs = model(query_tensors)     print(query_tensors)     gen_kwargs = {         ""max_length"": 64,         ""min_length"": 20,     }     with torch.no_grad():         unwrapped_model = accelerator.unwrap_model(model)          synced_gpus was necessary else resulted into indefinite hang         response_tensors = unwrapped_model.generate(query_tensors, synced_gpus=True, **gen_kwargs)     text_out = tokenizer.decode(response_tensors[0], skip_special_tokens=True)      Arbitrarily score generation     score = torch.tensor([1.0]).to(accelerator.device)     print(f""\nrank{rank}:\n   in={text_in}\n  out={text_out}"")      Now compute ppo loss      First compute logprobs and ref_logprobs     collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)     for i in range(10):         input_ids = collator([torch.cat([q, r]) for q, r in zip(query_tensors, response_tensors)])[""input_ids""]         with torch.no_grad():             logits, _, v = model(input_ids)             print('values', v)             ref_logits, _, _ = ref_model(input_ids.cpu())             ref_logits = ref_logits.to(accelerator.device)         logprobs = logprobs_from_logits(logits[:,:1,:], input_ids[:,1:])         ref_logprobs = logprobs_from_logits(ref_logits[:,:1,:], input_ids[:,1:])          Only care about logprobs for generated text         start = query_tensors.size()[1]  1         end = query_tensors.size()[1] + response_tensors.size()[1]  1         logprobs = logprobs[:, start:end]         ref_logprobs = ref_logprobs[:, start:end]         v = v[:, start1: end1]         print('logprob sizes', logprobs.size(), ref_logprobs.size(), v.size())          Compute rewards         kl = logprobs  ref_logprobs         non_score_reward = .2 * kl         reward = non_score_reward.clone()         reward[1] += score          Compute losses         lastgaelam = 0         advantages_reversed = []         gen_len = response_tensors.shape[1]         for t in reversed(range(gen_len)):             nextvalues = v[:, t+1] if t < gen_len  1 else 0.0             delta = reward[:, t] + 1.00 * nextvalues  v[:, t]             lastgaelam = delta + 1.00 * .99 * lastgaelam             advantages_reversed.append(lastgaelam)         advantages = torch.stack(advantages_reversed[::1]).transpose(0, 1)         returns = advantages + v         advantages = advantages.detach()          With grad this time         logits, _, vpred = model(input_ids)         logprob = logprobs_from_logits(logits[:, :1, :], input_ids[:, 1:])         logprob, vpred = logprob[:, gen_len:], vpred[:, gen_len1:1]         vf_loss = torch.mean((vpred  returns)**2)          Backpropagate         optimizer.zero_grad()         accelerator.backward(vf_loss)         optimizer.step() def logprobs_from_logits(logits, labels):     """"""     See:  CC(CrossEntropyLoss masking)issuecomment330103591     """"""     logp = F.log_softmax(logits, dim=2)     logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(1)     return logpy if __name__ == ""__main__"":     main() ``` A similar issue has already been opened on accelerate's repo with some discussion here  Versions PyTorch version: 1.12.1+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Amazon Linux release 2 (Karoo) (x86_64) GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.115) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.26 Python version: 3.8.5 (default, Feb 18 2021, 01:24:20)  [GCC 7.3.1 20180712 (Red Hat 7.3.112)] (64bit runtime) Python platform: Linux5.10.126117.518.amzn2.x86_64x86_64withglibc2.2.5 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration:  GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB GPU 4: NVIDIA A100SXM440GB GPU 5: NVIDIA A100SXM440GB GPU 6: NVIDIA A100SXM440GB GPU 7: NVIDIA A100SXM440GB Nvidia driver version: 470.129.06 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.2 [pip3] torch==1.12.1+cu113 [pip3] torchaudio==0.12.1+cu113 [pip3] torchvision==0.13.1+cu113 [conda] Could not collect ",2022-08-27T23:34:52Z,oncall: distributed module: fsdp,open,0,7,https://github.com/pytorch/pytorch/issues/84175,"Upon checking, the reason for this warning is when FSDP finds that not same parameter is used by using an allgather to check that all ranks are running ``forward()``. But the code looks legit to me. :  , varma ", ,"I suspect what is happening is that the dummy forward pass via `outputs = model(query_tensors)` is being considered as part of the 1st iteration. Specifically, the 1st iteration includes `outputs = model(query_tensors)`, `logits, _, v = model(input_ids)`, and `logits, _, vpred = model(input_ids)`, while the subsequent iterations only include `logits, _, v = model(input_ids)` and `logits, _, vpred = model(input_ids)`. For FSDP, the end of an iteration is determined by the end of a backward pass via `.backward()` in this case triggered by `accelerator.backward(vf_loss)`. If your model can train without hanging or erroring, then you can simply ignore the warning.","Interestingly the dummy forward pass is required as a hacky fix to another error thrown if this is not done: here.  I believe I encounter this warning even if all iterations follow the first iteration's format, but I need to verify this.",Unfortunately training does not proceed properly. :(. Reward is not increasing., Would you be able to follow up here?,>  Would you be able to follow up here? My concern is that debugging this requires peeking into the HF accelerate code and requires setting up the appropriate libraries. It is most likely not an FSDPspecific issue but rather an integration issue somewhere.
rag,Create codeql.yml to leverage github Code Scanning,Enables Code Scanning on Python and JS. It detects bugs like https://github.com/pytorch/pytorch/blob/1dabb51a16eb6cf81475efecb1d39c4683af50fb/benchmarks/distributed/rpc/rl/launcher.pyL99 and others. /pytorchdevinfra,2022-08-27T22:41:28Z,module: ci triaged open source cla signed Stale topic: not user facing,closed,0,15,https://github.com/pytorch/pytorch/issues/84174,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84174**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 2 New Failures As of commit ba78edc868 (more details on the Dr. CI page): Expand to see more  * **2/2** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/8053531809?check_suite_focus=true) Lint / lintrunner (1/2) **Step:** ""Run lintrunner on all files"" (full log  : 20220827T22:43:51.1593993Z [command]/usr/bin/git config local nameonly getregexp http\.https\:\/\/github\.com\/\.extraheader ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",Thank you for adding this! You can do a rebase to fix those build failures due to a missing CI action.,"As expected, there are too many errors (or noise depending on how people view them) https://github.com/pytorch/pytorch/pull/84174/checks?check_run_id=8278980270 and a failed codeql scan will block the PR in its current state: * Do you know if it's possible to tune the scan to run only on the content of the PR and commit during pull_request and push events instead of scanning the whole repo? * I wonder if it's possible to config the scanner to report only high severity issue and above to make it less noisy * Let's not run this in every pull request until it's proven to be useful there","> * Do you know if it's possible to tune the scan to run only on the content of the PR and commit during pull_request and push events instead of scanning the whole repo? Yes! I think this is the default behavior. This PR reported all errors because it's the first scan. So everything is ""new"". Afterwards only new alerts should be reported in future PRs.  > * I wonder if it's possible to config the scanner to report only high severity issue and above to make it less noisy Yes to this too. Any query can be turned on or off.  https://docs.github.com/en/codesecurity/codescanning/automaticallyscanningyourcodeforvulnerabilitiesanderrors/configuringcodescanningexcludingspecificqueriesfromanalysis Something like  ``` queryfilters:  exclude:     problem.severity:        warning        recommendation ``` Can also be used to turn off alerts based on severity. I find some low severity queries helpful for finding bugs and higher severity ones noisy. So selecting individual may be helpful.  > * Let's not run this in every pull request until it's proven to be useful there We can also configure paths for it to run on https://docs.github.com/en/codesecurity/codescanning/automaticallyscanningyourcodeforvulnerabilitiesanderrors/configuringcodescanningavoidingunnecessaryscansofpullrequests", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `justinchu/codeql` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `git checkout justinchu/codeql && git pull rebase`)","> Yes! I think this is the default behavior. This PR reported all errors because it's the first scan. So everything is ""new"". Afterwards only new alerts should be reported. ~~Oh nice, it's good to know. Now the check is green~~ or is it? ","Hmm, should this be part of a lint workflow?","> > Yes! I think this is the default behavior. This PR reported all errors because it's the first scan. So everything is ""new"". Afterwards only new alerts should be reported. >  > ~Oh nice, it's good to know. Now the check is green~ or is it? It will only be show added alerts in other PRs after this ""base"" is merged. This PR will always contain all errors","> Hmm, should this be part of a lint workflow? That would be great. I haven't looked into how that is possible","> > Hmm, should this be part of a lint workflow? >  > That would be great. I haven't looked into how that is possible It looks like a reusable workflow would work here https://docs.github.com/en/actions/usingworkflows/reusingworkflowscreatingareusableworkflow having `lint.yml` calling `codeql.yml`","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Justin Chu . The commit (2772b4aef41c60ec0c7d53b45b814c9a6c57f867, 2111eba28aebc1c043488922ca7462ae4c13b10b, 914be8296efbdd0d4246c3489282df714cfbc234) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,mps: weird results given by Transformer CausalLM," 🐛 Describe the bug ``` python %env PYTORCH_ENABLE_MPS_FALLBACK=1 import transformers from transformers import AutoModelForCausalLM, AutoTokenizer import torch assert torch.backends.mps.is_built() assert torch.backends.mps.is_available() pretrain_model = ""bigscience/bloom560m"" device= ""mps"" tokenizer = AutoTokenizer.from_pretrained(pretrain_model) model = AutoModelForCausalLM.from_pretrained(pretrain_model).to(device) generator = transformers.pipeline(task=""textgeneration"", model=model, tokenizer=tokenizer, device=torch.device(device)) r = generator(""This is a conversation between A and B. \nA: Your should say something meaningful.\nB:"", max_length=50, use_cache=True) print(r[0]['generated_text']) ``` This should give something like: > using `device= cpu""` actually results in this: ``` This is a conversation between A and B.  A: Your should say something meaningful. B: I don't know what you mean. But I think you should say something meaningful. ``` However it actually gives: ``` This is a conversation between A and B.  A: Your should say something meaningful. B: is is is isThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThisThis ``` Same behavior not only applies to `bigscience/bloom560m`, all CausalLM seems to results in similar behavior.  Just FYI To get rid of warning and rule out weird behavior from MPS fallback, I added some code to Transformer source `sitepackages/transformers/models/bloom/modeling_bloom.py` This does not effect the reported bug behavior. ```python if attention_mask.shape[0] == 1 and not torch.any(attention_mask 1):   position_ids = torch.arange(attention_mask.shape[1], dtype=torch.long,                                device=attention_mask.device).expand(attention_mask.shape) else:   print(attention_mask)   position_ids = attention_mask.cumsum(1) ```  Versions ``` torch                   1.13.0.dev20220827 transformers            4.21.2 ``` ",2022-08-27T18:14:18Z,high priority triage review triaged module: mps,closed,0,9,https://github.com/pytorch/pytorch/issues/84169,"I had the same issue running `neox` on the M1. https://github.com/zphang/minimalgptneox20b/issues/5 With `mps`, I got ""...developed by EleutherAI. in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in"" and with cpu on the same machine, I got ""... developed by EleutherAI. It is a stateoftheart language model..."". That implementation doesn't use `transformers` at all, it's just plain pytorch. I tested on `1.13.0.dev20220803`.","I have the same issue with galactica6.7b used with huggingface's `transformers`. This is a minimal example to reproduce: ``` from transformers import AutoTokenizer, OPTForCausalLM tokenizer = AutoTokenizer.from_pretrained(""facebook/galactica6.7b"") model = OPTForCausalLM.from_pretrained(""facebook/galactica6.7b"").to(""mps"") input_text = ""The Transformer architecture [START_REF]"" input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""mps"") outputs = model.generate(input_ids, max_new_tokens = 20) print(tokenizer.decode(outputs[0])) ``` output with mps: ``` The Transformer architecture [START_REF]      results results results results results results results results results results results results results results results ________________________________________________________ Executed in   40.03 secs    fish           external    usr time   25.76 secs    0.17 millis   25.76 secs    sys time   24.76 secs    2.54 millis   24.76 secs ``` output when removing `to(""mps"")` (running in cpu mode): ``` The Transformer architecture [START_REF] Attention is All you Need, Vaswani[END_REF] is a sequencetosequence model that uses self ________________________________________________________ Executed in   56.61 secs    fish           external    usr time   42.90 secs    0.17 millis   42.90 secs    sys time   27.70 secs    2.38 millis   27.70 secs ``` torch version: `1.14.0.dev20221117` I'm using a Apple M1 Max Macbook with MacOS 13.0","Setting `use_cache` to `False` fixed it for me. ,e.g. ```python outputs = model.generate(input_ids=input_ids, do_sample=False, use_cache=False, max_new_tokens=max_length) ```",What can be the cause of this? I have the same problem in a different pacakage but my model does not have a genrate function with use_cache,> What can be the cause of this? I have the same problem in a different pacakage but my model does not have a genrate function with use_cache Is this still happening with latest nightly ?,"Hey, yes i just tested it. It is still happening wit pytorch2.1.0.dev202 (installed today). The quality of the output is way worse when unsing ""mps"" compares to ""cpu"" on mac","Thanks   and Zhang, we will investigate the issue. ",Zhang thanks for filling this issue. Could you please try latest nightly? This should be fixed there: pip3 install pre forcereinstall torch indexurl https://download.pytorch.org/whl/nightly/cpu ``` This is a conversation between A and B. A: Your should say something meaningful. B: I don't know what you mean. But I think you should say something meaningful. ```,I can confirm the problem is gone with `torch2.1.0.dev20230804` on macOS 13.5 (22G74).  > Zhang thanks for filling this issue. Could you please try latest nightly? This should be fixed there: pip3 install pre forcereinstall torch indexurl https://download.pytorch.org/whl/nightly/cpu >  > ``` > This is a conversation between A and B. > A: Your should say something meaningful. > B: I don't know what you mean. But I think you should say something meaningful. > ```
rag,Call jit decomposition in VariableType to increase forward AD coverage,"Stack from ghstack:  CC(Call jit decomposition in VariableType to increase forward AD coverage)  CC([reland] Move decompositions and helpers for jvp from functorch into core) This PR:  updates forward AD codegen in core to generate code that tries calling into decompositions registered to jit when     (1) the function is not inplace or out variant     AND (2) the function is differentiable (requires_derivative=True)     AND (3) there are no forward AD formulas registered     To simplify things we always generating the if/else (as long as (1) is true), but generate 'false' when either (2) or (3) are false.   removes the mechanism from functorch      (follow up) some functorch tests should be updated here so they no longer have to compute the Jacobian with vjp    factors out some logic to generate the any_has_forward_grad condition       (bcbreaking) when TensorList inputs unexpectedly have forward grad, the error will no longer contain the name See https://github.com/pytorch/pytorch/pull/84151issuecomment1238519247 for codegen output and more discussion.",2022-08-26T21:54:41Z,oncall: jit Merged cla signed Reverted release notes: autograd,closed,0,9,https://github.com/pytorch/pytorch/issues/84151,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84151**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit f234e34dc7 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"For your codegen output: `const auto& opt_op = c10::Dispatcher::singleton().findSchema(full_name);`  This looks wrong to me  `findSchema` returns a value, not a reference, unless this is some c++ism.  findSchema is expensive, so we could cache the value by assigning it to a static variable: `static auto opt_op = ...` ```cpp   auto _tmp = ([&]() {     if (true && (isFwGradDefined(grad_out)  isFwGradDefined(save_invstd))) {       c10::OperatorName full_name(""aten::native_batch_norm_backward"", """");       const auto& opt_op = c10::Dispatcher::singleton().findSchema(full_name);       TORCH_CHECK(opt_op.has_value());       return impl::run_jit_decomposition_with_args_for_jvp, const at::Tensor &, const at::Tensor &, const c10::optional &, const c10::optional &, const c10::optional &, const c10::optional &, const c10::optional &, bool, double, ::std::array>(""native_batch_norm_backward"", *opt_op, ks, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);     } else {       at::AutoDispatchBelowADInplaceOrView guard;       return at::redispatch::native_batch_norm_backward(ks & c10::after_autograd_keyset, grad_out_, input_, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);     }   })(); ```","Here is an entire codegened function in VariableType:   Click to show  ```cpp ::std::tuple native_batch_norm_backward(c10::DispatchKeySet ks, const at::Tensor & grad_out, const at::Tensor & input, const c10::optional & weight, const c10::optional & running_mean, const c10::optional & running_var, const c10::optional & save_mean, const c10::optional & save_invstd, bool train, double eps, ::std::array output_mask) {   auto& grad_out_ = unpack(grad_out, ""grad_out"", 0);   auto& input_ = unpack(input, ""input"", 1);   auto _any_requires_grad = compute_requires_grad( grad_out, input, weight, save_mean, save_invstd );   (void)_any_requires_grad;   check_no_requires_grad(running_mean, ""running_mean"", ""native_batch_norm_backward"");   check_no_requires_grad(running_var, ""running_var"", ""native_batch_norm_backward"");   std::shared_ptr grad_fn;   if (_any_requires_grad) {     grad_fn = std::shared_ptr(new NativeBatchNormBackwardBackward0(), deleteNode);     grad_fn>set_next_edges(collect_next_edges( grad_out, input, weight, save_mean, save_invstd ));     grad_fn>grad_out_ = SavedVariable(grad_out, false);     grad_fn>input_ = SavedVariable(input, false);     grad_fn>weight_ = SavedVariable(weight, false);     grad_fn>running_mean_ = SavedVariable(running_mean, false);     grad_fn>running_var_ = SavedVariable(running_var, false);     grad_fn>save_mean_ = SavedVariable(save_mean, false);     grad_fn>save_invstd_ = SavedVariable(save_invstd, false);     grad_fn>train = train;     grad_fn>eps = eps;   }   at::Tensor result0;   at::Tensor result1;   at::Tensor result2;   ifndef NDEBUG   c10::optional grad_out__storage_saved =     grad_out_.has_storage() ? c10::optional(grad_out_.storage()) : c10::nullopt;   c10::intrusive_ptr grad_out__impl_saved;   if (grad_out_.defined()) grad_out__impl_saved = grad_out_.getIntrusivePtr();   c10::optional input__storage_saved =     input_.has_storage() ? c10::optional(input_.storage()) : c10::nullopt;   c10::intrusive_ptr input__impl_saved;   if (input_.defined()) input__impl_saved = input_.getIntrusivePtr();   endif   auto _tmp = ([&]() {     if (true && (isFwGradDefined(grad_out)  isFwGradDefined(save_invstd))) {       static c10::OperatorName full_name(""aten::native_batch_norm_backward"", """");       static c10::optional opt_op = c10::Dispatcher::singleton().findSchema(full_name);       return impl::run_jit_decomposition_with_args_for_jvp, const at::Tensor &, const at::Tensor &, const c10::optional &, const c10::optional &, const c10::optional &, const c10::optional &, const c10::optional &, bool, double, ::std::array>(""native_batch_norm_backward"", *opt_op, ks, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);     } else {       at::AutoDispatchBelowADInplaceOrView guard;       return at::redispatch::native_batch_norm_backward(ks & c10::after_autograd_keyset, grad_out_, input_, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);     }   })();   std::tie(result0, result1, result2) = std::move(_tmp);   ifndef NDEBUG   if (grad_out__storage_saved.has_value() &&       !at::impl::dispatch_mode_enabled() &&       !at::impl::tensor_has_dispatch(grad_out_))     AT_ASSERT(grad_out__storage_saved.value().is_alias_of(grad_out_.storage()));   if (grad_out__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(grad_out_))     AT_ASSERT(grad_out__impl_saved == grad_out_.getIntrusivePtr());   if (input__storage_saved.has_value() &&       !at::impl::dispatch_mode_enabled() &&       !at::impl::tensor_has_dispatch(input_))     AT_ASSERT(input__storage_saved.value().is_alias_of(input_.storage()));   if (input__impl_saved && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(input_))     AT_ASSERT(input__impl_saved == input_.getIntrusivePtr());   if (result0.has_storage() && !at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0)) {     AT_ASSERT(result0.storage().use_count() == 1, ""function: native_batch_norm_backward"");   }   if (!at::impl::dispatch_mode_enabled() && !at::impl::tensor_has_dispatch(result0))     AT_ASSERT(result0.use_count()  One thing to note is that the backward graph will be created for both the nondecomposed and decomposed levels, but this is probably OK because the graph corresponding to the decomposed operations should be destroyed as soon as we call set_history at the higher level. An alternative to this is to actually modify codegen to no longer create the graph at this higher level when we know we are decomposition is met, but that requires more changes and increases complexity of the codegen and is probably slower because we would no longer be utilizing preexisting backward kernels. See https://docs.google.com/document/d/1k0RLf1GrGNYLglPf9sonjjoP_wghlfFbsDxca7ImQ/editheading=h.bcl5pn5tsxf9 for discussion of other alternatives.", merge g," successfully started a merge job. Check the current status here. The merge job was triggered with the green (g) flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""Regressed test_jvpvjp_nn_functional_layer_norm_cuda_float32, see https://hud.pytorch.org/pytorch/pytorch/commit/42d99e6f196233627a28b8e9efb26a0a166fa370"" c landrace", successfully started a revert job. Check the current status here. Please reach out to the PyTorch DevX Team with feedback or questions!, your PR has been successfully reverted.
agent,DISABLED test_dynamic_rpc_existing_rank_can_communicate_with_new_rank (__main__.TensorPipeTensorPipeAgentRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 failures and 1 successes. **Debugging instructions (after clicking on the recent samples link):** DO NOT BE ALARMED IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs. To find relevant log snippets: 1. Click on the workflow logs linked above 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work. 3. Grep for `test_dynamic_rpc_existing_rank_can_communicate_with_new_rank` 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs. ",2022-08-26T12:52:23Z,oncall: distributed module: flaky-tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/84116,I can repo it locally looks like calling RPC to call the method fails here..
agent,Add watchdog to TorchElastic agent and trainers,"Summary: D38604238 (https://github.com/pytorch/pytorch/commit/3b11b80fc3f9f9a0171abb5eb2299835feba8b04) introduced a named pipe based watchdog timer. This diff uses the named pipe based watchdog timer in TorchElastic agent and training worker processes (in the StuckJobDetector class) to allow the TorchElastic agent to detect the stuck of a training process, and kill the process to create a core dump. Test Plan: ``` buck test mode/devnosan //caffe2/test/distributed/elastic/agent/server/test:local_agent_test ``` ``` RemoteExecution session id: reSessionID0bfcacef24d142bca1d3f3058fc42b2ftpx Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/7318349503394739     ✓ ListingSuccess: caffe2/test/distributed/elastic/agent/server/test:local_agent_test : 55 tests discovered (22.699)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_barrier_failed_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (47.140)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_distributed_sum_homogeneous_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (49.198)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_happy_function_c10d (local_elastic_agent_test.LocalElasticAgentTest) (46.387)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_happy_function_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (46.094)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_bipolar_function_etcd (local_elastic_agent_test.LocalElasticAgentTest) (106.342)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_correct_rank_assignment_homogeneous_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (64.888)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_correct_rank_assignment_homogeneous_etcd (local_elastic_agent_test.LocalElasticAgentTest) (69.158)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_agent_local_watchdog_setup_enabled_etcd (local_elastic_agent_test.LocalElasticAgentTest) (46.965)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_double_agent_elastic_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (79.626)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_function_with_return_value_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (46.113)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_sad_function_etcd (local_elastic_agent_test.LocalElasticAgentTest) (46.487)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_shutdown_called_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (24.358)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_torch_rpc_c10d (local_elastic_agent_test.LocalElasticAgentTest) (48.216)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_distributed_sum_homogeneous_c10d (local_elastic_agent_test.LocalElasticAgentTest) (48.433)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_torch_rpc_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (47.029)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_simple_dist_sum_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (44.357)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_check_master_addr_port_override_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (45.176)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_check_nccl_async_error_handling_env_default_c10d (local_elastic_agent_test.LocalElasticAgentTest) (45.980)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_simple_dist_sum_c10d (local_elastic_agent_test.LocalElasticAgentTest) (47.151)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_simple_dist_sum_etcd (local_elastic_agent_test.LocalElasticAgentTest) (44.614)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_correct_rank_assignment_heterogeneous_etcd (local_elastic_agent_test.LocalElasticAgentTest) (69.099)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_agent_local_watchdog_setup_enabled_c10d (local_elastic_agent_test.LocalElasticAgentTest) (45.367)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_shutdown_called_etcd (local_elastic_agent_test.LocalElasticAgentTest) (22.804)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_double_agent_elastic_c10d (local_elastic_agent_test.LocalElasticAgentTest) (77.560)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_dummy_compute_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (46.050)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_distributed_sum_heterogeneous_c10d (local_elastic_agent_test.LocalElasticAgentTest) (48.088)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_double_agent_elastic_etcd (local_elastic_agent_test.LocalElasticAgentTest) (77.286)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_double_agent_fault_tolerance_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (50.670)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_check_master_addr_port_override_etcd (local_elastic_agent_test.LocalElasticAgentTest) (45.631)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_distributed_sum_heterogeneous_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (50.867)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_double_agent_fault_tolerance_etcd (local_elastic_agent_test.LocalElasticAgentTest) (51.095)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_happy_function_etcd (local_elastic_agent_test.LocalElasticAgentTest) (45.000)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_sad_function_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (45.197)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_distributed_sum_homogeneous_etcd (local_elastic_agent_test.LocalElasticAgentTest) (46.873)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_shutdown_called_c10d (local_elastic_agent_test.LocalElasticAgentTest) (23.160)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_barrier_failed_etcd (local_elastic_agent_test.LocalElasticAgentTest) (43.632)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_torch_rpc_etcd (local_elastic_agent_test.LocalElasticAgentTest) (44.536)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_bipolar_function_c10d (local_elastic_agent_test.LocalElasticAgentTest) (89.859)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_workers_drift_fail_etcd (local_elastic_agent_test.LocalElasticAgentTest) (48.277)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_check_nccl_async_error_handling_env_c10d (local_elastic_agent_test.LocalElasticAgentTest) (43.930)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_bipolar_function_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (87.677)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_workers_drift_success_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (48.965)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_workers_drift_fail_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (50.143)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_workers_drift_success_etcd (local_elastic_agent_test.LocalElasticAgentTest) (46.781)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_function_with_return_value_etcd (local_elastic_agent_test.LocalElasticAgentTest) (45.152)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_barrier_failed_c10d (local_elastic_agent_test.LocalElasticAgentTest) (44.832)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_function_with_return_value_c10d (local_elastic_agent_test.LocalElasticAgentTest) (45.281)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_correct_rank_assignment_heterogeneous_etcd_v2 (local_elastic_agent_test.LocalElasticAgentTest) (74.968)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_agent_local_watchdog_setup_disabled_c10d (local_elastic_agent_test.LocalElasticAgentTest) (46.141)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_dummy_compute_c10d (local_elastic_agent_test.LocalElasticAgentTest) (44.960)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_dummy_compute_etcd (local_elastic_agent_test.LocalElasticAgentTest) (45.292)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_agent_local_watchdog_setup_disabled_etcd (local_elastic_agent_test.LocalElasticAgentTest) (44.611)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_check_env_function_etcd (local_elastic_agent_test.LocalElasticAgentTest) (44.939)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_distributed_sum_heterogeneous_etcd (local_elastic_agent_test.LocalElasticAgentTest) (47.609)     ✓ Pass: caffe2/test/distributed/elastic/agent/server/test:local_agent_test  test_run_sad_function_c10d (local_elastic_agent_test.LocalElasticAgentTest) (45.628) Summary   Pass: 55   ListingSuccess: 1 Finished test run: https://www.internalfb.com/intern/testinfra/testrun/7318349503394739 ```  ``` buck test caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test ``` ``` RemoteExecution session id: reSessionID607a002840954dfcb65755f0807fe621tpx Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/8162774432794818     ✓ ListingSuccess: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test : 11 tests discovered (39.037)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_thrift_api_called (caffe2.torch.fb.trainer.stuck_detection.tests.collect_quickstack_test.CollectQuickstackTrace) (0.655)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_setup_local_watchdog (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (36.510)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_dont_print_when_job_normal (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (36.727)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_send_watchdog_request_on_batch_callbacks_no_server (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (37.060)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_quickstack_stuck_job (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (37.242)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_setup_local_watchdog_disabled (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (37.243)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_print_stack_trace_when_job_stuck (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (37.590)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_print_when_stuck (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (37.590)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_setup_local_watchdog_no_file (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (37.589)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_signposts_stack_trace_when_job_stuck (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (38.132)     ✓ Pass: caffe2/torch/fb/trainer/stuck_detection/tests:stuck_job_detector_test  test_send_watchdog_request_on_batch_callbacks (caffe2.torch.fb.trainer.stuck_detection.tests.stuck_job_detector_test.StuckJobDetectorTest) (38.133) Summary   Pass: 11   ListingSuccess: 1 Finished test run: https://www.internalfb.com/intern/testinfra/testrun/8162774432794818 ``` Differential Revision: D38930476",2022-08-25T20:16:10Z,oncall: distributed fb-exported Merged cla signed release notes: distributed (pipeline) topic: new features,closed,0,10,https://github.com/pytorch/pytorch/issues/84081,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84081**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b6a7e6b3bc (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476,This pull request was **exported** from Phabricator. Differential Revision: D38930476, merge (Initiating merge automatically since Phabricator Diff has merged)," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,sync AveragedModel buffers when use_buffers=False,"Fixes CC(Buffers in AveragedModel are not synchronized with the source model when use_buffers=False) As described in the issue, the AveragedModel will deep copy the model during initialization, which means that the buffers in the averaged model cannot be updated together with the model. One solution is to make the buffers equal to the source model every time when calling `update_parameters`.",2022-08-25T13:13:54Z,triaged open source Merged cla signed,closed,0,16,https://github.com/pytorch/pytorch/issues/84054,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84054**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (18 Pending) As of commit 8dea46b1f4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.",The committers listed above are authorized under a signed CLA.:white_check_mark: login: RangiLyu  (8dea46b1f4c8a23bf0d0b9e6de0368fdef5e0c57),"  Hi, sorry for bothering. Is there a plan to merge this PR?","Hi sorrysince it's approved, feel free to comment with ` merge` and it will merge it for you", merge, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here , Merge failed **Reason**: This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. You can rebase by leaving the following comment on this PR: ` rebase` Details for Dev Infra team Raised by workflow job , rebase, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `fix_avgmodel_buffer` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout fix_avgmodel_buffer && git pull rebase`)", merge,Just set up the job so I didn't forget. Sorry about that  and thanks for the PR, Merge started Your change will be merged once all checks pass (ETA 04 Hours). Learn more about merging in the wiki. Questions? Feedback? Please reach out to the PyTorch DevX TeamAdvanced Debugging Check the merge workflow status  here ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Buffers in AveragedModel are not synchronized with the source model when use_buffers=False," 🐛 Describe the bug The AveragedModel will deep copy the model during initialization, which means that the buffers in the averaged model cannot be updated together with the model.  This can easily lead to bugs when using AveragedModel if the model has some buffers that are updated during training.  Here is an example: ```python import torch import torch.nn as nn import torch.nn.functional as F from torchvision import datasets, transforms from torch.utils.data import DataLoader class ToyModel(nn.Module):     def __init__(self):         super().__init__()         self.mlp = nn.Sequential(             nn.Linear(28 * 28, 128),             nn.ReLU(),             nn.Linear(128, 128),             nn.ReLU(),             nn.Linear(128, 10))         self.register_buffer('input_mean', torch.tensor(0.))     def forward(self, x):         self.input_mean = 0.9 * self.input_mean + 0.1 * x.mean()         return self.mlp(x.flatten(1) / self.input_mean) model = ToyModel() ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\         0.05 * averaged_model_parameter + 0.95 * model_parameter ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg) optimzier = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimzier, milestones=[2]) train_dataset = datasets.MNIST(root=""MNIST"", download=True, train=True, transform=transforms.ToTensor()) train_dataloader = DataLoader(dataset=train_dataset, batch_size=100, num_workers=2) for epoch in range(3):     for input, target in train_dataloader:         x = model(input)         loss = F.cross_entropy(x, target)         optimzier.zero_grad()         loss.backward()         optimzier.step()         ema_model.update_parameters(model) print(ema_model.module.input_mean) print(model.input_mean) ``` The result would be: ``` >> tensor(0.) >> tensor(0.1336) ``` This leads to the completely wrong inference results of the ema model.  Suggestion Synchronize the averaged model's buffer with the source model when calling `update_parameters` if `use_buffers=False`. I created a simple PR to solve this:  CC(sync AveragedModel buffers when use_buffers=False)  Versions PyTorch version: 1.12.1 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Manjaro Linux (x86_64) GCC version: (GCC) 11.3.0 Clang version: 14.0.6 CMake version: version 3.23.3 Libc version: glibc2.36 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.10.1361MANJAROx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 11.7.99 GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 SUPER Nvidia driver version: 515.65.01 cuDNN version: Probably one of the following: /usr/lib/libcudnn.so.8.4.1 /usr/lib/libcudnn_adv_infer.so.8.4.1 /usr/lib/libcudnn_adv_train.so.8.4.1 /usr/lib/libcudnn_cnn_infer.so.8.4.1 /usr/lib/libcudnn_cnn_train.so.8.4.1 /usr/lib/libcudnn_ops_infer.so.8.4.1 /usr/lib/libcudnn_ops_train.so.8.4.1 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.1 [pip3] torch==1.12.1 [pip3] torchaudio==0.12.1 [pip3] torchvision==0.13.1 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.6.0              hecad31d_10    condaforge [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0            py38h95df7f1_0    condaforge [conda] mkl_fft                   1.3.1            py38h8666266_1    condaforge [conda] mkl_random                1.2.2            py38h1abd341_0    condaforge [conda] numpy                     1.23.1           py38h6c91a56_0   [conda] numpybase                1.23.1           py38ha15fc14_0   [conda] pytorch                   1.12.1          py3.8_cuda11.6_cudnn8.3.2_0    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torchaudio                0.12.1               py38_cu116    pytorch [conda] torchvision               0.13.1               py38_cu116    pytorch ",2022-08-25T13:10:29Z,module: optimizer triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/84053
rag,"""RuntimeError: src_total_size >= storage_byte_offset INTERNAL ASSERT FAILED"" when training using MPS on YOLOv5"," 🐛 Describe the bug I was training on the latest version of YOLOv5 by the latest PyTorch nightly using MPS acceleration on an Apple Silicon based MacBook, but after just ONE successful epoch, I got this: RuntimeError: src_total_size >= storage_byte_offset INTERNAL ASSERT FAILED at ""/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Copy.mm"":129, please report a bug to PyTorch. So I am reporting a bug here. Note: the command I used to train the network is an example command that comes with the YOLOv5 train.py, I just added device mps parameter, so I think the problem is not the way I train.  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220824 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.5.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS12.5.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.2 [pip3] torch==1.13.0.dev20220824 [pip3] torchaudio==0.13.0.dev20220824 [pip3] torchvision==0.14.0.dev20220824 [conda] Could not collect ",2022-08-25T07:24:01Z,triaged module: mps,closed,0,2,https://github.com/pytorch/pytorch/issues/84042,Hi Maxwell  thank you for the report! This issue is fixed in the nightly build  could you please try the latest PyTorch nightly ( torch1.13.0.dev20220930) and let me know if you still see any issues? Thanks!,"Yes, I tried this morning (UTC 2:47) and this issue is fixed. Thank you for your kind support and fix. Wish you a good day!"
yi,[WIP] symintifying reshape ,Fixes ISSUE_NUMBER ,2022-08-24T17:34:47Z,oncall: jit open source cla signed Stale release notes: jit,closed,0,6,https://github.com/pytorch/pytorch/issues/84000,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/84000**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 30 New Failures, 1 Base Failures, 1 Pending As of commit d5e6399226 (more details on the Dr. CI page): Expand to see more  * **30/31** failures introduced in this PR * **1/31** broken upstream at merge base 50ae5c9141 on Sep 07 from  8:37am to  9:19am   :detective: 13 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/8233920729?check_suite_focus=true) pull / linuxfocalpy3.7clang7asan / test (default, 5, 5, linux.2xlarge) (1/13) **Step:** ""Test"" (full log     :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * pull / winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) on Sep 07 from  8:37am to  9:19am (0dddefe242  a47bc96fb7)  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","The tests are failing in crazy ways, so we might want to find a less dangerous way of doing this","btw, re reshape_alias, the changes you have to make should be substantially simplified post this stack https://github.com/pytorch/pytorch/pull/84579 so I highly recommend doing this on the branch (where the change is already available)","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Nick Korovaiko . The commit (3e5b65fef6be636f1ad158778e8575408a7114af, 44cdb40a6a59d629b92326bf049d0adee2c7cc27, 2e2f56791d810c284afd059817284ca7fa7668f3, 18f00da8b21a3ec92330b45249bc8bae17be937d, ae94f95ae6d2462b279413ef8e4bd33fa220f314, 6ca4ad580c3aaec232cbd0b06a0d4eddea004997, 0e1092a25e1636a2d2d844984f643ee1bbbb3b61, 758c271a6618e3534b68ebb33c4be526ddf1de7e, 5389b06fe401cb9cbf222009278437a3e4940aa9, ed027406eef239b654bfb9e53a9bfb4648f0c756, f55e49fc6ca3fa125fdcbd83df55401877389bb1, d5e6399226c906f17d729178b766075d18c6e969) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
chat,"RuntimeError: dst.nbytes() >= (dst.storage_offset() * dst.element_size()) INTERNAL ASSERT FAILED at ""/Users/davidlaxer/pytorch/aten/src/ATen/native/mps/operations/Copy.mm"":130, please report a bug to PyTorch. "," 🐛 Describe the bug This assertion is being raised: ``` TORCH_INTERNAL_ASSERT(dst.nbytes() >= (dst.storage_offset() * dst.element_size())); ``` in  ~/pytorch/aten/src/ATen/native/mps/operations/Copy.mm ``` import torch NUM_SAMPLES=60 s = (0,1) X = torch.rand(8000, 3, dtype=torch.float32, device='mps') idx = torch.randint(0, X.shape[0], (1,)).repeat(len(s)) pts = torch.randint(0, X.shape[0], (NUM_SAMPLES, X.shape[1])) pts[:, s] = idx actual_pts = torch.zeros(NUM_SAMPLES, X.shape[1], dtype=torch.float) for i in range(NUM_SAMPLES):     for j in range(X.shape[1]):         actual_pts[i,j] = X[pts[i,j],j]  RuntimeError                              Traceback (most recent call last) Input In [20], in ()       1 for i in range(NUM_SAMPLES):       2     for j in range(X.shape[1]): > 3         actual_pts[i,j] = X[pts[i,j],j] RuntimeError: dst.nbytes() >= (dst.storage_offset() * dst.element_size()) INTERNAL ASSERT FAILED at ""~/pytorch/aten/src/ATen/native/mps/operations/Copy.mm"":130, please report a bug to PyTorch.  ```  Versions % python collect_env.py  Collecting environment information... PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: macOS 12.5.1 (x86_64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.22.1 Libc version: N/A Python version: 3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ] (64bit runtime) Python platform: macOS10.16x86_64i38664bit Is CUDA available: N/A CUDA runtime version: Could not collect GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] torch==1.13.0a0+gitb2ddef2 [pip3] torchmetrics==0.9.3 [pip3] torchvision==0.14.0a0+a61e6ef [conda] blas                      1.0                         mkl    anaconda [conda] captum                    0.5.0                         0    pytorch [conda] mkl                       2021.4.0           hecd8cb5_637    anaconda [conda] mklservice               2.4.0           py310hca72f7f_0    anaconda [conda] mkl_fft                   1.3.1           py310hf879493_0    anaconda [conda] mkl_random                1.2.2           py310hc081a56_0    anaconda [conda] numpy                     1.22.3          py310hdcd3fac_0    anaconda [conda] numpybase                1.22.3          py310hfd2de13_0    anaconda [conda] pytorch                   1.12.1                 py3.10_0    pytorch [conda] torch                     1.13.0a0+git09157c7          pypi_0    pypi [conda] torchmetrics              0.9.3              pyhd8ed1ab_0    condaforge [conda] torchvision               0.14.0a0+a61e6ef          pypi_0    pypi ",2022-08-24T17:04:56Z,triaged module: mps,closed,0,0,https://github.com/pytorch/pytorch/issues/83995
rag,General NestedTensor op coverage tracking issue," This issue is to have a centralized place to list and track work on adding support to new ops for the NestedTensor backend. There are  large number of operators in pytorch and so they are not all implemented yet for the NestedTensor backends as it is still in the prototype phase. We will be prioritizing adding new operators based on user feedback. If possible, please also provide link to the network or usecase where this op is getting used. If you want to work on adding support for such op, feel free to comment below to get assigned one. Please avoid picking up an op that is already being worked on or that already has a PR associated with it.  Op coverage requests  [ ] aten::convolution  [ ] aten::max.dim  [x] detach CC([Nested Tensor] detach)   [x] to CC(Add support for .to() for NestedTensor backends)  [ ] eq  [ ] masked_select  [ ] index_select  [ ] narrow  [ ] alias  [ ] Broadcasting Ufuncs along implicit NT dim  [ ] Zerocopy Nt construction from Size info  [ ] BCE/ Other loss functions for NT  [ ] nested tensor creation from arbitrary masks rather than leftaligned  Backward op coverage requests  [ ] gelu, relu backward  [ ] layernorm backward ",2022-08-24T16:57:07Z,feature triaged module: nestedtensor,closed,0,2,https://github.com/pytorch/pytorch/issues/83994," CC(Add eq, to, masked_select, index_select, narrow to nested tensors)","also, having clean nestedtensor support for depthwise conv is important for Conformer models which have them in convolution blocks"
rag,Support the XPU backend untyped storage,Simple add XPU backend in untyped torch storage.,2022-08-24T01:10:09Z,open source Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/83952,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83952**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (1 Pending) As of commit 7b8224a3f2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , It is glad to see the storage has been decoupled to be untyped. Add the necessary code to support the XPU backend in untyped storage., merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[deploy][inference] Load extra_files of package in cpp (needed for saved requests and warmup),"Summary:  Adds a method to the package struct to enable reading of data saved in ""extra_files"" of the package  D38378610 **[torchrec] Allow storing binary data in extra_files**  added how to save and read the extra files in python  For warmup, I need a way to save the requests, which can only be read as binary in python and the extra_files provides a convenient way to do so Test Plan: Tested that I can save a recordio file in python as binary, and read it as a string in cpp Differential Revision: D38914842",2022-08-23T01:25:12Z,fb-exported cla signed Stale,closed,0,5,https://github.com/pytorch/pytorch/issues/83882,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83882**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :white_check_mark: 1 Base Failures As of commit da0081924e (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 7cfc8b7820 on Aug 22 from  4:25pm to  8:36pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * Lint / lintrunner on Aug 22 from  4:25pm to  8:36pm (3db6859c4e  658f958bc4)  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D38914842,"/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."," :x:  login:  / name: Shabab Ayub . The commit (da0081924e2e9351341f7814e6617d907f233882) is not authorized under a signed CLA. Please click here to be authorized. For further assistance with EasyCLA, please submit a support request ticket.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
agent,Named pipe based watchdog timer,"Summary: This diff implements a named pipe based watchdog timer (`FileTimerClient` and `FileTimerServer`). This is similar to the existing `LocalTimerClient` and `LocalTimerServer` (https://fburl.com/code/j4b9pyya). The motivation is from the need of handling various timeout issues. The training process occasionally get stuck. We need a proper watchdog to monitor the liveness of the training processes. This timer allows the TorchElastic agent (as the watchdog) to monitor the progress of the training processes that it spawned. If a timeout occurred, he TorchElastic agent can take some action to kill the stuck process and creating a core dump for it. `LocalTimerClient` and `LocalTimerServer` require  a `multiprocessing.Queue()` to work. So they can only be used between `multiprocessing` parent and child processes. `FileTimerClient` and `FileTimerServer` does not have such limitation. Test Plan:  Unit Test ``` buck test mode/opt caffe2/test/distributed/elastic/timer:file_based_timer_test ``` ``` RemoteExecution session id: reSessionID06d70a77043c4d9db0f294c24460740atpx Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/844425186732666     ✓ ListingSuccess: caffe2/test/distributed/elastic/timer:file_based_timer_test : 12 tests discovered (2.177)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_happy_path (file_based_local_timer_test.FileTimerTest) (2.463)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_expired_timers (file_based_local_timer_test.FileTimerServerTest) (1.889)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_send_request_release (file_based_local_timer_test.FileTimerServerTest) (1.700)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_valid_timers (file_based_local_timer_test.FileTimerServerTest) (1.873)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_watchdog_call_count (file_based_local_timer_test.FileTimerServerTest) (1.715)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_watchdog_empty_queue (file_based_local_timer_test.FileTimerServerTest) (1.609)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_exception_propagation (file_based_local_timer_test.FileTimerTest) (1.633)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_multiple_clients_interaction (file_based_local_timer_test.FileTimerTest) (2.189)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_get_timer_recursive (file_based_local_timer_test.FileTimerTest) (2.295)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_no_client (file_based_local_timer_test.FileTimerTest) (1.753)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_timer (file_based_local_timer_test.FileTimerTest) (2.151)     ✓ Pass: caffe2/test/distributed/elastic/timer:file_based_timer_test  test_client_interaction (file_based_local_timer_test.FileTimerTest) (1.895) Summary   Pass: 12   ListingSuccess: 1 Finished test run: https://www.internalfb.com/intern/testinfra/testrun/844425186732666 ``` Differential Revision: D38604238",2022-08-18T18:41:00Z,oncall: distributed fb-exported Merged cla signed,closed,0,10,https://github.com/pytorch/pytorch/issues/83695,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83695**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (3 Pending) As of commit 051ff2bbb2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D38604238,This pull request was **exported** from Phabricator. Differential Revision: D38604238,This pull request was **exported** from Phabricator. Differential Revision: D38604238,This pull request was **exported** from Phabricator. Differential Revision: D38604238, ,This pull request was **exported** from Phabricator. Differential Revision: D38604238, merge (Initiating merge automatically since Phabricator Diff has merged)," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Huggingface Transformers Trainer Test,"Transformers trainer API uses TorchDynamo. As we cleaned up Dynamo, we did not percolate the changes to trainer API, leading to some failures  https://github.com/huggingface/transformers/issues/18127 This is a tracker to improve the situation 1) Better API  Currently, we pass strings and then depending on the strings, we find the backend for Dynamo. Instead, we could just simplify all this, and directly do torchdynamo.optimze(backend_str)  https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.pyL639L666 2) Remove ctx manager. 3) Decompositions might not be working correctly for relu. I had to switch to cos instead of relu in this PR to see the memory footprint reduction  https://github.com/huggingface/transformers/pull/18685 4) Add a test in Dynamo CI that brings huggingface and tests Dynamo related tests ~~~ pytest tests/trainer/test_trainer.py k torchdynamo ~~~ ",2022-08-18T17:48:34Z,triaged oncall: pt2 module: dynamo,closed,2,1,https://github.com/pytorch/pytorch/issues/93632,Close the issue tracker as it seems fixed.
rag,"Coverage for nondeterministic_seeded, respect it in constant prop","  CC(Don't extract tensor metadata from sparse tensors)  CC(Refactor is_X_like, better invariant checking for SymInt overload)  CC(Refactor CppSignatureGroup to collect signatures as list.)  CC(Coverage for nondeterministic_seeded, respect it in constant prop)  CC(Be more conservative about propagating constants.)  CC(Address CR comments for ""Delete ProxyTensor wrapper subclass"")  nondeterministic_seeded was not applied to enough functions.  I added   some heuristics to codegen for identifying functions that are likely   to be random and added a bunch of these tags to functions.  Not sure   I got all of them.  Don't constant propagate through nondeterministic functions in FX   tracing. It would be better to do some testing for the tag but this would be quite an effort. Signedoffby: Edward Z. Yang ",2022-08-18T03:30:15Z,Merged cla signed fx,closed,0,4,https://github.com/pytorch/pytorch/issues/83650,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83650**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (2 Pending) As of commit 240d82b943 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Fix issue in softmax.cu with transformer error when mask seqlen > 1024 ,Fixes CC(Transformer encoder error when encoding long sequence (more than 1024 tokens)) Adds  test to catch this issue.   fix to softmax.cu that broadcasts src_key_padding_mask to regular attention_mask shape,2022-08-18T00:59:56Z,Merged cla signed release notes: cuda release notes: nn topic: bug fixes,closed,0,10,https://github.com/pytorch/pytorch/issues/83639,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83639**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 02020155aa (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , rebase s, successfully started a rebase job. Check the current status here,"Successfully rebased `erichan1/btseqlen1024fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout erichan1/btseqlen1024fix && git pull rebase`)", rebase s, successfully started a rebase job. Check the current status here,"Successfully rebased `erichan1/btseqlen1024fix` onto `refs/remotes/origin/viable/strict`, please pull locally before adding more changes (for example, via `git checkout erichan1/btseqlen1024fix && git pull rebase`)", merge," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[functorch] reclassifying max_unpool failures,Stack from ghstack:  CC([functorch] reenable some linalg.det tests)  CC([functorch] reclassifying max_unpool failures)  CC([functorch] reclassify linalg.eigh in vmap testing)  CC([functorch] reclassify svd as an allowed failure; add test)  CC([functorch] add some vmap+jvp inplace+view tests)  CC([functorch] relax as_strided batching rule)  CC([functorch] annotate test_jvpvjp) They're expected because this variant of max_unpool is nondetermnistic Also removed a skip that seems to pass alright on my machine,2022-08-17T19:13:15Z,cla signed Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/83618,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83618**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 38b4f1560f (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7900726444?check_suite_focus=true) pull / linuxfocalpy3.7gcc7 / test (default, 1, 2, linux.2xlarge) (1/1) **Step:** ""Get workflow job id"" (full log | diagnosis details)   20220818T15:10:18.1833888Z   test_add_done_ca...arg() takes 0 positional arguments but 1 was given  ``` 20220818T15:10:18.1808138Z   /opt/conda/lib/python3.7/unittest/suite.py(122): run 20220818T15:10:18.1808392Z   /opt/conda/lib/python3.7/unittest/suite.py(84): __call__ 20220818T15:10:18.1808732Z   /opt/conda/lib/python3.7/sitepackages/xmlrunner/runner.py(67): run 20220818T15:10:18.1809011Z   /opt/conda/lib/python3.7/unittest/main.py(271): runTests 20220818T15:10:18.1809250Z   /opt/conda/lib/python3.7/unittest/main.py(101): __init__ 20220818T15:10:18.1809628Z   /opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py(786): run_tests 20220818T15:10:18.1809905Z   test_futures.py(331):  20220818T15:10:18.1810026Z  20220818T15:10:18.1810084Z ok (0.246s) 20220818T15:10:18.1827609Z   test_add_done_callback_maintains_callback_order (__main__.TestFuture) ... ok (0.002s) 20220818T15:10:18.1833888Z   test_add_done_callback_no_arg_error_is_ignored (__main__.TestFuture) ... [E pybind_utils.h:212] Got the following error when running the callback: TypeError: no_arg() takes 0 positional arguments but 1 was given 20220818T15:10:18.1834872Z ok (0.001s) 20220818T15:10:18.1845904Z   test_add_done_callback_simple (__main__.TestFuture) ... ok (0.001s) 20220818T15:10:18.1881424Z   test_chained_then (__main__.TestFuture) ... ok (0.003s) 20220818T15:10:18.2900228Z   test_collect_all (__main__.TestFuture) ... ok (0.102s) 20220818T15:10:18.2908494Z   test_done (__main__.TestFuture) ... ok (0.001s) 20220818T15:10:18.2921615Z   test_done_exception (__main__.TestFuture) ... ok (0.001s) 20220818T15:10:18.2938320Z   test_interleaving_then_and_add_done_callback_maintains_callback_order (__main__.TestFuture) ... ok (0.002s) 20220818T15:10:18.2948403Z   test_interleaving_then_and_add_done_callback_propagates_error (__main__.TestFuture) ... [E pybind_utils.h:212] Got the following error when running the callback: ValueError: Expected error 20220818T15:10:18.2948938Z  20220818T15:10:18.2949056Z At: ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
finetuning,Multi-Grad Hooks," 🚀 The feature, motivation and pitch For FSDP, one advantage of the `FlatParameter` is that registering a hook on its `AccumulateGrad` object gives us the correct time to schedule a reducescatter: The hook runs when all constituent original parameters' gradients are ready. An alternative to this paradigm is to have a ""multigrad hook"" that only runs once all passedin tensors' gradients are ready. In that case, we can explore a new design for FSDP where we are not constrained to a single (large) `flat_param.grad`. Under such a new design, the `FlatParameter` would **only be for communication** and no longer relevant for autograd. One effort this half is to provide an option to expose the original parameters in FSDP. The current approach manually sets `.data` to be views into the `FlatParameter`, and another approach is to use a tensor subclass like `IndirectParameter`. Regardless of the approach, as long as we have the `FlatParameter` interface with autograd and thus have its `.grad` attribute populated, we cannot save memory when only _some_ of the original parameters require gradients. The parts of the `.grad` corresponding to parameters that do not require gradients will simply be zeros, but the entire `.grad` needs to be materialized. This use case surfaces in finetuning, where there is a large model with only some small subset of parameters that require gradients and those parameters are spread throughout the module tree. PyTorch FSDP does not have good support for this at the moment. One option is to support a module having multiple `FlatParameter`s, where we have the ones that do require gradient be in the same smaller `FlatParameter` and the ones that do not require gradient in another larger `FlatParameter`. This can work but is not ideal since it (unexpectedly) couples communication and autograd. The smaller `FlatParameter` will be less efficient to communicate.  Alternatives The alternative is the current approach of having the `FlatParameter` interface with autograd.  Additional context _No response_ ",2022-08-17T19:01:52Z,feature module: autograd triaged actionable,closed,0,9,https://github.com/pytorch/pytorch/issues/83617,"Does the following work for you? ```python import torch def register_multi_grad_hook(tensors, fn):     count = 0     nb_calls = len(tensors)     buffer = [None] * nb_calls     def get_inner_hook(idx):         def inner_hook(grad):             nonlocal count             buffer[idx] = grad             count += 1             if count == nb_calls:                 fn(buffer)         return inner_hook     for i, t in enumerate(tensors):         t.register_hook(get_inner_hook(i)) t1 = torch.rand(2, requires_grad=True) t2 = torch.rand(2, requires_grad=True) t3 = torch.rand(2, requires_grad=True) t4 = torch.rand(2, requires_grad=True) def hook(grads):     print(f""Multihook called with {len(grads)} gradients"") register_multi_grad_hook((t2, t3), hook) def get_hook(name):     def hook(grad):         print(f""{name} hook called"")     return hook t1.register_hook(get_hook(""t1"")) t3.register_hook(get_hook(""t3"")) out = t1.clone() out = out + t2 out = out + t3 out = out + t4 out.sum().backward() ``` This will output ``` t3 hook called Multihook called with 2 gradients t1 hook called ```",No because the multihook is not called if some of the hooks are unused,"From offline discussion:  There is no way to do without this at python level  There are two main avenues to do this:    We can change the engine to do it:      Add a new special kind of hook that the engine knows about      We can discover if/when it needs to be called ""for free"" by reusing the traversal that we already do to compute dependencies_      This new state will need to be stored on graph_task to ensure that multiple backward can properly run in parallel    Create a custom Node that is always needed_ if linked to the graph.      This hook will just be another Node in the graph      Will need to allow for an edge to have multiple endpoint < major change","I created https://github.com/pytorch/pytorch/pull/84773 for a different design, which aims to make fewer modifications to the engine/graph itself, but may require more changes on the Python side. It does the job by exposing a way to query the TLS graph task's exec_info which is a map mapping the Node to a bool indicating whether it will be executed in the current backward pass (as determined by the inputs= argument for .grad or .backward). Below, I've modified Alban's example above using the new API to address Ed's comment. ```python import torch def register_multi_grad_hook(tensors, fn):     count = 0     nb_calls = None     buffer = None     def get_grad_fn(t):          or grad accumulator         if t.requires_grad and t.grad_fn is None:             return t.clone().grad_fn.next_functions[0][0]         else:             return t.grad_fn     grad_fns = list(map(get_grad_fn, tensors))     def get_inner_hook(idx):         def inner_hook(grad):             nonlocal count, nb_calls, buffer             if count == 0:                  On the first call, compute the actual nb_calls and buffer                 nb_calls = sum(1 for g in grad_fns if torch._C._will_engine_execute_node(g))                 buffer = [None] * nb_calls             buffer[idx] = grad             count += 1             if count == nb_calls:                 fn(buffer)         return inner_hook     for i, t in enumerate(tensors):         t.register_hook(get_inner_hook(i)) t1 = torch.rand(2, requires_grad=True) t2 = torch.rand(2, requires_grad=True) t3 = torch.rand(2, requires_grad=True) t4 = torch.rand(2, requires_grad=True) def hook(grads):     print(f""Multihook called with {len(grads)} gradients"") register_multi_grad_hook((t2, t3), hook) def get_hook(name):     def hook(grad):         print(f""{name} hook called"")     return hook t1.register_hook(get_hook(""t1"")) t3.register_hook(get_hook(""t3"")) out = t1.clone() out = out + t2 out = out + t3 out = out + t4 out.sum().backward(inputs=(t2, t3))  t3 hook called  Multihook called with 2 gradients  t1 hook called out.sum().backward(inputs=(t1, t2))  Multihook called with 1  ```",This alternate API seems fine too!,"I think the primary risk is if this adds overhead, but  can probably test this",Thanks  for prototyping this so quickly! I will find some time to try it out soon.,"Hey , we're landing https://github.com/pytorch/pytorch/pull/86260, which is a cleaned up version of  CC(Multi-Grad Hooks)issuecomment1244048429. One improvement is that it now support multiple backwards on different threads on the same graph. One thing to note is that if an error happens during backward the state of the hook is still kept alive by the graph, so that can be problematic if someone catches the error and runs backward on the same graph. We may find a more automatic solution later, but for now to get around this one should just remove and reregister the hook to reset its state (this would reset the state for all backwards running concurrently). Let us know how it works for you!","Hi . Thanks so much for pushing this forward! Sadly, from the FSDP side, there have been a series of blockers that have emerged between our current state and adoption of multigrad hooks. Integrating multigrad hooks will not be in scope for this half :/"
transformer,Move odd num_head in TransformerEncoder to slow_path,"Summary: odd nhead is not supported for masked softmax, therefore we just move it to use old slow_path Test Plan: CI Differential Revision: D38720086",2022-08-15T22:27:50Z,fb-exported Merged cla signed,closed,0,13,https://github.com/pytorch/pytorch/issues/83483,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83483**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 052ed6f068 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D38720086,"Failure is because we did not add odd nhead check to creating a NestedTensor. So when nhead is odd, we are sending a NestedTensor to slow path, which cannot accept NestedTensor. Best fix is probably to check if the first or any encoder layer in TransformerEncoder has odd nhead. Kind of ugly... but what can you do. ",This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086,This pull request was **exported** from Phabricator. Differential Revision: D38720086, merge (Initiating merge automatically since Phabricator Diff has merged)," successfully started a merge job. Check the current status here. The merge job was triggered without a flag. This means that your change will be merged once all checks on your PR have passed (ETA: 04 Hours). If this is not the intended behavior, feel free to use some of the other merge options in the wiki. Please reach out to the PyTorch DevX Team with feedback or questions!","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Only support when num_heads is even in transformer," 🐛 Describe the bug I have verified that up until v.1.11 I can set `nhead=1` in `TransformerEncoderLayer`, but in v.1.12 this throws a Runtime Error: ``` Exception has occurred: RuntimeError Only support when num_heads is even in transformer ``` As far as I can tell the docs do not specify that `nhead` must be even, unless I'm missing it.   Versions ``` Collecting environment information... PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.22.4 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.13.01025awsx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 11.0.221 GPU models and configuration: GPU 0: Tesla V100SXM216GB Nvidia driver version: 510.47.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.1 [pip3] pytorchlightning==1.7.1 [pip3] torch==1.12.0 [pip3] torchtbprofiler==0.4.0 [pip3] torchinfo==1.7.0 [pip3] torchmetrics==0.9.3 [pip3] torchtext==0.13.0 [conda] Could not collect ``` ",2022-08-12T22:37:27Z,high priority triage review module: cuda triaged module: regression,closed,0,7,https://github.com/pytorch/pytorch/issues/83355,Reproduced! We are fixing. Thanks  ,"It could be tricky to fix the masked softmax kernel, so probably the easiest thing is to send odd number of heads to the slow path. ",Yep we'll send to slow path. Root cause is here https://github.com/pytorch/pytorch/blob/b2363520363d8c85496f6954fde005834c188b5d/aten/src/ATen/native/cuda/SoftMax.cuL1009. I would hope there aren't too many users of odd num MHA heads... but this is definitely a major issue. ,"To be clear, I really was looking just to maintain support for 1 head, not an odd number of heads generally. In my case testing showed that 2 heads was not better than 1 lol.","Then probably you want to use 2 heads, because 2 heads would go to a fast code path, whereas one head will go to a slow one, so it'll probably end up being slower than 2.","In my testing 2 heads is also slower than using 1. I also am relying on a pretrained model with 1 head, so having to use 2 heads would be an issue there as well.",https://github.com/pytorch/pytorch/pull/83483 Fixes and adds test
yi,"when input type of torch.cholesky_inverse is complex128, cpu's running result is inconsistent with gpu"," 🐛 Describe the bug ``` results = dict() import torch arg_1 = torch.rand([3, 3, 5, 5], dtype=torch.complex128) try:   results[""res_cpu""] = torch.cholesky_inverse(arg_1,) except Exception as e:   results[""err_cpu""] = ""ERROR:""+str(e) arg_2 = arg_1.clone().cuda() try:   results[""res_gpu""] = torch.cholesky_inverse(arg_2,) except Exception as e:   results[""err_gpu""] = ""ERROR:""+str(e) print(results) ``` **The result on cpu begins as follows:**        tensor([[[[ 2.2772e+01+0.0000e+00j, 1.4934e+011.6002e+01j,             7.4759e+00+1.0612e+01j, 1.6203e+002.7951e+00j,            5.8908e01+7.1717e01j],           [1.4934e+01+1.6002e+01j,  2.3012e+01+0.0000e+00j,            1.2838e+012.2871e+00j,  3.2950e+00+2.4935e01j,            5.4043e018.5046e01j],           [ 7.4759e+001.0612e+01j, 1.2838e+01+2.2871e+00j,             8.5023e+00+0.0000e+00j, 2.0702e+004.6753e01j,             5.8712e02+6.5857e01j],           [1.6203e+00+2.7951e+00j,  3.2950e+002.4935e01j,            2.0702e+00+4.6753e01j,  1.1360e+00+0.0000e+00j,            3.8658e014.2250e01j],           [5.8908e017.1717e01j, 5.4043e01+8.5046e01j,             5.8712e026.5857e01j, 3.8658e01+4.2250e01j,             5.4342e013.6619e01j]].......... **The result on gpu begins as follows:**        tensor([[[[ 2.3904e+014.5969e15j, 1.6702e+011.5623e+01j,             9.0184e+00+9.8650e+00j, 3.6530e+001.1465e+00j,            1.0580e01+1.1141e+00j],           [1.6702e+01+1.5623e+01j,  2.4514e+012.7789e15j,            1.3767e+015.5089e01j,  3.3668e+002.2533e+00j,            1.1135e+004.8628e01j],           [ 9.0184e+009.8650e+00j, 1.3767e+01+5.5089e01j,             8.8595e+00+6.3713e17j, 2.1199e+00+1.2197e+00j,             5.0250e01+6.1900e01j],           [3.6530e+00+1.1465e+00j,  3.3668e+00+2.2533e+00j,            2.1199e+001.2197e+00j,  1.7951e+00+1.7584e16j,            6.7129e011.6200e01j],           [1.0580e011.1141e+00j, 1.1135e+00+4.8628e01j,             5.0250e016.1900e01j, 6.7129e01+1.6200e01j,             7.9018e01+1.9773e17j]].......... It's obvious that cpu's result is quite different from gpu's result.  Versions pytorch: 1.8.1 python: 3.8.3 os: win11 ",2022-08-12T03:18:53Z,triaged module: complex module: linear algebra,closed,0,5,https://github.com/pytorch/pytorch/issues/83311,"Still reproducible on 1.12:  ``` $ python c ""import torch; arg=torch.rand([5, 5], dtype=torch.float);print(torch.max(torch.abs(torch.cholesky_inverse(arg)torch.cholesky_inverse(arg.to('cuda')).to('cpu'))))"" tensor(9.5367e07) $ python c ""import torch; arg=torch.rand([5, 5], dtype=torch.cfloat);print(torch.max(torch.abs(torch.cholesky_inverse(arg)torch.cholesky_inverse(arg.to('cuda')).to('cpu'))))"" tensor(30.9700) ```","The input matrix has values between 0 and 1 on the diagonal (it's eigenvalues), so it is likely to be badly conditioned for inverse. Especially after the ""outer"" product which will square the singular values of the input and blow up the condition number of the product. So the behavior is expected, that is, the input is badly conditioned and that the algorithms on CPU and on CUDA are likely different and unstable for closetosingular inputs.",'s comment shows that we still have an issue in master.,"It is very likely a singularity issue, take a look at: ```python In [1]: import torch In [2]: cond_inv = torch.empty(1000, dtype=torch.double) In [3]: for i in range(1000):    ...:     x = torch.rand(5, 5).tril()    ...:     y = x @ x.transpose(1, 2)    ...:     _, s, _ = y.svd()    ...:     cond_inv[i] = s[0] / s[1]    ...:  In [4]: cond_inv.mean() Out[4]: tensor(8959341.4300, dtype=torch.float64) In [5]: cond_inv.std() Out[5]: tensor(1.4241e+08, dtype=torch.float64) In [6]: cond_inv.max() Out[6]: tensor(4.0407e+09, dtype=torch.float64) In [7]: cond_inv.min() Out[7]: tensor(16.4844, dtype=torch.float64) ```","Yup,  is right. I take that back. This is expected as the matrix is very illconditioned."
transformer,Pytorch based Bert NER for transfer learning/retraining,"I trained an Bertbased NER model using Pytorch framework by referring the below article. https://www.dependsonthedefinition.com/namedentityrecognitionwithbert/. After training the model using this approach, I saved that model using torch.save() method. Now, I want to retrain the model with new dataset. Can someone please help me on how to perform retraining/transfer learning as I'm new to NLP and transformers. Thanks in advance.",2022-08-11T19:44:16Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/83290
transformer,Transformer encoder error when encoding long sequence (more than 1024 tokens)," 🐛 Describe the bug In evaluation mode, Transformer encoder shows a runtime error when the number of tokens is greater than 1024. ```python import torch import torch.nn as nn encoder_layer = nn.TransformerEncoderLayer(     d_model=512, nhead=8, dim_feedforward=2048, batch_first=True) encoder = nn.TransformerEncoder(     encoder_layer, num_layers=6, enable_nested_tensor=True) encoder.cuda() x = torch.randn(1, 1026, 512) x_mask = [[0]*1025+[1]*1] x_mask = torch.Tensor(x_mask).bool() x = x.cuda() x_mask = x_mask.cuda() encoder.eval() with torch.no_grad():     y = encoder(x, src_key_padding_mask=x_mask) print(y.shape) ``` runtime error: ``` Traceback (most recent call last):   File ""/home/xmr/miniconda3/envs/py39/lib/python3.9/runpy.py"", line 197, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/xmr/miniconda3/envs/py39/lib/python3.9/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/home/xmr/.vscodeserver/extensions/mspython.python2022.12.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in      cli.main()   File ""/home/xmr/.vscodeserver/extensions/mspython.python2022.12.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main     run()   File ""/home/xmr/.vscodeserver/extensions/mspython.python2022.12.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file     runpy.run_path(target, run_name=""__main__"")   File ""/home/xmr/.vscodeserver/extensions/mspython.python2022.12.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path     return _run_module_code(code, init_globals, run_name,   File ""/home/xmr/.vscodeserver/extensions/mspython.python2022.12.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code     _run_code(code, mod_globals, init_globals,   File ""/home/xmr/.vscodeserver/extensions/mspython.python2022.12.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code     exec(code, run_globals)   File ""/mnt/e/test/test.py"", line 19, in      y = encoder(x, src_key_padding_mask=x_mask)   File ""/home/xmr/miniconda3/envs/py39/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/xmr/miniconda3/envs/py39/lib/python3.9/sitepackages/torch/nn/modules/transformer.py"", line 236, in forward     output = mod(output, src_mask=mask)   File ""/home/xmr/miniconda3/envs/py39/lib/python3.9/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/xmr/miniconda3/envs/py39/lib/python3.9/sitepackages/torch/nn/modules/transformer.py"", line 437, in forward     return torch._transformer_encoder_layer_fwd( RuntimeError: Mask shape should match input shape; transformer_mask is not supported in the fallback case. ``` But if the number of tokens is less than or equal to 1024, then it can run without error. ```python import torch import torch.nn as nn encoder_layer = nn.TransformerEncoderLayer(     d_model=512, nhead=8, dim_feedforward=2048, batch_first=True) encoder = nn.TransformerEncoder(     encoder_layer, num_layers=6, enable_nested_tensor=True) encoder.cuda() x = torch.randn(1, 1026, 512) x_mask = [[0]*1024+[1]*2]      Here I change the number of tokens to 1024 x_mask = torch.Tensor(x_mask).bool() x = x.cuda() x_mask = x_mask.cuda() encoder.eval() with torch.no_grad():     y = encoder(x, src_key_padding_mask=x_mask) print(y.shape)   torch.Size([1, 1024, 512]) ```  Versions PyTorch version: 1.12.1+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 8.4.03ubuntu2) 8.4.0 Clang version: 9.0.0 (https://github.com/condaforge/clangdevfeedstock 284a3d5d88509307bcfba64b055653ee347371db) CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.12 (main, Jun  1 2022, 11:38:51)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.10.16.3microsoftstandardWSL2x86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU Nvidia driver version: 516.59 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.12.1+cu116 [pip3] torchaudio==0.12.1+cu116 [pip3] torchvision==0.13.1+cu116 [conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free [conda] mkl                       2021.4.0           h06a4308_640    defaults [conda] mklservice               2.4.0            py39h7e14d7c_0    condaforge [conda] mkl_fft                   1.3.1            py39h0c7bc48_1    condaforge [conda] mkl_random                1.2.2            py39hde0f152_0    condaforge [conda] numpy                     1.22.3           py39he7a7128_0    defaults [conda] numpybase                1.22.3           py39hf524024_0    defaults [conda] torch                     1.12.1+cu116             pypi_0    pypi [conda] torchaudio                0.12.1+cu116             pypi_0    pypi [conda] torchvision               0.13.1+cu116             pypi_0    pypi ",2022-08-10T04:11:41Z,module: nn triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/83142,I'm aware of where this issue is called  we take a fastpath at 1024 tokens for mask addition and there's a bug in there. Debugging. ,Working on the fix. Put up test first.  Edit: Done! https://github.com/pytorch/pytorch/pull/83639
transformer,"Fix typo in norm_first description, respectivaly - > respectively","Fix typo in norm_first description, respectivaly  > respectively Fixes CC(Typo in norm_first parameter description of transformer layer, respectivaly  > respectively)",2022-08-10T01:48:08Z,triaged open source Merged cla signed,closed,1,7,https://github.com/pytorch/pytorch/issues/83139,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/83139**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit fd486799dd (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,Merge failed due to HTTP Error 403: Forbidden Raised by https://github.com/pytorch/pytorch/actions/runs/2833997951, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"Typo in norm_first parameter description of transformer layer, respectivaly - > respectively", 🐛 Describe the bug https://github.com/pytorch/pytorch/blob/b2363520363d8c85496f6954fde005834c188b5d/torch/nn/modules/transformer.pyL358L359 https://github.com/pytorch/pytorch/blob/b2363520363d8c85496f6954fde005834c188b5d/torch/nn/modules/transformer.pyL563L565  Versions We don't need a version to fix a typo.,2022-08-10T01:47:51Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/83138
rag,Remove unbalanced `pragma diagnostic pop`,"Detected by internal CI, we should have it in OSS as well: ``` aten/src/ATen/native/cpu/Loops.h:398:24: error: pragma diagnostic pop could not pop, no matching push [Werror,Wunknownpragmas] pragma GCC diagnostic pop ``` Regression introduced by https://github.com/pytorch/pytorch/pull/82883",2022-08-09T19:09:18Z,Merged cla signed topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/83095," merge f ""benign change, needed for diff train import""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[FSDP] specifying `device_id` will move ignored parameters,"If the wrapped module is on CPU, then specifying `device_id` will move the entire wrapped module to the device corresponding to `device_id`: https://github.com/pytorch/pytorch/blob/6c60a656b02fbd09661b282bec53940b184db3ca/torch/distributed/fsdp/fully_sharded_data_parallel.pyL1048 We should clarify the semantics for the `ignored_modules` argument and whether FSDP should move the ignored modules' parameters. ",2022-08-09T15:52:23Z,triaged module: fsdp,closed,0,2,https://github.com/pytorch/pytorch/issues/83078," Today in the latest version, the ignored modules are not moved to the device. Is it expected and going to stay this way in the future?"," Yep! I think not moving ignored modules to device will be the future behavior. In that case, I think that this issue can be closed."
transformer,DPP training incompatibility with checkpoint and detach," 🐛 Describe the bug I am using pytorch ddp to train my model. Turns out if I use ddp, then I can not use checkpoint or detach gradient. The incompatibility is a big problem, because these techniques are important for my use. My model consists of two part roughly, a language model for generate representation,  where weights are detached, another part of the model is trained  with gradients.  the code of the language model: ```python if exists(config.msa_bert.msa_bert_config.model_weight) and not config.msa_bert.skip_load_msa_bert:                 self.bert_model = load_pretrain(self.bert_model, config.msa_bert.msa_bert_config.model_weight) if config.msa_bert.msa_bert_config.freeze:     print('    frezze pretrained msa transformer')     for param in self.bert_model.parameters():         param.detach_()     self.bert_model.eval() ``` Note in the other part of my model, there are recycles with detach. ```python for i in range(n_recycle):             msa_fea, pair_fea = self.feat_extractor(msa_fea, pair_fea)             msa_fea, pair_fea = msa_fea.detach_(), pair_fea.detach_() ``` When using ddp, I have to turn on the `find_unused_parameters=True `, otherwise a error would be raised: `RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. ` Seems like if you have a model with detached params, you have to turn on this. Here comes the problem, if I keep `find_unused_parameters=True ` and enable checkpoint, an error would be raised because a variable is marked twice. I conjecture that during forward, those detached parameters are marked as ready because of  `find_unused_parameters=True `, and somehow they are marked ready again and causes this error. I am wondering in what cases  a param would be marked as ready again? And, what does it means for a param to be marked as ready? I think it is something to do with the autograd and the gradient compute map. I accidentally find a solution that turn off the recycle ( i.e.,  turn off detach) and checkpoint while keep  `find_unused_parameters=True `, the ddp training works. However, the problem is I can not turn off them as they are important for the efficiency. Without checkpoint, the gpu memory would explode.  Versions python3.8 ",2022-08-09T15:15:27Z,oncall: distributed triaged module: ddp,open,1,2,https://github.com/pytorch/pytorch/issues/83074,"Thanks for filing this issue! Would it be possible to get a minimal repro (DDP unused params + module + checkpoint + detach) that raises the error that you're seeing? Assuming that you're using `torch.utils.checkpoint.checkpoint` for checkpointing, you could consider passing `use_reentrant=False` flag into checkpoint function: https://github.com/pytorch/pytorch/blob/a58876ace78df1cfeb136cad592487f34d7e02f1/torch/utils/checkpoint.pyL164. Please note that to use this you should use a nightly build of pytorch as there are some recently landed bugfixes in the nonreentrant checkpoint code.  With the reentrant checkpointing implementation, DDP with unused parameters should work for checkpointing and we have unittests here: https://github.com/pytorch/pytorch/blob/a58876ace78df1cfeb136cad592487f34d7e02f1/test/distributed/test_c10d_common.pyL566, but would be valuable to get a repro of your use case to see if we missed any gaps.",Thanks. Would you like to explain the reason here? why this incompatibility occurs? How this argument help solve the error?
agent,Can't pickle local object 'CDLL.__init__.<locals>._FuncPtr',"I am using `torch.multiprocessing.Process` in a reinforcement learning project.  Here's bit of my codes: ```python class Agent(mp.Process):   def __init__():      super()...      self.dll = CDLL('load a dll here which work fine without mp.Process')      self.env = some_env   def run(self):      action = ...      self.env.step(action, self.dll)      ... if __name__ == ""__main__"":   mp.set_start_method(""forkserver"")    workers = [Agent(..) for i in range(n)]   [w.start() for w in workers]   [w.join() for w in workers] ``` Here's the errors I got: ``` Traceback (most recent call last):   File ""gym_test.py"", line 173, in      [w.start() for w in workers]   File ""gym_test.py"", line 173, in      [w.start() for w in workers]   File ""/usr/lib/python3.7/multiprocessing/process.py"", line 112, in start     self._popen = self._Popen(self)   File ""/usr/lib/python3.7/multiprocessing/context.py"", line 223, in _Popen     return _default_context.get_context().Process._Popen(process_obj)   File ""/usr/lib/python3.7/multiprocessing/context.py"", line 291, in _Popen     return Popen(process_obj)   File ""/usr/lib/python3.7/multiprocessing/popen_forkserver.py"", line 35, in __init__     super().__init__(process_obj)   File ""/usr/lib/python3.7/multiprocessing/popen_fork.py"", line 20, in __init__     self._launch(process_obj)   File ""/usr/lib/python3.7/multiprocessing/popen_forkserver.py"", line 47, in _launch     reduction.dump(process_obj, buf)   File ""/usr/lib/python3.7/multiprocessing/reduction.py"", line 60, in dump     ForkingPickler(file, protocol).dump(obj) AttributeError: Can't pickle local object 'CDLL.__init__.._FuncPtr' ``` I wonder how to load cdll properly in multiprocessing, any solutions? ",2022-08-09T12:07:41Z,module: multiprocessing triaged,closed,0,1,https://github.com/pytorch/pytorch/issues/83065,"`CDLL` is not pickable, and cannot be passed between processes. Avoid loading dll inside of the class init, do it lazily on demand."
transformer,'Wav2Vec2ForCTC' object has no attribute 'conv'," 🐛 Describe the bug hi there. i run my code on Colab. i want to statically quantize my Wav2Vec model. before that i try dynamic quantization but it was not useful because i didn't speed up inference time ,unfortunetly got slower than regular model. but i got error: `'Wav2Vec2ForCTC' object has no attribute 'conv'` here is my code: ``` from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer tokenizer = Wav2Vec2Tokenizer.from_pretrained(""facebook/wav2vec2base960h"") model = Wav2Vec2ForCTC.from_pretrained(""facebook/wav2vec2base960h"") input_values = tokenizer(audio, return_tensors = ""pt"").input_values ``` Quantize snippet: ``` model.eval() model.qconfig = torch.quantization.get_default_qconfig('fbgemm') model_fp32_fused = torch.quantization.fuse_modules(model, [['conv', 'relu']],inplace=True) model_fp32_prepared = torch.quantization.prepare(model_fp32_fused) model_fp32_prepared(input_values) model_int8 = torch.quantization.convert(model_fp32_prepared) res = model_int8(input_values) ``` and stacktrace:    ```      return modules[name]    1207         raise AttributeError(""'{}' object has no attribute '{}'"".format( > 1208             type(self).__name__, name))    1209     1210     def __setattr__(self, name: str, value: Union[Tensor, 'Module']) > None: AttributeError: 'Wav2Vec2ForCTC' object has no attribute 'conv' ```  Versions ``` PyTorch version: 1.12.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.6 Libc version: glibc2.26 Python version: 3.7.13 (default, Apr 24 2022, 01:04:09)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.188+x86_64withUbuntu18.04bionic Is CUDA available: False CUDA runtime version: 11.1.105 GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.0.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.12.0+cu113 [pip3] torchaudio==0.12.0+cu113 [pip3] torchsummary==1.5.1 [pip3] torchtext==0.13.0 [pip3] torchvision==0.13.0+cu113 [conda] Could not collect ``` ",2022-08-08T20:17:03Z,oncall: quantization triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/83020, Is this still an issue in the latest master? Can you try using FX graph mode quantization to see if this is also an issue there? We don't maintain this model so unless it's specifically a quantization bug I don't think we can fix this from our side.,Closing this for now. Please feel free to reopen if this is still an issue for you
transformer,[FSDP] TypeError: load_state_dict() got an unexpected keyword argument 'strict'," 🐛 Describe the bug When loading trained FSDP model, the below error is observed. The expected behaviour should be to have `load_state_dict` consistent with the PyTorch API. A similar issue is raised here https://github.com/huggingface/transformers/issues/18511 when using FSDP integration of transformers. ```bash .... load_result = model.load_state_dict(state_dict, strict=False) TypeError: load_state_dict() got an unexpected keyword argument 'strict' ```  Versions Collecting environment information... PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0122genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration: GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX Nvidia driver version: 510.73.08 cuDNN version: Probably one of the following: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda10.2/targets/x86_64linux/lib/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.0 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] Could not collect ",2022-08-08T12:46:53Z,high priority triage review oncall: distributed triaged module: fsdp,closed,0,5,https://github.com/pytorch/pytorch/issues/82963,cc:  varma ,"This seems like a bug in FSDP where we don't propagate kwargs: https://github.com/pytorch/pytorch/blob/a58876ace78df1cfeb136cad592487f34d7e02f1/torch/distributed/fsdp/fully_sharded_data_parallel.pyL2316, so we should be able to have a quick fix from our side. Although, one workaround could be to pass in strict as an arg instead of kwarg: ``` load_result = model.load_state_dict(state_dict, False) ``` which is not great for readability, but would be good if you can try that and see if it works around the issue.","Hello, Thank you for the reply and suggestion for bypassing the issue, as per https://github.com/huggingface/transformers/issues/18511issuecomment1211512941, the above workaround works. As a longterm thing, it would be great to have the FSDP function match the API of PyTorch for better interoperability.",Fixed https://github.com/pytorch/pytorch/pull/83309,I'm still seeing this error in torch 2
rag,"Slice operation on ""ragged"" dimension in NestedTensor"," 🚀 The feature, motivation and pitch  Motivation In preproc we often wants to operates over variablewidth list, such as token ids in text domain, or sparse features in recommendation domain; one common operation is to slice over each list (e.g. only need first k elements). One way is to use Arrow's List type:  ```python >>> import torcharrow as ta >>> id_list = ta.column([[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10]]) >>> id_list 0  [0, 1, 2, 3] 1  [4, 5, 6, 7, 8] 2  [9, 10] dtype: List(int64), length: 3, null_count: 0 >>> id_list.list.slice(stop=3) 0  [0, 1, 2] 1  [4, 5, 6] 2  [9, 10] dtype: List(Int64(nullable=True)), length: 3, null_count: 0 ``` I was thinking nested tensor may also work well for this use case (especially when doing preproc after Tensor collate). But looks like slice is not yet supported on ragged dimension? ```python >>> import torch >>> a, b, c = torch.arange(4), torch.arange(5) + 4, torch.arange(2) + 9 >>> id_list = torch.nested_tensor([a, b, c]) >>> id_list nested_tensor([   tensor([0, 1, 2, 3]),   tensor([4, 5, 6, 7, 8]),   tensor([9, 10]) ]) >>> id_list[:, :3] raceback (most recent call last):   File """", line 1, in  NotImplementedError: Could not run 'aten::slice.Tensor' with arguments from the 'NestedTensorCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::slice.Tensor' is only available for these backends: [CPU, CUDA, HIP, XLA, MPS, IPU, XPU, HPU, VE, Lazy, Meta, PrivateUse1, PrivateUse2, PrivateUse3, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedXLA, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedLazy, QuantizedMeta, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, CustomRNGKeyId, MkldnnCPU, SparseCPU, SparseCUDA, SparseHIP, SparseXLA, SparseMPS, SparseIPU, SparseXPU, SparseHPU, SparseVE, SparseLazy, SparseMeta, SparsePrivateUse1, SparsePrivateUse2, SparsePrivateUse3, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, Batched, VmapMode, PythonTLSSnapshot]. ...... ``` Wondering if there is any plan to support this? Thanks!  Alternatives _No response_  Additional context Variable width data is often modelled as the flattened value and the offset tensor. For the above (simplified 1D) case, one way is to model it as the following internal representation (which is the Arrow Layout, other layout variations exist, such as use the `lengths`): ```python values=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), offsets=tensor([ 0,  4,  9, 11]),  Logically, represent the following variablewidth data:    0  [0, 1, 2, 3]  1  [4, 5, 6, 7, 8]  2  [9, 10]  dtype: List(int64), length: 3 ``` So we kind wants to to a ""batched slice"" over `values` over the ranges `(0, 3), (4, 7), (9, 11)`. The ranges is kind of like `offsets, offsets + 3` (needs to capped by the end of each list. General nD Tensor slice support is more complicated, but the similar idea may still work?  The request originally posted in the NestedTensor repo: https://github.com/pytorch/nestedtensor/issues/473 . But now realized new feature about NestedTensor should be posted in PyTorch repo.  Thanks! ",2022-08-06T04:56:02Z,triaged enhancement module: nestedtensor,open,0,1,https://github.com/pytorch/pytorch/issues/82926, 
transformer,Adding a warning of non-compatibility with forward hooks for the fast path of TransformerEncoderLayer," 📚 The doc issue In TransformerEncoderLayer, it would be helpful if it explicitly points out that naive hooks tend not to work under `fast path`, see this discussion.  The reason for such a notification is that **attention maps** are generally plotted when using Transformers, and hooking is debatably the most direct way to get attention weights.  Suggest a potential alternative/fix Add a warning notifying users that forward hooks might not be compatible with the fast path of `nn.TransformerEncoderLayer`. ",2022-08-05T23:01:59Z,module: nn triaged,open,0,0,https://github.com/pytorch/pytorch/issues/82919
yi,Introducing DependencyViewer for querying Node's depedency relationships,  CC(Introducing DependencyViewer for querying Node's depedency relationships) This PR refactor the dependency map builder from the CapabilityBasedPartitioner into a standalone component.  ,2022-08-05T20:28:15Z,cla signed Stale release notes: fx fx,closed,0,3,https://github.com/pytorch/pytorch/issues/82906,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82906**  * :x: Python docsfailed to build  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit f58aff6b95 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7922882980?check_suite_focus=true) pull / linuxdocs / builddocs (python) (1/1) **Step:** ""Unknown"" (full log | diagnosis details)   20220819T23:16:28.3252550Z [error]The operation was canceled.  ``` 20220819T17:23:21.7342534Z copying images... [ 97%] _static/img/tensorboard/add_images.png 20220819T17:23:21.7344225Z copying images... [100%] _static/img/tensorboard/add_hparam.png 20220819T17:23:21.7345637Z  20220819T17:23:21.7568279Z copying static files... done 20220819T17:23:21.7568519Z copying extra files... done 20220819T17:23:22.1758077Z dumping search index in English (code: en)... done 20220819T17:23:22.2779754Z dumping object inventory... done 20220819T17:23:22.2781863Z build succeeded. 20220819T17:23:22.2782193Z  20220819T17:23:22.2783235Z The HTML pages are in build/html. 20220819T23:16:28.3252550Z [error]The operation was canceled. 20220819T23:16:28.3275356Z Prepare all required actions 20220819T23:16:28.3292484Z [group]Run ./.github/actions/chownworkspace 20220819T23:16:28.3292696Z [endgroup] 20220819T23:16:28.3306352Z [group]Run docker run rm v ""$(pwd)"":/v w /v ""${ALPINE_IMAGE}"" chown R ""$(id u):$(id g)"" . 20220819T23:16:28.3306700Z [36;1mdocker run rm v ""$(pwd)"":/v w /v ""${ALPINE_IMAGE}"" chown R ""$(id u):$(id g)"" .[0m 20220819T23:16:28.3317036Z shell: /usr/bin/bash noprofile norc e o pipefail {0} 20220819T23:16:28.3317240Z env: 20220819T23:16:28.3317474Z   ALPINE_IMAGE: 308535385114.dkr.ecr.useast1.amazonaws.com/tool/alpine 20220819T23:16:28.3317710Z [endgroup] 20220819T23:16:28.3548801Z Unable to find image '308535385114.dkr.ecr.useast1.amazonaws.com/tool/alpine:latest' locally ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
agent,LayerNorm+CUDA+JIT," 🐛 Describe the bug File In an RL workflow, when I use CUDA, JIT, and a LayerNorm together in this script, I get the following error ```python /home/david/Documents/GitHub/cleanoc/cleanrl/ppo_lngru_jit.py:281: UserWarning: FALLBACK path has been taken inside: compileCudaFusionGroup. This is an indication that codegen Failed for some reason. To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback` To report the issue, try enable logging via setting the envvariable ` export PYTORCH_JIT_LOG_LEVEL=manager.cpp`  (Triggered internally at  /opt/conda/condabld/pytorch_1656352657443/work/torch/csrc/jit/codegen/cuda/manager.cpp:237.)   next_gru_state = agent.get_next_state(next_obs, next_gru_state, next_done, prev_actions[step]) /home/david/Documents/GitHub/cleanoc/cleanrl/ppo_lngru_jit.py:281: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason. To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`  (Triggered internally at  /opt/conda/condabld/pytorch_1656352657443/work/torch/csrc/jit/codegen/cuda/manager.cpp:329.)   next_gru_state = agent.get_next_state(next_obs, next_gru_state, next_done, prev_actions[step]) Traceback (most recent call last):   File ""/home/david/Documents/GitHub/cleanoc/cleanrl/ppo_lngru_jit.py"", line 281, in      next_gru_state = agent.get_next_state(next_obs, next_gru_state, next_done, prev_actions[step]) RuntimeError: The following operation failed in the TorchScript interpreter. Traceback of TorchScript (most recent call last): RuntimeError: The following operation failed in the TorchScript interpreter. Traceback of TorchScript (most recent call last): RuntimeError: Expected weight to be of same shape as normalized_shape, but got weight of shape [4, 128] and normalized_shape = [384] ``` Following the advice above (manager.cpp for debugging, disable NVFUSER), I get ```python Traceback (most recent call last):   File ""/home/david/Documents/GitHub/cleanoc/cleanrl/ppo_lngru_jit.py"", line 277, in      next_gru_state = agent.get_next_state(next_obs, next_gru_state, next_done, prev_actions[step]) RuntimeError: Invalid broadcast, number of false entries in is_broadcast_dim expected to be 2 but received 1 ``` This does not occur if: 1. I run on CPU 2. I uncomment any of the print statements inside the LNGRU 3. I turn off the first layer norms 4. I assert that both i_n and i_h have a shape (any shape that tests True) 5. I attempt to reproduce with JUST the LNGRU or even the Agent (I tried making a minimal working example with both but could not reproduce the bug). See below: ```python import torch import torch.nn as nn import torch.nn.functional as F class LNGRUCell(nn.RNNCellBase):     n_preact: torch.jit.Final[bool]     """"""Layernormalized GRU as in https://arxiv.org/pdf/1607.06450.pdf      CC([Feature request]:  add `LayerNormLSTMCell`)issuecomment440485163""""""     def __init__(self, input_size, hidden_size, bias=True, n_preact=True):         super().__init__(input_size, hidden_size, bias, num_chunks=3)         self.n_preact = n_preact         if n_preact:             self.n_ih = nn.LayerNorm(int(3 * self.hidden_size))             self.n_hh = nn.LayerNorm(int(3 * self.hidden_size))         self.n_in = nn.LayerNorm(self.hidden_size)         self.n_hn = nn.LayerNorm(self.hidden_size)          Orthogonal initialization         nn.init.orthogonal_(self.weight_hh, 2 ** 0.5)         nn.init.orthogonal_(self.weight_ih, 2 ** 0.5)         if self.bias:             nn.init.constant_(self.bias_hh, 0)             nn.init.constant_(self.bias_ih, 0)     def forward(self, x, gru_state):         ih = x @ self.weight_ih.T + self.bias_ih         hh = gru_state @ self.weight_hh.T + self.bias_hh         if self.n_preact:   In CUDA, with jit, breaks here             ih = self.n_ih(ih)             hh = self.n_hh(hh)         i_r, i_z, i_n = ih.chunk(3, dim=1)         h_r, h_z, h_n = hh.chunk(3, dim=1)          No idea why I need to do this, but ok...          assert i_n.shape [1] == self.hidden_size          assert h_n.shape [1] == self.hidden_size         i_n = self.n_in(i_n)         h_n = self.n_hn(h_n)         r = torch.sigmoid(i_r + h_r)         z = torch.sigmoid(i_z + h_z)         n = torch.tanh(i_n + r * h_n)         h = (1  z) * n + z * gru_state         return h class LNGRUCell_WithAssert(nn.RNNCellBase):     n_preact: torch.jit.Final[bool]     """"""Layernormalized GRU as in https://arxiv.org/pdf/1607.06450.pdf      CC([Feature request]:  add `LayerNormLSTMCell`)issuecomment440485163""""""     def __init__(self, input_size, hidden_size, bias=True, n_preact=True):         super().__init__(input_size, hidden_size, bias, num_chunks=3)         self.n_preact = n_preact         if n_preact:             self.n_ih = nn.LayerNorm(int(3 * self.hidden_size))             self.n_hh = nn.LayerNorm(int(3 * self.hidden_size))         self.n_in = nn.LayerNorm(self.hidden_size)         self.n_hn = nn.LayerNorm(self.hidden_size)          Orthogonal initialization         nn.init.orthogonal_(self.weight_hh, 2 ** 0.5)         nn.init.orthogonal_(self.weight_ih, 2 ** 0.5)         if self.bias:             nn.init.constant_(self.bias_hh, 0)             nn.init.constant_(self.bias_ih, 0)     def forward(self, x, gru_state):         ih = x @ self.weight_ih.T + self.bias_ih         hh = gru_state @ self.weight_hh.T + self.bias_hh         if self.n_preact:             ih = self.n_ih(ih)             hh = self.n_hh(hh)         i_r, i_z, i_n = ih.chunk(3, dim=1)         h_r, h_z, h_n = hh.chunk(3, dim=1)          No idea why I need to do this, but ok...         assert i_n.shape         assert h_n.shape         i_n = self.n_in(i_n)         h_n = self.n_hn(h_n)         r = torch.sigmoid(i_r + h_r)         z = torch.sigmoid(i_z + h_z)         n = torch.tanh(i_n + r * h_n)         h = (1  z) * n + z * gru_state         return h if __name__ == ""__main__"":     na, batch_size, input_size, hidden_size = 2, 8, 128, 256     class Agent(nn.Module):         num_actions: torch.jit.Final[int]         def __init__(self, with_assert=False, with_preact=True):             super().__init__()             self.num_actions = 2             if with_assert: self.rnn = LNGRUCell_WithAssert(input_size + na, hidden_size, n_preact=with_preact)             self.rnn = LNGRUCell(input_size + na, hidden_size, n_preact=with_preact)         .jit.export         def get_next_state(self, x, gru_state, is_init, prev_action):             return self.rnn(torch.cat([x, F.one_hot(prev_action, self.num_actions)], 1),                             (1.  is_init.unsqueeze(1)) * gru_state)      Input and hidden state     x = torch.ones((batch_size, input_size)); xc = x.to('cuda')     a = torch.randint(2, (batch_size,), dtype=torch.int64); ac = a.to('cuda')     is_init = torch.zeros(batch_size); is_initc = is_init.to('cuda')     h = torch.zeros((batch_size, hidden_size)); hc = h.to('cuda')      Without JIT, cuda and cpu. Works!     cuda_lngru = Agent().to('cuda')     cpu_lngru = Agent()     _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)     _ = cpu_lngru.get_next_state(x, h, is_init, a)      With JIT, cuda and cpu. CPU works! CUDA doesn't...     cuda_lngru = torch.jit.script(Agent().to('cuda'))     cpu_lngru = torch.jit.script(Agent())     _ = cpu_lngru.get_next_state(x, h, is_init, a)     try:         _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)         _ = cuda_lngru.get_next_state(xc, _, is_initc, ac)     except Exception as e:         print(f'CUDA size error: \n{e}')      With JIT and assert, CUDA works     cuda_lngru = torch.jit.script(Agent(with_assert=True).to('cuda'))     _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)     print('Assert works!')      With JIT and without the ""preactivation"" LayerNorm, CUDA works     cuda_lngru = torch.jit.script(Agent(with_preact=False).to('cuda'))     _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)     print('No preactivate works') ```  Versions PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Linux Mint 20.3 (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.9.13  (main, May 27 2022, 16:56:21)  [GCC 10.3.0] (64bit runtime) Python platform: Linux5.4.0122genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.7.64 GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Nvidia driver version: 515.43.04 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] blas                      2.115                       mkl    condaforge [conda] blasdevel                3.9.0            15_linux64_mkl    condaforge [conda] cudatoolkit               11.6.0              hecad31d_10    condaforge [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] libblas                   3.9.0            15_linux64_mkl    condaforge [conda] libcblas                  3.9.0            15_linux64_mkl    condaforge [conda] liblapack                 3.9.0            15_linux64_mkl    condaforge [conda] liblapacke                3.9.0            15_linux64_mkl    condaforge [conda] mkl                       2022.1.0           h84fe81f_915    condaforge [conda] mkldevel                 2022.1.0           ha770c72_916    condaforge [conda] mklinclude               2022.1.0           h84fe81f_915    condaforge [conda] numpy                     1.22.4                   pypi_0    pypi [conda] pytorch                   1.12.0          py3.9_cuda11.6_cudnn8.3.2_0    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torchaudio                0.12.0               py39_cu116    pytorch [conda] torchvision               0.13.0               py39_cu116    pytorch",2022-08-05T16:26:12Z,oncall: jit module: nvfuser,closed,1,13,https://github.com/pytorch/pytorch/issues/82889, does the same error occur with LayerNorm+CUDA but without JIT?,"No, only the combination of all 3, plus whatever's going on in the gist I linked. Sorry I couldn't refine it down to a smaller reproduction script :(. It occurs on the **2nd** step through the network", ,It's probably both TS and nvfuser here. Looks like even fallback path causes failures in layer here. I'll take a better look,Am I supposed to see the exception thrown within the try block?   ``` root:/jiej/playground python repro_82899.py Assert works! No preactivate works ``` Doesn't seem to give me any error. I tried this on our nightlies as well as on 1.12 pytorch container. `pytorch/pytorch:1.12.0cuda11.3cudnn8runtime`.,"To be clear, that code block was me attempting to create a more minimal working example and failing! I put the print statements in prematurely.  I can only provoke the failure using the file linked in the original post or variants like it. Regardless of the training loop, it occurs on the second step through the network, before any actual training has been done, and only when CUDA and JIT are enabled, with no print statements inside the LNGRU",Got it. Have the repro locally. Something went wrong in our fusion pass.. I'm working on it.,I think there's something funny with ConstantChunk here.... We are definitely abusing that and should not have push it for nonpw operations...., so does the issue go away when we turn off nvfuser?,>  so does the issue go away when we turn off nvfuser? It does if I run with `PYTORCH_JIT_USE_NNC_NOT_NVFUSER=1 python repro_82899.py cuda True` Does NNC default not fusing normalizations? Is NNC using the same ConstantChunk optimization?,"ConstantChunk is inserted as part of CanonicalizeOps, which I think will happen either way: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/runtime/profiling_graph_executor_impl.cppL340 Is that the optimization you're talking about or is there another detail ConstantChunk that you're referring to?","I was referring to this https://github.com/pytorch/pytorch/blob/aad3b8e4d3b00f9bd95bc52fd5242dd0f1c43557/torch/csrc/jit/passes/tensorexpr_fuser.cppL485L503 nvfuser doesn't fuse ConstantChunk, instead we rely on this old optimization pass that we stole from legacy fuser: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/graph_fuser.cppL875 I patched it in CC(Limits constant chunk propagation for pwnodeonly), which seems to run the original script without issue. I'll try to reverseengineer a python test for it after local test passes.","FYI, a simpler repro: ``` import torch import torch.nn as nn import torch.nn.functional as F class Cell(nn.Module):     """"""Layernormalized GRU as in https://arxiv.org/pdf/1607.06450.pdf      CC([Feature request]:  add `LayerNormLSTMCell`)issuecomment440485163""""""     def __init__(self):         super().__init__()         self.layer0 = nn.LayerNorm(384)         self.layer1 = nn.LayerNorm(128)     def forward(self, x, y):         ih = self.layer0(x)         i_r, i_z, i_n = ih.chunk(3, dim=1)         i_n = self.layer1(i_n)         r = torch.sigmoid(i_r)         n = torch.tanh(i_n + r * i_z)         h = n + r * y         return h if __name__ == ""__main__"":     with torch.no_grad():          Input and hidden state         x = torch.ones((4, 384)).to('cuda')         h = torch.zeros((4, 128)).to('cuda')         cuda_cell = torch.jit.script(Cell().to('cuda'))         for _ in range(5):             cuda_cell(x, h) ``` I'll put this in a python test."
transformer,Fix profiling with record_shapes=True and nested tensor,"We probably want more systematic fix for other tensorImpls that don't support sizes (and also there are other places in profiler that call sizes unchecked, which should be fixed), but this is to unblock Transformer profiling. ",2022-08-04T23:30:09Z,Merged cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/82854,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82854**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (1 Pending) As of commit 4ccd3b8f94 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Is the goal to actually get shapes out, or just to not break? Because if it's the latter I think we should just change ``` if (t.defined()) { ``` to ``` if (t.defined() && !t.is_nested()) {  // TODO: Handle this better ``` The reason is that we're looking at better support in https://github.com/pytorch/pytorch/pull/81824 (currently being refactored to use the underlying data for better overhead) and then we could handle `SizesStridesPolicy` in a fully robust way. WDYT?","Yeah the goal is to just not break, looking forward to proper support. ","Unit test passes as it should (nested tensors call regular mm underneath, and regular mm has correct sizes), and I'd rather not rip it out because otherwise CC([TorchTidy] Refactor profiler to use SizesAndStrides) will break nested tensor profiling again. ","> Unit test passes as it should (nested tensors call regular mm underneath, and regular mm has correct sizes), and I'd rather not rip it out because otherwise CC([TorchTidy] Refactor profiler to use SizesAndStrides) will break nested tensor profiling again. Gotcha. Thanks for the explanation.", merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,TransformerEncoderLayer fast path errors under `torch.autocast`," 🐛 Describe the bug Pytorch errors when the fast path is taken in `TransformerEncoderLayer` under an `autocast` context. Minimal reproducer below. ```python import torch device = torch.device(""cuda"") x = torch.randn(4, 1, 16).to(device=device) encoder_layer = torch.nn.TransformerEncoderLayer(     d_model=16,     nhead=1,     batch_first=True,     dropout=0.1).to(device=device) encoder_layer.eval() with torch.inference_mode():     with torch.autocast(""cuda"", dtype=torch.float16):         y = encoder_layer(x) ``` Fails with error:  ``` Traceback (most recent call last):   File ""test.py"", line 23, in      main()   File ""test.py"", line 17, in main     y = encoder_layer(x)   File ""/mnt/home/wzhou/miniconda3/envs/pytorch/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/mnt/home/wzhou/miniconda3/envs/pytorch/lib/python3.8/sitepackages/torch/nn/modules/transformer.py"", line 437, in forward     return torch._transformer_encoder_layer_fwd( RuntimeError: expected scalar type Half but found Float ```  Versions Note: I checked that the error happens on colab for both 1.12 release and current 1.13 nightly. Collecting environment information... PyTorch version: 1.13.0.dev20220803+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.5 Libc version: glibc2.26 Python version: 3.7.13 (default, Apr 24 2022, 01:04:09)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.188+x86_64withUbuntu18.04bionic Is CUDA available: True CUDA runtime version: 11.1.105 GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 460.32.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.0.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.13.0.dev20220803+cu102 [pip3] torchaudio==0.12.0+cu113 [pip3] torchsummary==1.5.1 [pip3] torchtext==0.13.0 [pip3] torchvision==0.13.0+cu113 [conda] Could not collect ",2022-08-04T00:06:46Z,,closed,2,2,https://github.com/pytorch/pytorch/issues/82783, ,Closing as a duplicate of  CC(Runtime Error raised by `torch._native_multi_head_attention` working with `torch.cuda.amp.autocast`). Fix in progress https://github.com/pytorch/pytorch/pull/84722.
rag,[NestedTensor]Remove tensor buffer replace with Storage type, Description In order to enable NestedTensor views NestedTensorImpl no longer stores its data in a at::Tensor buffer_ instead it conforms to the practice of most TensorImpls and uses a Storage class. This change will enable NestedTensor to use the view constructor defined on the base TensorImpl.  Issue CC(Swap Nested Tensor buffer_ with a buffer_ of type Storage.)  Testing  The existing nested_tensor tests are utilized since this is core functionality and would break these tests if not successful.  Performance One change that has potentially large performance impact is that most nested_tensor kernels call `get_buffer` to get the buffer in Tensor form and perform ops on this buffer. Previously this was free since we stored the data as a Tensor but now each kernel must construct a Tensor from the storage. The most performance critical/heavy user of nested tensors is BetterTransformer. I would be curious to see if this change significantly impacts performance for this and other workloads.,2022-08-03T21:32:28Z,module: nestedtensor Merged cla signed ciflow/trunk release notes: nested tensor,closed,0,5,https://github.com/pytorch/pytorch/issues/82757,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82757**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (8 Pending) As of commit b493a2dbb1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," Benchmarking Linear with a an nt of nested size [[1,1]] and a weight of size [1,1] in order to profile new overhead of get_buffer() ``` The blocked_autorange output for running this for 15 seconds on master before the change to NestedTensorImpl: Minimal nt linear  Dtype:torch.float16, device:cuda   Median: 30.40 us   IQR:    2.11 us (29.75 to 31.86)   477590 measurements, 1 runs per measurement, 48 threads Minimal nt linear  Dtype:torch.float32, Device:cpu   Median: 9.45 us   IQR:    0.24 us (9.41 to 9.65)   157 measurements, 10000 runs per measurement, 48 threads ``` ``` The blocked_autorange output for running this for 15 seconds after the change to NestedTensorImpl adding storage: Minimal nt linear  Dtype:torch.float16, device:cuda   Median: 29.95 us   IQR:    2.15 us (29.49 to 31.64)   481336 measurements, 1 runs per measurement, 48 threads Minimal nt linear  Dtype:torch.float32, Device:cpu   Median: 9.49 us   IQR:    0.43 us (9.45 to 9.88)   157 measurements, 10000 runs per measurement, 48 threads ```", merge l, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Add Mask Identifier for Better Transformer Fast-Path,Differential Revision: D38398590,2022-08-03T19:13:09Z,fb-exported cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/82747,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82747**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures, 5 Pending As of commit 96d6dca58f (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7661996109?check_suite_focus=true) pull / linuxbioniccuda11.6py3.10gcc7 / test (default, 2, 4, linux.4xlarge.nvidia.gpu) (1/1) **Step:** ""Test"" (full log | diagnosis details)   20220803T23:36:23.2734647Z FAIL [0.026s]: tes...tal_tree_cuda_detailed (__main__.TestProfilerTree)  ``` 20220803T23:36:23.2395033Z ok (0.017s) 20220803T23:36:23.2425808Z   test_profiler_experimental_tree_with_stack_and_modules (__main__.TestProfilerTree) ... STAGE:20220803 23:36:23 14858:14858 ActivityProfilerController.cpp:294] Completed Stage: Warm Up 20220803T23:36:23.2447413Z STAGE:20220803 23:36:23 14858:14858 ActivityProfilerController.cpp:300] Completed Stage: Collection 20220803T23:36:23.2531204Z STAGE:20220803 23:36:23 14858:14858 ActivityProfilerController.cpp:294] Completed Stage: Warm Up 20220803T23:36:23.2548716Z STAGE:20220803 23:36:23 14858:14858 ActivityProfilerController.cpp:300] Completed Stage: Collection 20220803T23:36:23.2632320Z STAGE:20220803 23:36:23 14858:14858 ActivityProfilerController.cpp:294] Completed Stage: Warm Up 20220803T23:36:23.2651578Z STAGE:20220803 23:36:23 14858:14858 ActivityProfilerController.cpp:300] Completed Stage: Collection 20220803T23:36:23.2732740Z ok (0.034s) 20220803T23:36:23.2733652Z  20220803T23:36:23.2733925Z ====================================================================== 20220803T23:36:23.2734647Z FAIL [0.026s]: test_profiler_experimental_tree_cuda_detailed (__main__.TestProfilerTree) 20220803T23:36:23.2735495Z  20220803T23:36:23.2736345Z Traceback (most recent call last): 20220803T23:36:23.2737205Z   File ""/var/lib/jenkins/workspace/test/test_profiler_tree.py"", line 55, in begin_unit_test_marker 20220803T23:36:23.2737918Z     out = f(self) 20220803T23:36:23.2738738Z   File ""/var/lib/jenkins/workspace/test/test_profiler_tree.py"", line 785, in test_profiler_experimental_tree_cuda_detailed 20220803T23:36:23.2739445Z     self.assertTreesMatch( 20220803T23:36:23.2740446Z   File ""/var/lib/jenkins/workspace/test/test_profiler_tree.py"", line 189, in assertTreesMatch 20220803T23:36:23.2741516Z     self.assertExpectedInline(actual, expected, skip=1) 20220803T23:36:23.2742106Z   File ""/opt/conda/lib/python3.10/sitepackages/expecttest/__init__.py"", line 262, in assertExpectedInline 20220803T23:36:23.2742581Z     self.assertMultiLineEqualMaybeCppStack(expect, actual, msg=help_text) ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D38398590,This pull request was **exported** from Phabricator. Differential Revision: D38398590,This pull request was **exported** from Phabricator. Differential Revision: D38398590
transformer,[Apple Silicon M1 MPS device] bad performance metrics for BERT model training," 🐛 Describe the bug When using `mps` device, BERT finetuning on MRPC task leads to bad performance metrics in comparison to CPU training. Also, speedup is only ~30% when compared to CPU training.  Steps to reproduce 1. Code is given below. The is run on a Mac Pro with M1 chip having 8 CPU performance cores (+2 efficiency cores), 14 GPU cores and 16GB of unified memory. ```python import argparse import torch from torch.optim import AdamW from torch.utils.data import DataLoader import evaluate from accelerate import Accelerator, DistributedType from datasets import load_dataset from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed MAX_GPU_BATCH_SIZE = 16 EVAL_BATCH_SIZE = 32 def get_dataloaders(batch_size: int = 16):     """"""     Creates a set of `DataLoader`s for the `glue` dataset,     using ""bertbasecased"" as the tokenizer.     Args:         batch_size (`int`, *optional*):             The batch size for the train and validation DataLoaders.     """"""     tokenizer = AutoTokenizer.from_pretrained(""bertbasecased"")     datasets = load_dataset(""glue"", ""mrpc"")     def tokenize_function(examples):          max_length=None => use the model max length (it's actually the default)         outputs = tokenizer(examples[""sentence1""], examples[""sentence2""], truncation=True, max_length=None)         return outputs     tokenized_datasets = datasets.map(         tokenize_function,         batched=True,         remove_columns=[""idx"", ""sentence1"", ""sentence2""],     )      We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the      transformers library     tokenized_datasets = tokenized_datasets.rename_column(""label"", ""labels"")     def collate_fn(examples):         return tokenizer.pad(examples, padding=""longest"", return_tensors=""pt"")      Instantiate dataloaders.     train_dataloader = DataLoader(         tokenized_datasets[""train""], shuffle=True, collate_fn=collate_fn, batch_size=batch_size     )     eval_dataloader = DataLoader(         tokenized_datasets[""validation""], shuffle=False, collate_fn=collate_fn, batch_size=EVAL_BATCH_SIZE     )     return train_dataloader, eval_dataloader def training_function(config, device):      Sample hyperparameters for learning rate, batch size, seed and a few other HPs     lr = config[""lr""]     num_epochs = int(config[""num_epochs""])     seed = int(config[""seed""])     batch_size = int(config[""batch_size""])     metric = evaluate.load(""glue"", ""mrpc"")      If the batch size is too big we use gradient accumulation     gradient_accumulation_steps = 1     set_seed(seed)     train_dataloader, eval_dataloader = get_dataloaders(batch_size)     model = AutoModelForSequenceClassification.from_pretrained(""bertbasecased"", return_dict=True)     model = model.to(device)      Instantiate optimizer     optimizer = AdamW(params=model.parameters(), lr=lr)      Instantiate scheduler     lr_scheduler = get_linear_schedule_with_warmup(         optimizer=optimizer,         num_warmup_steps=100,         num_training_steps=(len(train_dataloader) * num_epochs) // gradient_accumulation_steps,     )     for epoch in range(num_epochs):         model.train()         for step, batch in enumerate(train_dataloader):             batch.to(device)             outputs = model(**batch)             loss = outputs.loss             loss = loss / gradient_accumulation_steps             loss.backward()             if step % gradient_accumulation_steps == 0:                 optimizer.step()                 lr_scheduler.step()                 optimizer.zero_grad()         model.eval()         samples_seen = 0         for step, batch in enumerate(eval_dataloader):             batch.to(device)             with torch.no_grad():                 outputs = model(**batch)             predictions = outputs.logits.argmax(dim=1)             metric.add_batch(                 predictions=predictions,                 references=batch[""labels""],             )         eval_metric = metric.compute()          Use accelerator.print to print only on the main process.         print(f""epoch {epoch}:"", eval_metric) def main(device):     config = {""lr"": 2e5, ""num_epochs"": 3, ""seed"": 42, ""batch_size"": 16}     training_function(config, device) from time import time for device in [""cpu"", ""mps""]:     device = torch.device(device)     print(device)     start_time = time()     main(device)     end_time = time()     print(f""time taken in seconds for training using device {device}: {end_timestart_time}"") ``` 2. The output logs: ```bash CPU ... epoch 0: {'accuracy': 0.7696078431372549, 'f1': 0.8540372670807453} epoch 1: {'accuracy': 0.8357843137254902, 'f1': 0.8858603066439524} epoch 2: {'accuracy': 0.8480392156862745, 'f1': 0.8934707903780068} time taken in seconds for training using device cpu: 1001.8208646774292 mps ... epoch 0: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079} epoch 1: {'accuracy': 0.7058823529411765, 'f1': 0.8214285714285715} epoch 2: {'accuracy': 0.6985294117647058, 'f1': 0.8183161004431315} time taken in seconds for training using device mps: 771.7576680183411 ```  Expected behaviour: Same performance metrics when using `mps` device in comparison to `cpu` training.  Versions Collecting environment information... PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.5 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.10.5 (v3.10.5:f377153967, Jun  6 2022, 12:36:10) [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS12.5arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.1 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] Could not collect ",2022-08-03T08:19:42Z,triaged module: correctness (silent) module: mps,closed,0,17,https://github.com/pytorch/pytorch/issues/82707,"This is related to  CC(MPS device appears much slower than CPU on M1 Mac Pro). I suspect it's because of overhead of using `MPSGraph` for everything. On the Apple M1 Max, there is:  10 µs overhead to create a new `MTLCommandBuffer` for each op  15 µs overhead to encode the `MPSGraph` for each op, if it's already compiled into an `MPSGraphExecutable`. This doesn't change even if you put multiple ops into the `MPSGraph`; overhead just becomes (15 µs) x (ops)  100 µs (I need to recreate the benchmark that proved this) overhead to encode an `MPSGraph` if it isn't already compiled into an `MPSGraphExecutable` That means 25  110 µs of overhead for a single operation, like the `negate` operator. Multiply that by 100s of thousands, and you get many, many milliseconds of extra execution time. This overhead can be avoided and brought down to 0.5 µs (encode time) / 3.0 µs (GPU sequential throughput), but you must use custom Metal shaders for trivial ops instead of a separate `MPSGraph` instance for each op. On the CPU, everything is instantaneous. Overhead of calling into something might be 25  110 _nanoseconds_, not 25  110 _microseconds_. RNNs like BERT have smaller tensors and more sequential in nature, so they often run better on CPUs  which have smaller vector width and higher clock speed. Perhaps this would not be true if a GPU had overhead of  For a longer explanation, you might be interested in this PyTorch Forums post. Since making the post, I have not only fused multiple unary operations into a single shader dispatch  I also fused binary/ternary operations and created a fullfledged JIT graph compiler. Unlike XLA, this has such low overhead that it takes *less* time to compile than to execute once. It can recompile the same set of ops countless times, much like how OoOE happens on a modern CPU. But it's limited to elementwise ops, which are the most bandwidthintensive kind of op. Matrix multiplications happen outside of the graph. TL;DR  I hypothesize that BERT could run 10100 times faster on the M1 GPU if certain optimizations are made.",Speedup is important but I am more concerned about performance metrics `accuracy` and `F1` scores. Performance drop of 17% in `Accuracy` and 9% in `F1` score compared to CPU training when there are no code changes expect for using `mps` device.,"For more context, inference of a trained/finetuned transformer model seems to give expected performance metrics. Hence, training is going haywire while using `mps` device.","Is it possible to unravel the BERT model's layers into a sequence of primitive operations? Then, perhaps I could replicate the results from scratch using the `MPSGraph` framework. I'm using an M1family Mac as well, so numerical precision should be exactly the same. I have noticed that CPU and GPU return slightly different values for activation functions. They're the same within a factor of (1 + 1e5), but that means transcendental functionals may behave differently. In the Metal Standard Library, there are often two variants of complex math functions. One is in the `fast::` namespace, and it's fast and less precise. The other is in the `precise::` namespace, and it's slower but more precise. I suspect that `MPSGraph` uses lowerprecision versions of complex math functions, which don't handle infinities and denormals.", could you please try latest pytorch nightly (such as 1.13.0.dev20220805) and let me know if you are still seeing the issue? (`pip3 install forcereinstall pre torch extraindexurl https://download.pytorch.org/whl/nightly/cpu` will bring the latest nightly version),"Thank you  for the detailed insights into what might be happening behind the scenes.  Thank you  for the suggestion to test out `mps` support using PyTorch nightly. Observations: 1. With PyTorch nightly, the performance is similar (same for the first 2 decimal points) (0.3% F1 drop and 0.6% Accuracy drop) as seen below.  **Therefore, model correctness/performance metrics seem to be resolved.** 2.  We can also observe **~60%** speedup compared to the ~30% speedup from the torch 1.12.0 version. However, this is nowhere near the 10X eval speedup for Bert mentioned in the blog Introducing Accelerated PyTorch Training on Mac | PyTorch. To provide more context, I observed a speedup of ~7.5X over CPU for the task of image classification using ResNet which is in line with the blog post. **Therefore, now the pressing question is of performance/speedup for transformerbased models.** ```bash cpu epoch 0: {'accuracy': 0.7696078431372549, 'f1': 0.8540372670807453} epoch 1: {'accuracy': 0.8357843137254902, 'f1': 0.8858603066439524} epoch 2: {'accuracy': 0.8480392156862745, 'f1': 0.8934707903780068} time taken in seconds for training using device cpu: 982.4634261131287 ... mps epoch 0: {'accuracy': 0.7475490196078431, 'f1': 0.8422664624808577} epoch 1: {'accuracy': 0.8431372549019608, 'f1': 0.8810408921933086} epoch 2: {'accuracy': 0.8431372549019608, 'f1': 0.8900343642611684} time taken in seconds for training using device mps: 608.2880346775055 ```","> **Therefore, now the pressing question is of performance/speedup for transformerbased models.** I've been waiting for this chance for a very long time. I wanted a realworld use case (specifically an RNN) to showcase measurable speedups from reducing driverside overhead. If you could humor me, I'd like to research the performance regression by benchmarking PyTorch against the rebooted S4TF. Please let me know whether you're interested in helping with this investigation.  Stage 1 My custom backend isn't yet in the state where it can test this BERT model. There's still quite a bit of work and debugging to complete. However, lowering it down to `MPSGraph` is a good start. This is something called narrowing a bug  eliding as much context as possible, to remove confounding variables and examine only the root cause. Despite creating ML frameworks, I'm not an expert at using ML frameworks. I have never used S4TF, PyTorch, or TensorFlow to train an actual model, except when running a precreated tutorial notebook on SwiftColab. I care more about making the software accessible to other people, who want to do training. So I may need help understanding or extracting the model's size parameters, sequence of raw ops, etc.   Stage 2 Investigate ways to run the model faster than one `MPSGraph` per op. Try fusing hotpaths or creating custom Metal shaders, just for demonstration. These emulate the final product of my prototype Metal backend, and may signal whether my 10100x speedup hypothesis is reasonable. Then, see whether such optimizations are possible in PyTorch. Odds are, they aren't. There's something unique about Swift, how it handles ARC and does automatic differentiation in the language compiler. There's also the high maintenance cost of my highly tuned command stream + memory management. It requires writing _lots_ of lowlevel code in a safe, productive programming language. It's counterproductive to translate it into C++, or worse, ObjectiveC (Swift's predecessor). Still, this investigation could reveal limitations of PyTorch's MPS backend, including _reproducible_ evidence of why MPS is sometimes slower than the CPU. ","> > **Therefore, now the pressing question is of performance/speedup for transformerbased models.** >  > I've been waiting for this chance for a very long time.  Apologies if you all know and have digested this a while back but related to speedup of Transformers and RNNs on Apple devices, specifically ANE, there are lessons and code to be learned in two links below. Obviously that is yetanotherbackend compared to CPU and MPS+GPU. The data layout discussion might be relevant for MPS speedup while the 4D format for convolutions seems very ANE specific. Just some additional background info for Transformer speedup on devices. https://github.com/apple/mlanetransformers https://machinelearning.apple.com/research/neuralenginetransformers","> don't handle infinities and denormals I am getting **infinite loss** in my project when enabling **MPS**, which never occurs in either CPU or CUDA. Can this be related to what  describes? It is a bit hard to isolate the issue, unfortunately. ","The project I'm making always uses the `precise::` namespace. This isn't currently possible in PyTorch, which uses `MPSGraph` for activations. The biggest bottleneck is memory bandwidth, not ALU time  clock cycles are basically free. There's no reason to avoid `precise::`.   could you show some of the source code that's causing infinite loss? If so, I might be able to trace back the culprit and show how using `precise::` would change results.","The full project is not open source, however the loss function is available here: https://github.com/alin256/multimodepredictionwithmtploss/blob/09b1ba9a7fb9e3d19528c230d63c8d89f5bb8895/mtp_loss.pyL39 From what I am getting from the testing, it actually seems that the initialization of the neural network causes the problem: ``` prediction tensor(7.8854e+29, device='mps:0', grad_fn=) ``` It is really weird that neither 'CPU' nor 'CUDA' versions get this error. I am wondering if the initialization of ANN is different for MPS. The full output that I get printing the mean for every line: ``` data flat tensor(0.0006, device='mps:0') data flat size 32 prediction tensor(7.8854e+29, device='mps:0', grad_fn=) unscaled norm tensor(9.5908e+32, device='mps:0', grad_fn=) scaled norm tensor(2.9971e+31, device='mps:0', grad_fn=) distances tensor(2.9971e+31, device='mps:0', grad_fn=) prob raw tensor(7.4528e+30, device='mps:0', grad_fn=) log prob tensor(inf, device='mps:0', grad_fn=) prob contrib tensor(inf, device='mps:0', grad_fn=) norm contrib tensor(2.7200e+31, device='mps:0', grad_fn=) mtp loss tensor(inf, device='mps:0', grad_fn=) data flat tensor(0.0008, device='mps:0') ```","I looked deeper, and something really dodgy was going on: My input tensors' means on GPU are nans ```             data_input1, data_input2, drill_traj, data_output = data             print(""inputs cpu"")             print(torch.mean(data_input1.float()))             print(torch.mean(data_input2.float()))             print(torch.mean(drill_traj.float()))             print(""inputs gpu"")             data_input1 = data_input1.float().to(device=device, non_blocking=True)             data_input2 = data_input2.float().to(device=device, non_blocking=True)             data_input3_drill = drill_traj.float().to(device=device, non_blocking=True)             data_output = data_output.float().to(device=device, non_blocking=True) ``` ``` inputs cpu tensor(0.1111) tensor(0.1129) tensor(1.0027) inputs gpu /opt/anaconda3/envs/multimodalm1/lib/python3.8/sitepackages/torch/_tensor_str.py:103: UserWarning: The operator 'aten::bitwise_and.Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/_temp/anaconda/condabld/pytorch_1659484780698/work/aten/src/ATen/mps/MPSFallback.mm:11.)   nonzero_finite_vals = torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)) tensor(0.1111, device='mps:0') tensor(nan, device='mps:0') tensor(nan, device='mps:0') ```","Could you extract the raw data of one of those tensors before computing the mean? We can start by putting it into a calculator or simple script and validating that the CPU has it right. Understanding what values trigger the `NAN` will help investigate the GPU problems. I'm hoping for the three raw data's that cause: ``` tensor(0.1111) tensor(0.1129) vs. tensor(nan) tensor(1.0027) vs. tensor(nan) ``` Use a ` ` if the data is quite large, otherwise you will flood the GitHub thread.","> 1. With PyTorch nightly, the performance is similar (same for the first 2 decimal points) (0.3% F1 drop and 0.6% Accuracy drop) as seen below.  **Therefore, model correctness/performance metrics seem to be resolved.** > 2. We can also observe **~60%** speedup compared to the ~30% speedup from the torch 1.12.0 version. However, this is nowhere near the 10X eval speedup for Bert mentioned in the blog Introducing Accelerated PyTorch Training on Mac | PyTorch. To provide more context, I observed a speedup of ~7.5X over CPU for the task of image classification using ResNet which is in line with the blog post. **Therefore, now the pressing question is of performance/speedup for transformerbased models.** Thanks  for trying out the torchnightly. I will consider the Correctness issue fixed with latest torch nightly. For the performance I would like to provide more context. The numbers in the Blog post were collected on M1 Ultra machine which has 4x the number of GPU cores and 128GB of unified memory. Also the benchmark used was the `torchbench` benchmark `hf_Bert` model at the specified Batch sizes.   We are continuing to make perf improvements so stay tuned on that.  ","> > don't handle infinities and denormals >  > I am getting **infinite loss** in my project when enabling **MPS**, which never occurs in either CPU or CUDA. Can this be related to what  describes? It is a bit hard to isolate the issue, unfortunately.  , can you please file a separate issue on this and we will take a look. Thanks!",">  we were still discussing performance issues on this thread, and a potential investigation. I don't think it's correct to mark it as closed on GitHub. The bug was tracking the `accuracy` and `f1` performance metrics in the BERT network, which was addressed. The other point on Performance was raised about not matching the Blogpost which I provided more clarification. If this is not satisfactory  , feel free to reopen the issue. About discussing the performance issues on this thread, I would appreciate we create a specific issue and track it there. This issue is getting conflated with parallel threads. ","(Bumping up this old bug, let me know if there's a better place) I'm also noticing that mps produces consistently lower results than what model is trained on cpu. However, the results are much smaller than OP's original scenar, are are usually maybe .02 lower. There's an argument for this to be within the room for error, however I'm noticing this across runs extremely consistently (failed to see one where mps converges on a higher f1 score) Mps  epoch 0: {'accuracy': 0.7769607843137255, 'f1': 0.8330275229357799} epoch 1: {'accuracy': 0.8406862745098039, 'f1': 0.8873483535528597} epoch 2: {'accuracy': 0.8382352941176471, 'f1': 0.8862068965517241} time taken in seconds for training using device mps: 576.6726970672607 cpu epoch 0: {'accuracy': 0.7573529411764706, 'f1': 0.8070175438596492} epoch 1: {'accuracy': 0.8504901960784313, 'f1': 0.8924162257495591} epoch 2: {'accuracy': 0.8676470588235294, 'f1': 0.9049295774647886} time taken in seconds for training using device cpu: 1144.207025051117"
rag,Swap Nested Tensor buffer_ with a buffer_ of type Storage., Summary Currently Nested Tensors store their data on an `const at::Tensor buffer_` . This poses a problem though when constructing a view of a NestedTensor most of the view machinery is expecting a Storage storage_. This issues is to track the changes needed to update NestedTensors storage.   PRs  Issues ,2022-08-02T21:01:44Z,triaged module: nestedtensor,closed,0,0,https://github.com/pytorch/pytorch/issues/82671
yi,Use enable_tracing flag for ProxyTorchDispatchMode instead of modifying torch dispatch mode stack inner attributes, Description This PR removes fiddling with the mode stack using copies and ExitStack in favor of a simpler and more straightforward approach.  Issue https://github.com/pytorch/pytorch/pull/81764discussion_r934917799  Testing No new tests are needed.,2022-08-02T11:34:09Z,open source better-engineering Merged cla signed fx,closed,0,10,https://github.com/pytorch/pytorch/issues/82643,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82643**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (4 Pending) As of commit 9d7de52326 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) pull failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2784245585, merge g, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) pull failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2788006900, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[WIP] TP + FSDP Integration change,"  CC([WIP] TP + FSDP Integration change) This is PR or diff is not aiming for land now. Context: We want to have FSDP + TP 2D Parallel enabled for transformer based model. Original PR: https://github.com/pytorch/pytorch/pull/82166 Andrew's Rebased commit: https://github.com/awgu/pytorch/commit/5b7ef1a6ea2e802fb6f1276baea6a030090320f4 This PR includes the use of Distributed Tensor. Down the road, we can discuss whether we only want to enable it for distributed tensor or not. For now, I just keep the logic both for sharded tensor and distributed tensor. Will update a unit test for TP + FSDP using DT soon. Differential Revision: D38266184",2022-08-01T17:10:06Z,oncall: distributed cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/82581,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82581**  * :x: Python docsfailed to build  * :x: C++ docsfailed to build  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 15 New Failures, 4 Pending As of commit 1f1caee096 (more details on the Dr. CI page): Expand to see more  * **15/15** failures introduced in this PR   :detective: 15 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7680998044?check_suite_focus=true) pull / linuxdocs / builddocs (cpp) (1/15) **Step:** ""Unknown"" (full log  diagnosis details)   20220804T22:26:48.8672507Z NameError: name 'DeviceMesh' is not defined  ``` 20220804T22:26:48.8667635Z   File ""/opt/conda/lib/python3.10/sitepackages/torch/distributed/algorithms/_optimizer_overlap/__init__.py"", line 1, in  20220804T22:26:48.8668096Z     from .optimizer_overlap import _as_overlapped_optim 20220804T22:26:48.8668686Z   File ""/opt/conda/lib/python3.10/sitepackages/torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py"", line 5, in  20220804T22:26:48.8669204Z     from torch.distributed.fsdp import FullyShardedDataParallel 20220804T22:26:48.8669761Z   File ""/opt/conda/lib/python3.10/sitepackages/torch/distributed/fsdp/__init__.py"", line 1, in  20220804T22:26:48.8670263Z     from .flat_param import FlatParameter 20220804T22:26:48.8670810Z   File ""/opt/conda/lib/python3.10/sitepackages/torch/distributed/fsdp/flat_param.py"", line 84, in  20220804T22:26:48.8671216Z     class STShardingInfo(NamedTuple): 20220804T22:26:48.8671744Z   File ""/opt/conda/lib/python3.10/sitepackages/torch/distributed/fsdp/flat_param.py"", line 90, in STShardingInfo 20220804T22:26:48.8672136Z     device_mesh: Optional[DeviceMesh] 20220804T22:26:48.8672507Z NameError: name 'DeviceMesh' is not defined 20220804T22:26:48.8672702Z  20220804T22:26:48.8672968Z  20220804T22:26:48.8673282Z Ran 5 tests in 1.630s 20220804T22:26:48.8673445Z  20220804T22:26:48.8673586Z FAILED (errors=1, expected failures=3) 20220804T22:26:48.8673783Z  20220804T22:26:48.8673909Z Generating XML reports... 20220804T22:26:48.8703382Z Generated XML report: testreports/pythonunittest/test_public_bindings/TESTTestPublicBindings20220804222647.xml 20220804T22:26:49.1910017Z Traceback (most recent call last): 20220804T22:26:49.1910821Z   File ""/var/lib/jenkins/workspace/test/run_test.py"", line 974, in  ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: fduwjj / name: Hugo  (6963562bf2d20aa13bdd752d98bb5e3343415e62, 1f1caee096ff66815aa1a9b93937825382c9b14c)",Not needed.
transformer,RFC: Add flag for RNN decomposition to all RNN modules,"**tl;dr** The basic proposal here is to add a flag to RNN (and subclasses like GRU or LSTM) where instead of running the RNN kernel, it will run the linear, dropout, etc. calls that create an equivalent decomposition. Without this, the monolithic rnn functions and buffers returned from the _cudnn_rnn function make it difficult to extend RNNs in the cases of extending RNNs, computing per sample gradients, and AOTAutograd. The proposed API adds a flag to the RNN class that determines whether or not to use the decomposed version, which will be defaulted to False in order to not incur perf penalties  Problem The basic problem with the RNN kernel in particular is that the cuda versions pass around buffers that are used during the backward computation. This is particularly problematic when someone wants to use a custom derivative for the RNN since CUDA doesn't have any stability guarantees for what is being passed back in the buffers. Therefore, a developer cannot even try to recompute the intermediate values and pass those to CUDA's RNN backwards function and hope to produce correct results.  Use Cases  RNN Experimentation For a long time (even since issue 1932)), people have been asking for ways to adapt RNNs. Some of the asks include using layer norm) as the activation to having different hidden sizes per layer). Although RNNs have somewhat fallen out of style with the rise of transformers, new research on them is still hitting ICML. Right now, everything exists in monolithic kernels (like rnn_tanh and rnn_relu) that are performant but make it difficult to understand what's happening. Although a user could write the same decomposition we plan to in Python, there's so many flags to an RNN that make it difficult to know if you've implemented the decomposition correctly. Having a deomposed Python version that we know works correctly will let users experiment with these new versions easily  Expanded Weights and per sample gradients These kernels are also problematic for Expanded Weights, our new system for computing per sample gradients. The mechanism behind this uses torch function and autograd.Function since we need to change the autograd behavior. In doing this, we also need to recompute the batched gradient with respect to the input. So, we would need to decide which backwards to use and then pass the correct byffers if we're using _cudnn_rnn_backward. As mentioned, this won't work because NVIDIA doesn't guarantee that the values in the buffers will be consistent between versions. To work around this, libraries that want to support RNNs while computing per sample gradients like Opacus have hacky solutions that we shouldn't copy upstream. Specifically, they implement RNNs as two linear layers, which gets them the correct behavior. However, in order to make it exportable, they reset the names so that it looks like it's an RNN module. More concretely, a vanilla Pytorch RNN may have a weight with a ""weight_hh_l0"". An Opacus version of this would be decomposed into multiple Linear layers where the equivalent parameter should have the name ""l0.hh.weight"". In order to make their models save and loadable, they patch it to have the same name as the vanilla PyTorch RNN. However, we should not be copying this hack upstream since it breaks mechanisms like make_stateless that assumes that the name of the weights follows the structure of the nn.Module.  AOTAutograd AOTAutograd has mentioned that they've noticed these functions show up in traces. Although they are able to support the current mechanism, having a decomposition can help support backends that don't have an RNN kernel and allow for custom optimizations for different backends. This would also fix https://github.com/pytorch/functorch/issues/586, which is an issue that stems from LSTMs not properly forwarding the `requires_grad_`ness of its weights through to the `_cudnn_rnn` kernel  Proposed API Our proposal is to add a flag to the RNN module that determines whether to use the decomposed version or the RNN kernel like before. By keeping this flag off by default, users should not see any changes from the original behavior. User should be able to determine this while building the RNN while also toggle the flag without rebuilding their RNN, similar to a training flag. Unlike the training flag, we should be able to set this on a layerbylayer basis instead of only at a whole model level. `net = RNN(input_size, hidden_size, num_layers=4, use_decomposition=True)` `net.set_decomposition_(False)`  Perf Concerns The decompositions written in Python will have worse perf than the custom C++ kernels. First, these decompositions will be necesssary for backends that don't have a custom RNN kernel, as noted in the AOTAutograd section. Additionally, systems like Opacus that currently require this decomposition to do their per sample gradient computation currently pay this cost. So, we will not be worsening their perf metrics. Finally, with incoming systems like torchdynamo, we can hope to recover some of this performance for backends that have custom implemented kernels. Until then, by leaving the flag as the default of `use_decomposition=False`, we should not see any performance hits for users that do not use this flag.  Alternatives  `__torch_dispatch__` Decomposition One alternate would be to implement the decomposition at the torch dispatch level, like the decompositions that AOTAutograd use. Since users will still be able to use the undecomposed version, AOTAutograd will still need a torch dispatch decomposition and we will probably want to even use the same decomposition. The only issue is if we only have a torch dispatch decomposition. Since Expanded Weights needs to be at the torch function level in order to extend autograd, it won't work to have the decomposition only exist at the torch dispatch level  Add a `__torch_function__` for RNN Currently, RNN doesn't have a functional version nor a torch function intercept. One argument may be to add these and then have a user intercept the RNN at that level and decompose if necessary. Given how monolithic the RNN kernels are, we won't be able to decompose it much between the module's forward call and this call. So if we just added this extension point, we end up with the same issue where a user could decompose the function themselves but runs a lot of risk of implementing it incorrectly. Additionally, this is BC breaking since we're intercepting torch function calls where we weren't before.  Additional context _No response_ ",2022-08-01T16:31:07Z,feature module: rnn triaged,open,0,4,https://github.com/pytorch/pytorch/issues/82577,"> Expanded Weights and per sample gradients The problem also extends to things like functorch transforms, right? We can't write an efficient batching rule for cudnn_rnn_forward, because who knows what it is saving into the buffers it gets passed. > AOTAutograd The AOTAutograd example is interesting. There's a question of if the user should pass `use_decomposition=True` before using AOTAutograd. If we want ""dynamo on by default"", then the user shouldn't need to change their code. This means that somewhere, AOTAutograd needs to intercept cudnn_rnn_forward and decompose it and we would want to ensure that the cudnn_rnn_backward call doesn't make it to the AOTAutograd graph (because that is not something we know how to decompose).","> This means that somewhere, AOTAutograd needs to intercept cudnn_rnn_forward and decompose it and we would want to ensure that the cudnn_rnn_backward call doesn't make it to the AOTAutograd graph (because that is not something we know how to decompose). Is this not possible?  It does seem ideal if users don't have to change their code, like with our other decompositions.","> > This means that somewhere, AOTAutograd needs to intercept cudnn_rnn_forward and decompose it and we would want to ensure that the cudnn_rnn_backward call doesn't make it to the AOTAutograd graph (because that is not something we know how to decompose). >  > Is this not possible? It does seem ideal if users don't have to change their code, like with our other decompositions. We can't really do this with torch_dispatch mode today (because that happens after autograd), but we could do it with a torch_function mode, or figure out how to get a torch_dispatch mode to occur before autograd.",.Module side
transformer,Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment," 🐛 Describe the bug Hi there. i run my code on Colab. i want to qunatize my **Wav2Vec** model but i got error. Code: ``` import torch from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer tokenizer = Wav2Vec2Tokenizer.from_pretrained(""facebook/wav2vec2base960h"") model = Wav2Vec2ForCTC.from_pretrained(""facebook/wav2vec2base960h"") input_values = tokenizer(audio, return_tensors = ""pt"").input_values model_int8 = torch.quantization.quantize_dynamic(     model,     {torch.nn.Linear},      dtype=torch.qint8)  ``` The **ERROR**:  `Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment`  Versions ``` Collecting environment information... PyTorch version: 1.12.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.5 Libc version: glibc2.26 Python version: 3.7.13 (default, Apr 24 2022, 01:04:09)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.188+x86_64withUbuntu18.04bionic Is CUDA available: False CUDA runtime version: 11.1.105 GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.0.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.12.0+cu113 [pip3] torchaudio==0.12.0+cu113 [pip3] torchsummary==1.5.1 [pip3] torchtext==0.13.0 [pip3] torchvision==0.13.0+cu113 [conda] Could not collect ``` ",2022-08-01T15:05:21Z,oncall: quantization triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/82570,i solved it buy adding `inplace=True` but another problem showed up is that the average execution time of quantized model is more than nonquantized!,Let's close this for now since the original problem was solved.  if the execution time thing is still a problem for you please open a new issue.
rag,MacOS MPS: src_total_size >= storage_byte_offset," 🐛 Describe the bug When converting a result from MPS backend to CPU, I get this error: src_total_size >= storage_byte_offset INTERNAL ASSERT FAILED at ""/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Copy.mm"":129, please report a bug to PyTorch.  This is the code that fails: ```C++         if (!o.device().is_cpu())         {             if (o.dtype() != torch::kFloat)             {                 o = o.to(torch::kCPU, torch::kFloat);             }             else             {                 o = o.to(torch::kCPU);             }         } ``` This, as a workaround, appears to not fail (note: the tensor is already a float): ``` if (!o.device().is_cpu()) {     o = o.to(torch::kCPU); } ``` It says to report it so I have put it here.  Versions Nightly pip for 1st August ",2022-08-01T13:01:55Z,triaged module: mps,closed,0,2,https://github.com/pytorch/pytorch/issues/82566,Thanks  for reporting the issue. Can you please provide a small testcase to reproduce this INTERNAL_ASSERT ?,The code above is what triggers the assert. Typically this code works for all other libtorch backends. This 'o' is a tensor that is from the result of a model that used MPS. You could also just create a tensor and move it to MPS. I haven't been able to reproduce in python.
rag,Saved tensor hooks checkpoint implementation cannot robustly clear storage," 🐛 Describe the bug In the saved tensor hooks based checkpointing approach (https://github.com/pytorch/pytorch/blob/386b39831745d9b87f24481b7c919816a80686cb/torch/utils/checkpoint.pyL349),  when autograd needs to unpack an activation, it potentially reruns the forward to recompute all activations, and then returns the activation for the index it is unpacking. However, we currently do a `storage.pop()` for this to ensure we don't hold references to the tensor after the backward is over. This raises the issue that if the same tensor is unpacked twice, without a pack in between, we'll run into an error. A (silly) example repro is here: https://github.com/pytorch/pytorch/blob/386b39831745d9b87f24481b7c919816a80686cb/test/test_autograd.pyL4598 Another concern is if a tensor is packed by autograd but never unpacked, thus leading to `storage` leaking. Although, we are unsure if this can occur in practice.  Versions main ",2022-07-29T16:24:36Z,module: checkpoint module: autograd triaged needs design,closed,0,1,https://github.com/pytorch/pytorch/issues/82482, 
transformer,[FSDP] caffe2 error in forward method when using fsdp," 🐛 Describe the bug When using FSDP, during inference/evaluation using transformers (gpt2, blenderbot, t5 ...) for generation, i.e., `model.generate()`, caffe2 error is thrown.  Steps to reproduce the error: 1. Code is here: run_seq2seq_no_trainer.py 2. Using 🤗 Accelerate's FSDP integration with config.yaml being below.  Using 2 Nvidia Titan RTX GPUs.  ```yaml compute_environment: LOCAL_MACHINE deepspeed_config: {} distributed_type: FSDP downcast_bf16: 'no' fsdp_config:   fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP   fsdp_backward_prefetch_policy: BACKWARD_PRE   fsdp_offload_params: false   fsdp_sharding_strategy: 1   fsdp_state_dict_type: FULL_STATE_DICT   fsdp_transformer_layer_cls_to_wrap: BlenderbotDecoderLayer machine_rank: 0 main_process_ip: null main_process_port: null main_training_function: main mixed_precision: 'fp16' num_machines: 1 num_processes: 2 use_cpu: false ``` 3. Running below launch command: ```bash accelerate launch config_file config.yaml \     run_seq2seq_no_trainer.py \     dataset_name ""smangrul/MuDoConv"" \     max_source_length 128 \     source_prefix ""chatbot: "" \     max_target_length 64 \     val_max_target_length 64 \     val_min_target_length 20 \     n_val_batch_generations 5 \     n_train 10000 \     n_val 1000 \     pad_to_max_length \     num_beams 10 \     model_name_or_path ""facebook/blenderbot400Mdistill"" \     per_device_train_batch_size 100 \     per_device_eval_batch_size 50 \     learning_rate 1e6 \     weight_decay 0.0 \     num_train_epochs 1 \     gradient_accumulation_steps 1 \     num_warmup_steps 100 \     output_dir ""/tmp/fsdp_test"" \     seed 25 \     logging_steps 100 ``` 4. Output with the error and trace stack: ```bash Traceback (most recent call last):                                                                                                File ""run_seq2seq_no_trainer.py"", line 911, in                                                                            bleu_score = evaluate(args, model, metric, tokenizer, eval_dataloader, accelerator, config.max_length)                        File ""run_seq2seq_no_trainer.py"", line 392, in evaluate                                                                           generated_tokens = unwrapped_model.generate(                                                                                  File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/autograd/grad_mode.py"", line 27, in decorate_context                     main()                                                                                                                        File ""run_seq2seq_no_trainer.py"", line 840, in main                                                                               return func(*args, **kwargs)                                                                                                  File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 1181, in generate                                     bleu_score = evaluate(args, model, metric, tokenizer, eval_dataloader, accelerator, config.max_length)                        File ""run_seq2seq_no_trainer.py"", line 392, in evaluate                                                                           model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(                                                           File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 525, in _prepare_encoder_decoder_kwargs_for_gener ation                                                                                                                               generated_tokens = unwrapped_model.generate(                                                                                  File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/autograd/grad_mode.py"", line 27, in decorate_context                     return func(*args, **kwargs)                                                                                                  File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 1181, in generate                                     model_kwargs[""encoder_outputs""]: ModelOutput = encoder(**encoder_kwargs)                                                      File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl                          model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(                                                           File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 525, in _prepare_encoder_decoder_kwargs_for_gener ation                                                                                                                               return forward_call(*input, **kwargs)                                                                                         File ""/home/sourab/transformers/src/transformers/models/blenderbot/modeling_blenderbot.py"", line 736, in forward                  model_kwargs[""encoder_outputs""]: ModelOutput = encoder(**encoder_kwargs)                                                      File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl                          inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale                                                               File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl                          return forward_call(*input, **kwargs)                                                                                         File ""/home/sourab/transformers/src/transformers/models/blenderbot/modeling_blenderbot.py"", line 736, in forward                  return forward_call(*input, **kwargs)                                                                                         File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/sparse.py"", line 158, in forward     return F.embedding(   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/functional.py"", line 2199, in embedding     inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/sparse.py"", line 158, in forward     return F.embedding(   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/functional.py"", line 2199, in embedding     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: The tensor has a nonzero number of elements, but its data is not allocated yet. Caffe2 uses a lazy allocation, s o you will need to call mutable_data() or raw_mutable_data() to actually allocate memory.     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) RuntimeError: The tensor has a nonzero number of elements, but its data is not allocated yet. Caffe2 uses a lazy allocation, s o you will need to call mutable_data() or raw_mutable_data() to actually allocate memory. ``` 5. The error disappears and everything works if I run `model(**dummy_batch)` before prediction loop. The concerned snippet in run_seq2seq_no_trainer.py is shown below: ```python model.eval()     if args.val_max_target_length is None:         args.val_max_target_length = args.max_target_length     gen_kwargs = {         ""max_length"": args.val_max_target_length if args is not None else max_length,         ""num_beams"": args.num_beams,         ""min_length"": args.val_min_target_length,         ""length_penalty"": False,         ""no_repeat_ngram_size"": 3,         ""encoder_no_repeat_ngram_size"": 3,         ""repetition_penalty"": 1.2,     }     samples_seen = 0     for step, batch in enumerate(eval_dataloader):          had to run this 1 time at the start of eval loop else was giving device `caffe error`.          So, before directly using `model.generate` pass a batch with dummy data through the model          uncomment below lines for successful run with FSDP          if samples_seen == 0 and accelerator.distributed_type == DistributedType.FSDP:              model(**batch)         with torch.no_grad():             unwrapped_model = accelerator.unwrap_model(model)             generated_tokens = unwrapped_model.generate(                 batch[""input_ids""],                 attention_mask=batch[""attention_mask""],                 synced_gpus=True,                 **gen_kwargs,             ) ``` On uncommenting the corresponding lines, everything works and below is the output: !Screenshot 20220729 at 2 50 40 PM  Expected Output No caffe2 allocation error is thrown.  Versions Collecting environment information... PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0122genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration:  GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX Nvidia driver version: 510.73.08 cuDNN version: Probably one of the following: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda10.2/targets/x86_64linux/lib/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.0 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] Could not collect ",2022-07-29T09:24:57Z,high priority triage review oncall: distributed triaged module: fsdp,closed,0,11,https://github.com/pytorch/pytorch/issues/82461,Similar issue faced by another user: https://github.com/huggingface/accelerate/issues/570,", hello,  just pinging you here in case you have the idea or have experienced this issue before.", ,"Hi    sorry I didn't see this earlier.  To your issue  we've run with T5 and validation loops, with many T5 variations, and did not hit this issue. However, in reviewing the above it looks like your code is going through some additional HF code before actually passing the inputs to the model for validation, vs while we are using HF T5 model, but then doing everything else in pure PyTorch. To pin this down a bit more:                                                       1  Can you confirm you see this with T5 in your setup above (original text states it happens with all transformers including T5, etc), but most of the specifics seem to revolve around blenderbot.   Would be a fast test to swap in T5 and see if it shows the same just to make sure if we can reference working T5 code for this issue. 2  The issue arises immediately upon the first validation minibatch correct?  At which point you push in a dummy batch which apparently allocates some embedding related memory for the input, and then things are fine after that.  I can setup and run blenderbot directly if that would help isolate (vs t5). Another fast test of this would be to modify your code to work directly with the sharded model in the validation loop, rather than the HF generate function: ala: ~~~ with torch.no_grad():         for batch in test_loader:             for key in batch.keys():                 batch[key] = batch[key].to(local_rank)             output = model(                 input_ids=batch[""source_ids""],                 attention_mask=batch[""source_mask""],                 labels=batch[""target_ids""],             )             ddp_loss[0] += output[""loss""].item()   sum up batch loss             ddp_loss[1] += len(batch) ~~~ You can review the code for T5 here if that helps to compare: https://github.com/lessw2020/t5_11 If that works then you can work backwards to isolate the issue in the HF utility code. If it's specific to blenderbot, then please let me know and I can setup blenderbot here in place of T5.  Thanks for bringing up the issue and look forward to getting it pinned down. ",I had the same problem.,"I think this issue is because FSDP shards the model params, so after you wrap: FSDP(model) model will have sharded parameters. In the inference path, it appears that unwrapped_model.generate calls an `encoder` directly, which is not an individually wrapped unit, and we call the forward pass:  ```  File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 1181, in generate                                     model_kwargs[""encoder_outputs""]: ModelOutput = encoder(**encoder_kwargs)                                                      File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl                          model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation( ``` we can tell encoder is not individually wrapped in FSDP because it doesn't appear to go through FSDP codepath.  I think to help confirm this, can you confirm `model` is FSDP instance and `encoder` is not, and also print out the FSDP wrapping of model? I think the reason `model(batch)` works might be because encoder's params are bubbled up to the top, and the root unit does not reshard after forward, so this sort of works due to an implementation detail.", any update from your side? Would it be possible to get a print out of the FSDP wrapped model? ,">  any update from your side? Would it be possible to get a print out of the FSDP wrapped model? Hi, I have encountered the same problem, and I have tested your comments and what you said is correct. `model` is a FSDP instance while `encoder` is not. So is there any solutions? Thank you very much!","We have no plans to support this at the moment, the workaround is to ensure the desired submodule is wrapped in its own FSDP unit. Then as a result, the submodule's forward pass will trigger the appropriate parameter rebuild.  Added https://github.com/pytorch/pytorch/pull/86343 to clarify this in the docs.",">  🐛 Describe the bug > When using FSDP, during inference/evaluation using transformers (gpt2, blenderbot, t5 ...) for generation, i.e., `model.generate()`, caffe2 error is thrown. >  >  Steps to reproduce the error: > 1. Code is here: run_seq2seq_no_trainer.py > 2. Using 🤗 Accelerate's FSDP integration with config.yaml being below.  Using 2 Nvidia Titan RTX GPUs. >  > ```yaml > compute_environment: LOCAL_MACHINE > deepspeed_config: {} > distributed_type: FSDP > downcast_bf16: 'no' > fsdp_config: >   fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP >   fsdp_backward_prefetch_policy: BACKWARD_PRE >   fsdp_offload_params: false >   fsdp_sharding_strategy: 1 >   fsdp_state_dict_type: FULL_STATE_DICT >   fsdp_transformer_layer_cls_to_wrap: BlenderbotDecoderLayer > machine_rank: 0 > main_process_ip: null > main_process_port: null > main_training_function: main > mixed_precision: 'fp16' > num_machines: 1 > num_processes: 2 > use_cpu: false > ``` >  > 3. Running below launch command: >  > ```shell > accelerate launch config_file config.yaml \ >     run_seq2seq_no_trainer.py \ >     dataset_name ""smangrul/MuDoConv"" \ >     max_source_length 128 \ >     source_prefix ""chatbot: "" \ >     max_target_length 64 \ >     val_max_target_length 64 \ >     val_min_target_length 20 \ >     n_val_batch_generations 5 \ >     n_train 10000 \ >     n_val 1000 \ >     pad_to_max_length \ >     num_beams 10 \ >     model_name_or_path ""facebook/blenderbot400Mdistill"" \ >     per_device_train_batch_size 100 \ >     per_device_eval_batch_size 50 \ >     learning_rate 1e6 \ >     weight_decay 0.0 \ >     num_train_epochs 1 \ >     gradient_accumulation_steps 1 \ >     num_warmup_steps 100 \ >     output_dir ""/tmp/fsdp_test"" \ >     seed 25 \ >     logging_steps 100 > ``` >  > 4. Output with the error and trace stack: >  > ```shell > Traceback (most recent call last):                                                                                              >   File ""run_seq2seq_no_trainer.py"", line 911, in                                                                        >     bleu_score = evaluate(args, model, metric, tokenizer, eval_dataloader, accelerator, config.max_length)                      >   File ""run_seq2seq_no_trainer.py"", line 392, in evaluate                                                                       >     generated_tokens = unwrapped_model.generate(                                                                                >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/autograd/grad_mode.py"", line 27, in decorate_context                 >     main()                                                                                                                      >   File ""run_seq2seq_no_trainer.py"", line 840, in main                                                                           >     return func(*args, **kwargs)                                                                                                >   File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 1181, in generate                                 >     bleu_score = evaluate(args, model, metric, tokenizer, eval_dataloader, accelerator, config.max_length)                      >   File ""run_seq2seq_no_trainer.py"", line 392, in evaluate                                                                       >     model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(                                                         >   File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 525, in _prepare_encoder_decoder_kwargs_for_gener > ation                                                                                                                           >     generated_tokens = unwrapped_model.generate(                                                                                >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/autograd/grad_mode.py"", line 27, in decorate_context                 >     return func(*args, **kwargs)                                                                                                >   File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 1181, in generate                                 >     model_kwargs[""encoder_outputs""]: ModelOutput = encoder(**encoder_kwargs)                                                    >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl                      >     model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(                                                         >   File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 525, in _prepare_encoder_decoder_kwargs_for_gener > ation                                                                                                                           >     return forward_call(*input, **kwargs)                                                                                       >   File ""/home/sourab/transformers/src/transformers/models/blenderbot/modeling_blenderbot.py"", line 736, in forward              >     model_kwargs[""encoder_outputs""]: ModelOutput = encoder(**encoder_kwargs)                                                    >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl                      >     inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale                                                             >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl                      >     return forward_call(*input, **kwargs)                                                                                       >   File ""/home/sourab/transformers/src/transformers/models/blenderbot/modeling_blenderbot.py"", line 736, in forward              >     return forward_call(*input, **kwargs)                                                                                       >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/sparse.py"", line 158, in forward >     return F.embedding( >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/functional.py"", line 2199, in embedding >     inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl >     return forward_call(*input, **kwargs) >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/sparse.py"", line 158, in forward >     return F.embedding( >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/functional.py"", line 2199, in embedding >     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) > RuntimeError: The tensor has a nonzero number of elements, but its data is not allocated yet. Caffe2 uses a lazy allocation, s > o you will need to call mutable_data() or raw_mutable_data() to actually allocate memory. >     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) > RuntimeError: The tensor has a nonzero number of elements, but its data is not allocated yet. Caffe2 uses a lazy allocation, s > o you will need to call mutable_data() or raw_mutable_data() to actually allocate memory. > ``` >  > 5. The error disappears and everything works if I run `model(**dummy_batch)` before prediction loop. The concerned snippet in run_seq2seq_no_trainer.py is shown below: >  > ```python > model.eval() >     if args.val_max_target_length is None: >         args.val_max_target_length = args.max_target_length >  >     gen_kwargs = { >         ""max_length"": args.val_max_target_length if args is not None else max_length, >         ""num_beams"": args.num_beams, >         ""min_length"": args.val_min_target_length, >         ""length_penalty"": False, >         ""no_repeat_ngram_size"": 3, >         ""encoder_no_repeat_ngram_size"": 3, >         ""repetition_penalty"": 1.2, >     } >     samples_seen = 0 >     for step, batch in enumerate(eval_dataloader): >          had to run this 1 time at the start of eval loop else was giving device `caffe error`. >          So, before directly using `model.generate` pass a batch with dummy data through the model >          uncomment below lines for successful run with FSDP >          if samples_seen == 0 and accelerator.distributed_type == DistributedType.FSDP: >              model(**batch) >         with torch.no_grad(): >             unwrapped_model = accelerator.unwrap_model(model) >             generated_tokens = unwrapped_model.generate( >                 batch[""input_ids""], >                 attention_mask=batch[""attention_mask""], >                 synced_gpus=True, >                 **gen_kwargs, >             ) > ``` >  > On uncommenting the corresponding lines, everything works and below is the output: >  > !Screenshot 20220729 at 2 50 40 PM >  >  Expected Output > No caffe2 allocation error is thrown. >  >  Versions > Collecting environment information... PyTorch version: 1.12.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A >  > OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: 10.0.04ubuntu1 CMake version: version 3.16.3 Libc version: glibc2.31 >  > Python version: 3.8.10 (default, Jun 22 2022, 20:18:18) [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0122genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration: GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX >  > Nvidia driver version: 510.73.08 cuDNN version: Probably one of the following: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda10.2/targets/x86_64linux/lib/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True >  > Versions of relevant libraries: [pip3] numpy==1.23.0 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] Could not collect >  > , I tried what you suggested, but weirdly enough when I run model(**inputs), it instantly reaches GPU capacity of 100%, making the whole pipeline stuck. Have you encountered such behavior?","> I think this issue is because FSDP shards the model params, so after you wrap: >  > FSDP(model) >  > model will have sharded parameters. In the inference path, it appears that unwrapped_model.generate calls an `encoder` directly, which is not an individually wrapped unit, and we call the forward pass: >  > ``` >  File ""/home/sourab/transformers/src/transformers/generation_utils.py"", line 1181, in generate                                 >     model_kwargs[""encoder_outputs""]: ModelOutput = encoder(**encoder_kwargs)                                                    >   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl                      >     model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation( > ``` >  > we can tell encoder is not individually wrapped in FSDP because it doesn't appear to go through FSDP codepath. >  > I think to help confirm this, can you confirm `model` is FSDP instance and `encoder` is not, and also print out the FSDP wrapping of model? I think the reason `model(batch)` works might be because encoder's params are bubbled up to the top, and the root unit does not reshard after forward, so this sort of works due to an implementation detail. I had a similar error message inferring a nonHF LLM model trained using FSDP, and this explanation solved my problem: it turned out that I was inferring the model with a different number of GPUs from what the model was trained on. Thanks varma !"
transformer,Quantization issue in transformers," 🐛 Describe the bug This issue happened when quantizing a simple transformer model Example ```     class M(torch.nn.Module):         def __init__(self):             super(DynamicQuantModule.M, self).__init__()             self.transformer = nn.Transformer(d_model=2, nhead=2, num_encoder_layers=1, num_decoder_layers=1)         def forward(self):             return self.transformer(torch.randn(1, 16, 2))     torch.quantization.quantize_dynamic(M(), dtype=torch.qint8) ``` The error is ```   File ""/Users/linbin/opt/anaconda3/lib/python3.8/sitepackages/torch/jit/_recursive.py"", line 516, in init_fn     scripted = create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)   File ""/Users/linbin/opt/anaconda3/lib/python3.8/sitepackages/torch/jit/_recursive.py"", line 542, in create_script_module_impl     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)   File ""/Users/linbin/opt/anaconda3/lib/python3.8/sitepackages/torch/jit/_recursive.py"", line 393, in create_methods_and_properties_from_stubs     concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults) RuntimeError: method cannot be used as a value:   File ""/Users/linbin/opt/anaconda3/lib/python3.8/sitepackages/torch/nn/modules/transformer.py"", line 468                 self.norm2.weight,                 self.norm2.bias,                 self.linear1.weight,                 ~~~~~~~~~~~~~~~~~~~ < HERE                 self.linear1.bias,                 self.linear2.weight, ```  Versions PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: macOS 12.4 (x86_64) GCC version: Could not collect Clang version: 13.0.0 (clang1300.0.18.6) CMake version: version 3.21.3 Libc version: N/A Python version: 3.7.5 (default, Oct 22 2019, 10:35:10)  [Clang 10.0.1 (clang1001.0.46.4)] (64bit runtime) Python platform: Darwin21.5.0x86_64i38664bit Is CUDA available: N/A CUDA runtime version: Could not collect GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A Versions of relevant libraries: [pip3] No relevant packages [conda] blas                      1.0                         mkl   [conda] mkl                       2021.2.0           hecd8cb5_269   [conda] mklinclude               2022.0.0           hecd8cb5_105   [conda] mklservice               2.4.0            py38h9ed2024_0   [conda] mkl_fft                   1.3.0            py38h4a7008c_2   [conda] mkl_random                1.2.2            py38hb2f4e1b_0   [conda] numpy                     1.20.0                   pypi_0    pypi [conda] numpybase                1.20.2           py38he0bd621_0   [conda] numpydoc                  1.4.0            py38hecd8cb5_0   [conda] pytorch                   1.13.0.dev20220728         py3.8_0    pytorchnightly [conda] torch                     1.11.0                   pypi_0    pypi [conda] torchaudio                0.13.0.dev20220728        py38_cpu    pytorchnightly [conda] torchvision               0.13.0                   pypi_0    pypi Another way to trigger it is just run: ``` python3 test/mobile/model_test/gen_test_model.py dynamic_quant_ops ``` in the latest nightly build. ",2022-07-28T23:46:27Z,oncall: quantization low priority triaged,open,2,19,https://github.com/pytorch/pytorch/issues/82443,"The PR that introduced this likely landed Jul 15 2022, judging from when iOS tests started to fail (it's not clear when exactly, because the test uses nightly build to generate models). This issue currently blocking enabling DynamicQuantModule in iOS simulator tests:  CC(Re-enable DynamicQuantModule in iOS simulator tests)  /pytorchdevinfra ",Another way to trigger it is to run: ``` python3 test/mobile/model_test/gen_test_model.py dynamic_quant_ops ``` in the latest nightly build.,"https://github.com/pytorch/pytorch/pull/81277 was landed on July 15, and https://github.com/pytorch/pytorch/pull/81013 on July 14","Oooh, I don't think we can quantize that module in eager mode. Once quantized, the `.weight` becomes a method for ""unpacking"" the quantized weights. I think the only way to quantize this type of layers is by using the ""fx quantization"".  ","yeah eager mode quantization does not support quantizing a linear that is used in this way (direct access to weight), is fx graph mode quantization an option here? Does this work before?",yes it works before. What is the right way? The goal is just to include certain quantization ops in the test model.,I'm wondering what changed? we didn't touch eager mode quantization code for a long time I think. Maybe one thing you can do is to add `` for https://github.com/pytorch/pytorch/blob/master/torch/nn/quantized/modules/linear.pyL229 not sure if it would work or not though,"Any updates on this? DynamicQuantModule in iOS simulator tests is still not running, it's better to be able to reenable it soon to prevent regressions.","I can't repro the issue actually, are you using pytorch master?","maybe not the root cause, but the crash site was touched by https://github.com/pytorch/pytorch/pull/81277 and https://github.com/pytorch/pytorch/pull/81013.  To repro just run this command on the latest nightly: python3 test/mobile/model_test/gen_test_model.py dynamic_quant_ops If it works, then we can just enable the ios test.",I've just tried rebasing https://github.com/pytorch/pytorch/pull/82439 and the issue is still there., is this the code to repro the issue?,"I see, it failed during torch.jit.script, basically after we quantize the model, linear.weight became a method instead of a value, that's why torch.jit.script failed. I really doubt that this worked before actually","> I see, it failed during torch.jit.script, basically after we quantize the model, linear.weight became a method instead of a value, that's why torch.jit.script failed. I really doubt that this worked before actually It worked before Jul 16th 2022. See this PR https://github.com/pytorch/pytorch/pull/82027 Before Jul 16th DynamicQuantModule in the test worked, after that it needed to be disabled  otherwise it fails with the error from the first post in this issue.","> > I see, it failed during torch.jit.script, basically after we quantize the model, linear.weight became a method instead of a value, that's why torch.jit.script failed. I really doubt that this worked before actually >  > It worked before Jul 16th 2022. See this PR CC(Reenable iOS simulator tests) Before Jul 16th DynamicQuantModule in the test worked, after that it needed to be disabled  otherwise it fails with the error from the first post in this issue. is it possible to trace to the PR when this PR is broken?",  Is there an update on this?,"I don't have any updates, we haven't found the root cause of this issue I think","Hi    I am experiencing this issue when trying to quantize a PyTorch Transformer instance. I need to use ONNX Runtime instead, but it comes with its own drawbacks. We'd love to stick to pure PyTorch. Is anyone looking at this issue?",Hi folks  wondering if anyone successfully was able to quantize a vanilla Transformer model or a `nn.Transformer` layer within PyTorch? I face the same issue.
rag,Rename `_Typed/_UntypedStorage` to `Typed/UntypedStorage` and update docs," Description Since the major changes for `_TypedStorage` and `_UntypedStorage` are now complete, they can be renamed to be public. `TypedStorage._untyped()` is renamed to `TypedStorage.untyped()`. Documentation for storages is improved as well.  Issue Fixes CC(Rename `_Typed/_UntypedStorage` to `Typed/UntypedStorage` and update docs)  Testing N/A",2022-07-28T22:09:52Z,module: internals open source Merged cla signed topic: docs,closed,0,4,https://github.com/pytorch/pytorch/issues/82438,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82438**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: 1 Base Failures As of commit 05d62bd8fd (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base ff5399e528 on Jul 29 from 11:15am to 12:17pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * pull / linuxbionicpy3_7clang8xla / test (xla, 1, 1, linux.2xlarge) on Jul 29 from 11:15am to 12:17pm (ff5399e528  61b21f28d8)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," merge f ""upstream failure""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Rename `_Typed/_UntypedStorage` to `Typed/UntypedStorage` and update docs,,2022-07-28T21:59:56Z,module: internals topic: docs,closed,0,0,https://github.com/pytorch/pytorch/issues/82436
transformer,"[ONNX] When using Apex FusedLayerNorm, Transformers model has static shape (batch size)."," 🐛 Describe the bug If I use apex FusedLayerNorm to replace torch.nn.LayerNorm in transformers, the batch size of the model becomes static.  Versions Pytorch nightly",2022-07-27T17:06:58Z,module: onnx triaged onnx-triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/82330
chat,Make pytest ending output less chatty,"  CC(Make pytest ending output less chatty) If you still want this in CI, we should have a separate CI only configuration.  The current config is pretty unfriendly for local development. Signedoffby: Edward Z. Yang ",2022-07-26T21:42:45Z,Merged cla signed,closed,0,15,https://github.com/pytorch/pytorch/issues/82262,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82262**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 8b16a9db5a (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7549065995?check_suite_focus=true) pull / winvs2019cpupy3 / test (default, 1, 2, windows.4xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220727T22:30:58.7151485Z FAIL [0.016s]: test_simple_out (__main__.TestFunctionalization)  ``` 20220727T22:30:58.7147859Z   File ""test_functionalization.py"", line 209, in test_multi_out 20220727T22:30:58.7148141Z     self.assertExpectedInline(reinplaced_logs, """"""\ 20220727T22:30:58.7148631Z   File ""C:\Jenkins\Miniconda3\lib\sitepackages\expecttest\__init__.py"", line 262, in assertExpectedInline 20220727T22:30:58.7149027Z     self.assertMultiLineEqualMaybeCppStack(expect, actual, msg=help_text) 20220727T22:30:58.7149472Z   File ""C:\Jenkins\Miniconda3\lib\sitepackages\expecttest\__init__.py"", line 281, in assertMultiLineEqualMaybeCppStack 20220727T22:30:58.7149871Z     self.assertMultiLineEqual(expect, actual[:len(expect)], *args, **kwargs) 20220727T22:30:58.7150287Z AssertionError: ""\n\n[55 chars]mpty.SymInt([4], dtype = torch.float32, device[360 chars]    "" != ""\n\n[55 chars]mpty.memory_format([4], dtype = torch.float32,[359 chars]etur"" 20220727T22:30:58.7150820Z Diff is 929 characters long. Set self.maxDiff to None to see it. : To accept the new output, rerun test with envvar EXPECTTEST_ACCEPT=1 (we recommend staging/committing your changes before doing this) 20220727T22:30:58.7151156Z  20220727T22:30:58.7151254Z ====================================================================== 20220727T22:30:58.7151485Z FAIL [0.016s]: test_simple_out (__main__.TestFunctionalization) 20220727T22:30:58.7151779Z  20220727T22:30:58.7152033Z Traceback (most recent call last): 20220727T22:30:58.7152304Z   File ""test_functionalization.py"", line 171, in test_simple_out 20220727T22:30:58.7152595Z     self.assertExpectedInline(reinplaced_logs, """"""\ 20220727T22:30:58.7152980Z   File ""C:\Jenkins\Miniconda3\lib\sitepackages\expecttest\__init__.py"", line 262, in assertExpectedInline 20220727T22:30:58.7153370Z     self.assertMultiLineEqualMaybeCppStack(expect, actual, msg=help_text) 20220727T22:30:58.7153821Z   File ""C:\Jenkins\Miniconda3\lib\sitepackages\expecttest\__init__.py"", line 281, in assertMultiLineEqualMaybeCppStack 20220727T22:30:58.7154202Z     self.assertMultiLineEqual(expect, actual[:len(expect)], *args, **kwargs) 20220727T22:30:58.7154622Z AssertionError: ""\n\n[318 chars]mpty.SymInt([], dtype = torch.float32, device [249 chars]    "" != ""\n\n[318 chars]mpty.memory_format([], dtype = torch.float32, [248 chars]tens"" 20220727T22:30:58.7155150Z Diff is 901 characters long. Set self.maxDiff to None to see it. : To accept the new output, rerun test with envvar EXPECTTEST_ACCEPT=1 (we recommend staging/committing your changes before doing this) ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `gh/ezyang/1285/orig` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/82262`)", rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `gh/ezyang/1285/orig` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/82262`)", merge g, successfully started a merge job. Check the current status here, merge g, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) pull failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2749245960," merge f ""known master breakage""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,UserWarning: operator() sees varying value in profiling," 🐛 Describe the bug Hello, I have a problem with the following code, causing a warning when executed with `torch.jit.script`. The code seems to work ok, I just find that the warning is a bit weird. This is a minimal example to reproduce the bug. ```python from typing import Dict import torch import torch.nn as nn from torch import Tensor class MyModule(nn.Module):     def forward(self, x: Tensor) > Tensor:         x = x.permute(0, 2, 3, 1).contiguous()         x = x.view(x.size(0), 1, x.size(3))         return x class AnotherModule(nn.Module):     def __init__(self) > None:         super().__init__()         self._my_module = MyModule()     def forward(self, x: Dict[str, Tensor]) > Dict[str, Tensor]:         return {key: self._my_module(x[key]) for key in x} if __name__ == ""__main__"":     model = AnotherModule()     model: AnotherModule = torch.jit.script(model)   type: ignore     out = model(         {             ""a"": torch.rand(1, 5, 32, 32),             ""b"": torch.rand(1, 3, 32, 32),             ""c"": torch.rand(1, 5, 32, 32),         }     )     print({k: v.shape for k, v in out.items()}) ``` Spefically, I get this warning: ```  /home/luca/venvs/dev38/lib/python3.8/sitepackages/torch/nn/modules/module.py:1130: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3513.) return forward_call(*input, **kwargs) ``` Some insight on the bug:  The warning appears only when using `torch.jit.script`: if I comment the line in which jit compilation is called everything works fine.  It seems to be caused by the forward method of `MyModule`, if I uncomment at least one line of the `forward` method, no warning is raised.   It also has interactions with the second dimension of input tensors in the dictionary with keys ""a"", ""b"" and ""c"". If they have the same number of channes, no warning is raised. 👀   I cannot reproduce this with previous versions of PyTorch.  Another question: is it safe to ignore this kind of warning?  Versions PyTorch version: 1.12.0+cu116 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.15.041genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti Nvidia driver version: 510.47.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.23.1 [pip3] pytorchlightning==1.6.5 [pip3] torch==1.12.0+cu116 [pip3] torchmetrics==0.9.3 [pip3] torchvision==0.13.0+cu116 [conda] Could not collect",2022-07-25T11:01:00Z,oncall: jit,open,14,3,https://github.com/pytorch/pytorch/issues/82099,any update on this issue?,Same issue!,"Hi! After some digging it seems that torch script calls nvfuser to optimize computational graph for a particular model. It usually takes few forward passes to completely profile and then optimize the computational graph as suggested here. Therefore, it seems that before profiling is ready there is bunch of loose ends that raise this warning. After ~5 forward passes this warning should be gone as the profiling is done. So I think that this warning can be ignored"
transformer,Fix deserialization of TransformerEncoderLayer (#81832) (#81832),"Summary: When `activation` is a module, it is not saved directly in the state dictionary but instead in `_modules`. When deserialized, the old version of this code would think that activation was missing and set it to RELU. This version first reconstructions the module and then sees if activation is neither a module nor a function before setting it to RELU. Pull Request resolved: https://github.com/pytorch/pytorch/pull/81832 Approved by: https://github.com/kit1980, https://github.com/zrphercule Test Plan: contbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/e68583b4d180066b8e4f108e0d23176a2676421c Test plan from GitHub: pytorch oss tests Reviewed By: jeanschmidt, zrphercule Differential Revision: D38014872 Pulled By: zdevito fbshipitsourceid: 938079d768f7981ca55eed3c8828b29a92e06f41",2022-07-25T09:23:21Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/82094,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82094**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 74e5021fc4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
rag,[Tracking] Operator coverage for symbolic shape tracing," 🚀 The feature, motivation and pitch Op Coverage mostly taken from https://github.com/pytorch/pytorch/blob/master/test/test_proxy_tensor.pyL387 Decompositions Needed (i.e. we need to write a Primtorch decomposition or meta function)  [ ] `aten._to_copy.default`  [ ] `aten.mm.default`  [ ] `aten.slice.Tensor` SymInt overloads needed (i.e. we need to support calling it with SymInts as an argument)  [ ] `aten.new_empty`  [ ] `aten.reshape` (tricky since it's CompositeImplicitAutograd?) cc:    ",2022-07-23T05:57:14Z,triaged oncall: pt2 module: dynamic shapes,closed,0,0,https://github.com/pytorch/pytorch/issues/82048
rag,[vulkan] enable storage_ access in VulkanOpaqueTensorImpl to enable serialization,  CC([vulkan] enable storage_ access in VulkanOpaqueTensorImpl to enable serialization) Differential Revision: D38086377,2022-07-22T20:09:12Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/82015,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/82015**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 03e4ebda15 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
transformer,Issue with FSDP memory reduction scaling up GPUs," 🐛 Describe the bug We are running FSDP with HuggingFace T5 3B model using the transformer wrapping policy and mixed precision (BF16) with A100 gpus. Scaling up GPUs, by extending to multi nodes (each node has 8 gpus),  we are expecting to have smaller shard on each and overall proportional memory reduction. However, we see that the reserved memory for between 14 nodes does not change very much, whereas the allocated memory reduces to almost half.  Revered memory has been captured using ""torch.cuda.memory_reserved()"" and allocated memory using ""torch.cuda.memory_allocated()"".  As shown in the table below, the reserved memory on 1 node is ~ 28Gb > 2 nodes ~ 26Gb >3 nodes ~25Gb> 4 nodes ~24Gb.   This is the same pattern for TF32 and FP32 where the memory reduction by extending nodes is even less (table below).  This results in OOM when training T5, 11B model, as the memory reduction is not happening as expected. It would be great to narrow down the root cause if this is due to the transformer wrapping policy or other design choices in FSDP.  Code to repro.  Versions PyTorch Nightlies ",2022-07-22T17:49:11Z,oncall: distributed module: fsdp,closed,1,1,https://github.com/pytorch/pytorch/issues/82001,"I think active memory is the appropriate stat to track over allocated memory due to the multiple stream usage in FSDP. Nonetheless, the rate limiter should help with this issue: https://github.com/pytorch/pytorch/pull/83917 I will close this for now, and we can open a new issue if there are similar issues even with the limiter enabled. The limiter targets the high active memory and high reserved memory case and can bring those values down. However, if the workload has low active memory and high reserved memory, then it suffers from fragmentation, which the rate limiter cannot help on the first order."
yi,[resubmit][FX] Fix PyTree unpacking carrying forward type annotations,Differential Revision: D38077793,2022-07-22T17:08:15Z,fb-exported Merged cla signed Reverted fx,closed,0,19,https://github.com/pytorch/pytorch/issues/81999,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81999**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 65fdbc200a (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7505713799?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (dynamo, 2, 2, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220725T18:42:49.8938560Z RuntimeError: test_nn failed!  ``` 20220725T18:42:47.7108767Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestModuleGlobalHooks20220725183504.xml 20220725T18:42:47.8905173Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestNN20220725183504.xml 20220725T18:42:47.9615914Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestNNDeviceTypeCPU20220725183504.xml 20220725T18:42:47.9638682Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestNNInit20220725183504.xml 20220725T18:42:47.9645087Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestStateDictHooks20220725183504.xml 20220725T18:42:49.8919480Z Traceback (most recent call last): 20220725T18:42:49.8919749Z   File ""test/run_test.py"", line 966, in  20220725T18:42:49.8937108Z     main() 20220725T18:42:49.8937378Z   File ""test/run_test.py"", line 944, in main 20220725T18:42:49.8938272Z     raise RuntimeError(err_message) 20220725T18:42:49.8938560Z RuntimeError: test_nn failed! 20220725T18:42:50.1849182Z  20220725T18:42:50.1849467Z real	45m32.035s 20220725T18:42:50.1849833Z user	220m44.643s 20220725T18:42:50.1850138Z sys	11m38.118s 20220725T18:42:50.1881676Z [error]Process completed with exit code 1. 20220725T18:42:50.1917832Z Prepare all required actions 20220725T18:42:50.1918138Z Getting action download info 20220725T18:42:50.3716844Z [group]Run ./.github/actions/getworkflowjobid 20220725T18:42:50.3717065Z with: 20220725T18:42:50.3717678Z   githubtoken: *** ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D38077793,This pull request was **exported** from Phabricator. Differential Revision: D38077793, merge f flaky,This pull request was **exported** from Phabricator. Differential Revision: D38077793," merge f (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)"," merge f '[MINOR] Unrelated, unrepreproducible failure'", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.", revert ," revert m  ""test_bce_with_logits_has_correct_forward_grad consistently fails with an error that it takes 2 positional arguments but 3 were given"" c ignoredsignal", successfully started a revert job. Check the current status here, your PR has been successfully reverted.," please investigate the broken test, this test started failing consistently once this PR was merged.  For the future, please note that the Dr. CI comment at the top of this PR provides hints as to which failures are likely to be caused by flakiness and which ones might be legitimate errors You can see builds failing consistently starting with this PR in http://hud.pytorch.org "," As mentioned in the commit message, the test does not reproduce locally, and that part of the code has nothing to do with this PR","Hi James, I hear you on this failure being hard to debug.  It def frustrating that failures don't always reproduce locally and that’s an area that we want to make easier for developers. That said, looking at the build failures, there was clearly *something* about that PR which caused the build break on that particular configuration. If you look at the failures, the build started failing when the PR was checked in and stopped failing when your PR was reverted.  Looking at the failure in particular, it's saying `TypeError: forward() takes 2 positional arguments but 3 were given`  I see that the PR makes changes related to the forward() method and even adds what looks kind of like a new invocation in each of the two `gen_fn_def` methods https://github.com/pytorch/pytorch/pull/81999/filesdiff4fc0f305bb3a7821b97dc183a9370957e4ce8bbb83dc547bcb51a1de8b79e448R271 Apologies for this being hard to debug, but it does seem like something caused by this PR. Let me pull up some instructions that may help you debug deeper…","You can use the `withssh` label to ssh into the failing machine, which should let you reproduce the error Instructions: https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions","[edit] Hmm,  have you try running it with torchdynamo installed/enabled?",
agent,DISABLED test_rref_as_arg_synchronization3 (__main__.TensorPipeTensorPipeAgentCudaRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 red and 1 green. ",2022-07-22T04:01:37Z,oncall: distributed module: flaky-tests skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/81962
transformer,1.12.1/bt fix,"Fixes  CC(Transformer and CPU path with `src_mask` raises error with torch 1.12). Also fixes a separate crash when enable_nested_tensor is enabled (caught by tests added in test_transformers.py).  These three PRs:  CC(disable src mask for transformer and multiheadattention fastpath): Fix CC(Transformer and CPU path with `src_mask` raises error with torch 1.12) (activation.py and transformer.py changes. primary reason for this PR)  CC(Add numerical test comparing BetterDecoder and fairseq decoder): Add tests for the fix (test_transformers.py and test_nn.py changes)  CC([PyTorch] Round T up to next multiple of 8 in NestedTensor case): Fix crash when BetterTransformer gets NestedTensor input (Pulled only the change to NestedTensorMath.cpp, not all changes to keep code minimal)",2022-07-21T23:47:21Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/81952,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81952**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b99b16f2cc (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
transformer,Add mask identifier for multiplexed src_mask/src_key_padding_mask in BT,"Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/81947 Transformer fastpath multiplexes two arguments, src_mask [seq_len x seq_len] and src_key_padding_mask [batch_size x seq_len], and later deduces the type based on mask shape. In the event that batch_size == seq_len, any src_mask is wrongly interpreted as a src_key padding_mask. This is fixed by requiring a mask_type identifier be supplied whenever batch_size == seq_len. Additionally, added support for src_mask in masked_softmax CPU path. Test Plan: existing unit tests + new unit tests (batch_size == seq_len) Differential Revision: D37932240",2022-07-21T23:19:11Z,fb-exported Merged cla signed,closed,0,22,https://github.com/pytorch/pytorch/issues/81947,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81947**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 688b668a91 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240, Could you please approve the above workflows,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240, merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[ONNX] Test and address autograd.func (FusedLayerNorm) shape inference,"This PR addresses the ONNX exporter issue of wrongly inferred static shape by unreliable nodes: 1. Specifically, this unblocks the usage of apex `FusedLayerNorm` (autograd.function) in transformer. Before this PR, the downstream nodes of apex `FusedLayerNorm` are inferred with static shape even though they are unreliable (should be dynamic). 2. Add a general test case using autograd function to wrap `torch.nn.layernorm` which can repro the same issue as apex `FusedLayerNorm` did in transformersembedding layer. 3. Remove a legacy test `test_empty_like_opset7` which still uses deprecated ConstantFill op. As this node is not supported by onnx (checker) anymore, the output of its shape inference leading to unexpected outcome, and is exposed by this PR. ```python Warning: Checker does not support models with experimental ops: ConstantFill ``` Please advise if there is a better place for the test case. Fixes CC([ONNX] When using Apex FusedLayerNorm, Transformers model has static shape (batch size).) ",2022-07-21T22:21:07Z,oncall: jit module: onnx triaged open source Merged cla signed release notes: onnx topic: bug fixes,closed,0,8,https://github.com/pytorch/pytorch/issues/81931,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81931**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit ab6431a880 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,I just saw ConstantFill being used in opset8 too. Do you know what should be the alternative?,"> I just saw ConstantFill being used in opset8 too. Do you know what should be the alternative? As far as I know, ConstantFill was treated as invalid onnx node during my testing, and providing a different result than the expect file (Not sure if it's used sort of like onnx::Constant at the time). And considering we have another `test_empty_like` with more current version, I didn't thoroughly explore the alternative way to that test. If we decide to keep that test in an alternative way, I will have to talk to onnx team to know that node more.","  folks, Pytorch will release 1.12.1 soon. Do you think we can expedite the code review to try to fit in this fix in time?",">   folks, Pytorch will release 1.12.1 soon. Do you think we can expedite the code review to try to fit in this fix in time?  We have created a release tracker for some fixes CC([ONNX] Cherry pick quantization for 1.12.1 release), CC(ONNX 1.12.1). If needed let's work together to include this fix.","> >   folks, Pytorch will release 1.12.1 soon. Do you think we can expedite the code review to try to fit in this fix in time? >  >  We have created a release tracker for some fixes CC([ONNX] Cherry pick quantization for 1.12.1 release), CC(ONNX 1.12.1). If needed let's work together to include this fix. Can  just include this PR in it after you approve it?", merge g, successfully started a merge job. Check the current status here
yi,[FX] Fix PyTree unpacking carrying forward type annotations,Stack from ghstack:  CC([FX] Fix PyTree unpacking carrying forward type annotations) Resolves  CC([FX] `concrete_args` unpacking erroneously carries over type annotations),2022-07-21T18:32:03Z,Merged cla signed Reverted fx,closed,0,12,https://github.com/pytorch/pytorch/issues/81906,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81906**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 634af35278 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7457762974?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (dynamo, 2, 2, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220721T21:43:39.5162842Z RuntimeError: test_nn failed!  ``` 20220721T21:43:36.9434048Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestModuleGlobalHooks20220721213552.xml 20220721T21:43:37.5399857Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestNN20220721213552.xml 20220721T21:43:37.6114516Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestNNDeviceTypeCPU20220721213552.xml 20220721T21:43:37.6136728Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestNNInit20220721213552.xml 20220721T21:43:37.6142636Z Generated XML report: testreports/pythonunittest/test_nn/TESTTestStateDictHooks20220721213552.xml 20220721T21:43:39.5158412Z Traceback (most recent call last): 20220721T21:43:39.5158835Z   File ""test/run_test.py"", line 940, in  20220721T21:43:39.5160843Z     main() 20220721T21:43:39.5161182Z   File ""test/run_test.py"", line 918, in main 20220721T21:43:39.5162450Z     raise RuntimeError(err_message) 20220721T21:43:39.5162842Z RuntimeError: test_nn failed! 20220721T21:43:39.7555904Z  20220721T21:43:39.7556248Z real	43m37.030s 20220721T21:43:39.7556514Z user	207m31.202s 20220721T21:43:39.7556684Z sys	12m10.391s 20220721T21:43:39.7591445Z [error]Process completed with exit code 1. 20220721T21:43:39.7653621Z Prepare all required actions 20220721T21:43:39.7653918Z Getting action download info 20220721T21:43:39.9182542Z [group]Run ./.github/actions/getworkflowjobid 20220721T21:43:39.9182761Z with: 20220721T21:43:39.9183090Z   githubtoken: *** ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) pull failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2716282268, merge f," merge f ""flaky test""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""breaking internal builds"" c ""ghfirst"" ", successfully started a revert job. Check the current status here, your PR has been successfully reverted.,"  I saw this got reverted, but we still need this fix.  how is it breaking internal builds?"
transformer,Fix deserialization of TransformerEncoderLayer,"Summary: When `activation` is a module, it is not saved directly in the state dictionary but instead in `_modules`. When deserialized, the old version of this code would think that activation was missing and set it to RELU. This version first reconstructions the module and then sees if activation is neither a module nor a function before setting it to RELU. Test Plan: pytorch oss tests Reviewed By: zrphercule Differential Revision: D38014872",2022-07-20T23:56:32Z,fb-exported Merged cla signed,closed,0,6,https://github.com/pytorch/pytorch/issues/81832,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81832**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 56e0a4c3c4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D38014872,This pull request was **exported** from Phabricator. Differential Revision: D38014872, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Add mask identifier for multiplexed src_mask/src_key_padding_mask in BT,"Summary: Transformer fastpath multiplexes two arguments, src_mask [seq_len x seq_len] and src_key_padding_mask [batch_size x seq_len], and later deduces the type based on mask shape. In the event that batch_size == seq_len, any src_mask is wrongly interpreted as a src_key padding_mask. This is fixed by requiring a mask_type identifier be supplied whenever batch_size == seq_len. Additionally, added support for src_mask in masked_softmax CPU path. Test Plan: existing unit tests + new unit tests (batch_size == seq_len) Differential Revision: D37932240",2022-07-20T23:51:20Z,fb-exported cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/81830,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81830**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 11 New Failures As of commit 21373ebb2d (more details on the Dr. CI page): Expand to see more  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7460034510?check_suite_focus=true) pull / winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) (1/11) **Step:** ""Test"" (full log  :repeat: rerun)   20220722T02:28:21.6464390Z RuntimeError: expected `mask_type` when `mask` supplied  ``` 20220722T02:28:21.6459244Z ====================================================================== 20220722T02:28:21.6459500Z ERROR [0.006s]: test_masked_softmax_xla (__main__.TestNNDeviceTypeXLA) 20220722T02:28:21.6459842Z  20220722T02:28:21.6460094Z Traceback (most recent call last): 20220722T02:28:21.6460508Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_device_type.py"", line 390, in instantiated_test 20220722T02:28:21.6460770Z     raise rte 20220722T02:28:21.6463028Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_device_type.py"", line 377, in instantiated_test 20220722T02:28:21.6463555Z     result = test(self, **param_kwargs) 20220722T02:28:21.6463846Z   File ""/var/lib/jenkins/workspace/xla/test/../../test/test_nn.py"", line 16968, in test_masked_softmax 20220722T02:28:21.6464144Z     native_res = torch._masked_softmax(input, mask, dim) 20220722T02:28:21.6464390Z RuntimeError: expected `mask_type` when `mask` supplied 20220722T02:28:21.6464542Z  20220722T02:28:21.7374029Z  20220722T02:28:21.7374537Z Ran 934 tests in 681.056s 20220722T02:28:21.7374764Z  20220722T02:28:21.7374973Z FAILED (errors=3, skipped=723, expected failures=9) 20220722T02:28:21.7375332Z  20220722T02:28:21.7375420Z Generating XML reports... 20220722T02:28:21.7375879Z Generated XML report: testreports/pythonunittest/test.......test.test_nn/TESTTestNNDeviceTypeXLA20220722021700.xml 20220722T02:28:22.2701662Z + sccache_epilogue 20220722T02:28:22.2702258Z + echo '::group::Sccache Compilation Log' ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D37932240,This pull request was **exported** from Phabricator. Differential Revision: D37932240
yi,403 when trying to get multiple pytorch components," 🐛 Describe the bug As part of a dockerfile we're conda installing multiple components from a yml. Today we are getting these errors: CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.298984 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.073712 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.304133 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.075025 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. I tried to build the docker image multiple times.  Then, got the same 403 error just trying to access any of the components directly via a browser.  Versions N/A",2022-07-20T19:04:13Z,,closed,2,4,https://github.com/pytorch/pytorch/issues/81798,Can confirm. Created a fresh conda environment and ran `conda install pytorch torchvision torchaudio cudatoolkit=10.2 c pytorch`. Fails with the same `CondaHTTPError: HTTP 403 FORBIDDEN` errors as  mentioned.,I accidentally logged a duplicate of this bug ( CC(Pytorch fails to install on Windows with HTTP 403 errors)).  It's the same issue I'm sure. Seems similar to a past bug: CC(404 when trying to get pytorchmutex1.0cuda.tar.bz2 from Conda)  which was flagged as high priority. Is there a way to escalate the priority of this one as well?  As it's been down for hours...,"Same here. ``` (base) root4814debian:~ conda install pytorch torchvision torchaudio cpuonly c pytorch y Collecting package metadata (current_repodata.json): done Solving environment: done  Package Plan    environment location: /root/miniconda3   added / updated specs:      cpuonly      pytorch      torchaudio      torchvision The following packages will be downloaded:     package                       0%  CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.909810 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.225389 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.225191 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. CondaHTTPError: HTTP 403 FORBIDDEN for url  Elapsed: 00:00.225416 An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. ```",This is a Conda issue  https://github.com/conda/conda/issues/11638 Closing this as a duplicate of  CC(pytorch conda repo broke today)
yi,[MPS] Get the correct size of the view tensor when copying from cpu to mps ,"Fixes:  CC(Macbook M1 GPU support ""mps"" results in wrong (random) conversion when casting a tensor to LongTensor type.),  CC(MPS device neural net in validation gives the same results after n-th batch element) * Get the correct size of the view tensor when copying from cpu to mps * Use 'computeStorageNbytesContiguous' to get the size just when src is a view * Add asserts and tests to check for storage_offset  * Add testcase for  CC(MPS device neural net in validation gives the same results after n-th batch element) * Replace assert_allclose with assertEqual * Replace TORCH_CHECK with TORCH_INTERNAL_ASSERT",2022-07-19T21:21:47Z,open source Merged cla signed ciflow/trunk,closed,0,4,https://github.com/pytorch/pytorch/issues/81730,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81730**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 09900ad8d0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Remove remaining `eval` calls from `torch/storage.py`,,2022-07-19T15:38:28Z,open source Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81701,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81701**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 6cc61c08a2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Remove `eval` from `torch.storage._TypedStorage.__new__`,,2022-07-19T02:56:07Z,open source Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81679,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81679**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 399a4a9d3e (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Error if trying to enable_reentrant_dispatch but no TorchDispatchMode is set,"Stack from ghstack:  CC(Error if trying to enable_reentrant_dispatch but no TorchDispatchMode is set)  CC(Enable reentrant dispatch for decompositions) If you enable_reentrant_dispatch when TorchDispatchMode is not set, and then dispatch again, then torch dispatch will fail because no mode can be found. This throws a warning that should be easier to debug.",2022-07-19T01:03:50Z,cla signed Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/81677,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81677**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 7 New Failures As of commit 123d82b6a4 (more details on the Dr. CI page): Expand to see more  * **7/7** failures introduced in this PR   :detective: 7 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7400947934?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (dynamo, 2, 2, linux.2xlarge) (1/7) **Step:** ""Test"" (full log  :repeat: rerun)   20220719T02:08:11.6299596Z RuntimeError: test_ops failed!  ``` 20220719T02:08:10.0927122Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestCompositeComplianceCPU20220719014344.xml 20220719T02:08:10.1529732Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestFakeTensorNonErroringCPU20220719014344.xml 20220719T02:08:10.2905751Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestMathBitsCPU20220719014344.xml 20220719T02:08:10.3230349Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestRefsOpsInfoCPU20220719014344.xml 20220719T02:08:10.3892200Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestTagsCPU20220719014344.xml 20220719T02:08:11.6291729Z Traceback (most recent call last): 20220719T02:08:11.6292041Z   File ""test/run_test.py"", line 940, in  20220719T02:08:11.6295723Z     main() 20220719T02:08:11.6295963Z   File ""test/run_test.py"", line 918, in main 20220719T02:08:11.6299124Z     raise RuntimeError(err_message) 20220719T02:08:11.6299596Z RuntimeError: test_ops failed! 20220719T02:08:12.1593566Z  20220719T02:08:12.1593881Z real	33m57.728s 20220719T02:08:12.1594139Z user	40m27.337s 20220719T02:08:12.1594365Z sys	1m56.724s 20220719T02:08:12.1626199Z [error]Process completed with exit code 1. 20220719T02:08:12.1662592Z Prepare all required actions 20220719T02:08:12.1662880Z Getting action download info 20220719T02:08:12.3370048Z [group]Run ./.github/actions/getworkflowjobid 20220719T02:08:12.3370276Z with: 20220719T02:08:12.3370609Z   githubtoken: *** ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign."
yi,optimize_for_mobile has an issue with constant operations at the end of a loop," 🐛 Describe the bug I've been working on a model that I want to optimize for mobile, but when optimizing the model for mobile, I ran into an issue with an operation at the end of a loop. I'll use the code example below to show the issue: ``` import torch from torch import nn from torch.utils.mobile_optimizer import optimize_for_mobile class testModel(nn.Module):     def __init__(self):         super(testModel, self).__init__()          Input is 5, output is 5         self.m = nn.ModuleList([nn.Linear(5, 5) for i in range(5)])      Forward takes noise as input and returns 5 outputs      from the network     def forward(self, X):          the output is initially all ones         Y = torch.ones((5))          array to hold the outputs         out = []          Iterate 5 times to get 5 outputs         for i in range(0, 5):              Send the inputs through the blocks             for b in self.m:                 Y = b(Y)              Save the ith output             out.append(Y[i])              Add a constant to the ith value of Y. This              is what breaks the optimized model             Y[i] *= 2         return torch.stack(out) def main():      Create a new network     model = testModel()      Trash input     X = torch.zeros((5))      Get the network output     Y = model(X)      Create torch script form of the model     ts_model = torch.jit.trace(model, X)     ts_model_mobile = optimize_for_mobile(ts_model)      Get the output from the torch script models     Y_ts_model = ts_model(X)     Y_ts_model_mobile = ts_model_mobile(X)     print(ts_model_mobile.code)      What are the outputs from the model?     print(""Original: "", Y)     print(""Optimized: "", Y_ts_model)     print(""Mobile Optimized: "", Y_ts_model_mobile)      The output of the model should be the same as the      output from the optimized models     assert torch.all(Y.eq(Y_ts_model)), ""Torch script different from original model""     assert torch.all(Y.eq(Y_ts_model_mobile)), ""Mobile torch script different from original model"" if __name__=='__main__':     main() ``` If you run the code, you would expect the output of the torch script model to be the same as the output of the original model, but the outputs of the models are slightly off. The original traced module works perfectly fine, but the mobile script runs into an issue where the output is slightly off for all values in the output vector (excluding the first). Taking a look at the mobile torch script code, I think I see what the issue is: !image At the beginning of each loop, it applies the constant multiplication operation. The first operation is applied at _3 and _4. This operation should be applied after the loop instead of at the beginning. Another issue (which I think is the main problem) is after the multiplication operation is performed on the Y tensor (at _3 and _4), the value is never used. As seen in _5, the _1 tensor is used which is the tensor before the constant operation is applied. Instead, the forward method at _5 should probably be using _4.  Versions torch: 1.12.0",2022-07-18T18:50:32Z,oncall: mobile,open,0,2,https://github.com/pytorch/pytorch/issues/81651,"I was able to simplify the repro a bit: ```python import torch from torch import nn from torch.utils.mobile_optimizer import optimize_for_mobile COUNT = 3 class testModel(nn.Module):     def __init__(self):         super(testModel, self).__init__()         self.m = nn.Linear(COUNT, COUNT)         self.m.weight.detach().copy_(torch.arange(COUNT*COUNT).reshape((COUNT, COUNT)))         self.m.bias.detach().copy_(torch.arange(COUNT))     def forward(self, X):         for i in range(COUNT):             X = self.m(X)             X[i].mul_(2)         return X def main():     X = torch.ones((COUNT))     model = testModel().eval()     ts_model = torch.jit.trace(model, X)     frozen = torch.jit.freeze(ts_model)     semi_optimized = optimize_for_mobile(ts_model, optimization_blocklist=set([torch.utils.mobile_optimizer.MobileOptimizerType.INSERT_FOLD_PREPACK_OPS]))     optimized = optimize_for_mobile(ts_model)     models = [         (""eager"", model),         (""traced"", ts_model),         (""frozen"", frozen),         (""semi_optimized"", semi_optimized),         (""optimized"", optimized),     ]     for n, m in models:         Y = m(X).detach()         print(n, Y)     print()     print(semi_optimized.code)     print(optimized.code) if __name__=='__main__':     main() ``` ```txt eager tensor([  998.,  3231., 10928.]) traced tensor([  998.,  3231., 10928.]) frozen tensor([  998.,  3231., 10928.]) semi_optimized tensor([  998.,  3231., 10928.]) optimized tensor([ 767., 2361., 7910.]) def forward(self,     X: Tensor) > Tensor:   X0 = torch.linear(X, CONSTANTS.c0, CONSTANTS.c1)   _0 = torch.mul_(torch.select(X0, 0, 0), CONSTANTS.c2)   X1 = torch.linear(X0, CONSTANTS.c0, CONSTANTS.c1)   _1 = torch.mul_(torch.select(X1, 0, 1), CONSTANTS.c2)   X2 = torch.linear(X1, CONSTANTS.c0, CONSTANTS.c1)   _2 = torch.mul_(torch.select(X2, 0, 2), CONSTANTS.c2)   return X2 def forward(self,     X: Tensor) > Tensor:   _0 = ops.prepacked.linear_clamp_run(X, CONSTANTS.c0)   _1 = ops.prepacked.linear_clamp_run(_0, CONSTANTS.c0)   _2 = ops.prepacked.linear_clamp_run(_1, CONSTANTS.c0)   _3 = torch.mul_(torch.select(_2, 0, 2), CONSTANTS.c1)   return _2 ``` Looks like the issue is related to the INSERT_FOLD_PREPACK_OPS pass.", thanks for the simplified repro. I can take a look.
yi,Modifying the source code,"I have been trying to make some changes in the source code of PyTorch library, particularly in aten/src files. I want to modify the files on a Linux system.  Issue description I had downloaded the Pytorch Library from source code. I have made made the changes in the .cpp files, particularly, I have added a few printf() statements and then I build using `python3 setup.py develop` .  The source code gets built but on running the Pytorch code, I am unable to see the anything from the printf()'s I added. Any help would be appreciated.",2022-07-18T07:22:43Z,,closed,0,4,https://github.com/pytorch/pytorch/issues/81627,"That should work. If those `printf`s didn't fire it's because that code didn't run. That can be because your code didn't exercise that path, or because you are not using the version of PyTorch you just compiled (you may be using one installed from pip or conda). In any case, the right place to discuss these points is the forum: https://discuss.pytorch.org/","I want to find in which C file the thread is created when I use `torch.set_num_thread()`. For this I did a grep in pytorch source code and then in all those places where there was an occurance of set_num_threads, I added a `printf()`. I built it again and still cannot have anything on the terminal from the `printf()` statements added.","Consider debugging your program for example with `gdb`. But again, the right place to discuss these points is the forums.",Can you please tell how to use gdb in source code. I have also posted on discussions forum
transformer,fix view_copy kernel striding check logic,"The composite kernel for `view_copy` that we generate is specialcased a bit for efficiency to avoid having to do extra clones in some cases. That logic was slightly wrong though, and is fixed here (it needs to mirror the logic in `reshape()`). It manifested as a debug assert firing for Lazy Tensor, which I confirmed no longer fires when running this script: ```  ran with ""python test_ltc_only_torch.py device=lazy sync=1 nvtx=1"" import torch import torch._lazy from torch._lazy.ts_backend import init as init_ts_backend init_ts_backend() torch.manual_seed(42) from transformers import BertForSequenceClassification def parse_args():   import argparse   parser = argparse.ArgumentParser(description='')   parser.add_argument('device', type=str, default='cuda')   parser.add_argument('sync', type=bool, default=False)   parser.add_argument('nvtx', type=bool, default=False)   return parser.parse_args() args = parse_args() device = args.device model = BertForSequenceClassification.from_pretrained('bertbaseuncased', return_dict=True) from transformers import AdamW from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained('bertbaseuncased') text_batch = [""I love Pixar."", ""I don't care for Pixar.""] encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True) input_ids = encoding['input_ids'].to(device) attention_mask = encoding['attention_mask'].to(device) model = model.to(device) model.train() no_decay = ['bias', 'LayerNorm.weight'] optimizer_grouped_parameters = [     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0} ] optimizer = AdamW(optimizer_grouped_parameters, lr=1e5) labels = torch.tensor([1,0]).unsqueeze(0).to(device) for _ in range(6):   torch.cuda.nvtx.range_push(f'Iter{_}')   torch.cuda.nvtx.range_push('F')   outputs = model(input_ids, attention_mask=attention_mask, labels=labels)   if args.sync:     torch._lazy.mark_step()     torch._lazy.wait_device_ops()   torch.cuda.nvtx.range_pop()   loss = outputs.loss   torch.cuda.nvtx.range_push('B')   optimizer.zero_grad()   loss.backward()   if args.sync:     torch._lazy.mark_step()     torch._lazy.wait_device_ops()   torch.cuda.nvtx.range_pop()   torch.cuda.nvtx.range_push('O')   optimizer.step()   if args.sync:     torch._lazy.mark_step()     torch._lazy.wait_device_ops()   torch.cuda.nvtx.range_pop()   torch.cuda.nvtx.range_pop() torch._lazy.mark_step() torch._lazy.wait_device_ops() ``` Stack from ghstack:  CC(fix view_copy kernel striding check logic)",2022-07-15T14:56:47Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81553,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81553**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 2a9c14b548 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Remove unused storage_size,,2022-07-14T22:38:17Z,oncall: distributed better-engineering Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81514,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81514**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 6027b9f607 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Make C10_DEFINE_TLS_static use a template functor rather than a lambda. (For non C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE cases),"  CC(Optimize ThreadLocalDebugInfo lookup)  CC(Make C10_DEFINE_TLS_static use a template functor rather than a lambda. (For non C10_PREFER_CUSTOM_THREAD_LOCAL_STORAGE cases))  CC([Trivial] Delete C10_DECLARE_TLS_class_static and C10_DEFINE_TLS_class_static) There is a nontrivial cost to storing an accessor lambda as a pointer and calling it at each access: https://godbolt.org/z/s8zf19TsM By contrast, if we pass a trivial struct then the compiler can inline and optimize everything away. It seems that roughly 4% of the time to make an empty Tensor comes from this overhead when checking whether to profile memory in the allocator. That said, it's hard to get a precise estimate (even with multiple runs and 50+ replicates). But the benchmark does confirm that the extra instructions actually matter. Differential Revision: D37842249",2022-07-14T02:03:56Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/81452,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81452**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 17 New Failures As of commit e913b8db12 (more details on the Dr. CI page): Expand to see more  * **17/17** failures introduced in this PR   :detective: 17 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7332425282?check_suite_focus=true) pull / linuxjammycuda11.6cudnn8py3.8clang12 / build (1/17) **Step:** ""Build"" (full log  :repeat: rerun)   20220714T02:09:50.8661826Z [error]Process completed with exit code 1.  ``` 20220714T02:09:50.8656302Z Average cache read hit            0.040 s 20220714T02:09:50.8656508Z Failed distributed compilations       0 20220714T02:09:50.8656642Z  20220714T02:09:50.8656751Z Noncacheable reasons: 20220714T02:09:50.8656956Z multiple input files                 15 20220714T02:09:50.8657151Z unknown source language               1 20220714T02:09:50.8657275Z  20220714T02:09:50.8657772Z Cache location                  S3, bucket: Bucket(name=osscicompilercachecircleciv2, base_url=http://osscicompilercachecircleciv2.s3.amazonaws.com/) 20220714T02:09:50.8658168Z + echo ::endgroup:: 20220714T02:09:50.8658578Z [endgroup] 20220714T02:09:50.8661826Z [error]Process completed with exit code 1. 20220714T02:09:50.8686792Z Prepare all required actions 20220714T02:09:50.8705587Z [group]Run ./.github/actions/teardownlinux 20220714T02:09:50.8705791Z with: 20220714T02:09:50.8705938Z [endgroup] 20220714T02:09:50.8720707Z [group]Run .github/scripts/wait_for_ssh_to_drain.sh 20220714T02:09:50.8720953Z [36;1m.github/scripts/wait_for_ssh_to_drain.sh[0m 20220714T02:09:50.8732250Z shell: /usr/bin/bash noprofile norc e o pipefail {0} 20220714T02:09:50.8732481Z [endgroup] 20220714T02:09:50.8769023Z Holding runner for 2 hours until all ssh sessions have logged out 20220714T02:09:50.8808966Z [group]Run  ignore expansion of ""docker ps q"" since it could be empty ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "
transformer,[FSDP] `test_mp_embedding_reduce()` fails with `transformer_auto_wrap_policy()`,"`test_mp_embedding_reduce()` fails if we wrap the `TransformerWithSharedParams` with the typical `transformer_auto_wrap_policy()` with `transformer_layer_cls={TransformerEncoderLayer, TransformerDecoderLayer}`. For now, we only wrap with a single toplevel `FullyShardedDataParallel`, but we should investigate whether this failure is due to how the unit test is written or due to a gap in the mixed precision implementation. ",2022-07-13T20:41:38Z,triaged module: fsdp,open,0,0,https://github.com/pytorch/pytorch/issues/81426
agent,Implement shape/size functions for nestedtensor," 🚀 The feature, motivation and pitch Currently, nt.shape does not work. It just throws an error: ``` >>> import torch >>> x = torch.nested_tensor([torch.rand(2), torch.rand(3)]) :1: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:99.) >>> x.shape Traceback (most recent call last):   File """", line 1, in  RuntimeError: Internal error: NestedTensorImpl doesn't support sizes. Please file an issue on https://github.com/pytorch/nestedtensor ``` The shape function has been implemented, as seen in this colab, it just hasn't been migrated to torch. I do not see any downside to implementing this function, as it is easy it implement (the code is already written; it just needs to be migrated), and it's much better than throwing an error.  Alternatives _No response_  Additional context My use case: I am working with graph neural networks, which involve a variable number of agents, each of which can observe a variable number of objects in the environment. if fully functional, nestedtensor would significantly simply the problem, as it would obviate the need to store and manipulate tensors of batch indices. ",2022-07-13T14:50:48Z,triaged module: nestedtensor,open,2,6,https://github.com/pytorch/pytorch/issues/81405, ,"`my_dataset = TensorDataset(tensor_x,tensor_y)  create your dataset`  I am trying to create a dataset. `tensor_x` data used for a model that has 2 inputs. I am running a Multitask learning experiment. Running into the same error: ``` python3.9/sitepackages/torch/utils/data/dataset.py"", line 184, in      assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors), ""Size mismatch between tensors"" RuntimeError: Internal error: NestedTensorImpl doesn't support sizes. Please file an issue on https://github.com/pytorch/nestedtensor ^CProcess Process1: ```","  which version of PyTorch are you using? The snippet you're referencing calls into `.size(0)`, which we support for NestedTensor.",PyTorch version `1.12.1` Python version `3.9.2`, do I need to update anything? How do I solve this error?, use the latest pytorch nightly
rag,"[Profiler] Make KinetoEvent a view of Result (Part 4 (final), stragglers)","  CC([Profiler] Start moving python bindings out of autograd)  CC([Profiler] Make KinetoEvent a view of Result (Part 4 (final), stragglers))  CC([Profiler] Make KinetoEvent a view of Result (Part 3, forwarded from `result_`))  CC([Profiler] Make KinetoEvent a view of Result (Part 2, python and stacks))  CC([Profiler] Make KinetoEvent a view of Result (Part 1: trivial fields)) This PR just moves all the KinetoEvent methods which didn't fit the previous categories. Now that we no longer need to set Kineto event fields in `EventFieldsVisitor` we can remove the reference wrapper and rename the visitor to `AddKinetoMetadata` since that's all it does now. Differential Revision: D37490053",2022-07-12T14:59:33Z,Merged cla signed ciflow/trunk release notes: profiler ciflow/periodic,closed,0,10,https://github.com/pytorch/pytorch/issues/81322,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81322**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 3 New Failures As of commit 0da3ad1e5e (more details on the Dr. CI page): Expand to see more  * **3/3** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7717258108?check_suite_focus=true) periodic / ios1251arm64metal / build (1/3) **Step:** ""Unknown"" (full log  : ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge l, successfully started a merge job. Check the current status here,"Merge failed due to 2 additional jobs have failed, first few of them are: periodic ,periodic / buckbuildtest / buckbuildtest Raised by https://github.com/pytorch/pytorch/actions/runs/2810411058", merge l, successfully started a merge job. Check the current status here,"Merge failed due to 1 additional jobs have failed, first few of them are: periodic Raised by https://github.com/pytorch/pytorch/actions/runs/2815669061"," merge f ""I have tried to land this responsibly but infra is too flaky.""", successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,disable src mask for transformer and multiheadattention fastpath,Disable fastpath if src_mask passed to TransformerEncoderLayer and MultiheadAttention.   Refactored test_transformerencoder from test_nn.py to test_transformers.py. Added a src_mask test there.  Added a specific src_mask test in test_transformers.py Fixes  CC(Transformer and CPU path with `src_mask` raises error with torch 1.12),2022-07-11T21:22:52Z,Merged cla signed release notes: nn topic: bug fixes,closed,0,4,https://github.com/pytorch/pytorch/issues/81277,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81277**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 5b06d9ad70 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,fixing call_module on subscripting into generator,"named_modules() return a generator, which is not subscriptable and causes node support query to fail",2022-07-11T18:02:35Z,triaged open source Merged cla signed module: fx,closed,0,4,https://github.com/pytorch/pytorch/issues/81258,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81258**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c0f077e898 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,grad not preserved during copying or pickling," 🐛 Describe the bug I would like to (cloud)pickle some Tensors. However, their `grad` attribute is lost during pickling. The following snippet  ```python import cloudpickle import copy import torch def square(x):     return x*x x = torch.tensor([2.0], requires_grad=True) loss = square(x) loss.backward() print (type(x), x, x.grad) y = copy.copy(x) print (type(y), y, y.grad) y = cloudpickle.loads(cloudpickle.dumps(x)) print (type(y), y, y.grad) ``` yields the following output: ```  tensor([2.], requires_grad=True) tensor([4.])  tensor([2.], requires_grad=True) None  tensor([2.], requires_grad=True) None ``` I can work around this using a custom reduction function for `torch.Tensor` like ```python import copyreg def reconstruct_tensor(val, requires_grad, grad, device):     t = torch.tensor(val, device=device, requires_grad=requires_grad)     if grad:         t.grad = torch.from_numpy(grad)     return t def reduce_tensor(t):     if t.requires_grad:         return reconstruct_tensor, (t.detach().numpy(), True, t.grad.numpy(), t.device.type)     else:         return reconstruct_tensor, (t.numpy(), False, None, t.device.type) copyreg.pickle(torch.Tensor, reduce_tensor)  Include previous snippet here ``` which yields the expected output ```  tensor([2.], requires_grad=True) tensor([4.])  tensor([2.], requires_grad=True) tensor([4.])  tensor([2.], requires_grad=True) tensor([4.]) ``` This is basically what Dask does. But a better place to fix this would seem to be in `Tensor.__reduce_ex__`  Versions PyTorch version: 1.12.0+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Fedora Linux 35 (Container Image) (x86_64) GCC version: (GCC) 11.2.1 20220127 (Red Hat 11.2.19) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.34 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.18.9200.fc36.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.0 [pip3] torch==1.12.0+cpu [pip3] torchaudio==0.12.0+cpu [pip3] torchvision==0.13.0+cpu [conda] numpy                     1.23.0                   pypi_0    pypi [conda] torch                     1.12.0+cpu               pypi_0    pypi [conda] torchaudio                0.12.0+cpu               pypi_0    pypi [conda] torchvision               0.13.0+cpu               pypi_0    pypi",2022-07-10T18:28:35Z,triaged module: python frontend,open,0,2,https://github.com/pytorch/pytorch/issues/81186,Can you confirm that `copy.deepcopy()` does preserve the grad field right?  ," Yes, `copy.deepcopy()` works."
yi,[di] avoid copying optional input for get_real_inputs_from_optional_inputs_v2 when possible,Summary: avoid copies and casting by moving out of the inputs list Differential Revision: D37572556,2022-07-08T20:42:22Z,oncall: jit fb-exported Merged cla signed,closed,0,17,https://github.com/pytorch/pytorch/issues/81137,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81137**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 93077fbfdb (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556,This pull request was **exported** from Phabricator. Differential Revision: D37572556, merge (Initiating merge automatically since Phabricator Diff has merged), successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Transformer and CPU path with `src_mask` raises error with torch 1.12," 🐛 Describe the bug The following code, which runs on torch 1.11 cpu, doesn't anymore on torch 1.12: ```python import torch model = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True) src = torch.rand(32, 10, 512) src_mask = torch.zeros(10, 10).to(torch.bool) model.eval() with torch.no_grad():     print(model(src, src_mask)) ``` It raises  ```  Traceback (most recent call last):   File ""/Users/adm/Desktop/main.py"", line 9, in      print(model(src, src_mask))   File ""/Users/adm/pytorch/torch/nn/modules/module.py"", line 1186, in _call_impl     return forward_call(*input, **kwargs)   File ""/Users/adm/pytorch/torch/nn/modules/transformer.py"", line 439, in forward     return torch._transformer_encoder_layer_fwd( RuntimeError: Expected attn_mask>sizes()[0] == batch_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.) ``` The `.to(torch.bool)` is only here to silence the warning: `UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly.`, the original code on v1.11 doesn't use it. This also happens on an x86 ubuntu machine, when using cpu, but it does not happens when using CUDA. Because of this condition: https://github.com/pytorch/pytorch/blob/e9b3bc2eadb8ffe10c002abcd5a34a5b7d36f390/aten/src/ATen/native/transformers/attention.cppL136L144 using `src_mask = torch.zeros(32, 10)` makes the error go away, but it must not be right because I believe the size of `src_mask` should be `(seq_len, seq_len)`.  Instead, on torch 1.11, using `(32, 10)` was raising `RuntimeError: The shape of the 2D attn_mask is torch.Size([32, 10]), but should be (10, 10).`.  Also, the comment saying CPU path doesn't support mask makes me wonder if I'm looking at the right code 🤔   Versions Collecting environment information... PyTorch version: 1.13.0a0+git4c57cf9 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.23.2 Libc version: N/A Python version: 3.10.5  (main, Jun 14 2022, 07:07:06) [Clang 13.0.1 ] (64bit runtime) Python platform: macOS12.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.0 [pip3] torch==1.13.0a0+git4c57cf9 [conda] numpy                     1.23.0          py310h0a343b5_0    condaforge [conda] torch                     1.13.0a0+git4c57cf9           dev_0     ",2022-07-08T19:11:15Z,high priority triage review module: regression,closed,0,6,https://github.com/pytorch/pytorch/issues/81129,This is a new transformer implementation issue. Thank you for bringing this up  we'll get a fix asap. ,"Relevant function is  https://github.com/pytorch/pytorch/blob/e9b3bc2eadb8ffe10c002abcd5a34a5b7d36f390/aten/src/ATen/native/transformers/attention.cppL118L150 Tagging . Looks like cpu path assumed src_key_padding_mask with shape (batch size, seqlen) and not src_mask with shape (seqlen, seqlen)? src_mask works fine on the cuda path since _masked_softmax takes care of it.  I think the solution is to pass a flag saying whether this is a src_mask or a src_key_padding_mask and deal with the mask accordingly. There's not an easy way to tell if the given attention_mask is one or the other if the batch size and sequence length are the same.  Edit: For now solution I think is to just disable fast path if src_mask is passed in. ","IMO, best solution is probably to have parity between the CPU and CUDA paths in masked_softmax w.r.t. ""transformer mask"" support, then we don't have to continue layering more special cases onto the specialcase fix for the gap in support.","Main PR to fix is https://github.com/pytorch/pytorch/pull/81277. I've also taken the liberty to increase our test coverage in test/test_transformers.py. For cherrypicking onto the 1.12.1 release we will also need https://github.com/pytorch/pytorch/pull/79796 because it created test_transformers.py but didn't get into the 1.12 release. : In addition to the above two PRs, we'll also need https://github.com/pytorch/pytorch/pull/81013 because my fix sits on top of that PR. ", Fix was CC(disable src mask for transformer and multiheadattention fastpath) which landed yesterday. Should work now  let me know if issues remain. ,Thanks for the quick fix !
yi,Add `scalarType` to JitType pyi,Fixes mypy errors when calling `.type().scalarType()` on an `_C.Value` object. Reference https://github.com/pytorch/pytorch/blob/28776c45e393ec7d6731a5613f5d79d309036d3a/torch/csrc/jit/python/python_ir.cppL891L896,2022-07-08T15:51:18Z,module: typing triaged open source Merged cla signed topic: not user facing,closed,0,11,https://github.com/pytorch/pytorch/issues/81112,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81112**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 01ff7db3b2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , could you help take a look? thanks, merge, successfully started a merge job. Check the current status here,Merge failed due to This PR is too stale; the last push date was more than 3 days ago. Please rebase and try again. Raised by https://github.com/pytorch/pytorch/actions/runs/2715553287, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `justinchu/jittypehint` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `git checkout justinchu/jittypehint && git pull rebase`)", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[FSDP] Rename `transformer_auto_wrap_policy()`,"Per varma's suggestion, we can rename `transformer_auto_wrap_policy()` to reflect that it works for any `layer_classes: Set[Type[nn.Module]]`, not just transformer layer classes. https://github.com/pytorch/pytorch/blob/c0e15fe3f4d5622c7a687cf0d503902be9296596/torch/distributed/fsdp/wrap.pyL81L86 https://github.com/pytorch/pytorch/blob/c0e15fe3f4d5622c7a687cf0d503902be9296596/torch/distributed/fsdp/wrap.pyL117L122  We need to be wary of any existing model code assuming this `transformer_auto_wrap_policy()` name. If breaking this backward compatibility, then we may need to deprecate `transformer_auto_wrap_policy()` (e.g. adding a warning) and add a new function with the new name that does the exact same thing. ",2022-07-07T16:17:17Z,triaged module: fsdp,closed,0,2,https://github.com/pytorch/pytorch/issues/81050,"transformer_auto_wrap_policy() possibly has already been used for multiple use cases, since we are working on nonrecursive wrapping, once it is finalized, we can revisit naming here later on?",https://github.com/pytorch/pytorch/pull/88450
yi,Add doc string for Library.impl,Stack from ghstack:  CC(Add doc string for Library.impl),2022-07-07T15:30:33Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/81047,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81047**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 4a3e4eb46e (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,torch.utils.bottleneck spams output and crashes," 🐛 Describe the bug I have a Python script that trains an NN, and am trying to optimize it using bottleneck, but I can't. The command `python m torch.utils.bottleneck TA_generator.py` runs through my code about 3 times and then fills the console with spam. It should be noted that `python TA_generator.py` runs without issue: ```import os from collections import OrderedDict from copy import deepcopy from datetime import datetime import torch from torch.utils.data import DataLoader, Dataset from transformers import BertTokenizerFast, BertForPreTraining, BertConfig  paths proj_dir = '/scratch/ddegenaro' def in_proj_dir(dir):     return os.path.join(proj_dir, dir) pretraining_test = in_proj_dir('pretraining_test.txt') pretraining_txt = in_proj_dir('pretraining.txt') inits = in_proj_dir('inits') ckpts = in_proj_dir('ckpts') trained = in_proj_dir('trained') print('Getting tokenizer.')  get tokenizer and initialize teacher model mBERT tokenizer = BertTokenizerFast.from_pretrained(""bertbasemultilingualcased"", do_lower_case=False) print('Done.') print('Getting mBERT.')  this line will complain that decoder bias was not in the checkpoint mBERT = BertForPreTraining.from_pretrained(""bertbasemultilingualcased"") print('Done.') teacher = mBERT  first network to copy from MSELoss = torch.nn.MSELoss()  loss between logits of two models batch_size = 65536  batch size epochs = 1  num epochs  SHOULD BE 32 class BertData(Dataset):     def __init__(self):         print('Reading in corpus. Warning: requires ~ 50 GB of RAM.')         self.corpus = open(pretraining_test).readlines()         print('Done.')     def __len__(self):         return len(self.corpus)     def __getitem__(self, idx):       return tokenizer(self.corpus[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=512) dataset = BertData() data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=1, pin_memory=True)  should have num_workers=8 or 12 for i in reversed(range(11,12)):  TA builder loop  SHOULD BE 2, 12   teacher_state_dict = teacher.state_dict()    create a BertConfig with a multilingual vocabulary for the next TA   config_obj = BertConfig(vocab_size=119547, num_hidden_layers=i)   student = BertForPreTraining(config_obj)  initialize next model and state dict   student_state_dict = OrderedDict()   torch.cuda.empty_cache()   teacher.to('cuda')  use GPU   student.to('cuda')   print('Building student.')   for key in teacher_state_dict:  copy architecture and weights besides top layer     if str(i) not in key:       student_state_dict[key] = deepcopy(teacher_state_dict[key])   print('Done.')    save init for this TA   print('Saving student.')   torch.save(student_state_dict, os.path.join(inits, 'ta' + str(i)))   print('Done.')    load next state dict into the next model   student.load_state_dict(student_state_dict)   student.train()  ensure training mode    generate Adam optimizer close to mBERT's   optimizer = torch.optim.Adam(student.parameters(), lr=(batch_size/256*1e4),                              betas=(0.9, 0.999), eps=1e06, weight_decay=0)   optimizer.zero_grad(set_to_none=True)  just to be sure   with torch.set_grad_enabled(True):     for k in range(epochs):       start = datetime.now()       print(f'Begin epoch {k+1}/{epochs}. Current time: {datetime.now()}.')       loss = 0  initialize       for batch_idx, inputs in enumerate(data_loader):         for j in inputs:           inputs[j] = inputs[j][0]         inputs = inputs.to('cuda')          get teacher and student predictions         teacher_logits = teacher(**inputs).prediction_logits         student_logits = student(**inputs).prediction_logits          calculate the loss between them and update         loss = MSELoss(teacher_logits, student_logits) / batch_size          learning step         loss.backward()         optimizer.step()         optimizer.zero_grad(set_to_none=True)         loss = 0         print(batch_idx+1, (datetime.now()start)/(batch_idx+1))       torch.save(student.state_dict(), os.path.join(ckpts, 'ta' + str(i) + '_ckpt' + str(k)))    save trained model for this TA   torch.save(student.state_dict(), os.path.join(trained, 'ta' + str(i)))   teacher = student  prepare to initialize next network  end for ``` `pretraining_test.txt` is simply a text file of 100,000 lines. I would post a stack trace, but the console is filled with spam so quickly that it is impossible to get any Traceback. Attached is a screencap of some spam  it's the same message on repeat. !spam  Versions Collecting environment information... PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.10.2 Libc version: glibc2.27 Python version: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0] (64bit runtime) Python platform: Linux4.15.0180genericx86_64withglibc2.27 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: GeForce RTX 2080 Ti GPU 1: GeForce RTX 2080 Ti GPU 2: GeForce RTX 2080 Ti Nvidia driver version: 460.106.00 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.3.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [pip3] torchvision==0.13.0 [conda] blas                      1.0                         mkl [conda] cudatoolkit               11.3.1               h2bc3f7f_2 [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklservice               2.4.0           py310h7f8727e_0 [conda] mkl_fft                   1.3.1           py310hd6ae3a3_0 [conda] mkl_random                1.2.2           py310h00e6091_0 [conda] numpy                     1.22.3          py310hfa59a62_0 [conda] numpybase                1.22.3          py310h9585f30_0 [conda] pytorch                   1.12.0          py3.10_cuda11.3_cudnn8.3.2_0    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torchaudio                0.12.0              py310_cu113    pytorch [conda] torchvision               0.13.0              py310_cu113    pytorch",2022-07-07T01:30:18Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/81026,"Hi, We use github issues only for bug reports or feature requests. Please use the forum to ask questions: https://discuss.pytorch.org/","I'm sorry if this is unclear, but I don't see how this isn't a bug. My script runs fine until I try to profile it using the bottleneck module, then it spams my output. I feel that something is wrong with bottleneck if this is the case.","themememan  The issue referenced in the output is  CC([utils.bottleneck] Bottleneck crashes with multi-threaded data loader) . The message suggests to not use this feature in conjunction with DataLoader and `num_workers > 0`. It's not spam output, but useful information that references an existing issue, which is therefore not a bug."
transformer,[Bootcamp T124004534] Better Transformer fastpath diagnostics,Summary: update the boolean logic for TransformerEncoder => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/transformer.py?lines=206212%2C228230 and TransformerEncoderLayer => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/transformer.py?lines=410417%2C432436 according to the decision logic => https://www.internalfb.com/code/fbsource/[0da9d46c97ca76432303e55492b1e8b4131c781a]/fbcode/caffe2/torch/nn/modules/activation.py?lines=10591091%2C11031109 Differential Revision: D37666451,2022-07-06T22:54:53Z,fb-exported Merged cla signed,closed,0,16,https://github.com/pytorch/pytorch/issues/81013,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/81013**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 0e8900f836 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451,This pull request was **exported** from Phabricator. Differential Revision: D37666451, merge (Initiating merge automatically since Phabricator Diff has merged), successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[ONNX] Fix bug using std::copy_if,"  CC([ONNX] Fix bug using std::copy_if) `std::copy_if` requires preallocated destination. Hence `scopes` needs to either be pre allocated, or use `std::inserter` for copy. Ref: https://en.cppreference.com/w/cpp/algorithm/copy",2022-07-06T20:55:34Z,oncall: jit module: onnx open source Merged cla signed release notes: onnx topic: bug fixes onnx-needs-import,closed,0,8,https://github.com/pytorch/pytorch/issues/80999,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80999**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c20d981e9c (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Curious what the bug was? (Helpful to include in the PR description),, rebase, successfully started a rebase job. Check the current status here,"Successfully rebased `gh/BowenBao/152/orig` onto `refs/remotes/origin/master`, please pull locally before adding more changes (for example, via `ghstack checkout https://github.com/pytorch/pytorch/pull/80999`)", merge g, successfully started a merge job. Check the current status here
yi,[bug][nvfuser] Applying nvfuser to the model leads to runtime error," 🐛 Describe the bug ``` Traceback (most recent call last):   File ""main.py"", line 202, in      all_metrics = trainer.train(args.steps, args.val_steps, args.save_every, args.eval_every)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 127, in train     d_loss, g_loss, d_aux_losses, g_aux_losses = self.train_step(args_i)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 189, in train_step     d_base_loss_i, d_aux_losses_i = self._discriminator_step(args)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 383, in _discriminator_step     aux_losses[loss_fct.name] = loss_fct(self.discriminator, self.generator, candidate_batch, fake_batch, args, ""discriminator"", reference_batch)   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1111, in _call_impl     return forward_call(*input, **kwargs)   File ""/h/zhengboj/SetGan/setgan/training_utils.py"", line 204, in forward     return self._discriminator_loss(discriminator, generator, real_batch, fake_batch, args, *loss_args, **loss_kwargs)   File ""/h/zhengboj/SetGan/setgan/training_utils.py"", line 236, in _discriminator_loss     scaled_gradients = torch.autograd.grad(outputs=self.scaler.scale(disc_interpolates), inputs=interpolates,   File ""/opt/conda/lib/python3.8/sitepackages/torch/autograd/__init__.py"", line 275, in grad     return Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass RuntimeError: 0INTERNAL ASSERT FAILED at ""/opt/pytorch/pytorch/torch/csrc/jit/ir/alias_analysis.cpp"":602, please report a bug to PyTorch. We don't have an op for aten::cat but it isn't a special case.  Argument types: Tensor, int,  Candidates:         aten::cat(Tensor[] tensors, int dim=0) > (Tensor)         aten::cat.names(Tensor[] tensors, str dim) > (Tensor)         aten::cat.names_out(Tensor[] tensors, str dim, *, Tensor(a!) out) > (Tensor(a!))         aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) > (Tensor(a!)) Generated: ``` I tried to enable nvfuser when training my model but got the above runtime error. I also tried running my code without scripting the model and everything goes fine. It seems that the error is caused by invoking the `torch.cat` operator with a single tensor. However, after checking the source code, I can verify that each `torch.cat` operator is invoked with a list of tensors. Therefore, I am not sure what is causing this issue. Any help is appreciated. Thanks.  Versions ``` Collecting environment information... PyTorch version: 1.12.0a0+2c916ef Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04) 9.4.0 Clang version: Could not collect CMake version: version 3.22.3 Libc version: glibc2.31 Python version: 3.8.12  (default, Jan 30 2022, 23:42:07)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.0110genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.6.112 GPU models and configuration:  GPU 0: Tesla T4 GPU 1: Tesla T4 GPU 2: Tesla T4 GPU 3: Tesla T4 Nvidia driver version: 470.103.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.3 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.3 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.3 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.3.3 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.3.3 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] pytorchquantization==2.1.2 [pip3] torch==1.12.0a0+2c916ef [pip3] torchtensorrt==1.1.0a0 [pip3] torchtext==0.12.0a0 [pip3] torchvision==0.13.0a0 [conda] magmacuda110             2.5.2                         5    local [conda] mkl                       2019.5                      281    condaforge [conda] mklinclude               2019.5                      281    condaforge [conda] numpy                     1.22.3           py38h05e7239_0    condaforge [conda] pytorchquantization      2.1.2                    pypi_0    pypi [conda] torch                     1.12.0a0+2c916ef          pypi_0    pypi [conda] torchtensorrt            1.1.0a0                  pypi_0    pypi [conda] torchtext                 0.12.0a0                 pypi_0    pypi [conda] torchvision               0.13.0a0                 pypi_0    pypi ``` FYI, ",2022-07-04T22:00:51Z,triaged module: nvfuser,open,0,23,https://github.com/pytorch/pytorch/issues/80851,Can you please provide a script reproducing the error?  ,"> Can you please provide a script reproducing the error? >  >  , Thank you so much for checking on this issue! Since we are working on a closesourced repo, would you (or anyone else who would be helping to debug this) be open to being added into this private repo? Alternatively we could prepare a code snippet that reproduce this error, however, that might take some time (this error message arises during 'torch.autograd.grad' and we are not yet certain which layers/ops led to this bug). Thanks!","I'm taking a wild guess that there's something with the concat/chunk optimization pass inside fusion. Would you mind running your script with debug dump? `PYTORCH_JIT_LOG_LEVEL=graph_fuser python your_script.py &> log`, We should be able to see the last fusion graph and figure out a repro from there. FYI, there's a `debug nvfuser` section in https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/codegen/cuda/README.md","I won't be working on debugging this issue, but Nvidia's nvfuser team might be interested. ,  can you guys please discuss what the options are?","> I won't be working on debugging this issue, but Nvidia's nvfuser team might be interested. ,  can you guys please discuss what the options are? Hopefully it'll be easy to get some debug dump so we can reverse engineer a repro from there. Not sure how tricky it would be for me to get into their closesourced repo, I would at least need to get management approval. Let's consider that as a backup plan~",",  can you guys start by following the steps here https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/cudageneralideasofdebugnvfusermalfunctioning, or is something else a better approach?","Yeah, I think doing this `PYTORCH_JIT_LOG_LEVEL=graph_fuser python your_script.py &> log` would be a reasonable first step. Should be able to workout a repro with that."," Thanks. I have attached the compilation log here. nvfuser_compilation.log FYI,  ","errr, I actually don't see anything went wrong in the graph log (all aten::cat are operating on list of tensors), looks like we are not trying to insert any chunk neither.... Unfortunately we don't have a graph dump in aliasdb. Let's verify if this is nvfuser specific. We can run things with `PYTORCH_JIT_USE_NNC_NOT_NVFUSER=1 PYTORCH_JIT_LOG_LEVEL=profiling_executor_graph_impl python your_script.py &> log` Would be nice to also run that without NNC (if NNC runs fine without any issue) and give me both of the logs `PYTORCH_JIT_LOG_LEVEL=profiling_executor_graph_impl python your_script.py &> log`","Hi   Thanks for your suggestions. The first command gives the following error: ``` Traceback (most recent call last):   File ""main.py"", line 202, in      all_metrics = trainer.train(args.steps, args.val_steps, args.save_every, args.eval_every)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 127, in train     d_loss, g_loss, d_aux_losses, g_aux_losses = self.train_step(args_i)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 189, in train_step     d_base_loss_i, d_aux_losses_i = self._discriminator_step(args)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 383, in _discriminator_step     aux_losses[loss_fct.name] = loss_fct(self.discriminator, self.generator, candidate_batch, fake_batch, args, ""discriminator"", reference_batch)   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1111, in _call_impl     return forward_call(*input, **kwargs)   File ""/h/zhengboj/SetGan/setgan/training_utils.py"", line 204, in forward     return self._discriminator_loss(discriminator, generator, real_batch, fake_batch, args, *loss_args, **loss_kwargs)   File ""/h/zhengboj/SetGan/setgan/training_utils.py"", line 236, in _discriminator_loss     scaled_gradients = torch.autograd.grad(outputs=self.scaler.scale(disc_interpolates), inputs=interpolates,   File ""/opt/conda/lib/python3.8/sitepackages/torch/autograd/__init__.py"", line 275, in grad     return Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass RuntimeError: 0INTERNAL ASSERT FAILED at ""/opt/pytorch/pytorch/torch/csrc/jit/ir/alias_analysis.cpp"":602, please report a bug to PyTorch. We don't have an op for aten::cat but it isn't a special case.  Argument types: Tensor, int,  Candidates: 	aten::cat(Tensor[] tensors, int dim=0) > (Tensor) 	aten::cat.names(Tensor[] tensors, str dim) > (Tensor) 	aten::cat.names_out(Tensor[] tensors, str dim, *, Tensor(a!) out) > (Tensor(a!)) 	aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) > (Tensor(a!)) Generated: ``` and the second command gives the same error.","So this is not nvfuser specific, as NNC also repros this one. I can still help tho. Would you be able to push the full log somewhere when running with the second config: `PYTORCH_JIT_LOG_LEVEL=profiling_executor_graph_impl python your_script.py &> log`. < this should dump a graph somewhere and we can stare at that to figure out a repro....","Here is the full log: ``` Profiling application [python3 main.py set_gan task omniglot n 2 batch_size 128 latent_size 128 hidden_size 256 lr_d 1e4 lr_g 1e4 reference_size 8 12 candidate_size 5 8 steps 3000000 val_steps 250 warmup_steps 200000 anneal_steps 1000000 num_heads 8 disc_blocks 2 gen_blocks 2 output_layers 1 dropout 0.1 noise_dim 100 l_smooth 0.2 p_flip 0.2 nu 3 mu0 0.3 s0 0.2 mu_scale 8 basedir omniglot_runs save_every 10000 eval_every 10000 conv_mode seq n_critic 5 criterion wgan eps 1e5 lambda_gp 10.0 lambda_cyc 0.1 model_type med imgsize 64 gen_sample_size 4 4 gaussian_noise 0 checkpoint_dir ./7937413 run_id 7937413 n_gpus 1 no_encoder gradient_penalty bn save_gen_samples use_apex use_amp] Output Filename: tmp Profile from start?: No (cudaProfilerStart/Stop required) {} 128, 256 {} 128, 256 Applying torch.jit.script to the model ... Traceback (most recent call last):   File ""main.py"", line 202, in      all_metrics = trainer.train(args.steps, args.val_steps, args.save_every, args.eval_every)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 127, in train     d_loss, g_loss, d_aux_losses, g_aux_losses = self.train_step(args_i)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 191, in train_step     d_base_loss_i, d_aux_losses_i = self._discriminator_step(args)   File ""/h/zhengboj/SetGan/setgan/trainer.py"", line 385, in _discriminator_step     aux_losses[loss_fct.name] = loss_fct(self.discriminator, self.generator, candidate_batch, fake_batch, args, ""discriminator"", reference_batch)   File ""/opt/conda/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1111, in _call_impl     return forward_call(*input, **kwargs)   File ""/h/zhengboj/SetGan/setgan/training_utils.py"", line 204, in forward     return self._discriminator_loss(discriminator, generator, real_batch, fake_batch, args, *loss_args, **loss_kwargs)   File ""/h/zhengboj/SetGan/setgan/training_utils.py"", line 236, in _discriminator_loss     scaled_gradients = torch.autograd.grad(outputs=self.scaler.scale(disc_interpolates), inputs=interpolates,   File ""/opt/conda/lib/python3.8/sitepackages/torch/autograd/__init__.py"", line 275, in grad     return Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass RuntimeError: 0INTERNAL ASSERT FAILED at ""/opt/pytorch/pytorch/torch/csrc/jit/ir/alias_analysis.cpp"":602, please report a bug to PyTorch. We don't have an op for aten::cat but it isn't a special case.  Argument types: Tensor, int,  Candidates: 	aten::cat(Tensor[] tensors, int dim=0) > (Tensor) 	aten::cat.names(Tensor[] tensors, str dim) > (Tensor) 	aten::cat.names_out(Tensor[] tensors, str dim, *, Tensor(a!) out) > (Tensor(a!)) 	aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) > (Tensor(a!)) ``` Sorry but I do not see a graph dumped somewhere.","Hi . Sorry for bothering you again, but I am wondering what your advice on the next step would be if  ``` PYTORCH_JIT_LOG_LEVEL=profiling_executor_graph_impl  ``` does not dump the graph information.","Oops, sorry I missed last message. My bad that I gave the wrong instruction :face_with_head_bandage: ... It should be `PYTORCH_JIT_LOG_LEVEL=profiling_graph_executor_impl`. It's using the native jit log https://github.com/pytorch/pytorch/blob/c94706c0118a41d0400e78b01dae897d0f1f9457/torch/csrc/jit/jit_log.hL8L39 We should be able to see a very long log dumping graph from this file: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp We'll be able to see which graph partition hits that assert then.",Hi  I have attached the compilation log here: nvfuser_compilation.log Thank you for your help.,"At the very end of the log, it prints out a `Profiled Graph` and then the assert failure. That graph is unfortunately a backward graph so it's somewhat cryptic and I can't figure out how to reverseengineer the forward graph from that.  :face_with_head_bandage:  I'm still leaning towards that one of the passes here (https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/runtime/profiling_graph_executor_impl.cppL658L678) introduced the issue. Let's dump the log in verbose mode with `PYTORCH_JIT_LOG_LEVEL="">>profiling_graph_executor_impl""`. We'll unfortunately see a much bigger log, but hopefully it would expose precisely which pass generates an invalid graph. :crossed_fingers:  Sorry for this back and forth. :cry: ",Hi. I have attached the verbose compilation log to this thread. Because the file is too large I have to zip it before uploading. Thank you. nvfuser_verbose_compilation.log.zip,"Looks like it's coming from autodiff. We just forgot to update the output type. A simpler repro: ``` import torch def fn(x):     o = x + 1     o = o.split(2, 2)     o = torch.cat(o, 0)     return o device = ""cuda"" scripted_ln = torch.jit.script(fn) x=torch.randn(4, 2, 4, device=device, requires_grad=True) for _ in range(4):     out=scripted_ln(x)     out.sum().backward() ``` Here we seems to forgot to update the output type for `AutogradAdd` https://github.com/pytorch/pytorch/blob/bcc8f592ba060cdc011c7534a2832ea9b816f733/torch/csrc/jit/runtime/autodiff.cppL354L357 I tried to patch that one and copy input type to output. Which seems to have fixed the original issue. But looks like `AutogradAdd` doesn't really handle list tensors. (Even though there's list in a few other places in symbolic_script.cpp) https://github.com/pytorch/pytorch/blob/bcc8f592ba060cdc011c7534a2832ea9b816f733/torch/csrc/jit/runtime/register_prim_ops.cppL2518L2549 Here's the updated graph with AutogradAdd return type patched ```  Forward graph:  graph(%x.1 : Tensor):    %11 : int = prim::Constant[value=1]()    %o.18 : Tensor = aten::add(%x.1, %11, %11)  repro_autodiff.py:4:8    %6 : int = prim::Constant[value=2]()    %o.15 : Tensor[] = aten::split(%o.18, %6, %6)  repro_autodiff.py:5:8    %3 : int = prim::Constant[value=0]()    %o.10 : Tensor = aten::cat(%o.15, %3)  repro_autodiff.py:6:8    %17 : bool = prim::Constant[value=1]()  :168:12    %size.1 : int = aten::len(%o.15)  :166:19    %20 : int[] = prim::ListConstruct(%3)    %split_sizes.4 : int[] = aten::mul(%20, %size.1)  :167:26     = prim::Loop(%size.1, %17)  :168:12      block0(%i.1 : int):        %23 : Tensor = aten::__getitem__(%o.15, %i.1)  :169:19        %24 : int[] = aten::size(%23)  :169:19        %26 : bool = aten::ne(%24, %20)  :169:19         = prim::If(%26)  :169:16          block0():            %27 : Tensor = aten::__getitem__(%o.15, %i.1)  :170:37            %28 : int[] = aten::size(%27)  :170:37            %29 : int = aten::__getitem__(%28, %3)  :170:37            %30 : int[] = aten::_set_item(%split_sizes.4, %i.1, %29)  :170:20            > ()          block1():            > ()        > (%17)    return (%o.10, %o.15, %3, %17, %20, %split_sizes.4)  Backward graph:  graph(%0 : Float(8, 2, 2, strides=[4, 2, 1], requires_grad=1, device=cuda:0),        %1 : Tensor[],        %o.14 : Tensor[],        %3 : int,        %4 : bool,        %5 : int[],        %split_sizes.2 : int[]):    %7 : int = prim::Constant[value=2]()    %8 : int = prim::Constant[value=0]()    %9 : Float(8, 2, 2, strides=[4, 2, 1], requires_grad=1, device=cuda:0) = prim::GradOfname=""prim::profile""      block0():        > (%0)    %grad_tensors.1 : Tensor[] = prim::GradOfname=""aten::cat""      block0():        %grad_tensors.2 : Tensor[] = aten::split_with_sizes(%9, %split_sizes.2, %8)  :173:31        > (%grad_tensors.2)    %12 : Tensor[] = prim::AutogradAdd(%1, %grad_tensors.1)                                   :133:28        > (%grad_self.2)    %grad_self.6 : Tensor = prim::GradOfname=""prim::profile""      block0():        > (%grad_self.4)    %grad_self.8 : Tensor = prim::GradOfname=""aten::add""      block0():        > (%grad_self.6)    %grad_self.1 : Tensor = prim::GradOfname=""prim::profile""      block0():        > (%grad_self.8)    %18 : (Tensor) = prim::TupleConstruct(%grad_self.1)    return (%18) ``` I can try to add list support in `AutogradAdd`. but I feel like I'm not on the right track.... Pinging  , not sure who's maintaining autodiff at this moment."," Sorry for bothering you again, but may I know whether there is any update on this bug? Thank you.","Oops, totally forgot about this one. Thanks for the reminder~~ I was trying to see if there's any Meta folks that can help us with Autodiff issue. Let me ping   to see if she can point us to the right person.", is the correct person for autodiff issues. ,"Hi,  May I know whether there is any update on this issue? Thanks.","Sorry I missed the message. I saw  self assigned, wondering if we have assigned any meta folks handling this issue?"
yi,"Got ""RuntimeError: y.get_desc().is_nhwc() INTERNAL ASSERT FAILED""  while applying conv2d over a transposed tensor"," 🐛 Describe the bug I got this error on pytorch version 1.12.0. The code that can reproduce the error: ```python import torch import torch.nn as nn conv = nn.Conv2d(     1,     128,     kernel_size=(5, 2),     stride=(2, 1),     padding=(0, 1),     dilation=(1, 1),     groups=1,     bias=True,     padding_mode='zeros') t = torch.rand([1, 2, 321, 201, 1]) t = torch.transpose(t, 1, 4) t2 = t[..., 0] r = conv(t2) ``` The error message: ``` Traceback (most recent call last):   File ""/Users/bin.xue/Codes/iot/maasAEC/xxx.py"", line 19, in      r = conv(t2)   File ""/Users/bin.xue/.pyenv/versions/3.7.3/lib/python3.7/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/Users/bin.xue/.pyenv/versions/3.7.3/lib/python3.7/sitepackages/torch/nn/modules/conv.py"", line 457, in forward     return self._conv_forward(input, self.weight, self.bias)   File ""/Users/bin.xue/.pyenv/versions/3.7.3/lib/python3.7/sitepackages/torch/nn/modules/conv.py"", line 454, in _conv_forward     self.padding, self.dilation, self.groups) RuntimeError: y.get_desc().is_nhwc() INTERNAL ASSERT FAILED at ""/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp"":143, please report a bug to PyTorch.  ```  Versions PyTorch version: 1.12.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (x86_64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.20.2 Libc version: N/A Python version: 3.7.3 (default, May 29 2019, 18:19:34)  [Clang 10.0.1 (clang1001.0.46.4)] (64bit runtime) Python platform: Darwin21.5.0x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.12.0 [pip3] torchaudio==0.12.0 [conda] Could not collect ",2022-07-04T12:37:46Z,triaged module: mkldnn,closed,0,2,https://github.com/pytorch/pytorch/issues/80837,"Root cause is this ambiguity between `is_contiguous`. the given input has shape `[1, 1, 321, 201]` and stride `[129042, 1, 201, 1]`, this is considered as contiguous (memory format is channels last) for pytorch. but onednn considered as not nhwc since ```     inline bool is_nhwc() const {       if (!is_plain()  data.ndims != 4) return false;       const auto &dims = data.dims;       const auto &strides = blocking_strides();       const auto n = 0, c = 1, h = 2, w = 3;       return strides[n] == dims[h] * dims[w] * dims[c]           && strides[h] == dims[w] * dims[c]           && strides[w] == dims[c]           && strides[c] == 1;     }; ``` and the dim0 stride do not match, but actually it does not matter since size0 is 1 and so the index could only be 0. we need to get rid of the ambiguity between pytorch and onednn, decide `use_channels_last` from pytorch side and pass it to onednn. On onednn side we don't make judgement, just believe in that pytorch passed down physically contiguous tensors in the aligned memory format (and it should be so).",it has been fixed by https://github.com/pytorch/pytorch/pull/83653. 
gpt,"When running GPT trainning with megatron,  the program quit due to torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers"," 🐛 Describe the bug 1.When running GPT trainning with megatron, the program quit due to torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers 2.code MegatronLM github branch master and I changed /MegatronLM/megatron/tokenizer/bert_tokenization.py and /MegatronLM/megatron/tokenizer/tokenizer.py for berttokenizer data preprocess needs. tokenizer.zip bert_tokenization.zip 3.training data ~103MB vocab_processed.txt mygpt2_test_0704_text_document.zip mygpt2_test_0704_text_document.bin is ~103MB which exceed size limit, if you need , i can send it. 4.bash 0704_gpt_train.sh 0704_gpt_train.zip 5.env: linux:Linux version 4.15.0167generic (builddamd64045) (gcc version 7.5.0 (Ubuntu 7.5.03ubuntu1~18.04)) python env.txt 6.error log: 0704.log  Versions Collecting environment information... PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.27 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.15.0167genericx86_64withglibc2.17 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration:  GPU 0: Tesla V100SPCIE32GB GPU 1: Tesla V100SPCIE32GB GPU 2: Tesla V100SPCIE32GB GPU 3: Tesla V100SPCIE32GB GPU 4: Tesla V100SPCIE32GB GPU 5: Tesla V100SPCIE32GB GPU 6: Tesla V100SPCIE32GB GPU 7: Tesla V100SPCIE32GB Nvidia driver version: 470.103.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.11.0 [pip3] torchaudio==0.11.0 [pip3] torchvision==0.12.0 [conda] blas                      1.0                         mkl    defaults [conda] cudatoolkit               10.2.89             h713d32c_10    condaforge [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640    defaults [conda] mklservice               2.4.0            py38h95df7f1_0    condaforge [conda] mkl_fft                   1.3.1            py38h8666266_1    condaforge [conda] mkl_random                1.2.2            py38h1abd341_0    condaforge [conda] numpy                     1.22.3           py38he7a7128_0    defaults [conda] numpybase                1.22.3           py38hf524024_0    defaults [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torch                     1.11.0                   pypi_0    pypi [conda] torchaudio                0.11.0               py38_cu102    pytorch [conda] torchvision               0.12.0               py38_cu102    pytorch ",2022-07-03T23:41:43Z,oncall: distributed module: elastic,open,0,1,https://github.com/pytorch/pytorch/issues/80824,Do you run the program with nohup?
yi,Improve readability of cuda_lazy_init,This PR cleans up the implementation of `cuda_lazy_init.cpp` and improves its readability. No behavioral changes are introduced.,2022-07-01T23:00:39Z,Merged cla signed release notes: cuda topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/80788,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80788**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 6b04de21b7 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,wt1 - trying a reusealbe workflow,Fixes ISSUE_NUMBER,2022-06-30T14:55:02Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/80610,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80610**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 12 New Failures, 20 Pending As of commit e8d5df4a50 (more details on the Dr. CI page): Expand to see more  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7138406429?check_suite_focus=true) linuxbinarymanywheel / manywheelpy3_10rocm5_1_1build / build (1/12) **Step:** ""Build PyTorch binary"" (full log  :repeat: rerun)   20220630T19:45:39.4424294Z CMake Error at caffe2/CMakeLists.txt:1609 (target_link_libraries):  ``` 20220630T19:45:39.4421626Z CMake Error at caffe2/CMakeLists.txt:1461 (target_compile_definitions): 20220630T19:45:39.4421914Z   Cannot specify compile definitions for target ""torch_cuda_cu"" which is not 20220630T19:45:39.4422155Z   built by this project. 20220630T19:45:39.4422267Z  20220630T19:45:39.4422271Z  20220630T19:45:39.4422408Z CMake Error at caffe2/CMakeLists.txt:1462 (target_compile_definitions): 20220630T19:45:39.4422712Z   Cannot specify compile definitions for target ""torch_cuda_cpp"" which is not 20220630T19:45:39.4423009Z   built by this project. 20220630T19:45:39.4423127Z  20220630T19:45:39.4423132Z  20220630T19:45:39.4424294Z CMake Error at caffe2/CMakeLists.txt:1609 (target_link_libraries): 20220630T19:45:39.4424620Z   Cannot specify link libraries for target ""torch_cuda_cu"" which is not built 20220630T19:45:39.4424895Z   by this project. 20220630T19:45:39.4425002Z  20220630T19:45:39.4425007Z  20220630T19:45:39.4782375Z  Configuring incomplete, errors occurred! 20220630T19:45:39.4782715Z See also ""/pytorch/build/CMakeFiles/CMakeOutput.log"". 20220630T19:45:39.4782984Z See also ""/pytorch/build/CMakeFiles/CMakeError.log"". 20220630T19:45:39.5547868Z [error]Process completed with exit code 1. 20220630T19:45:39.5574928Z [group]Run  Ensure the working directory gets chowned back to the current user 20220630T19:45:39.5575269Z [36;1m Ensure the working directory gets chowned back to the current user[0m ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "
yi,[GHA] Remove new lines from PR_BODY too to appease batch env var copying,"https://github.com/pytorch/pytorch/pull/80543 doesn't work in preventing batch from interpreting the multiline env vars. We will remove the lines from these env vars instead, since PR_BODY and COMMIT_MESSAGES are both used to determine what disabled tests to not skip. Test plan is using the following below and making sure tests still pass, which they do. Summary: previous versions of sparsity utils either allowed for a leading '.' for fqns, or would only allow for that. Per discussion with ao team about fqns don't have a leading '.' fqn of root module is '' these utilities have been updated to align with these definitions. module_to_fqn was changed to not generate a leading '.' and output '' for root module fqn_to_module was changed to output the root rather than None for path='' get_arg_info_from_tensor_fqn had explicit handling for a leading '.' that was removed. The previous implementation overwrote the tensor_fqn if it had a leading '.' which resulted in undesirable behavior of rewriting arguments provided by the user. Also refactored utils to be simpler and added comments, formatting and test Test Plan: python test/test_ao_sparsity.py python test/test_ao_sparsity.py TestSparsityUtilFunctions Reviewers: Subscribers: Tasks: Tags:",2022-06-29T17:42:56Z,module: rocm Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/80548,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80548**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b10da2e9c2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge l trying out land validation, successfully started a merge job. Check the current status here,Successfully started land time checks. See progress here: https://hud.pytorch.org/pytorch/pytorch/commit/40ba6dd2b195b6dca672cbee883dcf08c9c3b6b3,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Playing around,Fixes ISSUE_NUMBER,2022-06-29T17:38:04Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/80546,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80546**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 7cab6dd4a5 (more details on the Dr. CI page): Expand to see more  *Commit 7cab6dd4a5 was recently pushed. Waiting for builds...*  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
rag,Forward AD mismatched stride when size = 0 becomes mismatched storage offset after slice," 🐛 Describe the bug `gradcheck` triggers INTERNAL ASSERT for `torch.diff` in forward mode. ```python import torch input = torch.tensor([1.0+1.0j], dtype=torch.complex128, requires_grad=True) torch.autograd.gradcheck(torch.diff, (input), check_forward_ad=True, check_backward_ad=False)  !this_view_meta>has_fw_view()INTERNAL ASSERT FAILED at ""/opt/conda/condabld/pytorch_1646756402876/work/torch/csrc/autograd/autograd_meta.cpp"":186, please report a bug to PyTorch. Expected the output of forward differentiable view operations to have the tangent have the same layout as primal ```  Versions pytorch: 1.11.0 ",2022-06-29T12:51:51Z,triaged module: forward ad,closed,0,7,https://github.com/pytorch/pytorch/issues/80507, ,"fwiw, I can reproduce this in `master`.","This is happening because forward AD doesn't really care when strides are different between primal and tangent (when the size in that dimension is 1), but when we do a certain slice on these primal and tangent (for example `[1:]`), they both become zero numel tensors (expected) except the storage offset is now different  and forward AD complains about this. What we might want to do is just make forward AD not care about storage offset being different in some cases as well, but I'm not sure about the implications of that yet. Minimal repro: ``` import torch import torch.autograd.forward_ad as fwAD a = torch.tensor([1.]).as_strided((1,), (2,), 0) b = torch.tensor([1.]).as_strided((1,), (1,), 0) with fwAD.dual_level():     dual_input = fwAD.make_dual(a, b)     dual_input[1:] ```","From offline discussion with , in the case when there is no exact match in metadata, but the underlying storage representation is the same, we should update forward AD to perform a outofplace view during set_fw_grad to ensure the metadata matches exactly.",Can we instead ignore storage_offset on 0element tensors?,> Can we instead ignore storage_offset on 0element tensors? Hmm yeah I think this makes sense. The invariant that the same indices in the tangent and primal tensors must index the same locations in storage would be vacuously true., Actually another alternate way to make the old solution work would be to allow this specific usage of as_strided in the batching rule since it is basically a noop
transformer,[bug] libtorch bug in nn::MultiheadAttention and nn::Transformer," 🐛 Describe the bug the attn_mask does not work in nn::MultiheadAttention ~~~ include  namespace nn = torch::nn; // seq_length x batch_size x feature_size torch::Tensor x = torch::randn({3,1,4}); torch::Tensor attn_mask = nn::TransformerImpl::generate_square_subsequent_mask(3); torch::Tensor attn_mask_bool = attn_mask.to(torch::kBool); nn::MultiheadAttention multihead_attention(4,1); std::tuple output_with_attn_mask = multihead_attention >forward(x,x,x,{},true,attn_mask_bool); torch::Tensor attn_output, attn_output_weights; std::tie(attn_output, attn_output_weights) = output_with_attn_mask; //unpacking tuple into variables std::cout << attn_output_weights << std::endl; ~~~ ~~~ attn_mask: /*  0  1  1  0  0  1  0  0  0 [ CPUBoolType{3,3} ] */ attn_output_weights: (1,.,.) =    0.1918  0.5302  0.2780   0.2074  0.1919  0.6007   0.1948  0.5092  0.2960 [ CPUFloatType{1,3,3} ] ~~~ * 1 the attn_mask does not affect the attn_output_weights. * 2  the API in libtorch for MultiheadAttention and Transformer is not comparable with that of in pytorch. e.g. libtorch api have no batch_first param.  * 3 more detail comparison see https://github.com/walkacross/pytorchlibtorchAPItranslation/tree/main/translation/torch.nn/transformer_layers  Versions libtorch: 1.11.0+cu113 ",2022-06-29T03:12:17Z,module: cpp module: nn triaged module: correctness (silent),open,1,7,https://github.com/pytorch/pytorch/issues/80494,Hey there ! Thanks for bringing this up. Responding to your points: 1. This is a problem and I will look to repro and fix this week.  2. Unfortunately the cpp api is not as well developed as the python api and the lag will probably continue. We will do our best to add features like batch_first but cannot guarantee. ," thanks for your response, have a good day.","It's been 9 months, and this bug still exists. why?? (everyone who use libtorch to inference experiences the wrong code)     ","Hi, I'm afraid the c++ API does not have a maintainer at the moment. So development in that area is quite slow.","hi , thanks for your kind response. this situation  c++ API does not have a maintainer at the moment   seems stay a long time, does pytorch team has any plan to change this situation?  considering every version release involves in libtorch, but unfortunately, little improvement happens.  Besides, nn::MultiheadAttention is a key part of Transformer model, but the code is wrong, it has affacted many researchers.","We are very much open to onboarding a maintainer for the C++ API! Unfortunately, we don't have anyone interested at the moment. Also since the C++ API is not used as much as other features, we tend to focus more on these other features (while maintaining releases for libtorch of course). Broadly, we don't expect the C++ API to bring a lot of benefits compared to the python one.","got it, anyway, thanks for your feedback"
yi,Mark underlying type as C10_UNUSED,"Fixes regression detected in https://github.com/pytorch/pytorch/pull/79978 As result some of the internal workflows fail as follows: ``` aten/src/ATen/native/cpu/CopyKernel.cpp:28:5: error: unused type alias 'underlying_t' [Werror,Wunusedlocaltypedef]     AT_DISPATCH_QINT_TYPES(dtype, ""copy_kernel"", [&] {     ^ src/ATen/native/cpu/CopyKernel.cpp:28:5: error: unused type alias 'underlying_t' [Werror,Wunusedlocaltypedef] aten/src/ATen/Dispatch.h:393:34: note: expanded from macro 'AT_DISPATCH_QINT_TYPES'   AT_DISPATCH_SWITCH(TYPE, NAME, AT_DISPATCH_CASE_QINT_TYPES(__VA_ARGS__)) ```",2022-06-28T00:59:49Z,Merged cla signed ciflow/trunk,closed,0,5,https://github.com/pytorch/pytorch/issues/80415,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80415**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit a216c4c30d (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge f, successfully started a merge job. Check the current status here, your PR has been successfully merged.,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,support nested_tensor * scalar,"In transformer, the scale step in attention has a `nested_tensor / scalar` operation. There are two ways to support that: 1. directly support `nested_tensor / scalar`: * pro: straightforward, good UX * con: is dispatching `mul(nested tensor, regular tensor)` a good practice? 2. let user manually convert `scalar` to `nested_scalar = torch.nested_tensor([broadcast_scalar])` * pro: dispatcher only has to deal with `mul(nested tensor, nested tensor)` * con: confusing manual conversions, bad UX",2022-06-25T15:50:12Z,Merged cla signed topic: improvements release notes: nested tensor,closed,0,6,https://github.com/pytorch/pytorch/issues/80284,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80284**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b3d9565f73 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Stuff like op(cuda_tensor, scalar_tensor) is allowed, so I think mixing scalar tensors with nested tensors should be allowed too. FWIW, that should work fine with the dispatcher you’ll always dispatch to a nested tensor kernel.","I'm very much in favor of Option 1. Please see my inline comment for a tour of our Python frontend bindings. I'd have also expected mul.Scalar to be used, but it seems like it's not.", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Modifying Adam to support complex numbers as 2d real numbers,This commit addresses issues in CC(Cleanup optimizer handling of complex numbers.),2022-06-25T11:09:53Z,triaged open source Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/80279,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80279**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 3e6f0a0c50 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , does this look alright?, merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Disable AVX512 CPU dispatch by default,"As it can be slower, see  CC(AVX512 CPU kernels result in 5+% slower Transformer training on Cascade Lake CPU) Update trunk test matrix to test AVX512 config in `nogpu_AVX512` flavor. Kill `nogpu_noAVX` as AVX support were replaced with AVX512 when https://github.com/pytorch/pytorch/pull/61903 were landed",2022-06-24T22:49:24Z,Merged cla signed ciflow/trunk,closed,0,4,https://github.com/pytorch/pytorch/issues/80253,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80253**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 419899ecbd (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/7066712836?check_suite_focus=true) pull / linuxfocalpy3.7gcc7 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220627T05:30:09.4877508Z The PR is introduc...m to confirm whether this change is wanted or not.  ``` 20220627T05:30:09.4862116Z processing existing schema:  duration_ns(__torch__.torch.classes.profiling.InstructionStats _0) > int _0 20220627T05:30:09.4863743Z processing existing schema:  source(__torch__.torch.classes.profiling.SourceStats _0) > __torch__.torch.classes.profiling.SourceRef _0 20220627T05:30:09.4865877Z processing existing schema:  line_map(__torch__.torch.classes.profiling.SourceStats _0) > Dict(int, __torch__.torch.classes.profiling.InstructionStats) _0 20220627T05:30:09.4867171Z processing existing schema:  __init__(__torch__.torch.classes.profiling._ScriptProfile _0) > NoneType _0 20220627T05:30:09.4868673Z processing existing schema:  enable(__torch__.torch.classes.profiling._ScriptProfile _0) > NoneType _0 20220627T05:30:09.4870114Z processing existing schema:  disable(__torch__.torch.classes.profiling._ScriptProfile _0) > NoneType _0 20220627T05:30:09.4872226Z processing existing schema:  _dump_stats(__torch__.torch.classes.profiling._ScriptProfile _0) > __torch__.torch.classes.profiling.SourceStats[] _0 20220627T05:30:09.4873707Z processing existing schema:  __init__(__torch__.torch.classes.c10d.ProcessGroup _0, int _1, int _2) > NoneType _0 20220627T05:30:09.4875098Z processing existing schema:  __init__(__torch__.torch.classes.c10d.Work _0) > NoneType _0 20220627T05:30:09.4877046Z processing existing schema:  __init__(__torch__.torch.classes.dist_rpc.WorkerInfo _0, str _1, int _2) > NoneType _0 20220627T05:30:09.4877508Z The PR is introducing backward incompatible changes to the operator library. Please contact PyTorch team to confirm whether this change is wanted or not.  20220627T05:30:09.4877527Z  20220627T05:30:09.4877831Z Broken ops: [ 20220627T05:30:09.4878685Z 	c10d::broadcast(__torch__.torch.classes.c10d.ProcessGroup _0, Tensor[] _1, int _2, int _3, int _4) > __torch__.torch.classes.c10d.Work _0 20220627T05:30:09.4878758Z ] 20220627T05:30:09.6014046Z [error]Process completed with exit code 1. 20220627T05:30:09.6041099Z Prepare all required actions 20220627T05:30:09.6041209Z Getting action download info 20220627T05:30:09.7687896Z [group]Run ./.github/actions/getworkflowjobid 20220627T05:30:09.7688101Z with: 20220627T05:30:09.7688404Z   githubtoken: *** ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge f, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,AVX512 CPU kernels result in 5+% slower Transformer training on Cascade Lake CPU, 🐛 Describe the bug Benchmarking latest nightly using WLM_Transformers example  AVX512 kernels are 18% slower than using AVX2 ones: ``` (py38cpuperfregression) nshulgastc524xlarge1:/fsx/users/nshulga/examples/word_language_model$ ATEN_CPU_CAPABILITY=avx512 python3 main.py epochs 1 model Transformer  test ppl  1891.44 ========================================================================================= ```  Versions 1.13.0.dev20220610+cpu)  ,2022-06-24T22:45:49Z,high priority module: performance triaged module: regression,closed,0,9,https://github.com/pytorch/pytorch/issues/80252,"Oops! I hope this wouldn't affect the PyTorch 1.12 release plan. I'll submit a PR to disable AVX512 dispatch for those kernels that are slower with AVX512, i.e. enable AVX512 dispatch only for those kernels that are faster with AVX512. Perhaps we can also enable/disable AVX512 dispatch according to the platform, as more recent or upcoming platforms might have better AVX512 performance. Regardless, I'll see if there's a way to boost AVX512 performance of existing ATen kernels as well. Memorybound kernels are more likely to be slower with AVX512, than with AVX2, so maybe down the line we'll see if HBM can help. Thanks!","person for the time being I plan to disable it across the board by default, because at least on any of the workflows we usually benchmarked it did not yield any gains on EC2 C5 machine. Let's continue discussion here on some microbenchmarks one could have run to more easily identify what is going on.",By dropping (profile.py)[https://gist.github.com/malfet/0a063a0663230a4cdcd2eb75324e221a] into (https://github.com/pytorch/examples/tree/main/word_language_model) int looks like `aten::_log_softmax ` is the one that regressed the most: ``` (py38cpuperfregression) nshulgastc524xlarge1:/fsx/users/nshulga/examples/word_language_model$ ATEN_CPU_CAPABILITY=avx512 python3 profile.pyhead n10                                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     of Calls                                                       aten::_log_softmax        29.88%     287.279ms        29.88%     287.279ms      28.728ms            10                                                  aten::mm        17.55%     168.752ms        17.56%     168.810ms     937.833us           180                                               aten::addmm         6.72%      64.632ms        11.55%     111.066ms       1.234ms            90                          aten::_log_softmax_backward_data         6.57%      63.190ms         6.57%      63.190ms       6.319ms            10                                               aten::fill_         6.05%      58.173ms         6.05%      58.173ms     264.423us           220                                               aten::copy_         5.54%      53.288ms         5.54%      53.288ms     131.251us           406       autograd::engine::evaluate_function: AddmmBackward0         4.56%      43.840ms        24.00%     230.767ms       2.564ms            90   ```,Discussion:  let's file an issue to improve benchmarking  possibly creating a benchmarking job," , there are some existing ATen op benchmarks at https://github.com/pytorch/pytorch/tree/master/benchmarks/operator_benchmark/pt, and we can run them with different `ATEN_CPU_CAPABILITY`. We might need to add some more, though. Some ops work better with AVX512, but there's significant runtorun variation. I've noticed that preloading a thirdparty memory allocator (jemalloc/tcmalloc) produces more consistent results. I'll post results later. I guess we can also benchmark on AWS m6i.64xlarge AWS instance, which is Ice Lake, and has 48 KB L1D caches. Thanks!","Hi      ,  Even after we'd finish benchmarking & analyzing performance of ATen kernels (and potentially enable dispatch of some AVX512 kernels), can we still let AVX2 be the default CPU capability, so that users may use AVX512 ATen kernels by using the environment variable `ATEN_CPU_CAPABILITY=AVX512`? IMHO, there are at least 4 factors that should be considered while benchmarking & comparing AVX2 & AVX512 performance of each ATen kernel: 1. Input(s) size(s)  2. Number of threads 3. CPU platform 4. Memory allocator (AVX512 might do better with nondefault memory allocators with large pagesizes.)  For some combination of the aforementioned factors, some ATen kernels might exhibit better performance with AVX512, but worse with another combination.  However, currently, PyTorch dispatches an ATen kernel according to the highest CPU capability, so the dispatcher doesn't consider these factors. So, would it be okay to add a Python API to change `ATEN_CPU_CAPABILITY` at runtime, which would allow users to finetune performance of workloads, based on the analysis of data gleaned via prior benchmarking? Thanks!","person I've already switched to `avx2` by default in https://github.com/pytorch/pytorch/pull/80253 As for the rest, I think majority of the users would not know which kernel to pick, so if we want to use AVX512, then we should code the heuristics into the AVX512 kernel implementation","Hi , please help with a doubt. >  if we want to use AVX512, then we should code the heuristics into the AVX512 kernel implementation If  `ATEN_CPU_CAPABILITY` would be `AVX512`, but for some particular inputs of a kernel, if it's determined heuristically that the `AVX512` kernel might be slower, should its `AVX2` counterpart be dispatched instead? Thanks!",This specific issue was fixed in CC(Fix threadallocation in `_vec_log_softmax_lastdim`). Thanks!
yi,[FSDP] Clean up `_lazy_init()`,Stack from ghstack:  CC([FSDP] Move `_post_backward_called` to `_init_param_attributes`) [FSDP] Move `_post_backward_called` to `_init_param_attributes` * * CC([FSDP] Clean up `_lazy_init()`) [FSDP] Clean up `_lazy_init()`**  CC([Easy][FSDP] Add `zero_grad()` to unit test train loop) [Easy][FSDP] Add `zero_grad()` to unit test train loop  CC([FSDP] Remove `self.numel_padded_per_param` (unused)) [FSDP] Remove `self.numel_padded_per_param` (unused)  CC([FSDP] Move tensor sharding logic to `FlatParamHandle`) [FSDP] Move tensor sharding logic to `FlatParamHandle`  CC([FSDP] Deduplicate `_orig_size` and `_unsharded_size`) [FSDP] Deduplicate `_orig_size` and `_unsharded_size`  CC([FSDP] Introduce `FlatParamHandle`) [FSDP] Introduce `FlatParamHandle` This PR cleans up `_lazy_init()`. The explanations are left as PR comments. Differential Revision: D37726059,2022-06-23T22:20:31Z,oncall: distributed Merged cla signed,closed,0,9,https://github.com/pytorch/pytorch/issues/80185,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80185**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit f9ad1d74ca (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"> I think this might have a small semantic change, where cast_buffers() is no longer done every iteration  I think this is fine since it was probably redundant. But one case I can think of is we checkpoint in full precision for buffers. If user calls forward after that, we should make sure they were restored (I think they are already when exiting checkpoint logic, but need to check) It looks like there is no cast in the post load state dict hooks. I will test this and add the cast to the post load state dict hook as needed. IMO, it makes more sense (for both clarity and performance overhead) to have that cast once and directly where we need it rather than having it be a noop all but one of the time when calling it in `_lazy_init()` every iteration.","> I think this might have a small semantic change, where cast_buffers() is no longer done every iteration  I think this is fine since it was probably redundant. But one case I can think of is we checkpoint in full precision for buffers. If user calls forward after that, we should make sure they were restored (I think they are already when exiting checkpoint logic, but need to check) https://github.com/pytorch/pytorch/blob/10c689856d5f913b93daa404955927a5a5716805/torch/distributed/fsdp/fully_sharded_data_parallel.pyL1909L1930 The `_post_state_dict_hook()` casts buffers. https://github.com/pytorch/pytorch/blob/10c689856d5f913b93daa404955927a5a5716805/torch/distributed/fsdp/fully_sharded_data_parallel.pyL1992L2005 `state_dict()` is the only method that casts buffers (namely, back to the original dtype) other than `_cast_buffers()`. Since `state_dict()` and `_post_state_dict_hook()` always are called in pairs, there should be no need to `_cast_buffers()` every iteration."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,GEGLU activation," 🚀 The feature, motivation and pitch Seems to be the latest and greatest activation function for transformers. See https://arxiv.org/abs/2002.05202v1  Alternatives One could copypaste the implementation from https://github.com/pfnetresearch/deeptable/blob/237c8be8a405349ce6ab78075234c60d9bfe60b7/deep_table/nn/layers/activation.py  Additional context _No response_ ",2022-06-23T19:41:15Z,module: nn triaged enhancement needs research,open,22,1,https://github.com/pytorch/pytorch/issues/80168,"Hey , thanks for the request! Note that we are very conservative with adding new modules to PyTorch core. In general, there should be a very good performance or popularity reason for them to be officially maintained here rather than implemented in a separate repo. Let's see if the request becomes popular enough to make the addition worthwhile."
yi,[MPS] segmentation fault when multiplying two scalar tensors," 🐛 Describe the bug When multiplying two scalar tensors in mps kernel, a segmentation fault occurs. ```mm MPSGraphTensor* aTensor = [mpsGraph constantWithScalar:.4f                                                //shape:@[]   ",2022-06-23T19:06:53Z,needs reproduction triaged module: mps,closed,0,7,https://github.com/pytorch/pytorch/issues/80165,"Hmm, I'm not sure what this one as about. Can you please provide a reproducer? I can not repro the crash by running the following: ``` % python c ""import torch; print(torch.tensor(1, device='mps')+torch.tensor(2, device='mps'))"" tensor(3, device='mps:0') ```","  Please clone this branch https://github.com/qqaatw/pytorch/tree/add_huber_loss_mm Build and run the following:  ```bash python m pytest vv s test/test_mps.py k test_huber_loss  works fine ``` Then, comment out the shape arguments below like what I do in the issue content. https://github.com/qqaatw/pytorch/blob/d49ea958ee31478077567e239d2157680aa89f91/aten/src/ATen/native/mps/operations/LossOps.mmL1079L1084 Build and run the following again: ```bash python m pytest vv s test/test_mps.py k test_huber_loss  segmentation fault ```"," I struggle to understand what is has to do with PyTorch? This sounds like a bug that should be filed against Apple (which develops the framework), rather than PyTorch that just uses it.",Thanks  for filling this issue  I was able to reproduce locally as well. I'll check with the relevant engineers and update you back,"Hi ,  This issue has been fixed in macOS 13 beta 2. In order to avoid doing checks based on the OS in PyTorch's MPS backend, I think the best would be to specify directly the shape (`[mpsGraph constantWithScalar: x shape:@[] ... ];`.","Hi , Thank you so much for your quick feedback!  Maybe this issue is worth noting on the wiki, which prevents people who want to contribute new mps ops from running into the bug."," , I agree with , please use shape for now to avoid OS specific checks unless absolutely needed. That's a good point, we can add a section to note issues such as these."
transformer,[feature request] Add support for a custom DatasetFetcher in DataLoader ," 🚀 The feature, motivation and pitch I'm working on a timeseries dataset, and I stumbled upon a limitation withing pytorch's DataLoader that I'd like to address here. I have a custom mapstyle dataset that implements a `__getitem__` and `__len__` methods. `__getitem__`  has a following signature and returns by default features of shape `(seq_length, n_features)` and an integer label. ```python def __getitem__(self, i: int, seq_length: int = 100):      ....     return features, label ``` Sequence length is fixed by default, but can be changed by passing a `seq_length` argument. I'm using an LSTM + Transformer model that is able to process an input of variable sequence length.\ BUT I'm not leveraging this feature of my model because pytorch's DataLoader expects all the samples returned by the dataset to be of the same shape. A traditional approach to solving this problem is **sequence padding**. This doesn't really solve the problem in my opinion (the model still gets constant sequence length as an input) and leads to a range of further questions (which values to use for padding, how to interpolate ect.). **What I would like to do instead** is to use a custom DatasetFetcher to create batches that have constant sequence length within a batch, but **varying** sequence length over the dataset.\ So let's say or batch has a shape `batch_size, seq_length, n_features`.  (Let batch_size=32, n_features=10)\ Our DataLoader will sample indices for the first batch and then fetch samples with a constant `seq_length=100`, which will result in a batch of shape `32, 100, 10`.\ On the next batch, it will randomly (or by any other rule) pick a new sequence length (let's say 80)  and create a batch of shape `32, 80, 10`. ```python  Default behaviour data = [self.dataset[idx] for idx in possibly_batched_index]  New behaviour data = [self.dataset.__getitem__(idx, seq_length) for idx in possibly_batched_index] ``` I'm pretty sure it will have a regularization effect.  Alternatives _No response_  Additional context _No response_ ",2022-06-23T12:08:54Z,module: dataloader triaged enhancement module: data,open,0,1,https://github.com/pytorch/pytorch/issues/80134,Have you considered passing in a custom `collate_fn` to `DataLoader` that purpose? The input to `collate_fn` is a batch; you are free to pick a sequence length and pad/trim the sequence in there as you need. You can also import and call the `torch.utils.data.default_collate` after your custom step if you'd like as well.
transformer,create input tensor on GPU device takes extremely long time when inferencing with transformer models," 🐛 Describe the bug Hi Team, Good day! Problem: create input tensor on GPU device takes extremely long time when inferencing with transformer models Description:  I use below sample full code, to examine inferencing speed with BERT model, it uses bertbasemultilingualuncased from huggingface's implementation, takes random 5000 texts from SQuAD1. To speed up, the code is optimised so that it first tokenize all the 5000 texts, and sort the tokenized texts by number of tokens, and batching texts with similar length in one batch sent for inference.  Particularly, I have recorded the time used for model inferencing, below line  ```python model(input_ids = input_ids_batch, attention_mask = attention_mask_batch) ``` And the time took to put input tensor created to GPU device  ```python input_ids_batch = input_ids_batch.to(device) ``` Noticed, with maximum sequence length of 128, when set with a big batch size, say 512, the time used for model inference is blazingly fast, less than 0.1s for all 5000 texts, but it takes insanely long time, almost 2s in total to put input tensor to GPU for all 5000 texts.  However, with same maximum sequence length of 128, when set with a small batch size, say 16, the time used for model inference slows down to more than 2s for all 5000 texts, but it speeds up to put input tensor to GPU, which takes less than 0.2s for all 5000 texts.  My question is, irregardless of batch_size, put input tensor to GPU should not take long, as the size of input tensor is only batch_size * max_seq_len float matrix, which occupies extremely small memory. We would like to enjoy the fast inferencing speed brought by bigger batch, without being bottlenecked by the step of creating input tensor to GPU.  I have tried to create input tensor directly on GPU with below code, is still the same story. ```python  input_ids_batch = torch.tensor(input_ids_batch, device=device)[:,:args.max_seq_len] ``` Interestingly, I noticed,  Scenario 1   When I didnt order the text by number of tokens, it takes longer to put input tensors to GPU, when previous padded input tensor of batch is bigger matrix (i.e. batch_size * max_seq_len), and faster when previous padded input tensor is of smaller matrix.  Scenario 2  when I create the mode and put on GPU, using below code   ```python  model = BertModel.from_pretrained('bertbasemultilingualuncased', add_pooling_layer=False) model.eval() model = model.to(device) ``` but skip the inferencing in every batch, i.e. comment out below code ```python res = model(input_ids = input_ids_batch, attention_mask = attention_mask_batch) ``` The very first time, to put input tensors to GPU (max_seq_len=128, batch_size=512), is very long, takes more than 3s for a batch of 512 texts. But subsequent batches of the rest of 5000 texts, it is blazingly fast, ~0.01s or faster for every batch of 512 texts.  Scenario 3   when I didnt even create the model on GPU, i.e. comment out both of the code above in Scenario 1. Put input tensors to GPU (max_seq_len=128, batch_size=512) is always fast, less than 0.01s for every batch of 512 texts, including the very first one.  Many thanks!  The full code:  ```python from transformers import BertModel, BertTokenizerFast import numpy as np from tqdm.autonotebook import trange import time import torch import itertools from torchtext.datasets import SQuAD1 device = 0 tokenizer = BertTokenizerFast.from_pretrained('bertbasemultilingualuncased') model = BertModel.from_pretrained('bertbasemultilingualuncased', add_pooling_layer=False) model.eval() model = model.to(device) squad = SQuAD1(split='train') texts = [] for i in range(len(squad)):     texts.append(next(squad)[0]) np.random.seed(1) texts = np.random.choice(texts, size=5000) batch_size = 512 max_seq_len = 128 timings_inference = [] timings_put_input_ids_on_device = [] with torch.no_grad():  with or without this seems no difference      1. Tokenization     input_ids = tokenizer._batch_encode_plus(list(texts), return_attention_mask=False, return_token_type_ids=False)['input_ids']      2. sort by sequence length (so that we group length of similar length in one batch)        maybe this is not required? as majority time spent on replace input_ids_batch on device??     length_sorted_idx = np.argsort([len(emb) for emb in input_ids])     input_ids_sorted = [input_ids[idx] for idx in length_sorted_idx]     for start_index in trange(0, len(input_ids), batch_size, desc=""Batches"", disable=True):         input_ids_batch = input_ids_sorted[start_index:start_index+batch_size]                  3.1 padding          input_ids_batch = list(zip(*itertools.zip_longest(*input_ids_batch, fillvalue=0)))         input_ids_batch = torch.tensor(input_ids_batch)[:,:max_seq_len]          Tried to directly create the tensor on GPU (but no luck)          input_ids_batch = torch.tensor(input_ids_batch, device=device)[:,:max_seq_len]         start_time = time.time()          Takes long, if previous round in the loop, input_ids_batch is big matrix (batch_size * max_seq_len, int)         input_ids_batch = input_ids_batch.to(device)         timings_put_input_ids_on_device.append(time.time()start_time)          3.2 calculate attention mask         attention_mask_batch = torch.where(input_ids_batch!=0,1,0).to(device)          3.3 inferencing         start_time = time.time()         res = model(input_ids = input_ids_batch, attention_mask = attention_mask_batch)         timings_inference.append(time.time()start_time)         print(f""from {start_index} to {start_index+batch_size},  inferencing time {timings_inference[1]}s, put input_ids on gpu {timings_put_input_ids_on_device[1]}s,"") print(f""Total, inferencing time {sum(timings_inference)}s, put input_ids on gpu {sum(timings_put_input_ids_on_device)}s,"") ````  Versions torch==1.10.1+cu113 torchtext==0.11.1 transformers==4.12.5 Python==3.9.2 NVIDIASMI 515.43.04 Driver Version: 515.43.04 CUDA Version: 11.7   GPU: NVIDIA A100SXM, 81920MiB memory ",2022-06-23T09:05:17Z,module: cuda triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/80129,">Noticed, with maximum sequence length of 128, when set with a big batch size, say 512, the time used for model inference is blazingly fast, less than 0.1s for all 5000 texts, but it takes insanely long time, almost 2s in total to put input tensor to GPU for all 5000 texts. > However, with same maximum sequence length of 128, when set with a small batch size, say 16, the time used for model inference slows down to more than 2s for all 5000 texts, but it speeds up to put input tensor to GPU, which takes less than 0.2s for all 5000 texts. CUDA operations are executed asynchronously so you would need to synchronize the code via `torch.cuda.synchronize()` before starting and stopping the timers which is not the case in your posted code.  The very first CUDA operation will initialize the CUDA context, load the kernels etc. and is expected to be slow. Since you are not synchronizing your timers will accumulate the previous runtimes when a sync is enforced and give wrong results. ","Got it, Thank you very much! "
chat,[PyTorch][ATen][AMD] always declare ROCmBackwardPassGuard,"Summary:  Problem `ROCmBackwardPassGuard` is only defined and instantiated when `USE_ROCM` is set, which complicates our internal build system that distinguishes a CPUonly module from a GPUaware module.  Solution Despite its name, `ROCmBackwardPassGuard` does not depend on ROCm features. Regardless of its usefulness for nonROCm code, we don't need `ifdef USE_ROCM` for the declaration of `ROCmBackwardPassGuard` like other components like CUDAspecific `NoTF32Guard`. Note: this patch should not have any side effects (`ROCmBackwardPassGuard` is not instantiated when `USE_ROCM` is not set) except for additional tiny compilation time for these 10 lines. Reviewed By: minsii Differential Revision: D36670235",2022-06-22T19:31:55Z,module: rocm fb-exported cla signed Stale,closed,0,11,https://github.com/pytorch/pytorch/issues/80069,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/80069**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 4f7de45155 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36670235,This pull request was **exported** from Phabricator. Differential Revision: D36670235,This pull request was **exported** from Phabricator. Differential Revision: D36670235,This pull request was **exported** from Phabricator. Differential Revision: D36670235,This pull request was **exported** from Phabricator. Differential Revision: D36670235,  amd Would one of you mind taking a look at this PR? 1. Why needed? Our internal build system distinguishes a CPUonly module from a GPUaware module.  `context.cpp` belongs to CPUonly and does not take an AMDGPU flag.  The internal workaround does not work. This PR seems the best way to fix the issue so far. 2. Why `if !defined(_WIN32)`? `TORCH_API` + `thread_local` is not allowed on Windows.  This is okay since the path above does not use Windows.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",iwasaki Do you plan to merge this PR?,"Sorry, I completely missed this notification. Yes I would like to if the PyTorch reviewers can approve this change.",This pull request was **exported** from Phabricator. Differential Revision: D36670235
rag,[ONNX] Increase coverage of quantization operators,Uptodate supported list: https://pytorch.org/docs/master/onnx_supported_aten_ops.html?highlight=onnx  Extending support for operators used by quantization. A few thoughts/options. 1. Support all operators documented here https://pytorch.org/docs/stable/quantizationsupport.html?highlight=quantization. The list is nonexhaustive and there may exist undocumented quantized operators.     * torch          [x] quantize_per_tensor          [x] quantize_per_channel          [x] dequantize     * other aten          [ ] slice     * torch.Tensor          [x] view          [x] as_strided          [x] expand          [x] flatten          [x] select          [x] ne          [x] eq          [x] ge          [x] le          [x] gt          [x] lt          [ ] copy_          [x] clone          [ ] dequantize          [ ] equal          [ ] int_repr          [ ] max          [x] mean          [ ] min          [ ] q_scale          [ ] q_zero_point          [ ] q_per_channel_scales          [ ] q_per_channel_zero_points          [ ] q_per_channel_axis          [ ] ~resize_~ [resize_ is not traceable]          [ ] sort          [ ] topk     * torch.nn.quantized          [x] ReLU6          [x] Hardswish          [ ] ELU          [x] LeakyReLU          [x] Sigmoid          [ ] BatchNorm2d          [ ] BatchNorm3d          [ ] Conv1d          [x] Conv2d          [ ] Conv3d          [ ] ConvTranspose1d          [ ] ConvTranspose2d          [ ] ConvTranspose3d          [x] Embedding          [x] EmbeddingBag          [ ] FloatFunctional          [ ] FXFloatFunctional          [ ] QFunctional          [x] Linear          [x] LayerNorm          [x] GroupNorm          [x] InstanceNorm1d          [x] InstanceNorm2d          [x] InstanceNorm3d     * torch.nn.quantized.functional          [ ] avg_pool2d          [ ] avg_pool3d          [ ] adaptive_avg_pool2d          [ ] adaptive_avg_pool3d          [ ] conv1d          [x] conv2d          [ ] conv3d          [ ] interpolate          [x] linear          [ ] max_pool1d          [ ] max_pool2d          [ ] celu          [x] leaky_relu          [x] hardtanh          [x] hardswish          [ ] threshold          [x] elu          [x] hardsigmoid          [ ] clamp          [ ] upsample          [ ] upsample_bilinear          [ ] upsample_nearest 1. Support all existing supported full precision pytorch/vision models with quantization. 1. Automate the discovery (support) for quantized operators.,2022-06-22T15:56:11Z,module: onnx triaged onnx-triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/80039,"Seen:  Weights and biases: some are quantized, others not", can you link the other tracking issue here?
agent,RPC init fails and crashes when world_size is greater than 18," 🐛 Describe the bug Hi! When I use RPC, I find that when the `world_size` is greater than 18, the program will crash. I've tested it on two servers and got the same result. Program to reproduce: ```python import torch import torch.multiprocessing as mp import torch.distributed.rpc as rpc def init_process(rank, tot_processes):     print(f'here is rank {rank}', flush=True)     rpc_backend_options = rpc.TensorPipeRpcBackendOptions(         init_method=f'tcp://localhost:52521'     )     rpc.init_rpc(         name=f'test_{rank}', rank=rank, world_size=tot_processes,         rpc_backend_options=rpc_backend_options,     )     print(f'rank {rank} init successfully', flush=True)     rpc.shutdown() def main() > None:     mp.set_start_method('spawn')     tot_processes = 19     print(f'spawning {tot_processes} processes...')     mp.spawn(         fn=init_process,         args=(tot_processes, ),         nprocs=tot_processes,         join=True,     ) if __name__ == '__main__':     main() ``` When `tot_processes = 18`, the program can exit without any error. But if `tot_processes = 19`, it will crash as below. ```shell ❯ python rpc.py spawning 19 processes... here is rank 7 here is rank 12 here is rank 4 here is rank 11 here is rank 2 here is rank 6 here is rank 15 here is rank 10 here is rank 9 here is rank 14 here is rank 8 here is rank 5 here is rank 13 here is rank 17 here is rank 3 here is rank 1 here is rank 16 here is rank 0 here is rank 18 [W tensorpipe_agent.cpp:863] RPC agent for test_12 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_5 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_16 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_15 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_3 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_13 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:492] RPC agent for test_0 encountered error when accepting incoming pipe: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_16: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_1: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_3: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_9: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_11: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_2: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_18: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_4: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_17: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_14: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_8: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_7: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_12: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_6: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_5: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_10: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_15: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_13: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:863] RPC agent for test_6 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_8 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_11 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_10 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_14 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_4 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_7 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_2 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_17 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_1 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_18 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_9 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""/home/ubuntu/myenv/experiment/pponew/rpc.py"", line 30, in      main()   File ""/home/ubuntu/myenv/experiment/pponew/rpc.py"", line 22, in main     mp.spawn(   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 240, in spawn     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 198, in start_processes     while not context.join():   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 160, in join     raise ProcessRaisedException(msg, error_index, failed_process.pid) torch.multiprocessing.spawn.ProcessRaisedException:   Process 1 terminated with the following error: Traceback (most recent call last):   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 69, in _wrap     fn(i, *args)   File ""/home/ubuntu/myenv/experiment/pponew/rpc.py"", line 11, in init_process     rpc.init_rpc(   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/distributed/rpc/__init__.py"", line 190, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/distributed/rpc/__init__.py"", line 224, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/distributed/rpc/backend_registry.py"", line 97, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/distributed/rpc/backend_registry.py"", line 305, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 77, in wrapper     return func(*args, **kwargs)   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 204, in _all_gather     rpc_sync(   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 77, in wrapper     return func(*args, **kwargs)   File ""/home/ubuntu/miniconda3/envs/myenv/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 767, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) ```  Versions ```shell PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.2 LTS (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.01045awsx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration:  GPU 0: Tesla V100SXM216GB GPU 1: Tesla V100SXM216GB GPU 2: Tesla V100SXM216GB GPU 3: Tesla V100SXM216GB Nvidia driver version: 460.91.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.0.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.910 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] torch==1.11.0 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0           py310h7f8727e_0   [conda] mkl_fft                   1.3.1           py310hd6ae3a3_0   [conda] mkl_random                1.2.2           py310h00e6091_0   [conda] numpy                     1.22.3          py310hfa59a62_0   [conda] numpybase                1.22.3          py310h9585f30_0   [conda] pytorch                   1.11.0          py3.10_cuda11.3_cudnn8.2.0_0    pytorch [conda] pytorchmutex             1.0                        cuda    pytorch ``` ",2022-06-22T07:06:01Z,oncall: distributed triaged module: rpc,open,0,15,https://github.com/pytorch/pytorch/issues/80017,verified that I can repro this locally.  Huang  (for tensorpipe) could we take a look at this and see what happens as we go from 18 > 19 procs?,"Hi , thanks for the script and logs. I am able to repro and hitting the error during shutdown, but the reasoning is the same. Each RPC process select a thread from a pool of threads to handle requests to avoid blocking/stalling the event loop. During initialization and graceful shutdown, all RPC processes send RPC requests to each other to perform rendezvous which exhausts the threadpool and causes the error. A workaround is to increase this thread pool size (default is 16). Here is an updated script which works: ```python import torch import torch.multiprocessing as mp import torch.distributed.rpc as rpc def init_process(rank, tot_processes):     print(f'here is rank {rank}', flush=True)     rpc_backend_options = rpc.TensorPipeRpcBackendOptions(         init_method=f'tcp://localhost:52521',         num_worker_threads=32,   None:     mp.set_start_method('spawn')     tot_processes = 19     print(f'spawning {tot_processes} processes...')     mp.spawn(         fn=init_process,         args=(tot_processes, ),         nprocs=tot_processes,         join=True,     ) if __name__ == '__main__':     main() ``` TODO: we should error out with a more descriptive message, CC(Provide error message when thread pool is exhausted in RPC)","On one of my friend's environment, we get outputs like: ``` [W tensorpipe_agent.cpp:550] RPC agent for test_13 encountered error when reading incoming request from test_0: EOF: end of file (this is expected to happen during shutdown) [W tensorpipe_agent.cpp:550] RPC agent for test_0 encountered error when reading incoming request from test_17: EOF: end of file (this is expected to happen during shutdown) ``` But the program can finish other works and it seems like these errors don't affect the functionality. The difference here is that in his environment the program will not raise an error so the following works run well. For example, we can get the expected results for the program below even we use more than 20 processes. Note that he uses PyTorch 1.8.0. ```python import torch import torch.multiprocessing as mp import torch.distributed.rpc as rpc def work_func(x):     return x * 100 def init_process(rank, tot_processes):     print(f'here is rank {rank}', flush=True)     rpc_backend_options = rpc.TensorPipeRpcBackendOptions(         init_method=f'tcp://localhost:52521',     )     rpc.init_rpc(         name=f'test_{rank}', rank=rank, world_size=tot_processes,         rpc_backend_options=rpc_backend_options,     )     print(f'rank {rank} init successfully', flush=True)     if rank == 0:         ret_list = [             rpc.rpc_sync(f'test_{i}', work_func, args=(i,))             for i in range(1, tot_processes)         ]         print(ret_list)     rpc.shutdown() def main() > None:     mp.set_start_method('spawn')     tot_processes = 18     print(f'spawning {tot_processes} processes...')     mp.spawn(         fn=init_process,         args=(tot_processes, ),         nprocs=tot_processes,         join=True,     ) if __name__ == '__main__':     main() ```","Even add `num_worker_threads=32`, I still cannot run the program with 20 processes successfully. I guess there are some system settings that affect this.","> Even add num_worker_threads=32, I still cannot run the program with 20 processes successfully. I guess there are some system settings that affect this. Do you get the same error as the original post? Or is it a different one due to system settings?","Oh, I get a different error. ``` spawning 19 processes... here is rank 1 here is rank 5 here is rank 10 here is rank 4 here is rank 3 here is rank 2 here is rank 8 here is rank 17 here is rank 6 here is rank 16 here is rank 11 here is rank 9 here is rank 7 here is rank 0 here is rank 14 here is rank 13 here is rank 18 here is rank 15 here is rank 12 [W tensorpipe_agent.cpp:863] RPC agent for test_10 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_9 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_14 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_15 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:492] RPC agent for test_0 encountered error when accepting incoming pipe: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_3: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_6: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_16: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_4: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_1: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_15: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_18: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_13: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_12: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_5: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_7: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_2: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_11: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_9: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_10: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_8: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_14: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_17: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:863] RPC agent for test_18 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_1 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_16 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_2 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_13 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_5 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_12 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_8 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_11 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_7 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_17 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_4 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_3 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_6 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""/home/jinjun/quartz/experiment/pponew/rpc.py"", line 43, in      main()   File ""/home/jinjun/quartz/experiment/pponew/rpc.py"", line 35, in main     mp.spawn(   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 240, in spawn     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 198, in start_processes     while not context.join():   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 160, in join     raise ProcessRaisedException(msg, error_index, failed_process.pid) torch.multiprocessing.spawn.ProcessRaisedException:   Process 14 terminated with the following error: Traceback (most recent call last):   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 69, in _wrap     fn(i, *args)   File ""/home/jinjun/quartz/experiment/pponew/rpc.py"", line 15, in init_process     rpc.init_rpc(   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/__init__.py"", line 190, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/__init__.py"", line 224, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/backend_registry.py"", line 97, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/backend_registry.py"", line 305, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 77, in wrapper     return func(*args, **kwargs)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 204, in _all_gather     rpc_sync(   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 77, in wrapper     return func(*args, **kwargs)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 767, in rpc_sync     return fut.wait() RuntimeError: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) ```","Huang Hi! On my friend's machine, I tried the latest pytorch and still got the error reported above. However, using my friend's conda environment with a selfbuilt old version pytorch, the error disappeared. Here's the environment. ``` PyTorch version: 1.8.0a0+52ea372 Is debug build: False CUDA used to build PyTorch: 11.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.1 LTS (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: version 3.19.4 Libc version: glibc2.31 Python version: 3.8.5 (default, Sep  4 2020, 07:30:14)  [GCC 7.3.0] (64bit runtime) Python platform: Linux4.15.0112genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration:  GPU 0: NVIDIA A10 GPU 1: NVIDIA A10 GPU 2: NVIDIA A10 GPU 3: NVIDIA A10 Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.1.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.1.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.1.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.1.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.1.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.1.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.1.0 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] numpydoc==1.4.0 [pip3] pytorchtransformers==1.1.0 [pip3] torch==1.8.0a0+52ea372 [pip3] torchtext==0.9.0a0 [pip3] torchvision==0.9.0a0 [conda] magmacuda110             2.5.2                         5    local [conda] mkl                       2019.4                      243   [conda] mklinclude               2019.4                      243   [conda] nomkl                     3.0                           0   [conda] numpy                     1.22.4                   pypi_0    pypi [conda] numpydoc                  1.4.0                    pypi_0    pypi [conda] pytorchtransformers      1.1.0                    pypi_0    pypi [conda] torch                     1.8.0a0+52ea372          pypi_0    pypi [conda] torchtext                 0.9.0a0                  pypi_0    pypi [conda] torchvision               0.9.0a0                  pypi_0    pypi ```","> connect: Resource temporarily unavailable This looks system resource specific.  Check `ulimit a` for any limits set to your server, particularly the number of max processes. It could also be an issue with not enough RAM or swap space. ","I don't think so in my case. Because I use the same machine, and only the difference of conda environment causes the difference of results. `ulimit a` of both soft and hard limits are the same. ``` ❯ ulimit Ha t: cpu time (seconds)              unlimited f: file size (blocks)              unlimited d: data seg size (kbytes)          unlimited s: stack size (kbytes)             unlimited c: core file size (blocks)         unlimited m: resident set size (kbytes)      unlimited u: processes                       unlimited n: file descriptors                1048576 l: lockedinmemory size (kbytes)  64 v: address space (kbytes)          unlimited x: file locks                      unlimited i: pending signals                 2062058 q: bytes in POSIX msg queues       819200 e: max nice                        0 r: max rt priority                 0 N 15:                              unlimited ```","Well, you mentioned you are using different version of pytorch which will have different implementations and thus different resource usages. > l: lockedinmemory size (kbytes)  64 Your `lockedinmemory size` seems quite low, can you try increasing that (https://gemfire.docs.pivotal.io/97/geode/managing/heap_use/lock_memory.html) and trying again? Let me know, thanks!", Any updates on the follow up above? If there are no further updates I will close out the issue.,"I still have the error. ```shell ❯ ulimit Sa t: cpu time (seconds)              unlimited f: file size (blocks)              unlimited d: data seg size (kbytes)          unlimited s: stack size (kbytes)             unlimited c: core file size (blocks)         0 m: resident set size (kbytes)      unlimited u: processes                       982525 n: file descriptors                1048576 l: lockedinmemory size (kbytes)  1048576 v: address space (kbytes)          unlimited x: file locks                      unlimited i: pending signals                 982525 q: bytes in POSIX msg queues       819200 e: max nice                        0 r: max rt priority                 0 N 15:                              unlimited ❯ python try/rpc.py spawning 19 processes... here is rank 5 here is rank 10 here is rank 6 here is rank 11 here is rank 8 here is rank 13 here is rank 7 here is rank 12 here is rank 9 here is rank 3 here is rank 2 here is rank 0 here is rank 16 here is rank 17 here is rank 15 here is rank 14 here is rank 1 here is rank 18 here is rank 4 [W tensorpipe_agent.cpp:863] RPC agent for test_13 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_2 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_11 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_3 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:863] RPC agent for test_9 encountered error when sending outgoing request CC(未找到相关数据) to test_0: connect: Resource temporarily unavailable (this error originated at tensorpipe/common/socket.cc:114) [W tensorpipe_agent.cpp:492] RPC agent for test_0 encountered error when accepting incoming pipe: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_2: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_12: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_9: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_10: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_15: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_16: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_4: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_6: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_8: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_14: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_17: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_1: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_18: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_7: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_11: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_13: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_5: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:682] RPC agent for test_0 encountered error when reading incoming request from test_3: sendmsg: Broken pipe (this error originated at tensorpipe/common/socket.h:105) [W tensorpipe_agent.cpp:887] RPC agent for test_1 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_8 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_18 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_10 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_14 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_6 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_4 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_5 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_7 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_16 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_17 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:887] RPC agent for test_15 encountered error when reading incoming response from test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) [W tensorpipe_agent.cpp:863] RPC agent for test_12 encountered error when sending outgoing request CC(未找到相关数据) to test_0: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) Traceback (most recent call last):   File ""/home/jinjun/quartz/experiment/pponew/try/rpc.py"", line 50, in      main()   File ""/home/jinjun/quartz/experiment/pponew/try/rpc.py"", line 42, in main     mp.spawn(   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 240, in spawn     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 198, in start_processes     while not context.join():   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 160, in join     raise ProcessRaisedException(msg, error_index, failed_process.pid) torch.multiprocessing.spawn.ProcessRaisedException:  Process 6 terminated with the following error: Traceback (most recent call last):   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/multiprocessing/spawn.py"", line 69, in _wrap     fn(i, *args)   File ""/home/jinjun/quartz/experiment/pponew/try/rpc.py"", line 18, in init_process     rpc.init_rpc(   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/__init__.py"", line 190, in init_rpc     _init_rpc_backend(backend, store, name, rank, world_size, rpc_backend_options)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/__init__.py"", line 224, in _init_rpc_backend     rpc_agent = backend_registry.init_backend(   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/backend_registry.py"", line 97, in init_backend     return backend.value.init_backend_handler(*args, **kwargs)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/backend_registry.py"", line 305, in _tensorpipe_init_backend_handler     api._all_gather(None, timeout=rpc_backend_options.rpc_timeout)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 77, in wrapper     return func(*args, **kwargs)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 204, in _all_gather     rpc_sync(   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 77, in wrapper     return func(*args, **kwargs)   File ""/home/jinjun/miniconda3/envs/quartz/lib/python3.10/sitepackages/torch/distributed/rpc/api.py"", line 767, in rpc_sync     return fut.wait() RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187) ``` ```python import os import torch import torch.multiprocessing as mp import torch.distributed.rpc as rpc def work_func(x):     return x * 100 def init_process(rank, tot_processes):     print(f'here is rank {rank}', flush=True)     rpc_backend_options = rpc.TensorPipeRpcBackendOptions(         init_method=f'tcp://localhost:52524',         num_worker_threads=32,     )     rpc.init_rpc(         name=f'test_{rank}', rank=rank, world_size=tot_processes,         rpc_backend_options=rpc_backend_options,     )     print(f'rank {rank} init successfully', flush=True)     if rank == 0:         ret_list = [             rpc.rpc_sync(f'test_{i}', work_func, args=(i,))             for i in range(1, tot_processes)         ]         print(ret_list)     rpc.shutdown() def main() > None:     import resource     rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)     resource.setrlimit(resource.RLIMIT_NOFILE, (1000000, rlimit[1]))     mp.set_start_method('spawn')     tot_processes = 19     print(f'spawning {tot_processes} processes...')     mp.spawn(         fn=init_process,         args=(tot_processes, ),         nprocs=tot_processes,         join=True,     ) if __name__ == '__main__':     main() ```",HI  Huang  Have you solved this issue ?  I also met this issus and It doesn't work when I tried all approches above. ," No, I couldn't make it work."," I faced the same problem, and solved it with the solution mentioned in the discussion. https://discuss.pytorch.org/t/rpcbehaviordifferencebetweenpytorch170vs190/124772/16"
agent,DISABLED test_device_map_gpu_default_to_non_default (__main__.TensorPipeTensorPipeAgentCudaRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 red and 1 green. ",2022-06-22T03:55:39Z,oncall: distributed triaged module: flaky-tests module: rpc skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/80008
transformer,[release/1.12] nn: Disable nested tensor by default,Better transformers (and by extension nested tensor) are identified as a prototype feature and should not be enabled by default for the 1.12 release. Signedoffby: Eli Uriegas ,2022-06-20T18:38:49Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/79884,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79884**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9c125264db (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
yi,Trying to run all bazel tests on gpu machine,"This is exploration of CI capabilities, please don't review.",2022-06-18T22:26:59Z,open source cla signed Stale,closed,0,2,https://github.com/pytorch/pytorch/issues/79844,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79844**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 1 Flaky Failures As of commit 35c2db6352 (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** tentatively recognized as flaky :snowflake:     * Click here to rerun these jobs   :snowflake: 1 failure **tentatively classified as flaky** but reruns have not yet been triggered to confirm:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6955918249?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7bazeltest / buildandtest (1/1) **Step:** ""Test"" (full log  :repeat: rerun) :snowflake:   20220619T17:05:01.1648185Z unknown file: Failure  ``` 20220619T17:05:01.1644360Z AutocastCPU: fallthrough registered at aten/src/ATen/autocast_mode.cpp:482 [backend fallback] 20220619T17:05:01.1644826Z Autocast: fallthrough registered at aten/src/ATen/autocast_mode.cpp:324 [backend fallback] 20220619T17:05:01.1645259Z Batched: registered at aten/src/ATen/BatchingRegistrations.cpp:1068 [kernel] 20220619T17:05:01.1645742Z VmapMode: fallthrough registered at aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback] 20220619T17:05:01.1646263Z PythonTLSSnapshot: registered at aten/src/ATen/core/PythonFallbackKernel.cpp:137 [backend fallback] 20220619T17:05:01.1646551Z  20220619T17:05:01.1646812Z Exception raised from reportError at aten/src/ATen/core/dispatch/OperatorEntry.cpp:474 (most recent call first): 20220619T17:05:01.1647222Z (no backtrace available)"" thrown in the test body. 20220619T17:05:01.1647572Z [  FAILED  ] LazyShapeTest.TestMulBasic (1 ms) 20220619T17:05:01.1647913Z [ RUN      ] LazyShapeTest.TestCatBasic 20220619T17:05:01.1648185Z unknown file: Failure 20220619T17:05:01.1651720Z C++ exception with description ""Could not run 'aten::cat' with arguments from the 'Lazy' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::cat' is only available for these backends: [Dense, FPGA, ORT, Metal, Quantized, CustomRNGKeyId, MkldnnCPU, Sparse, SparseCsrCUDA, NestedTensor, BackendSelect, Python, Fake, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, AutogradOther, AutogradFunctionality, AutogradNestedTensor, Tracer, AutocastCPU, Autocast, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, DeferredInit, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, TESTING_ONLY_GenericMode, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, CPU, CUDA, XLA, MPS, IPU, XPU, HPU, VE, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseCPU, SparseCUDA, SparseHIP, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseXPU, UNKNOWN_TENSOR_TYPE_ID, SparseVE, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, NestedTensorCPU, NestedTensorCUDA, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID]. 20220619T17:05:01.1654016Z  20220619T17:05:01.1654519Z Undefined: registered at bazelout/k8fastbuild/bin/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:26815 [default backend kernel] 20220619T17:05:01.1655206Z CPU: registered at bazelout/k8fastbuild/bin/aten/src/ATen/RegisterCPU.cpp:39984 [kernel] 20220619T17:05:01.1655832Z CUDA: registered at bazelout/k8fastbuild/bin/aten/src/ATen/RegisterCUDA.cpp:55699 [kernel] 20220619T17:05:01.1656547Z HIP: registered at bazelout/k8fastbuild/bin/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:26815 [default backend kernel] 20220619T17:05:01.1657383Z MPS: registered at bazelout/k8fastbuild/bin/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:26815 [default backend kernel] 20220619T17:05:01.1658213Z IPU: registered at bazelout/k8fastbuild/bin/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:26815 [default backend kernel] 20220619T17:05:01.1659038Z XPU: registered at bazelout/k8fastbuild/bin/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:26815 [default backend kernel] 20220619T17:05:01.1659845Z HPU: registered at bazelout/k8fastbuild/bin/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:26815 [default backend kernel] ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
gpt,[ONNX] Internal assertion failure when export gpt2 model with `export_modules_as_functions=True`," 🐛 Describe the bug Hello, I'm using `torch.onnx.export` to convert my gpt2 model to onnx format. But when I set `        opset_version=15` and `export_modules_as_functions=True`, the program reports an error: ``` Traceback (most recent call last):   File ""/Users/haobogu/Projects/python/cosine/.venv/lib/python3.7/sitepackages/torch/onnx/__init__.py"", line 309, in export     export_modules_as_functions)   File ""/Users/haobogu/Projects/python/cosine/.venv/lib/python3.7/sitepackages/torch/onnx/utils.py"", line 122, in export     custom_opsets=custom_opsets, export_modules_as_functions=export_modules_as_functions)   File ""/Users/haobogu/Projects/python/cosine/.venv/lib/python3.7/sitepackages/torch/onnx/utils.py"", line 736, in _export     graph, export_modules_as_functions, list(params_dict.keys())) RuntimeError: ns_a[i]>kind() == ns_b[i]>kind() INTERNAL ASSERT FAILED at ""/Users/distiller/project/pytorch/torch/csrc/jit/passes/onnx/function_extraction.cpp"":146, please report a bug to PyTorch. ```  Versions Collecting environment information... PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (x86_64) GCC version: Could not collect Clang version: 13.0.1 CMake version: version 3.22.3 Libc version: N/A Python version: 3.7.9 (v3.7.9:13c94747c7, Aug 15 2020, 01:31:08)  [Clang 6.0 (clang600.0.57)] (64bit runtime) Python platform: Darwin21.5.0x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.11.0 [pip3] torchvision==0.12.0 [conda] Could not collect",2022-06-18T15:37:54Z,needs reproduction module: onnx triaged onnx-triaged,closed,0,8,https://github.com/pytorch/pytorch/issues/79839,Does `export_modules_as_functions=False` give you the same error?," No, setting `export_modules_as_functions=False` removes this error",Could you share a script that will help us reproduce the error?," here is the script ```python from transformers import GPT2Config from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel import torch def convert_gpt2_model_to_onnx() > str:     """"""     convert pytorch model to onnx format     """"""      Load model and set the model to eval mode     model: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained('sshleifer/tinygpt2')     model.eval()      batch_size, input_ids_length and past_sequence_length are dynamic axes      We have to initialize a random input(the value doesn't matter) for the model, because the converting requires execution of the model      See: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html     batch_size = 3     input_ids_length = 2     past_sequence_length = 1     config: GPT2Config = model.config     num_attention_heads = config.n_head     hidden_size = config.n_embd     num_layer = config.n_layer     vocab_size = config.vocab_size     config.is_decoder = True      past(`past_key_values` in model) is a list, its length is num_layer.      each element in the list is a tuple(key, value), and key/value's shape is past_shape,      aka [batch_size, n_heads, past_sequence_length, embd_size_each_head]     past_shape = [batch_size, num_attention_heads,                   past_sequence_length, int(hidden_size/num_attention_heads)]     past = [(torch.rand(past_shape, dtype=torch.float32, device='cpu'), torch.rand(past_shape, dtype=torch.float32, device='cpu'))             for _ in range(num_layer)]      input_ids is a [batch_length, input_ids_length] tensor     input_ids = torch.randint(         low=0,         high=vocab_size  1,         size=(batch_size, input_ids_length),         dtype=torch.long,         device='cpu',     )      attention_mask is a 0/1 tensor of [batch_size, past_sequence_length + input_ids_length]     attention_mask = torch.ones(         [batch_size, past_sequence_length + input_ids_length]).to(torch.long)      token_type_ids is not needed in our case, its size is [batch_size, input_ids_length]     token_type_ids = torch.zeros([batch_size, input_ids_length]).to(torch.long)      position_ids, size is [batch_size, input_ids_length]     position_ids = attention_mask.long().cumsum(1)  1     position_ids.masked_fill_(position_ids < 0, 0)     position_ids = position_ids[:, past_sequence_length:].to(torch.long)      Run the model and get output from the model     output = model(input_ids, past_key_values=past, attention_mask=attention_mask,                    position_ids=position_ids, return_dict=True, use_cache=True)      Set output names     output_names = ['logits']     for i in range(num_layer):         output_names.append(""present_key"" + str(i))         output_names.append(""present_value"" + str(i))      Set input_names     input_names = ['input_ids']     for i in range(num_layer):         input_names.append(""past_key"" + str(i))         input_names.append(""past_value"" + str(i))     input_names += ['attention_mask', 'position_ids']      Set dynamic axes     dynamic_axes = {}     dynamic_axes['input_ids'] = {0: 'batch_size', 1: 'input_ids_length'}     dynamic_axes['attention_mask'] = {0: 'batch_size', 1: 'total_length'}     dynamic_axes['position_ids'] = {0: 'batch_size', 1: 'input_ids_length'}     dynamic_axes['logits'] = {0: 'batch_size', 1: 'input_ids_length'}     for i in range(num_layer):         dynamic_axes['past_key' +                      str(i)] = {0: 'batch_size', 2: 'past_sequence_length'}         dynamic_axes['past_value' +                      str(i)] = {0: 'batch_size', 2: 'past_sequence_length'}         dynamic_axes['present_key' +                      str(i)] = {0: 'batch_size', 2: 'total_length'}         dynamic_axes['present_value' +                      str(i)] = {0: 'batch_size', 2: 'total_length'}      The first input is required, and other inputs can be passed to torch.onnx.export using dict     inputs = (input_ids, {         'attention_mask': attention_mask,         'position_ids': position_ids,         'past_key_values': past,     })      Do export using torch.onnx.export     exported_model = ""converted_model.onnx""     torch.onnx.export(         model,         args=inputs,         f=exported_model,         export_params=True,         verbose=False,         input_names=input_names,         output_names=output_names,         dynamic_axes=dynamic_axes,         opset_version=15,         export_modules_as_functions=True     )     return exported_model convert_gpt2_model_to_onnx() ```", any ideas?,Thanks for the script. We are tracking a few instabilities with export_modules_as_functions. Will update here, ,Fixed by CC([ONNX] Fix bug using std::copy_if)
agent,Separate faulty agent RPC tests,Separates faulty agent RPC tests from rpc_test.py to faulty_agent_rpc_test.py towards fixing issue CC(Split up and reorganize RPC tests). Run a specific test: cpurun pytest test/distributed/rpc/test_faulty_agent.py vsk  Run all FaultyAgentRpcTests:  cpurun pytest test/distributed/rpc/test_faulty_agent.py vs,2022-06-17T21:27:01Z,oncall: distributed Merged cla signed release notes: distributed (rpc) topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/79810,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79810**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit f9f5433090 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Looks good, thanks for sending this PR out! Let's fix these lint failures: ``` >>> Lint for torch/testing/_internal/distributed/rpc/rpc_test.py: Advice (FLAKE8) F401     'torch.distributed.rpc.api._delete_all_user_and_unforked_owner_rrefs'     imported but unused     See https://www.flake8rules.com/rules/F401.html   18      RPCExecMode, ``` And also add a title / description to the PR that describe the change. Let's also include the github issue in the description.","hanna has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge, successfully started a merge job. Check the current status here,"Hey hanna. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"Add test for torchscripting nn.TransformerEncoder, including fast path","Summary: Add test just to check if TransformerEncoder will crash when enumerating over params [with_no_grad, use_torchscript, training]. Motivation for this was that TransformerEncoder fast path (so with_no_grad=True) and use_torchscript=True would crash with the issue that NestedTensor doesn't have size. This was caused because the TransformerEncoder fast path generates a NestedTensor automatically as a perf optimization and torchscript attempts to find intermediate tensor sizes while it optimizes. But NestedTensor has not implemented a size method, so things fail. This test goes together with this fix https://github.com/pytorch/pytorch/pull/79480 Test Plan: ``` buck build showoutput mode/opt c fbcode.enable_gpu_sections=true c fbcode.nvcc_arch=a100 mode/inplace  //caffe2/test:transformers ./fbcode/buckout/gen/caffe2/test/transformersbinary.par ``` Test runs and passes together with the changes from the PR above (I made another diff on top of this with those changes). Does not pass without the fix. Reviewed By: mikekgfb Differential Revision: D37222923",2022-06-17T17:35:56Z,module: tests fb-exported Merged cla signed release notes: jit release notes: nn topic: bug fixes topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/79796,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79796**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 6d647a8ad3 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37222923, merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This PR also included this one https://github.com/pytorch/pytorch/pull/79576 (also approved). I intended to merge these as stacked PRs, but ended up just merging this one by mistake which included code for both PRs. "
finetuning,[FSDP] Test that module using mixed precision can be loaded into non-mp module," 🚀 The feature, motivation and pitch In FSDP, mixed precision checkpoint is taken with the full parameter precision, but we're missing a test that does this and restores into a nonmixed precision module. Having this test will help provide confidence that use cases which use these such as finetuning / inference work as expected.  Alternatives _No response_  Additional context _No response_ ",2022-06-17T01:38:57Z,triaged better-engineering module: fsdp,open,0,0,https://github.com/pytorch/pytorch/issues/79766
transformer,[AI Accelerators] softmax kernel for Nested Tensor (CPU),"Summary: Impl better softmax kernel for Nested Tensor CPU. Test Plan: Benchmark results: On CPU (command: buck run mode/opt c fbcode.platform=platform009 //pytext/fb/tools:benchmark_transformers   transformer large usetrtkernel False batchsize 16 avgsequencelength 64 maxsequencelength 256 iters 10 userealdatadistribution module native usent True usecpu True With mask (previous impl): NT: 4573.14 ms/iter, 0.14 TFLOP/s, Speedup: 2.33x; Without mask: NT: 3530.55 ms/iter, 0.18 TFLOP/s, Speedup: 1.51x Reviewed By: mikekgfb Differential Revision: D35679352",2022-06-16T22:29:21Z,fb-exported Merged cla signed release notes: nn release notes: cpp topic: performance,closed,0,22,https://github.com/pytorch/pytorch/issues/79756,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79756**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9d420b51e4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,  merge g, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2531813087,This pull request was **exported** from Phabricator. Differential Revision: D35679352, merge g, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2532363313," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D35679352,This pull request was **exported** from Phabricator. Differential Revision: D35679352,  merge g, successfully started a merge job. Check the current status here, your PR has been successfully merged.
agent,DISABLED test_custom_stream (__main__.TensorPipeTensorPipeAgentCudaRpcTest),"Platforms: linux This test was disabled because it is failing in CI. See recent examples and the most recent trunk workflow logs. Over the past 3 hours, it has been determined flaky in 1 workflow(s) with 1 red and 1 green. ",2022-06-16T21:41:35Z,oncall: distributed triaged module: flaky-tests module: rpc skipped,closed,0,0,https://github.com/pytorch/pytorch/issues/79750
yi,[DataPipe] Count number of successful yields for IterDataPipe,Stack from ghstack:  CC([DataPipe] Snapshotting prototype)  CC([DataPipe] Basic snapshotting with IterableWrapper)  CC([DataPipe] Full graph fastforwarding)  CC([DataPipe] Simple graph snapshotting)  CC([DataPipe] Count number of successful yields for IterDataPipe) This PR adds an attribute and logic to count the number of successful yields from `IterDataPipe`. This information can be useful to fastforward a DataPipe (or the entire graph) back to a certain state.,2022-06-15T23:38:32Z,Merged module: data cla signed release notes: dataloader topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/79657,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79657**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 55bdafcccc (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here, your PR has been successfully merged.
yi,[torch] Add more functions to __init__.pyi.in for torch._C for Node and Value,"Summary: https://github.com/pytorch/pytorch/pull/78757 recently added a lot of functions to the type stub, but it missed a few of them. This change will make sure every function is included, by making sure this list is uptodate with: `torch/csrc/jit/python/python_ir.cpp`. This change only does this for Node and Value. Differential Revision: D37189713",2022-06-15T22:55:05Z,fb-exported Merged cla signed topic: not user facing,closed,0,14,https://github.com/pytorch/pytorch/issues/79654,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79654**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 931c49008c (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,This pull request was **exported** from Phabricator. Differential Revision: D37189713,"Adding `destroy` as a function too, then will merge",This pull request was **exported** from Phabricator. Differential Revision: D37189713, merge g, successfully started a merge job. Check the current status here, your PR has been successfully merged.,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","I don't have write access, so I can't apply a label. So I'll leave this in a comment instead: topic: not user facing"
transformer,"[FSDP] RuntimeError when using FSDP with auto wrap for sequence-to-sequence language models such as T5, Pegasus"," 🐛 Describe the bug When using FSDP with the latest PyTorch Nightly version, it throws below error when using  sequencetosequence language models such as T5, Pegasus on tasks such as translation, summarization ...  Steps to reproduce the error: 1. Run below command for the official transformers script run_translation.py  to train T5 on translation task using FSDP integration. ``` torchrun nproc_per_node=2 examples/pytorch/translation/run_translation.py \\ 	model_name_or_path t5small per_device_train_batch_size 1   \\ 	output_dir output_dir overwrite_output_dir \\ 	do_train max_train_samples 500 num_train_epochs 1 \\ 	dataset_name wmt16 dataset_config ""roen"" \\ 	source_lang en target_lang ro \\ 	fsdp ""full_shard auto_wrap"" \\         fsdp_min_num_params 2000 \\  ``` 2. The following error is the output with the stack trace: ```bash File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 1852[51/1916] ard     outputs = self.module(*args, **kwargs)     inputs_embeds = self.embed_tokens(input_ids)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1129, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/distributed/fsdp/flatten_params_wrapper.py"", line 465, in forward     return self.module(*inputs, **kwinputs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1129, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/sourab/transformers/src/transformers/models/t5/modeling_t5.py"", line 1601, in forward     encoder_outputs = self.encoder(   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1129, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 1852, in forw ard     outputs = self.module(*args, **kwargs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1129, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1129, in _call_impl   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/distributed/fsdp/flatten_params_wrapper.py"", line 465, in forward     return self.module(*inputs, **kwinputs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1129, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/sourab/transformers/src/transformers/models/t5/modeling_t5.py"", line 934, in forward     inputs_embeds = self.embed_tokens(input_ids)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1129, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/sparse.py"", line 158, in forward     return F.embedding(   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/functional.py"", line 2148, in embedding     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)     return forward_call(*input, **kwargs)   File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/modules/sparse.py"", line 158, in forward RuntimeError: Output 0 of ViewBackward0 is a view and its base or another view of its base has been modified inplace. This view  is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace.  You should replace the inplace operation by an outofplace one.                                                                    return F.embedding(                                                                                                           File ""/home/sourab/dev/lib/python3.8/sitepackages/torch/nn/functional.py"", line 2148, in embedding                               return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)                                              RuntimeError: Output 0 of ViewBackward0 is a view and its base or another view of its base has been modified inplace. This view  is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace.  You should replace the inplace operation by an outofplace one. ```  Expected Output No errors and runs for sequencetosequence models.  Versions PyTorch version: 1.12.0.dev20220505+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 7.5.06ubuntu2) 7.5.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.090genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration:  GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX Nvidia driver version: 470.57.02 cuDNN version: Probably one of the following: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.5 /usr/local/cuda10.2/targets/x86_64linux/lib/libcudnn.so.7.6.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.12.0.dev20220505+cu113 [pip3] torchaudio==0.12.0.dev20220505+cu113 [pip3] torchvision==0.13.0.dev20220505+cu113 [conda] Could not collect ",2022-06-15T12:06:11Z,oncall: distributed module: fsdp,closed,1,16,https://github.com/pytorch/pytorch/issues/79605,Including similar issue raised in transformers repo  CC(Registering of kldivergence for independent distribution),"Hello, any update on this?","Hello, any update on this?"," Unfortunately our oncall missed this one, moving it back to the triage queue. Could you please link a pointer to the code that absorbs the following FSDP configuration, I couldn't find it in the HF file you linked: ``` fsdp ""full_shard auto_wrap"" \\         fsdp_min_num_params 2000 \\  ``` In PT 1.12, an autowrapping policy for transformers is available: https://github.com/pytorch/pytorch/blob/master/torch/distributed/fsdp/wrap.pyL82  could you try plugging in this policy as opposed to the sizebased wrapping policy and letting us know how that goes?",the error means the shared parameters are not wrapped in the same FSDP unit.  use transformer wrapper policy instead of sizebased wrapping policy should resolve the problem.  Wondering whether HF transformer integration codes can change the wrapping policy to use transformer wrapper policy. ?," varma   HF is not using the transformer wrapper atm, and this is definitely the issue here for T5 (and presumably pegasus).  I've contacted Stas at HF to try and get this corrected on their side.  ","Hello, Thank you, I'll work on it by testing out the Transformers wrapping.","   There's a full tutorial on how to implement transformer wrapper here, and has the import and code for T5 as one of the examples: https://github.com/lessw2020/transformer_central/tree/main/transformer_wrapping_tutorial Offhand, you should be able to modify their accelerator script to remove the min_num_params wrapper and replace it with the transformer wrapper for the model you are running. I've been using T5 HF + transformer wrapper, though not with HF trainer, so I know it works.  Please let us know how it goes and happy to help if you hit issues.","Hello , varma,   Thank you for your pointers and guidance. Transformer wrapping fixes the issue for T5. Also, I tried the `MixedPrecision` and it is also working but I don't know about correctness, I thought it was still an issue as per this  CC([FSDP] [Mixed Precision] using param_dtype breaks transformers ( in attention_probs matmul)).  Let me know if it is acceptable to use FSDP `Mixed Precision` with transformers. I have raised the following PRs to enable transformer wrapping in 🤗 accelerate and 🤗 Trainer: 1. 🤗 Accelerate  https://github.com/huggingface/accelerate/pull/522 2. 🤗 Trainer  https://github.com/huggingface/transformers/pull/18134 Successfully able to run the below command for the official transformers script run_translation.py  to train T5 on translation task using FSDP integration with `transformer wrapping` and mixed_precision. ``` torchrun nproc_per_node=2 examples/pytorch/translation/run_translation.py \\ 	model_name_or_path t5small per_device_train_batch_size 1   \\ 	output_dir output_dir overwrite_output_dir \\ 	do_train max_train_samples 500 num_train_epochs 1 \\ 	dataset_name wmt16 dataset_config ""roen"" \\ 	source_lang en target_lang ro \\         fp16 \\ 	fsdp ""full_shard auto_wrap"" \\         fsdp_transformer_layer_cls_to_wrap ""T5Block""  ``` Below is the screenshot of the output showing successful training and model architecture with FSDP wrapping on T5Block. !Screenshot 20220714 at 6 06 08 PM","Regarding  > Also, I tried the MixedPrecision and it is also working but I don't know about correctness Since you mentioned it is working I'm assuming it didn't run into the crash described in that PR, regarding correctness, in general we should see throughput speedup, memory reduction, at the possible cost of training loss being higher than full precision.","   thanks for putting in the PR over there to get this updated! As varma mentioned, mixed precision with FSDP works extremely well (for transformers etc).  Your loss may lag relative to fp32 for 13 of the first epochs but after that bfloat will catch up and end up at same place as fp32 ...but 2x+ as fast per epoch.   If you are an ampere, definitely use bfloat16 and esp for T5.   For fp16, you have to add a bit more code to implement the sharded grad scaler. I'm making the video + code tutorial for mixed precision today and tomorrow, so let me send that over for you as soon as it's finished. ","Hi     Here's the mixed precision tutorial for FSDP. https://github.com/lessw2020/transformer_central/blob/main/mixed_precision/readme.md It's pretty straightforward as it's controlled via custom policies.   Please make sure you do a BFloat16 supported check as shown in the tutorial and for FP16, you'll need to use the ShardedGradScaler (also shown in tutorial). Please let me know if any questions!","Thank you everyone 🤗. This is resolved, will close the issue.","This issue is caused by we tie `word_emb` with `lm_head`, which is the meaning of `multiple_views`. I solved it by disable `tie_word_embedding`","Hi, I'm facing the same issue but on the nn.Linear module, the stack trace is as follows, this is happening when I'm using FSDP. I have tried using `transformer_auto_wrap_policy` but that doesn't seem to help. Any ideas? ``` File "".../pnp_dist.runfiles/pip_torch/torch/nn/modules/linear.py"", line 114, in forward return F.linear(input, self.weight, self.bias) RuntimeError: Output 0 of ViewBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an outofplace one. ``` I am using the Linear module in a sequence, if that affects it? I am wrapping the entire model (it's a transformer based model) in one FSDP module, if it's recommended to wrap modules inside separately, let me know. ``` block = nn.Sequential(             nn.Linear(n_input, n_feat),             LayerNorm2DSlim(n_feat, relu=True),             nn.Linear(n_feat, n_feat),         ) ```",I'm having the same problem. Did you figure it out? 
transformer,Add numerical test comparing BetterDecoder and fairseq decoder,Summary: Add a new file test_transformers.py to put transformers tests in and move away from huge monolithic test_nn.py. A todo item is to move existing transformer tests from test_nn.py to test_transformers.py. Add a numerical test comparing torch.nn._transformer_decoder_layer_fwd and fairseq decoder. Both decoders use the weights of a common nn.TransformerEncoder. Contains both forced decoding and incremental decoding Stacked on top of https://github.com/pytorch/pytorch/pull/79438 Test Plan: ``` buck build showoutput mode/opt c fbcode.enable_gpu_sections=true c fbcode.nvcc_arch=a100 mode/inplace  //caffe2/test:transformers ./fbcode/buckout/gen/caffe2/test/transformersbinary.par ``` Test runs and passes! Differential Revision: D37157391,2022-06-14T23:10:29Z,fb-exported cla signed,closed,0,14,https://github.com/pytorch/pytorch/issues/79576,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79576**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b4d1fbc23f (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D37157391,Corresponding test for https://github.com/pytorch/pytorch/pull/79438,This pull request was **exported** from Phabricator. Differential Revision: D37157391,> LGTM! Thanks!   Can you also approve https://github.com/pytorch/pytorch/pull/79438? It's the PR this one is stacked on top of. ,This pull request was **exported** from Phabricator. Differential Revision: D37157391, merge g , successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2505610792,This pull request was **exported** from Phabricator. Differential Revision: D37157391, merge, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2518175005,Closing because code in this PR already committed in https://github.com/pytorch/pytorch/pull/79796
rag,[PyTorch] Don't create a new Storage in FreeMemory unnecessarily,"  CC([PyTorch] Don't create a new Storage in FreeMemory unnecessarily) No reason to go through an extra heap allocation. Differential Revision: D37157595 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-06-14T22:42:21Z,Merged cla signed topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/79573,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79573**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit cec0d99c7e (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,internal benchmarks look good on this one!,"clicked rerun on failed job, waiting for it before merging", merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,"Torch FX return error ""`__cuda_array_interface__` must be a dict"""," 🐛 Describe the bug During development of raftstereo model on torchvision, I get an error `""TypeError: ``__cuda_array_interface__`` must be a dict""` after testing it with `torch.fx.symbolic_trace`. The error comes from this particular line and this test is run on cpu device. Here is a minimum example to reproduce: ``` import torch import torch.fx class CustomModule(torch.nn.Module):     def forward(self, x):         batch_size, num_channels, h, w = x.shape         b = x / torch.tensor(num_channels)         return b if __name__ == ""__main__"":     m = CustomModule()     m_fx = torch.fx.symbolic_trace(m)   This will trigger error! ``` The error I got: ```   File ""/Users/yosuamichael/manual_build_lib/pytorch/vision/torchvision/prototype/models/depth/stereo/fx_test.py"", line 22, in forward     b = x / torch.tensor(num_channels) TypeError: `__cuda_array_interface__` must be a dict ``` Currently I have found a way to fix this by creating global function that is wrapped by `torch.fx.wrap`. Here is the simple code with the fix to illustrate: ``` import torch import torch.fx def foo(x, num_channels):     b = x / torch.tensor(num_channels)     return b torch.fx.wrap(""foo"") class CustomModule(torch.nn.Module):     def forward(self, x: torch.Tensor) > torch.Tensor:         batch_size, num_channels, h, w = x.shape         return foo(x, num_channels) if __name__ == ""__main__"":     m = CustomModule()     m_fx = torch.fx.symbolic_trace(m)   No error now ``` I would like to ask if my current fix is appropriate or is there a better suggestion for the fix? Also I think the current error message `""TypeError: ``__cuda_array_interface__`` must be a dict""` is not helpful. It would be great if we can have a better error message. Thanks in advance!  Versions ``` Collecting environment information... PyTorch version: 1.13.0.dev20220531 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2) CMake version: Could not collect Libc version: N/A Python version: 3.9.10 (main, Mar  8 2022, 16:21:53)  [Clang 13.0.0 (clang1300.0.29.30)] (64bit runtime) Python platform: macOS12.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.931 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] pytorchsphinxtheme==0.0.24 [pip3] torch==1.13.0.dev20220531 [pip3] torchaudio==0.10.2 [pip3] torchdata==0.5.0.dev20220531 [pip3] torchvision==0.13.0a0+d8654bb [conda] Could not collect ``` ",2022-06-14T12:46:11Z,triaged module: fx,closed,4,4,https://github.com/pytorch/pytorch/issues/79513,"I think I know what the problem is. FX tracers support ALL attribute accesses, returning proxies when this happens. torch.tensor tests to see if there is `__cuda_array_interface__` to see if it can treat it as if it were a tensor. If you pass a proxy tensor, proxy tensor is like ""yeah, I got one of those"" and then torch.tensor is like ""tf is this, this is a Proxy, not a dict"" and errors. We can make a nicer error message probably by explicitly testing if the input object is a Proxy and just erroring. But the upshot is you won't be able to symbolic trace through code that uses torch.tensor. Consider using `make_fx` instead!","Hey , do you have any news on this? Having the same issue..",Can you try `make_fx`? :),"Sorry I missed your first response . I have tried using `make_fx` and indeed it does work like a charm :) Code after using `make_fx`: ``` import torch import torch.fx import functorch class CustomModule(torch.nn.Module):     def forward(self, x):         batch_size, num_channels, h, w = x.shape         b = x / torch.tensor(num_channels)         return b if __name__ == ""__main__"":     m = CustomModule()      m_fx = torch.fx.symbolic_trace(m)   This will trigger error!     x = torch.rand(2, 3, 4, 4)     m_fx = functorch.make_fx(m)(x)  Work like a charm! ```"
yi,"Random number generation yields different values on different devices, despite the same manual seed."," 🐛 Describe the bug Given the **same** manual seed, cpu and cuda generate two different sets of random numbers. See the following code. ```python3 import torch cuda = torch.device(""cuda"") torch.manual_seed(10) torch.randn(3)  always yields tensor([0.6014, 1.0122, 0.3023]) torch.manual_seed(10) torch.randn(3, device=cuda)  always yields tensor([0.1029,  1.6810, 0.2708], device='cuda:0') ``` But that shouldn't be the case, as it breaks reproducibility. At the beginning of pytorch programs, you often select either the CPU to use as the device, or the GPU if its available. But now, depending on which device is selected, different sets of random numbers will be produced from a program that was otherwise seeded to run a single deterministic way.  Versions Collecting environment information... PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.10.16.3microsoftstandardWSL2x86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Ti Nvidia driver version: 512.95 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.20.2 [pip3] torch==1.11.0+cu113 [pip3] torchaudio==0.11.0+cu113 [pip3] torchvision==0.12.0+cu113 [conda] Could not collect ",2022-06-14T02:56:21Z,triaged module: random module: numerical-reproducibility module: determinism,closed,0,1,https://github.com/pytorch/pytorch/issues/79496,This is expected https://pytorch.org/docs/stable/notes/randomness.html
rag,Symbolic storage size,  CC(Add support for directly passing symint to empty)  CC(Add support for multiply on direct SymInt)  CC(Symbolic storage size) Signedoffby: Edward Z. Yang ,2022-06-14T02:25:11Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/79492,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79492**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 69ca6f4b1a (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Implement NestedTensor size function," 🚀 The feature, motivation and pitch NestedTensor does not implement a size method. NestedTensor has been integrated into nn.Transformer, which has revealed a couple of use cases where the sizes of intermediate tensors in the model are needed. The three main cases are:  PyTorch profiler fails on nn.Transformer because it attempts to find the size of all intermediate tensors, which includes NestedTensors in the transformer.   Autograd with NestedTensor fails because it requires the size of the tensor  Torchscript with NestedTensor fails because torch.jit.script(model) attempts to find the size of intermediate tensors for performance optimizations.  Alternatives Two main ways to fix: 1. In all code that tries to find size of intermediate tensors, check for NestedTensor, and if it's a NestedTensor, do not ask for size. This seems feasible for the above use cases (profiler: has a WIP patch to check for NestedTensor, autograd: WIP in  CC(Add Autograd Support for Nested Tensor), torchscript: could probably make a torchscriptspecific patch). But I imagine that there are many such use cases and it would be hard to cover them all.   2. Implement NestedTensor size method in a way that does not break existing use cases.   Additional context _No response_ ",2022-06-13T23:31:58Z,module: nestedtensor,closed,3,2,https://github.com/pytorch/pytorch/issues/79477,"Linking https://github.com/pytorch/pytorch/pull/79480 . Thanks for the fix! Unfortunately wasn't able to test this before the branch cut. It looks like it was failing some of the tests, so potentially needs more work. ",Note that the size of a nested tensor is a nested data structure itself. It's unlikely we would ever implement `sizes()` directly for nested tensors as its signature returns an `IntArrayRef`. I think it's better to abstract out callsitespecific querying of tensor size. A good example for autograd is providing an `is_same_size()` abstraction that can be implemented differently for nested vs. nonnested tensors (see CC(update is_same_size to work with nested tensor dispatch)).
yi,[META] Sign up to discuss significantly modifying CI,"Our CI currently tests tens of thousands of tests across many different platforms and compilers. As PyTorch grows and different modules would like to add to our CI, we should meet and discuss the added value of increased testing vs our constraints of CI capacity and time to signal (TTS). We should also discuss any changes that may largely affect CI. Please sign up for a slot (Tuesdays 4:055:00pm ET) below! Please add a topic and an RFC/document that should be prepared ahead of time so we can spend more time in discussion. The PyTorch Dev Infra team will be defacto attendees. **Please include emails so the invites could be sent out accordingly.**  6/14/22 [EXAMPLE] **Topic**: Let's Add More Fun(c) to CI **Presenter(s)**: Firstname Lastname (presenter.com) **RFC/Document**:  CC(RFC: Move functorch into pytorch/pytorch) is a good example **Invitees/Attendees**: Team MemberA (teammateA.com), Team MemberB (teammateB.com), Module ExpertA (expertA.com), etc..  6/21/22 **Topic**:  **Presenter(s)**:  **RFC/Document**:  **Invitees**:   7/12/22 **Topic**: **Presenter(s)**:  **RFC/Document**:  **Invitees**:   7/26/22 **Topic**: **Presenter(s)**:  **RFC/Document**:  **Invitees**: /pytorchdevinfra",2022-06-13T23:28:00Z,module: ci triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/79476
yi,RuntimeError: Found an argument of type numpy.int64 at index 0. ,"I got the following error when using `aot_module` on model fastNLP_Bert and speech_transformer . RuntimeError: Found an argument of type numpy.int64 at index 0. Nontensor arguments must be marked static. Please set the static_argnums correctly to mark the argument at index 0 static. The aot_module is used like below: ``` def save_fx(gm, example_inputs):     from functorch.compile import aot_module, aot_module_simplified     def graph_saver_forward(gm, _):         gm.to_folder(...)         return gm     def graph_saver_backward(gm, _):         gm.to_folder(...)         return gm     return aot_module(gm, fw_compiler=graph_saver_forward, bw_compiler=graph_saver_backward)  optimize_ctx = torchdynamo.optimize(                 save_fx,  aot_module(gm, fw_compiler=graph_saver_forward, bw_compiler=graph_saver_backward)             nopython=args.nopython,         ) with optimize_ctx:            model_iter_fn(model, example_inputs, collect_outputs=False) ```   ",2022-06-13T18:29:33Z,oncall: pt2,closed,0,1,https://github.com/pytorch/pytorch/issues/93766,This is not a supported way of calling `aot_module`
transformer,Refactor out QKV in projection,Summary: Refactor to reduce amount of copied code for decoder by finding common chunks for encoder and decoder. QKV in projection is a reasonable unit to copy out. Test Plan: buck run mode/opt c fbcode.platform=platform010 c fbcode.enable_gpu_sections=true c fbcode.nvcc_arch=a100 //pytext/fb/tools:benchmark_transformers  transformer batchsize 64 avgsequencelength 235 maxsequencelength 256 iters 100  module native Benchmark and numerical tests work fine. Reviewed By: mikekgfb Differential Revision: D36138504,2022-06-13T18:05:39Z,fb-exported Merged cla signed release notes: nn release notes: cpp topic: improvements topic: not user facing,closed,0,12,https://github.com/pytorch/pytorch/issues/79437,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79437**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit ef77e92aac (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36138504,This pull request was **exported** from Phabricator. Differential Revision: D36138504,This pull request was **exported** from Phabricator. Differential Revision: D36138504,This pull request was **exported** from Phabricator. Differential Revision: D36138504, merge g , successfully started a merge job. Check the current status here,This pull request was **exported** from Phabricator. Differential Revision: D36138504,Merge failed due to New commits were pushed while merging. Please rerun the merge command. Raised by https://github.com/pytorch/pytorch/actions/runs/2504577805, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Performance drops after running tensor multiplication for 15 seconds on M1 MAX (Pytorch MPS).," 🐛 Describe the bug Performance drops to half of the original performance after running tensor multiplication for 15 seconds on M1 MAX (Pytorch MPS).  Python version: 3.9.7 OS: macOS 12.4 Pytorch version: 1.13.0.dev20220612 The code below reproduces the error: ``` from tqdm import tqdm import torch x = torch.rand((1024 * 8, 1024 * 8), dtype=torch.float32, device=torch.device('mps')) y = torch.rand((1024 * 8, 1024 * 8), dtype=torch.float32, device=torch.device('mps')) .jit.script def foo():     x = torch.rand((1024 * 8, 1024 * 8), dtype=torch.float32, device=torch.device('mps'))     y = torch.rand((1024 * 8, 1024 * 8), dtype=torch.float32, device=torch.device('mps'))     z = x * y     return z if __name__ == '__main__':     z0 = None     for _ in tqdm(range(10000000000)):         zz = foo()         if z0 is None:             z0 = zz         else:             z0 += zz     torch.mps.empty_cache() ``` The above code works fine for RTX 3090 and performance never drops (by changing mps to cuda), but the performance will drop from 71 it/s to 25 it/s after 15 seconds running on M1 Max chip.  Please look into the issue, I suspect that's the reason why training speed of some transformer models dropped after running for a few minutes.  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220612 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.22.3 Libc version: N/A Python version: 3.9.7  (default, Sep 29 2021, 19:24:02)  [Clang 11.1.0 ] (64bit runtime) Python platform: macOS12.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.13.0.dev20220612 [pip3] torchaudio==0.14.0.dev20220603 [pip3] torchtext==0.10.0 [pip3] torchvision==0.14.0a0+f9f721d [conda] numpy                     1.22.3                   pypi_0    pypi [conda] torch                     1.13.0.dev20220612          pypi_0    pypi [conda] torchaudio                0.14.0.dev20220603          pypi_0    pypi [conda] torchtext                 0.10.0                   pypi_0    pypi [conda] torchvision               0.14.0a0+f9f721d          pypi_0    pypi ",2022-06-13T06:42:11Z,module: performance triaged module: mps,closed,0,5,https://github.com/pytorch/pytorch/issues/79402,"I don't think it is a matmul problem, this behaviour seems to stem from repeatedly calling `torch.rand`. For M1 Max this happen around `40` seconds mark. The speed drop from `~165 its/s` to `~110 its/s`, then `~60 it/s` ```python from tqdm import tqdm import torch if __name__ == '__main__':     for _ in tqdm(range(10000000000)):         torch.rand((1024 * 8, 1024 * 8), dtype=torch.float32, device=torch.device('mps')) ```",Are you sure it's not related to throttling at all? I only have the Mac Mini but I assume laptops may need to throttle after running for a while.,"> I don't think it is a matmul problem, this behaviour seems to stem from repeatedly calling `torch.rand`. For M1 Max this happen around `40` seconds mark. The speed drop from `~165 its/s` to `~110 its/s`, then `~60 it/s` >  > ```python > from tqdm import tqdm > import torch >  > if __name__ == '__main__': >     for _ in tqdm(range(10000000000)): >         torch.rand((1024 * 8, 1024 * 8), dtype=torch.float32, device=torch.device('mps')) > ``` Thanks. It does look like the issue of `torch.rand`. The following code works fine for me: ```[python] import torch import torch.nn.functional as F import numpy as np from tqdm import tqdm class NeuralNet(torch.nn.Module):      Set layers.     def __init__(self):         super(NeuralNet, self).__init__()          First fullyconnected hidden layer.         self.fc1 = torch.nn.Linear(256, 8192 * 8 * 2)      Set forward pass.     def forward(self, x):         x = F.relu(self.fc1(x))         return x  Build neural network model. neural_net = NeuralNet().to(torch.device('mps')) batch_size = 1024 x = torch.ones((batch_size, 256)).to(torch.device('mps')) for _ in tqdm(range(10000000)):     neural_net(x) ``` The performance of the above code never throttles. ","For comparison, on an M1 in a Mac Mini, the above is: 12 it/s If I move the x = torch.ones in to the loop, it is 10.5 it/s If I change torch.ones to torch.randn, it is 10.5 it/s It doesn't appear to reduce over time, but my version may be older. ``` >>> torch.__version__ '1.13.0.dev20220524' ``` Edit: Confirmed same numbers on today's nightly. So just to note, the torch.rand issue does not appear on the Mac Mini.","I will close the issue, as this is not a `matmul` problem but rather a `torch.rand` problem. Thanks! "
transformer,[PT-D] Use process group of the partial tensor so sub pg comm will be enabled during reshard,"  CC([PTD] Use process group of the partial tensor so sub pg comm will be enabled during reshard) During the debugging for TP enablement for Transformer model, looks like Partial tensor does not use its own pg during resharding while using the default pg instead. Switch to use its own pg as the fix. Differential Revision: D37093468",2022-06-11T23:48:52Z,oncall: distributed Merged cla signed sharded_tensor release notes: distributed (sharded) topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/79357,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79357**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 0e7e424955 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,"Autogen Tags enum, and allow specifying tags while defining an op","Stack from ghstack:  CC(Autogen Tags enum, and allow specifying tags while defining an op) clone of https://github.com/pytorch/pytorch/pull/77313. Internal Changes will be coming soon",2022-06-10T21:48:56Z,oncall: jit Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/79322,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79322**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit dc23a6b599 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"MPS: cherry-pick ""add layer_norm_backward"" (#79189)","Layernorm backward.  This is a much needed feature fix for running hf_Bert and many other Transformer networks on MPS backend.  Fixes ISSUE_NUMBER Pull Request resolved: https://github.com/pytorch/pytorch/pull/79189 Approved by: https://github.com/razarmehr, https://github.com/albanD Fixes ISSUE_NUMBER",2022-06-10T14:29:45Z,open source cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/79276,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79276**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 51a96db93a (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
yi,[ao][sparsity] Support for sparsifying data operations on raw torch tensors.,"  CC([ao][sparsity] Support for L2 norm based block data sparsifier)  CC([ao][sparsity] L1 norm based block data sparsifier)  CC([ao][sparsity] Support for embeddings and embedding bags in BaseDataSparsifier)  CC([ao][sparsity] Support for the nn.Parameter in BaseDataSparsifier)  CC([ao][sparsity] Implemented state dict and serialization functionalities)  CC([ao][sparsity] Support for sparsifying data operations on raw torch tensors.)  CC([ao][sparsity] Base class for Data Sparsifier) The users can now pass in raw torch tensors and the base class handles all the parametrizations and masking Example      >>> data_list = [('tensor_1', torch.randn(3,3)), ('tensor_2', torch.randn(4,4))]     >>> defaults = {'sparsity_level': 0.7}     >>> sparsifier = DerivedDataSparsifier(data_list = data_list, **defaults)  Some sparsifier that inherits BaseDataSparsifier     >>> new_tensor_to_add = {'name': 'tensor_3', 'data': torch.randn(5,5), 'sparsity_level': 0.3}     >>> sparsifier.add_data(**new_tensor_to_add)     >>>  tensor_1 and tensor_2 will have sparsity_level of 0.7 but tensor_3 will have sparsity_level=0.3 Test Plan: ```python test/test_ao_sparsity.py TestBaseDataSparsifier``` Differential Revision: D37164121",2022-06-10T00:09:43Z,Merged cla signed,closed,0,9,https://github.com/pytorch/pytorch/issues/79252,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79252**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 44833a20a0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,More descriptive name for the PR title is needed. `Support for toech tensors` doesn't tell a lot about where is this support added," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", merge (Initiating merge automatically since Phabricator Diff has merged), merge (Initiating merge automatically since Phabricator Diff has merged), successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Compilation failure seen on 1.12 rc2 when trying to use pretty_print_onnx function," 🐛 Describe the bug On 1.12rc2, Getting compilation error if trying to use function ""torch::jit::pretty_print_onnx"" by including file  Getting following error: /sitepackages/torch/include/torch/csrc/jit/serialization/export_bytecode.h:11:10: fatal error: torch/csrc/jit/mobile/function.h: No such file or directory 11 | include  This is because of no ""mobile"" folder in torch installation. Probably 'include/torch/csrc/jit/mobile/*.h' needs to be included in setup.py  Versions Collecting environment information... PyTorch version: 1.12.0a0+gite1edcac Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.2 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 12.0.1 (ssh://gerrit:29418/tpc_llvm10 4f0317d1e33979c9fd564a3e129129f6d711409d) CMake version: version 3.20.2 Libc version: glibc2.31 Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.090genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] pytorchranger==0.1.1 [pip3] torch==1.12.0a0+gite1edcac [pip3] torchoptimizer==0.0.1a15 [conda] mkl                       2022.0.2                 pypi_0    pypi [conda] mklinclude               2022.0.2                 pypi_0    pypi [conda] numpy                     1.22.3                   pypi_0    pypi [conda] torch                     1.12.0a0+gite1edcac          pypi_0    pypi",2022-06-09T06:57:26Z,oncall: jit,closed,0,2,https://github.com/pytorch/pytorch/issues/79190,Are you have a problem compiling PyTorch or compiling something that includes PyTorch headers?,"I am seeing compiliation failure with our builds that uses Pytorch headers. Pytorch compiles OK. It is due to diff in ""torch\\csrc\\jit\\serialization\\export.h"", which now includes include  which has references to  include  include  This ""mobile"" folder is not found in Torch .whl pkg."
transformer,[transformer] BT enablement on fairseq - pytorch change,"The fairseq diff is split into two parts. The first diff (this one) This diff is about creating a mask left align function to check the mask condition for nested tensor. It is necessary for torchscript deployment. The second diff (D37082681) Fork the inference path inside the forward function. If loaded the checkpoint file and perform the inference, we will deploy BT. Otherwise, fairseq take the position. Reviewed By: mikekgfb Differential Revision: D36057338",2022-06-09T05:45:46Z,fb-exported Merged cla signed release notes: nn topic: bug fixes topic: performance,closed,0,10,https://github.com/pytorch/pytorch/issues/79186,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79186**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9c624344d4 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36057338,This pull request was **exported** from Phabricator. Differential Revision: D36057338,This pull request was **exported** from Phabricator. Differential Revision: D36057338,This pull request was **exported** from Phabricator. Differential Revision: D36057338,"> Confused about this  I just see this PR adding a `_nested_tensor_from_mask_left_aligned` op. Some questions: >  > * How does this related to the PR summary? AFAICT just adding this op won't enable its use. The diff is optimized to split into two parts now. Part1 only includes this op change and associated with D36057338. Part2 only includes fairseq change (D37082681) but depend on Part1. The internal CI tests show the op runs well. > * How does this differ from `_nested_tensor_from_mask`  does it just avoid checks by making the assumption the mask is leftaligned? If so, couldn't we avoid quite a bit of duplication? It is the front part of ""_nested_tensor_from_mask"" implementation. But the purpose is to help check the left aligned in advance. ",This pull request was **exported** from Phabricator. Differential Revision: D36057338, merge (Initiating merge automatically since Phabricator Diff has merged), successfully started a merge job. Check the current status here,"Hey wei. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,MPSNDArray Error: buffer is not large enough," 🐛 Describe the bug Installed latest PyTorch nightly build for MPS on M1, 1.13.0dev20220608, set environmental variable PYTORCH_ENABLE_MPS_FALLBACK=1 to resolve issue with aten::index.Tensor. Ran StableBaselines3 to train a PPO agent. Conducted one successful iteration then received an error:  ``` import torch from stable_baselines3 import PPO from stable_baselines3.common.env_util import make_vec_env  Parallel environments env = make_vec_env(""CartPolev1"", n_envs=4) device = torch.device('mps') model = PPO(""MlpPolicy"", env, device=device, verbose=1) model.learn(total_timesteps=25000) model.save(""ppo_cartpole"") ``` ``` /AppleInternal/Library/BuildRoots/8d3bda538d9c11ecabd7fa6a1964e34e/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion '[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 1024 bytes ' Process finished with exit code 134 (interrupted by signal 6: SIGABRT) ```  Versions ``` Collecting environment information... PyTorch version: 1.13.0.dev20220608 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.8.13 (default, Mar 28 2022, 06:13:39)  [Clang 12.0.0 ] (64bit runtime) Python platform: macOS12.3.1arm64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.4 [pip3] pytorchignite==0.4.6 [pip3] torch==1.13.0.dev20220608 [pip3] torchvision==0.14.0a0+f9f721d [conda] pytorch                   1.13.0.dev20220608         py3.8_0    pytorchnightly [conda] torch                     1.13.0.dev20220602          pypi_0    pypi [conda] torchvision               0.14.0a0+f9f721d          pypi_0    pypi ``` ",2022-06-09T02:41:49Z,high priority triaged module: mps,closed,0,3,https://github.com/pytorch/pytorch/issues/79181,"Likely a duplicate of  CC([Device MPS] Error: buffer is not large enough. Must be 19200 bytes) And I can reproduce the crash easily (though `TORCH_SHOW_CPP_STACKTRACES=1` does not work for it :( ), so here is the backtrace with `lldb` attached: ```     frame CC(未找到相关数据): 0x000000019cd52d98 libsystem_kernel.dylib`__pthread_kill + 8     frame CC(Matrix multiplication operator): 0x000000019cd87ee0 libsystem_pthread.dylib`pthread_kill + 288     frame CC(Don't support legacy Python): 0x000000019ccc2340 libsystem_c.dylib`abort + 168     frame CC(PEP8): 0x000000019ccc1754 libsystem_c.dylib`__assert_rtn + 272   * frame CC(PEP8): 0x00000001a57787a8 Metal`MTLReportFailure.cold.1 + 56     frame CC(Checklist for Release): 0x00000001a57622bc Metal`MTLReportFailure + 480     frame CC(Remove dampening from SGD): 0x00000001a63b6984 MPSCore`___lldb_unnamed_symbol641$$MPSCore + 428     frame CC(ImportError: No module named _C): 0x00000002009bac90 MetalPerformanceShadersGraph`___lldb_unnamed_symbol2960$$MetalPerformanceShadersGraph + 536     frame CC(fake commit): 0x000000012bf040c0 libtorch_cpu.dylib`at::native::mps::_gatherViewTensor(at::Tensor const&, id, at::native::mps::MPSCachedGraph*, at::Tensor&) + 176     frame CC(Add Storage.from_buffer): 0x000000012bf0457c libtorch_cpu.dylib`at::native::mps::Placeholder::Placeholder(MPSGraphTensor*, at::Tensor const&, NSArray*) + 208     frame CC(Tensors don't print sometimes): 0x000000012bf7d808 libtorch_cpu.dylib`at::native::structured_gather_out_mps::impl(at::Tensor const&, long long, at::Tensor const&, bool, at::Tensor const&) + 1468     frame CC(add multiprocessing unit tests, working filedescriptor based solution and OSX): 0x0000000129b6de60 libtorch_cpu.dylib`at::(anonymous namespace)::wrapper_gather(at::Tensor const&, long long, at::Tensor const&, bool) + 128     frame CC(Initial utils implementation + bug fixes): 0x000000012a98ff2c libtorch_cpu.dylib`c10::impl::wrap_kernel_functor_unboxed_, at::Tensor, c10::guts::typelist::typelist >, at::Tensor (c10::DispatchKeySet, at::Tensor const&, long long, at::Tensor const&, bool)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, long long, at::Tensor const&, bool) + 1068     frame CC(Infinite recursion when indexing Variable): 0x0000000129222fd0 libtorch_cpu.dylib`at::_ops::gather::call(at::Tensor const&, long long, at::Tensor const&, bool) + 304     frame CC(Clean up Module forward and __call__): 0x00000001044875f0 libtorch_python.dylib`torch::autograd::THPVariable_gather(_object*, _object*, _object*) + 692 ```","Here is the one line reproducer to the problem: ``` % python3 c ""import torch;x=(torch.rand(64, 1, device='mps')*1000).to(dtype=torch.int64); y=x.as_strided(size=(64,2), stride=(1, 0));z=y.as_strided(size=(64, 2), stride=(1, 0));z.to('cpu')"" /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 1024 bytes ' zsh: abort      python3 c  ```","We aren't able to reproduce this issue, could you please update to latest nightly and try again?"
transformer,`cumsum` op: pytorch failed to run GPT-2 model in M1's MPS device," 🐛 Describe the bug My transformers inference script is running successfully in device CPU, but when using device MPS in MacOS M1 Pro, it will report 'aten::cumsum.out' op is missing, so I set environment variable 'PYTORCH_ENABLE_MPS_FALLBACK', but it will report the next error for huggingface transformers GPT2 model: ``` /Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/transformers/models/gpt2/modeling_gpt2.py:999: UserWarning: The operator 'aten::cumsum.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)   position_ids = attention_mask.long().cumsum(1)  1 Traceback (most recent call last):   File ""/Users/lihua.llh/Documents/codes/lab/python/gpt2_demo/inferences/demo/beam_generation_demo.py"", line 40, in      main()   File ""/Users/lihua.llh/Documents/codes/lab/python/gpt2_demo/inferences/demo/beam_generation_demo.py"", line 31, in main     outputs = model.generate(input_ids=input_ids, num_beams=2, max_length=500, num_return_sequences=2,   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/torch/autograd/grad_mode.py"", line 27, in decorate_context     return func(*args, **kwargs)   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/transformers/generation_utils.py"", line 1344, in generate     return self.beam_search(   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/transformers/generation_utils.py"", line 2192, in beam_search     outputs = self(   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/transformers/models/gpt2/modeling_gpt2.py"", line 1046, in forward     transformer_outputs = self.transformer(   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/transformers/models/gpt2/modeling_gpt2.py"", line 889, in forward     outputs = block(   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/transformers/models/gpt2/modeling_gpt2.py"", line 390, in forward     attn_outputs = self.attn(   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/transformers/models/gpt2/modeling_gpt2.py"", line 312, in forward     query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/Users/lihua.llh/miniconda3/envs/torchm1/lib/python3.8/sitepackages/transformers/pytorch_utils.py"", line 107, in forward     x = torch.addmm(self.bias, x.view(1, x.size(1)), self.weight) RuntimeError: tensors must be 2D ```  Script using huggingface transformers version 4.19.2 ``` import torch from transformers import (     GPT2LMHeadModel,     GPT2Tokenizer, ) MODEL_CLASSES = {     ""distilgpt2"": (GPT2LMHeadModel, GPT2Tokenizer),     ""gpt2large"": (GPT2LMHeadModel, GPT2Tokenizer),     ""gpt2"": (GPT2LMHeadModel, GPT2Tokenizer), } def main():     model_type = ""gpt2""     model_class, tokenizer_class = MODEL_CLASSES[model_type]     prompt_text = """"""In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered.""""""     tokenizer = tokenizer_class.from_pretrained(model_type)     model = model_class.from_pretrained(model_type)     input_ids = tokenizer(prompt_text, return_tensors=""pt"").input_ids     model.eval()     device = torch.device(""mps"")     model = model.to(device)     input_ids = input_ids.to(device)     outputs = model.generate(input_ids=input_ids, num_beams=2, max_length=500, num_return_sequences=2,                              repetition_penalty=1.2, length_penalty=1.2, no_repeat_ngram_size=5, top_p=1.0,                              early_stopping=True)     ret = tokenizer.batch_decode(outputs, skip_special_tokens=True)     for item in ret:         print(item) if __name__ == ""__main__"":     main() ```  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220601 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (arm64) GCC version: Could not collect Clang version: 13.0.0 (clang1300.0.29.30) CMake version: Could not collect Libc version: N/A Python version: 3.8.13  (default, Mar 25 2022, 06:05:16)  [Clang 12.0.1 ] (64bit runtime) Python platform: macOS12.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.23.0rc2 [pip3] torch==1.13.0.dev20220601 [pip3] torchaudio==0.14.0.dev20220601 [pip3] torchvision==0.14.0a0+f9f721d [conda] numpy                     1.23.0rc2                pypi_0    pypi [conda] torch                     1.13.0.dev20220601          pypi_0    pypi [conda] torchaudio                0.14.0.dev20220601          pypi_0    pypi [conda] torchvision               0.14.0a0+f9f721d          pypi_0    pypi ",2022-06-08T07:02:33Z,triaged module: mps,closed,6,2,https://github.com/pytorch/pytorch/issues/79112,"Hi , thanks for the issue. Currently we don't have support for `cumsum` op in MPS layer but looking into it. As we have the API support available we will enable it for MPS backend.","> x = torch.addmm(self.bias, x.view(1, x.size(1)), self.weight) > RuntimeError: tensors must be 2D  this crash is fixed in the latest pytorch nightly  please give a try and let me know if you still see any issues. >  it will report 'aten::cumsum.out' op is missing For op support, please add a request here:  CC(General MPS op coverage tracking issue)."
rag,Add operator and derivative coverage for nestedtensors, Summary Compared to the out of tree nested_tensor impl: https://github.com/pytorch/nestedtensor core does not have as many nestedtensor implementations.  As well since the out of tree version did not support autograd the derivative implementations are not defined yet.    Operator Coverage forward and backward defined  Factory Functions:  [x] _nested_tensor_from_mask  [x] _nested_tensor_from_padded  [x] to_padded_tensor  [x] nested_tensor  MHA Ops:  [x] linear  Other Ops  [x] numel  Linked PR:  CC(Add factory function derivatives)  CC(implement numel and tests for nested tensor)  CC(Register nested tensor linear kernel) ,2022-06-07T18:58:53Z,module: autograd triaged module: nestedtensor,closed,0,1,https://github.com/pytorch/pytorch/issues/79044,Looks like this is completed
yi,Add a test that shows that lazy_ir reuse breaks SizeNodes,I suspect Bin's lazy_ir reuse breaks `SizeNode` as these eventually point to DeviceData leaves and are updated after each mark_step whereas `SizeNode` persist across multiple mark_steps,2022-06-07T17:44:23Z,triaged lazy,open,0,2,https://github.com/pytorch/pytorch/issues/79032,cc:  ,"Okay, I wrote a test : https://github.com/Krovatkin/pytorch/pull/new/krovatkin/reuse_sym"
transformer,Add check for no grad in transformer encoder nestedtensor conversion …,"Cherry picked bug fix for 1.12 release. Summary: Before, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail. Fix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor. Original PR: https://github.com/pytorch/pytorch/pull/78832",2022-06-07T17:30:40Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/79029,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/79029**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 2c95c31d38 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6783296606?check_suite_focus=true) pull / linuxbionicrocm5.1py3.7 / test (default, 2, 2, linux.rocm.gpu) (1/1) **Step:** ""Download build artifacts"" (full log  :repeat: rerun)   20220607T22:04:23.0970066Z [error]No files ...ath: test/**/*.xml. No artifacts will be uploaded.  ``` 20220607T22:04:22.8949148Z   retentiondays: 14 20220607T22:04:22.8949866Z   ifnofilesfound: error 20220607T22:04:22.8950377Z   path: test/**/*.xml 20220607T22:04:22.8950826Z env: 20220607T22:04:22.8951233Z   IN_CI: 1 20220607T22:04:22.8951667Z   IS_GHA: 1 20220607T22:04:22.8952147Z   GIT_DEFAULT_BRANCH: master 20220607T22:04:22.8952888Z   DOCKER_HOST: unix:///run/user/1121/docker.sock 20220607T22:04:22.8953938Z   GPU_FLAG: device=/dev/mem device=/dev/kfd device=/dev/dri/renderD128 device=/dev/dri/renderD129 groupadd video groupadd daemon 20220607T22:04:22.8954813Z [endgroup] 20220607T22:04:23.0970066Z [error]No files were found with the provided path: test/**/*.xml. No artifacts will be uploaded. 20220607T22:04:23.1037148Z [group]Run set x 20220607T22:04:23.1037866Z [36;1mset x[0m 20220607T22:04:23.1038651Z [36;1mpython3 m pip install r requirements.txt[0m 20220607T22:04:23.1039551Z [36;1mpython3 m pip install boto3==1.19.12[0m 20220607T22:04:23.1040595Z [36;1mpython3 m tools.stats.print_test_stats uploadtos3 comparewiths3 test[0m 20220607T22:04:23.1066407Z shell: /bin/bash noprofile norc e o pipefail {0} 20220607T22:04:23.1066704Z env: 20220607T22:04:23.1066909Z   IN_CI: 1 20220607T22:04:23.1067137Z   IS_GHA: 1 20220607T22:04:23.1067391Z   GIT_DEFAULT_BRANCH: master ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","Test failure is numerically different 1/72 tensors and is for convolution, which this diff does not affect (only transformer). Likely a flaky test. "
transformer,Can we have Additive Attention?," 🚀 The feature, motivation and pitch Like MultiheadAttention which was proposed in **`Transformer`** paper similarly, can we have Additive attention  proposed in `**Neural Machine Translation by Jointly Learning to Align and Translate**`.  Alternatives `nn.MultiheadAttention`  Additional context _No response_ ",2022-06-06T18:38:56Z,triaged,open,0,2,https://github.com/pytorch/pytorch/issues/78954, ,"Hey there   apologies for the wait! I don't believe we have immediate plans to implement this, so if we were to do this the eta would look like a couple months out. "
rag,[PyTorch] (reapply) Avoid initializing storage for empty Optionals,"  CC([PyTorch] (reapply) Avoid initializing storage for empty Optionals) We don't need to initialize for the nonconstexpr case ever, or in the constexpr case after C++20. Differential Revision: D36519379",2022-06-06T17:30:03Z,oncall: distributed Merged cla signed ciflow/trunk topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/78947,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78947**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 1cd91cb799 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,reapply of https://github.com/pytorch/pytorch/pull/77858 which was reverted due to ASAN failures; I've piggybacked a patch (see rpc_agent.cpp),suddenly test_fs_sharing is failing. not a believer; rebasing, merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Support indexing of the underlying tensors for nested tensors,Fixes CC(Support indexing of the underlying tensors for nested tensors),2022-06-06T15:49:06Z,Merged cla signed topic: not user facing release notes: nested tensor,closed,0,7,https://github.com/pytorch/pytorch/issues/78934,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78934**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 3ce833bca2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Generally looks great! Thank you for sending this!,"  could you also try to rebase or merge master please? There is a conflict that seems to prevent CI from running, which can hide lint failures and the like.",">   could you also try to rebase or merge master please? There is a conflict that seems to prevent CI from running, which can hide lint failures and the like. just merged master. CI is running now", merge, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
finetuning,[Python 3/Pytorch] Error on Fine-tuning BERT for question answering -> RuntimeError: Overflow when unpacking long," 🐛 Describe the bug While trying to finetune a BERT model on COVIDQA dataset for the question answering task, the following error has occurred.  _RuntimeError: Overflow when unpacking long_ The instructions I am following are on this link. The error seems to occur on the following line: ` return {key: torch.tensor(val[idx],dtype=torch.int64) for key, val in self.encodings.items()}` I am trying to train the model on an ubuntu server, using python 3.6.6 and PyTorch 1.10.0+cu102 (using 1 GPU) Can anyone help?  Versions Collecting environment information... PyTorch version: 1.10.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.10.2 Libc version: glibc2.9 Python version: 3.6.6  (default, Oct  9 2018, 12:34:16)  [GCC 7.3.0] (64bit runtime) Python platform: Linux4.15.0180genericx86_64withdebianbustersid Is CUDA available: True CUDA runtime version: 10.2.89 GPU models and configuration: GPU 0: NVIDIA TITAN RTX GPU 1: NVIDIA TITAN RTX Nvidia driver version: 470.129.06 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.19.5 [pip3] pytorchtransformers==1.2.0 [pip3] torch==1.10.0 [pip3] torchaudio==0.10.0+cu113 [pip3] torchvision==0.11.1+cu113 [conda] numpy                     1.19.5                   pypi_0    pypi [conda] pytorchtransformers      1.2.0                    pypi_0    pypi [conda] torch                     1.3.0                    pypi_0    pypi [conda] torchaudio                0.10.1                   pypi_0    pypi [conda] torchvision               0.11.2                   pypi_0    pypi",2022-06-06T13:40:12Z,,closed,0,2,https://github.com/pytorch/pytorch/issues/78925,"This is probably not a framework problem, I suggest going to https://discuss.pytorch.org/",r
transformer,Add check for no grad in transformer encoder nestedtensor conversion,"Before, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail. Fix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor.",2022-06-03T17:52:03Z,Merged cla signed release notes: nn topic: bug fixes,closed,0,3,https://github.com/pytorch/pytorch/issues/78832,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78832**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 8df8a9f742 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Add methods to torch._C pyi,"Add python bound methods in `Block`, `Node`, `Graph` and `Value` to `torch._C.__init__.pyi` to enable proper type hints in editors.",2022-06-02T20:26:39Z,module: cpp module: typing triaged open source Merged cla signed release notes: jit,closed,0,10,https://github.com/pytorch/pytorch/issues/78757,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78757**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit cfe02c31e1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," given that these are all jit objects, can you take a look at this?", merge g, successfully started a merge job. Check the current status here, merge, successfully started a merge job. Check the current status here,Merge failed due to Refusing to merge as mandatory check(s) linuxdocs / builddocs (cpp) are pending/not yet run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2450881856, merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Getting NotImplementedError when trying to implement E2E support for `prim::is_nested` Op in torch-mlir.," 🐛 Describe the bug Hello, I am trying to add support for the Prim::is_nested Op in torchmlir. I have posted a similar issue in torchmlir: https://github.com/llvm/torchmlir/issues/880 and this PR https://github.com/llvm/torchmlir/pull/881 points to the commit in question. It has the corresponding lowering code and E2E test code. When trying to run the nested test case I get the following error summary. Kindly advice on the best way to debug the error. ```Unexpected outcome summary: ****** Failed tests  1 tests     FAIL  ""PrimIsNestedOpModule_nested""         Compilation error: Traceback (most recent call last):           File ""/home/vidush/nodAI/torchmlir/build/tools/torchmlir/python_packages/torch_mlir/torch_mlir_e2e_test/torchscript/framework.py"", line 282, in compile_and_run_test             golden_trace = generate_golden_trace(test)           File ""/home/vidush/nodAI/torchmlir/build/tools/torchmlir/python_packages/torch_mlir/torch_mlir_e2e_test/torchscript/framework.py"", line 276, in generate_golden_trace             test.program_invoker(tracer, TestUtils())           File ""/home/vidush/nodAI/torchmlir/build/tools/torchmlir/python_packages/torch_mlir/torch_mlir_e2e_test/test_suite/basic.py"", line 1922, in PrimIsNestedOpModule_nested             module.forward(nested_basic)           File ""/home/vidush/nodAI/torchmlir/build/tools/torchmlir/python_packages/torch_mlir/torch_mlir_e2e_test/torchscript/framework.py"", line 255, in __call__             inputs = [clone_torch_script_value(arg) for arg in args]           File ""/home/vidush/nodAI/torchmlir/build/tools/torchmlir/python_packages/torch_mlir/torch_mlir_e2e_test/torchscript/framework.py"", line 255, in              inputs = [clone_torch_script_value(arg) for arg in args]           File ""/home/vidush/nodAI/torchmlir/build/tools/torchmlir/python_packages/torch_mlir/torch_mlir_e2e_test/torchscript/framework.py"", line 60, in clone_torch_script_value             return v.clone()         NotImplementedError: Could not run 'aten::clone' with arguments from the 'NestedTensorCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::clone' is only available for these backends: [Dense, FPGA, ORT, Vulkan, Metal, Meta, Quantized, CustomRNGKeyId, MkldnnCPU, Sparse, SparseCsrCPU, SparseCsrCUDA, NestedTensor, BackendSelect, Python, Fake, Named, Conjugate, Negative, ZeroTensor, FuncTorchDynamicLayerBackMode, ADInplaceOrView, AutogradOther, AutogradFunctionality, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, Autocast, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, Functionalize, DeferredInit, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, TESTING_ONLY_GenericWrapper, TESTING_ONLY_GenericMode, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, CPU, CUDA, HIP, XLA, MPS, IPU, UNKNOWN_TENSOR_TYPE_ID, QuantizedXPU, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseCPU, SparseCUDA, SparseHIP, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseXPU, UNKNOWN_TENSOR_TYPE_ID, SparseVE, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, NestedTensorCUDA, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID].         Undefined: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         CPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         CUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         HIP: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         XLA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         MPS: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         IPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         XPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         HPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         VE: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         Lazy: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         PrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         PrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         PrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         FPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         ORT: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         Vulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         Metal: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         Meta: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1294 [kernel]         QuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         QuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         CustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         MkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:690 [kernel]         SparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1858 [kernel]         SparseCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         SparseHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         SparseXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         SparseVE: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         SparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1507 [kernel]         SparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:29545 [default backend kernel]         BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]         Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:133 [backend fallback]         Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]         Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]         Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]         ZeroTensor: fallthrough registered at ../aten/src/ATen/ZeroTensorFallback.cpp:90 [kernel]         ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]         AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         UNKNOWN_TENSOR_TYPE_ID: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:12167 [autograd kernel]         Tracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:12753 [kernel]         AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:481 [backend fallback]         Autocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:324 [backend fallback]         Batched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1068 [kernel]         VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]         Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:89 [backend fallback]         PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:137 [backend fallback]```  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220523+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Ubuntu 22.04 LTS (x86_64) GCC version: (Ubuntu 9.4.05ubuntu1) 9.4.0 Clang version: 13.0.0 CMake version: version 3.22.4 Libc version: glibc2.35 Python version: 3.9.0 (default, May 19 2022, 12:51:15)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.16.0051600genericx86_64withglibc2.35 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] torch==1.13.0.dev20220523+cpu [pip3] torchvision==0.13.0.dev20220523+cpu [conda] Could not collect ",2022-06-02T20:04:49Z,triaged module: nestedtensor,open,0,3,https://github.com/pytorch/pytorch/issues/78754,"Since ""clone"" is marked as a missing operation let's start with implementing that first, since it's an operation we'll probably want anyway. However, it's unlikely that this will resolve the issue.",999  I just landed support for clone (which will be in the next nightlies). Can you try again once it became available?,yup sounds good. Thanks!
agent,Update distributed/CONTRIBUTING.md to remove ProcessGroupAgent references and add test instructions,Stack from ghstack:  CC(Reenable assert after test update) Reenable assert after test update * * CC(Update distributed/CONTRIBUTING.md to remove ProcessGroupAgent references and add test instructions) Update distributed/CONTRIBUTING.md to remove ProcessGroupAgent references and add test instructions** ProcessGroupAgent was deprecated so these references to the tests should be removed from comments and docs.  ,2022-06-01T16:21:36Z,oncall: distributed Merged cla signed release notes: distributed (rpc) topic: docs,closed,0,3,https://github.com/pytorch/pytorch/issues/78625,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78625**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 1dfc0dd653 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6696616477?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220601T19:46:05.2303512Z The PR is introduc...m to confirm whether this change is wanted or not.  ``` 20220601T19:46:05.2291363Z processing existing schema:  text(__torch__.torch.classes.profiling.SourceRef _0) > (str _0) 20220601T19:46:05.2292627Z processing existing schema:  count(__torch__.torch.classes.profiling.InstructionStats _0) > (int _0) 20220601T19:46:05.2293789Z processing existing schema:  duration_ns(__torch__.torch.classes.profiling.InstructionStats _0) > (int _0) 20220601T19:46:05.2294979Z processing existing schema:  source(__torch__.torch.classes.profiling.SourceStats _0) > (__torch__.torch.classes.profiling.SourceRef _0) 20220601T19:46:05.2296680Z processing existing schema:  line_map(__torch__.torch.classes.profiling.SourceStats _0) > (Dict(int, __torch__.torch.classes.profiling.InstructionStats) _0) 20220601T19:46:05.2297489Z processing existing schema:  __init__(__torch__.torch.classes.profiling._ScriptProfile _0) > (NoneType _0) 20220601T19:46:05.2298771Z processing existing schema:  enable(__torch__.torch.classes.profiling._ScriptProfile _0) > (NoneType _0) 20220601T19:46:05.2299897Z processing existing schema:  disable(__torch__.torch.classes.profiling._ScriptProfile _0) > (NoneType _0) 20220601T19:46:05.2301789Z processing existing schema:  _dump_stats(__torch__.torch.classes.profiling._ScriptProfile _0) > (__torch__.torch.classes.profiling.SourceStats[] _0) 20220601T19:46:05.2303075Z processing existing schema:  __init__(__torch__.torch.classes.dist_rpc.WorkerInfo _0, str _1, int _2) > (NoneType _0) 20220601T19:46:05.2303512Z The PR is introducing backward incompatible changes to the operator library. Please contact PyTorch team to confirm whether this change is wanted or not.  20220601T19:46:05.2303632Z  20220601T19:46:05.2303743Z Broken ops: [ 20220601T19:46:05.2304012Z 	prims::minium_value(int dtype) > (Scalar) 20220601T19:46:05.2304252Z 	prims::maximum_value(int dtype) > (Scalar) 20220601T19:46:05.2304459Z 	prims::item(Tensor a) > (Scalar) 20220601T19:46:05.2304722Z 	prims::zeta(Tensor self, Tensor other) > (Tensor) 20220601T19:46:05.2304965Z 	prims::fmod(Tensor self, Tensor other) > (Tensor) 20220601T19:46:05.2305222Z 	prims::fmin(Tensor self, Tensor other) > (Tensor) 20220601T19:46:05.2305503Z 	prims::fmax(Tensor self, Tensor other) > (Tensor) 20220601T19:46:05.2305752Z 	prims::signbit(Tensor self) > (Tensor) ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge this on green,"Hey Huang. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Trying to get some OpInfo based test for MPS,"This does a few changes:  new `TORCH_CHECK` to make sure the mps backend returns nice error instead of hard crash  make print behavior smoother by converting to cpu   Add an OpInfobased test to make sure mps output matches the cpu one There are several issues with the opinfo test:  One test just hangs forever  Some function still hard crash and probably need more TORCH_CHECK  Many functions are failing in a flaky way: they sometimes pass, sometimes fail (usually because the output is all 0s) Next steps:  Fix all the things in the `BLOCKLIST`  Make `ALLOWLIST_OP` not depend on dtype anymore (so that we can just enable ops directly via a boolean flag). This is not done now because many op fail int/bool dtypes.  Move `ALLOWLIST_OP` to be a boolean flag directly in `common_method_invocations.py` like `supports_mps=True`.  Make `supports_mps=True` the default for all ops  Extend this to check that gradients match What needs to happen before merging:  Ensure all the `TORCH_CHECK()` are not overly restrictive  Ensure the newly added test is not flaky (add more stuff to the `BLOCKLIST`)",2022-05-30T22:30:44Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/78504,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78504**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit ea534eccd4 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6717530537?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7 / test (default, 1, 4, linux.4xlarge.nvidia.gpu) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220603T00:47:15.3661284Z RuntimeError: test_ops failed! Received signal: SIGIOT  ``` 20220603T00:47:13.1448468Z   test_backward_nn_functional_adaptive_max_pool2d_cuda_float32 (__main__.TestCompositeComplianceCUDA) ... ok (0.052s) 20220603T00:47:13.1887757Z   test_backward_nn_functional_adaptive_max_pool3d_cuda_float32 (__main__.TestCompositeComplianceCUDA) ... ok (0.044s) 20220603T00:47:13.2266398Z   test_backward_nn_functional_avg_pool1d_cuda_float32 (__main__.TestCompositeComplianceCUDA) ... ok (0.038s) 20220603T00:47:13.2450199Z   test_backward_nn_functional_avg_pool2d_cuda_float32 (__main__.TestCompositeComplianceCUDA) ... ok (0.018s) 20220603T00:47:13.2677302Z   test_backward_nn_functional_avg_pool3d_cuda_float32 (__main__.TestCompositeComplianceCUDA) ... ok (0.023s) 20220603T00:47:15.3654319Z   test_backward_nn_functional_batch_norm_cuda_float32 (__main__.TestCompositeComplianceCUDA) ... Traceback (most recent call last): 20220603T00:47:15.3654834Z   File ""test/run_test.py"", line 1077, in  20220603T00:47:15.3657510Z     main() 20220603T00:47:15.3658085Z   File ""test/run_test.py"", line 1055, in main 20220603T00:47:15.3660897Z     raise RuntimeError(err_message) 20220603T00:47:15.3661284Z RuntimeError: test_ops failed! Received signal: SIGIOT 20220603T00:47:16.4570669Z + cleanup 20220603T00:47:16.4570960Z + retcode=1 20220603T00:47:16.4571202Z + set +x 20220603T00:47:16.4619496Z [error]Process completed with exit code 1. 20220603T00:47:16.4666153Z [group]Run pytorch/pytorch/.github/actions/getworkflowjobid 20220603T00:47:16.4666506Z with: 20220603T00:47:16.4667045Z   githubtoken: *** 20220603T00:47:16.4667454Z env: 20220603T00:47:16.4667677Z   IN_CI: 1 20220603T00:47:16.4667889Z   IS_GHA: 1 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",Closing as this landed in another PR.
transformer,torch.fx: symbolic_trace: ones() received an invalid combination of arguments," 🐛 Describe the bug Run the following standalone python file. ``` import torch from torch.fx import symbolic_trace from transformers import AutoModelForSequenceClassification class OnlyLogitsHuggingFaceModel(torch.nn.Module):     """"""Wrapper that returns only the logits from a HuggingFace model.""""""     def __init__(self):         super().__init__()         self.model = AutoModelForSequenceClassification.from_pretrained(             ""ProsusAI/finbert"",   The pretrained model name.             num_labels=3,             output_attentions=False,             output_hidden_states=False,             torchscript=True,         )         self.model.eval()     def forward(self, input):          Return only the logits.         return self.model(input)[0] traced = symbolic_trace(OnlyLogitsHuggingFaceModel()) ``` I get the error: ``` TypeError: ones() received an invalid combination of arguments  got (tuple, device=Attribute), but expected one of:  * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)  * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad) ``` Full error log: https://gist.github.com/silvasean/a11f5219e8d931014ae4046a1fafcef7  Versions PyTorch version: 1.13.0.dev20220530+cpu Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: Debian GNU/Linux rodete (x86_64) GCC version: (Debian 11.2.019) 11.2.0 Clang version: 13.0.13+build2 CMake version: version 3.22.4 Libc version: glibc2.33 Python version: 3.9.12 (main, Mar 24 2022, 13:02:21)  [GCC 11.2.0] (64bit runtime) Python platform: Linux5.17.61rodete1amd64x86_64withglibc2.33 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] ireetorch==0.0.1 [pip3] numpy==1.23.0rc1 [pip3] torch==1.13.0.dev20220530+cpu [pip3] torchmlir==20220530.482 [pip3] torchvision==0.14.0.dev20220530+cpu [conda] Could not collect ",2022-05-30T12:59:44Z,triaged module: fx,open,0,2,https://github.com/pytorch/pytorch/issues/78487,"This also affects any model using deformable convolutions, e.g., trace_dcn_bug.py gives ```console brettworkhorse:~/repos/Autosensor/NN/tmp$ python3 trace_dcn_bug.py  torch.Size([1, 3, 10, 10]) Traceback (most recent call last):   File ""/home/brett/repos/Autosensor/NN/tmp/trace_dcn_bug.py"", line 53, in      symbolic_trace(m)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/fx/_symbolic_trace.py"", line 857, in symbolic_trace     graph = tracer.trace(root, concrete_args)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/fx/_symbolic_trace.py"", line 566, in trace     self.create_node('output', 'output', (self.create_arg(fn(*args)),), {},   File ""/home/brett/repos/Autosensor/NN/tmp/trace_dcn_bug.py"", line 47, in forward     return self.deform_conv2d(x)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/fx/_symbolic_trace.py"", line 556, in module_call_wrapper     return self.call_module(mod, forward, args, kwargs)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/fx/_symbolic_trace.py"", line 372, in call_module     return forward(*args, **kwargs)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/fx/_symbolic_trace.py"", line 552, in forward     return _orig_module_call(mod, *args, **kwargs)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1110, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/brett/repos/Autosensor/NN/tmp/trace_dcn_bug.py"", line 37, in forward     x = self.conv2d(x, offset)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/fx/_symbolic_trace.py"", line 556, in module_call_wrapper     return self.call_module(mod, forward, args, kwargs)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/fx/_symbolic_trace.py"", line 372, in call_module     return forward(*args, **kwargs)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/fx/_symbolic_trace.py"", line 552, in forward     return _orig_module_call(mod, *args, **kwargs)   File ""/home/brett/.local/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1110, in _call_impl     return forward_call(*input, **kwargs)   File ""/home/brett/.local/lib/python3.10/sitepackages/torchvision/ops/deform_conv.py"", line 170, in forward     return deform_conv2d(   File ""/home/brett/.local/lib/python3.10/sitepackages/torchvision/ops/deform_conv.py"", line 71, in deform_conv2d     mask = torch.zeros((input.shape[0], 0), device=input.device, dtype=input.dtype) TypeError: zeros() received an invalid combination of arguments  got (tuple, dtype=Attribute, device=Attribute), but expected one of:  * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)  * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad) ``` I suspect this is related to CC(torch.fx.symbolic_trace fails on torch.arange with inputdependent size) with a potential solution provided by ModelTC/MQBench CC(Data loading processes should disable OpenMP).",May I know whether there is any applicable solution to this issue? I'm also facing similar issue where I deserialize the torch.fx GraphModule which and dumped using pickle.
transformer,[onnx] RuntimeError: Attribute 'axes' is expected to have field 'ints'," 🐛 Describe the bug RuntimeError: Attribute 'axes' is expected to have field 'ints' When I want to export the transformer model containing the proj_adaptive_softmax layer to the onnx format, I get an error: RuntimeError: Attribute 'axes' is expected to have field 'ints', ==> Context: Bad node spec: input: ""567"" output: "" 584"" name: ""Unsqueeze_445"" op_type: ""Unsqueeze"" attribute { name: ""axes"" type: INTS } It means that axes type must be int. I found the corresponding line of code: ```` nll.index_copy_(0, indices_i, logprob_i) ```` It means that indices_i must be a tensor of type int, but the problem is that indices_i is already a tensor of int64. When I replace it with a constant tensor equal to indices_i, no error is reported ```` nll.index_copy_(0, indices_i, logprob_i) a = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21 , 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]) nll.index_copy_(0, a, logprob_i) print(indices_i == a) [true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true, true,true,true,true,true,true,true,true] ```` Why is this?  Versions Pytorch Version == 1.7.1+cu101 OS == Linux onnx == 1.10.2 CUDA Version == 10.1",2022-05-30T03:09:12Z,module: onnx triaged onnx-needs-info,closed,0,5,https://github.com/pytorch/pytorch/issues/78481,"hi , could you please try a newer version of PyTorch (current release is 1.11)? If error persists, please create a complete end to end repro for us to investigate. Thanks."," I have updated the version of torch but still the same error occurs Pytorch Version == 1.11.0+cu102 OS == Linux onnx == 1.10.2 CUDA Version == 10.2 Below is my main code：  model.py ``` from __future__ import absolute_import from __future__ import division from __future__ import print_function import sys import math import torch import torch.nn as nn import torch.nn.functional as F sys.path.append('utils') from proj_adaptive_softmax import ProjectedAdaptiveLogSoftmax class PositionalEncoding(nn.Module):     r""""""Inject some information about the relative or absolute position of the         tokens in the sequence. The positional encodings have the same dimension         as the embeddings, so that the two can be summed. Here, we use sine and         cosine functions of different frequencies.     .. math::         \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))         \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))         \\text{where pos is the word position and i is the embed idx)     Args:         d_model: the embed dim (required).         dropout: the dropout value (default=0.1).         max_len: the max. length of the incoming sequence (default=5000).     Examples:         >>> pos_encoder = PositionalEncoding(d_model)     """"""     def __init__(self, d_model, dropout=0.1, max_len=5000):         super(PositionalEncoding, self).__init__()         self.dropout = nn.Dropout(p=dropout)         pe = torch.zeros(max_len, d_model)         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model))         pe[:, 0::2] = torch.sin(position * div_term)         pe[:, 1::2] = torch.cos(position * div_term)         pe = pe.unsqueeze(0).transpose(0, 1)         self.register_buffer('pe', pe)     def forward(self, x):         r""""""Inputs of forward function         Args:             x: the sequence fed to the positional encoder model (required).         Shape:             x: [sequence length, batch size, embed dim]             output: [sequence length, batch size, embed dim]         Examples:             >>> output = pos_encoder(x)         """"""         x = x + self.pe[:x.size(0), :]         return self.dropout(x) class AdaptiveEmbedding(nn.Module):     def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1):         super(AdaptiveEmbedding, self).__init__()         self.n_token = n_token         self.d_embed = d_embed         self.cutoffs = cutoffs + [n_token]         self.div_val = div_val         self.d_proj = d_proj         self.emb_scale = d_proj ** 0.5         self.cutoff_ends = [0] + self.cutoffs         self.emb_layers = nn.ModuleList()         self.emb_projs = nn.ParameterList()         if div_val == 1:             self.emb_layers.append(                 nn.Embedding(n_token, d_embed, sparse=sample_softmax>0)             )             if d_proj != d_embed:                 self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_embed)))         else:             for i in range(len(self.cutoffs)):                 l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]                 d_emb_i = d_embed // (div_val ** i)                 self.emb_layers.append(nn.Embedding(r_idxl_idx, d_emb_i))                 self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_emb_i)))     def forward(self, inp):         if self.div_val == 1:             embed = self.emb_layers0             if self.d_proj != self.d_embed:                 embed  = F.linear(embed, self.emb_projs[0])         else:             param = next(self.parameters())             inp_flat = inp.view(1)             emb_flat = torch.zeros([inp_flat.size(0), self.d_proj],                 dtype=param.dtype, device=param.device)             for i in range(len(self.cutoffs)):                 l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]                 mask_i = (inp_flat >= l_idx) & (inp_flat  0:             self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))             self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))         self.out_layers = nn.ModuleList()         self.out_projs = nn.ParameterList()         if div_val == 1:             for i in range(len(self.cutoffs)):                 if d_proj != d_embed:                     self.out_projs.append(                         nn.Parameter(torch.Tensor(d_proj, d_embed))                     )                 else:                     self.out_projs.append(None)             self.out_layers.append(nn.Linear(d_embed, n_token))         else:             for i in range(len(self.cutoffs)):                 l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]                 d_emb_i = d_embed // (div_val ** i)                 self.out_projs.append(                     nn.Parameter(torch.Tensor(d_proj, d_emb_i))                 )                 self.out_layers.append(nn.Linear(d_emb_i, r_idxl_idx))         self.keep_order = keep_order     def _compute_logit(self, hidden, weight, bias, proj):         if proj is None:             logit = F.linear(hidden, weight, bias=bias)         else:              if CUDA_MAJOR bv', (hidden, proj, weight.t()))                  if bias is not None:                      logit = logit + bias         return logit     def forward(self, hidden, target, keep_order=False):         '''             hidden :: [len*bsz x d_proj]             target :: [len*bsz]         '''         if hidden.size(0) != target.size(0):             raise RuntimeError('Input and target should have the same size '                                'in the batch dimension.')         if self.n_clusters == 0:             logit = self._compute_logit(hidden, self.out_layers[0].weight,                                         self.out_layers[0].bias, self.out_projs[0])             nll = F.log_softmax(logit, dim=1) \\                     .gather(1, target.unsqueeze(1)).squeeze(1)         else:              construct weights and biases             weights, biases = [], []             for i in range(len(self.cutoffs)):                 if self.div_val == 1:                     l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]                     weight_i = self.out_layers[0].weight[l_idx:r_idx]                     bias_i = self.out_layers[0].bias[l_idx:r_idx]                 else:                     weight_i = self.out_layers[i].weight                     bias_i = self.out_layers[i].bias                 if i == 0:                     weight_i = torch.cat(                         [weight_i, self.cluster_weight], dim=0)                     bias_i = torch.cat(                         [bias_i, self.cluster_bias], dim=0)                 weights.append(weight_i)                 biases.append(bias_i)             head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]             head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)             head_logprob = F.log_softmax(head_logit, dim=1)             nll = torch.zeros_like(target,                     dtype=hidden.dtype, device=hidden.device)             offset = 0             cutoff_values = [0] + self.cutoffs             for i in range(len(cutoff_values)  1):                 l_idx, r_idx = cutoff_values[i], cutoff_values[i + 1]                 mask_i = (target >= l_idx) & (target >>>>>>the model is good"")      Check onnx output     print("">>>>>>>check output"")     ort_session = onnxruntime.InferenceSession(check_path)     def to_numpy(tensor):         return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()      Compute ONNX Runtime output prediction     ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(data),                   ort_session.get_inputs()[1].name: to_numpy(target)}     ort_outs = ort_session.run(None, ort_inputs)      Compare ONNX Runtime and PyTorch results     np.testing.assert_allclose(to_numpy(y), ort_outs[0], rtol=1e03, atol=1e05)     print("">>>>>>>The model has been tested with ONNXRuntime, and the result looks good!"")     return True torch_model.eval() tmp_data = [i for i in range(1)] tmp_data[0]=[0, 77, 7, 631, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] tmp_target = [i for i in range(1)] tmp_target[0]=[77, 7, 631, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] data = torch.LongTensor(tmp_data) target = torch.LongTensor(tmp_target) data = data.t().contiguous() target = target.t().contiguous().view(1) y = torch_model(data, target) input_names = ['data', 'target'] dynamic_axes = {'data': {0: 'seq_len', 1: 'batch_size'},                 'target': {0: 'full_size'},                 'loss': {0: 'full_size'}} torch.onnx.export(torch_model,                model being run                   (data, target),                          model input (or a tuple for multiple inputs)                   save_path,    where to save the model (can be a file or filelike object)                   verbose=False,                   export_params=True,         store the trained parameter weights inside the model file                   opset_version=12,           the ONNX version to export the model to                   do_constant_folding=True,   whether to execute constant folding for optimization                   input_names=input_names,    the model's input names                   output_names=['loss'],   the model's output names                   dynamic_axes=dynamic_axes) print("">>>>>>>model export is done."") print("">>>>>>>check onnx model"") check_model(save_path) ```","Hi  We are happy to help  please help us by providing a minimal script that can reproduce what you have seen, using the latest pytorch nightly build. Ideally it should be a single script so that we can run and diagnose. ","Single repro file by merging the above ```python from __future__ import absolute_import from __future__ import division from __future__ import print_function import io import os import argparse import numpy as np import onnx import onnxruntime import torch.onnx from torch import nn  From proj_adaptive_softmax.py from collections import defaultdict import numpy as np import torch import torch.nn as nn import torch.nn.functional as F CUDA_MAJOR = int(torch.version.cuda.split('.')[0]) CUDA_MINOR = int(torch.version.cuda.split('.')[1]) class ProjectedAdaptiveLogSoftmax(nn.Module):     def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1,                  keep_order=False):         super(ProjectedAdaptiveLogSoftmax, self).__init__()         self.n_token = n_token         self.d_embed = d_embed         self.d_proj = d_proj         self.cutoffs = cutoffs + [n_token]         self.cutoff_ends = [0] + self.cutoffs         self.div_val = div_val         self.shortlist_size = self.cutoffs[0]         self.n_clusters = len(self.cutoffs)  1         self.head_size = self.shortlist_size + self.n_clusters         if self.n_clusters > 0:             self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))             self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))         self.out_layers = nn.ModuleList()         self.out_projs = nn.ParameterList()         if div_val == 1:             for i in range(len(self.cutoffs)):                 if d_proj != d_embed:                     self.out_projs.append(                         nn.Parameter(torch.Tensor(d_proj, d_embed))                     )                 else:                     self.out_projs.append(None)             self.out_layers.append(nn.Linear(d_embed, n_token))         else:             for i in range(len(self.cutoffs)):                 l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]                 d_emb_i = d_embed // (div_val ** i)                 self.out_projs.append(                     nn.Parameter(torch.Tensor(d_proj, d_emb_i))                 )                 self.out_layers.append(nn.Linear(d_emb_i, r_idxl_idx))         self.keep_order = keep_order     def _compute_logit(self, hidden, weight, bias, proj):         if proj is None:             logit = F.linear(hidden, weight, bias=bias)         else:              if CUDA_MAJOR bv', (hidden, proj, weight.t()))                  if bias is not None:                      logit = logit + bias         return logit     def forward(self, hidden, target, keep_order=False):         '''             hidden :: [len*bsz x d_proj]             target :: [len*bsz]         '''         if hidden.size(0) != target.size(0):             raise RuntimeError('Input and target should have the same size '                                'in the batch dimension.')         if self.n_clusters == 0:             logit = self._compute_logit(hidden, self.out_layers[0].weight,                                         self.out_layers[0].bias, self.out_projs[0])             nll = F.log_softmax(logit, dim=1) \\                     .gather(1, target.unsqueeze(1)).squeeze(1)         else:              construct weights and biases             weights, biases = [], []             for i in range(len(self.cutoffs)):                 if self.div_val == 1:                     l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]                     weight_i = self.out_layers[0].weight[l_idx:r_idx]                     bias_i = self.out_layers[0].bias[l_idx:r_idx]                 else:                     weight_i = self.out_layers[i].weight                     bias_i = self.out_layers[i].bias                 if i == 0:                     weight_i = torch.cat(                         [weight_i, self.cluster_weight], dim=0)                     bias_i = torch.cat(                         [bias_i, self.cluster_bias], dim=0)                 weights.append(weight_i)                 biases.append(bias_i)             head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]             head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)             head_logprob = F.log_softmax(head_logit, dim=1)             nll = torch.zeros_like(target,                     dtype=hidden.dtype, device=hidden.device)             offset = 0             cutoff_values = [0] + self.cutoffs             for i in range(len(cutoff_values)  1):                 l_idx, r_idx = cutoff_values[i], cutoff_values[i + 1]                 mask_i = (target >= l_idx) & (target >> pos_encoder = PositionalEncoding(d_model)     """"""     def __init__(self, d_model, dropout=0.1, max_len=5000):         super(PositionalEncoding, self).__init__()         self.dropout = nn.Dropout(p=dropout)         pe = torch.zeros(max_len, d_model)         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model))         pe[:, 0::2] = torch.sin(position * div_term)         pe[:, 1::2] = torch.cos(position * div_term)         pe = pe.unsqueeze(0).transpose(0, 1)         self.register_buffer('pe', pe)     def forward(self, x):         r""""""Inputs of forward function         Args:             x: the sequence fed to the positional encoder model (required).         Shape:             x: [sequence length, batch size, embed dim]             output: [sequence length, batch size, embed dim]         Examples:             >>> output = pos_encoder(x)         """"""         x = x + self.pe[:x.size(0), :]         return self.dropout(x) class AdaptiveEmbedding(nn.Module):     def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1):         super(AdaptiveEmbedding, self).__init__()         self.n_token = n_token         self.d_embed = d_embed         self.cutoffs = cutoffs + [n_token]         self.div_val = div_val         self.d_proj = d_proj         self.emb_scale = d_proj ** 0.5         self.cutoff_ends = [0] + self.cutoffs         self.emb_layers = nn.ModuleList()         self.emb_projs = nn.ParameterList()         if div_val == 1:             self.emb_layers.append(                 nn.Embedding(n_token, d_embed, sparse=sample_softmax>0)             )             if d_proj != d_embed:                 self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_embed)))         else:             for i in range(len(self.cutoffs)):                 l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]                 d_emb_i = d_embed // (div_val ** i)                 self.emb_layers.append(nn.Embedding(r_idxl_idx, d_emb_i))                 self.emb_projs.append(nn.Parameter(torch.Tensor(d_proj, d_emb_i)))     def forward(self, inp):         if self.div_val == 1:             embed = self.emb_layers0             if self.d_proj != self.d_embed:                 embed  = F.linear(embed, self.emb_projs[0])         else:             param = next(self.parameters())             inp_flat = inp.view(1)             emb_flat = torch.zeros([inp_flat.size(0), self.d_proj],                 dtype=param.dtype, device=param.device)             for i in range(len(self.cutoffs)):                 l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]                 mask_i = (inp_flat >= l_idx) & (inp_flat >>>>>>the model is good"")      Check onnx output     print("">>>>>>>check output"")     ort_session = onnxruntime.InferenceSession(check_path)     def to_numpy(tensor):         return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()      Compute ONNX Runtime output prediction     ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(data),                   ort_session.get_inputs()[1].name: to_numpy(target)}     ort_outs = ort_session.run(None, ort_inputs)      Compare ONNX Runtime and PyTorch results     np.testing.assert_allclose(to_numpy(y), ort_outs[0], rtol=1e03, atol=1e05)     print("">>>>>>>The model has been tested with ONNXRuntime, and the result looks good!"")     return True torch_model.eval() tmp_data = [i for i in range(1)] tmp_data[0]=[0, 77, 7, 631, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] tmp_target = [i for i in range(1)] tmp_target[0]=[77, 7, 631, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] data = torch.LongTensor(tmp_data) target = torch.LongTensor(tmp_target) data = data.t().contiguous() target = target.t().contiguous().view(1) y = torch_model(data, target) input_names = ['data', 'target'] dynamic_axes = {'data': {0: 'seq_len', 1: 'batch_size'},                 'target': {0: 'full_size'},                 'loss': {0: 'full_size'}} torch.onnx.export(torch_model,                model being run                   (data, target),                          model input (or a tuple for multiple inputs)                   save_path,    where to save the model (can be a file or filelike object)                   verbose=False,                   export_params=True,         store the trained parameter weights inside the model file                   opset_version=12,           the ONNX version to export the model to                   do_constant_folding=True,   whether to execute constant folding for optimization                   input_names=input_names,    the model's input names                   output_names=['loss'],   the model's output names                   dynamic_axes=dynamic_axes) print("">>>>>>>model export is done."") print("">>>>>>>check onnx model"") check_model(save_path) ``` Now observing error `torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::_scaled_dot_product_attention' to ONNX opset version 12 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues.` on pytorch '2.0.0a0+git1997753'.","Closing this as although `scaled_dot_product_attention` op has been added to torch.onnx.export, aten::unflatten hasn't and we don't plan adding new operators to torch.onnx.export Please try the new ONNX exporter and reopen this issue if it also doesn't work for you: quick torch.onnx.dynamo_export API tutorial"
transformer,13% performance regression in MPS since d63db5234," 🐛 Describe the bug I have been testing PyTorch MPS support in spaCy over the last week or so. Performance has been great, though I have noted a somewhat large performance regression since yesterday. With bisecting I found that it occured in d63db52349ae3cffd6f762c9027e7363a6271d27. It's kind of hard to provide a minimal reproducer, but we are basically using the BERT model from Huggingface transformers with the following change: ``` diff git a/src/transformers/models/bert/modeling_bert.py b/src/transformers/models/bert/modeling_bert.py index 9da6258e9..1b7812dcf 100755  a/src/transformers/models/bert/modeling_bert.py +++ b/src/transformers/models/bert/modeling_bert.py @@ 324,7 +324,7 @@ class BertSelfAttention(nn.Module):              past_key_value = (key_layer, value_layer)           Take the dot product between ""query"" and ""key"" to get the raw attention scores.         attention_scores = torch.matmul(query_layer, key_layer.transpose(1, 2)) +        attention_scores = torch.matmul(query_layer, key_layer.contiguous().transpose(1, 2).contiguous())          if self.position_embedding_type == ""relative_key"" or self.position_embedding_type == ""relative_key_query"":              seq_length = hidden_states.size()[1] ``` Without this change, the attention scores are incorrect ( I still have to look a bit deeper to understand what is causing this particular problem). But with the change above, we can reproduce CPU accuracies. I have been testing with out German transformer model, which is a finetuned BERT model. * Before d63db52349ae3cffd6f762c9027e7363a6271d27: ~7700 words per second. * After d63db52349ae3cffd6f762c9027e7363a6271d27: ~6700 words per second.  Versions ``` Collecting environment information... PyTorch version: 1.13.0a0+gitd63db52 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.23.2 Libc version: N/A Python version: 3.10.4 (main, May 26 2022, 08:19:39) [Clang 13.1.6 (clang1316.0.21.2.5)] (64bit runtime) Python platform: macOS12.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.950 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.4 [pip3] torch==1.13.0a0+gitd63db52 [conda] Could not collect ```",2022-05-29T07:31:54Z,triaged module: mps,closed,0,4,https://github.com/pytorch/pytorch/issues/78472,"Inserting calls to `contiguous()` around transpose/view is likely to have performance impact. I guess it would be better to highlight wrong accuracy in your issue description; now it seems like a performance problem at the first glance, which it is not IMHO.  The workaround would surely be helpful for debugging though."," I don’t agree. Even though needing `contiguous` is an issue that should be addressed, it is still a performance regression. Doing the same sequence of operations is slower than before.","> I don’t agree. Even though needing `contiguous` is an issue that should be addressed, it is still a performance regression. Doing the same sequence of operations is slower than before. , could you please try the following build from this page https://hud.pytorch.org/pr/78496 and let me know if you are still seeing the issue? Latest nightly build also contains them (1.13.0.dev20220531). If you are manually building PyTorch, sha 017b0ae9431ae3780a4eb9bf6d8865dfcd02cd92 contains these changes (https://github.com/pytorch/pytorch/pull/78496). ","I think the performance regression is largely gone (it's about 7600 words per second now). Furthermore, I only need one `contiguous` call now. I opened a separate issue about that yesterday ( CC(MPS matrix multiplication fails on a permuted tensor without `contiguous`))."
rag,Some helper code for determining missing meta coverage for XLA ops,  CC(Make Meta into a backend component)  CC(Register std_mean ref as a decomposition)  CC(Don't check for linalg errors on meta tensors)  CC(Reenable TestMeta native_batch_norm and native_layer_norm)  CC(Reenable TestMeta slice)  CC(Some helper code for determining missing meta coverage for XLA ops)  CC(Reenable TestMeta testing for isnan)  CC(Reenable tensor_split meta testing)  CC(prod ref)  CC(Register PrimTorch sum as a decomposition.)  CC(Unconditionally transform dtype arguments to double for upcast) When I ran it I got this: ``` $ PYTORCH_COMPARE_XLA=/scratch/ezyang/xla/xla_native_functions.yaml python test/test_meta.py aten.inverse.default   SKIP aten.logdet.default aten._local_scalar_dense.default aten.cholesky.default aten.diag.default aten.empty.memory_format   SKIP aten.index.Tensor aten.kthvalue.default aten.log_sigmoid_forward.default aten.masked_select.default aten.max_pool3d_with_indices.default aten.max_unpool2d.default aten.max_unpool3d.default aten.native_batch_norm.default   SKIP aten.nll_loss2d_forward.default aten.nonzero.default aten.prelu.default aten.relu.default aten.roll.default aten.rrelu_with_noise.default aten.slice.Tensor   SKIP aten.std_mean.correction aten.symeig.default aten.take.default aten.trace.default aten.native_layer_norm.default   SKIP aten.native_group_norm.default ``` Signedoffby: Edward Z. Yang ,2022-05-29T02:03:53Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/78464,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78464**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 769bd464d1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
transformer,[cuDNN][TF32] Threshold adjustments for TF32 on `>=sm80`,CC    Change to transformer multilayer test can potentially be swapped in favor of an rtol change? (see also: CC(Sets rtol>0 for some tests that use tf32)).,2022-05-27T22:15:02Z,module: cudnn triaged open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/78437,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78437**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 1 New Failures As of commit e543f0199c (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6632612554?check_suite_focus=true) pull / linuxfocalpy3.7gcc7 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220527T22:29:54.0717919Z The PR is introduc...m to confirm whether this change is wanted or not.  ``` 20220527T22:29:54.0704211Z processing existing schema:  text(__torch__.torch.classes.profiling.SourceRef _0) > str _0 20220527T22:29:54.0705659Z processing existing schema:  count(__torch__.torch.classes.profiling.InstructionStats _0) > int _0 20220527T22:29:54.0707044Z processing existing schema:  duration_ns(__torch__.torch.classes.profiling.InstructionStats _0) > int _0 20220527T22:29:54.0708010Z processing existing schema:  source(__torch__.torch.classes.profiling.SourceStats _0) > __torch__.torch.classes.profiling.SourceRef _0 20220527T22:29:54.0710172Z processing existing schema:  line_map(__torch__.torch.classes.profiling.SourceStats _0) > Dict(int, __torch__.torch.classes.profiling.InstructionStats) _0 20220527T22:29:54.0711021Z processing existing schema:  __init__(__torch__.torch.classes.profiling._ScriptProfile _0) > NoneType _0 20220527T22:29:54.0712367Z processing existing schema:  enable(__torch__.torch.classes.profiling._ScriptProfile _0) > NoneType _0 20220527T22:29:54.0713918Z processing existing schema:  disable(__torch__.torch.classes.profiling._ScriptProfile _0) > NoneType _0 20220527T22:29:54.0715718Z processing existing schema:  _dump_stats(__torch__.torch.classes.profiling._ScriptProfile _0) > __torch__.torch.classes.profiling.SourceStats[] _0 20220527T22:29:54.0717437Z processing existing schema:  __init__(__torch__.torch.classes.dist_rpc.WorkerInfo _0, str _1, int _2) > NoneType _0 20220527T22:29:54.0717919Z The PR is introducing backward incompatible changes to the operator library. Please contact PyTorch team to confirm whether this change is wanted or not.  20220527T22:29:54.0717947Z  20220527T22:29:54.0718065Z Broken ops: [ 20220527T22:29:54.0718245Z 	prims::isfinite(Tensor self) > Tensor 20220527T22:29:54.0718311Z ] 20220527T22:29:54.1986890Z + cleanup 20220527T22:29:54.1987043Z + retcode=1 20220527T22:29:54.1987149Z + set +x 20220527T22:29:54.2024581Z [error]Process completed with exit code 1. 20220527T22:29:54.2062589Z [group]Run pytorch/pytorch/.github/actions/getworkflowjobid 20220527T22:29:54.2062851Z with: ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,fx.Tracer with param_shapes_constant=True not working for RobertaForMaskedLM," 🐛 Describe the bug param_shapes_constant=True can't bypass issue mentioned by    CC([FX] Limited control flow tracing for parameter shapes) ```python from transformers import RobertaForMaskedLM from transformers import RobertaConfig import torch.fx as fx import inspect config = RobertaConfig(  vocab_size=52_000,  max_position_embeddings=514,  num_attention_heads=12,  num_hidden_layers=12,  type_vocab_size=1, ) model = RobertaForMaskedLM(config) input_names = [""input_ids"", ""attention_mask"", ""decoder_input_ids"", ""decoder_attention_mask"",'token_type_ids'] sig = inspect.signature(model.forward) concrete_args = {p.name: None for p in sig.parameters.values() if p.name not in input_names} tracer = fx.Tracer(param_shapes_constant=True) graph = tracer.trace(model, concrete_args) ``` Error Message: ```  TraceError                                Traceback (most recent call last) Input In [20], in ()       1 tracer = fx.Tracer(param_shapes_constant=True) > 2 graph = tracer.trace(model, concrete_args) File ~/anaconda3/lib/python3.9/sitepackages/torch/fx/_symbolic_trace.py:615, in Tracer.trace(self, root, concrete_args)     613         for module in self._autowrap_search:     614             _autowrap_check(patcher, module.__dict__, self._autowrap_function_ids) > 615         self.create_node('output', 'output', (self.create_arg(fn(*args)),), {},     616                          type_expr=fn.__annotations__.get('return', None))     618 self.submodule_paths = None     620 return self.graph File ~/anaconda3/lib/python3.9/sitepackages/transformers/models/roberta/modeling_roberta.py:1098, in RobertaForMaskedLM.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)    1088 r""""""    1089 labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):    1090     Labels for computing the masked language modeling loss. Indices should be in `[100, 0, ...,    (...)    1094     Used to hide legacy arguments that have been deprecated.    1095 """"""    1096 return_dict = return_dict if return_dict is not None else self.config.use_return_dict > 1098 outputs = self.roberta(    1099     input_ids,    1100     attention_mask=attention_mask,    1101     token_type_ids=token_type_ids,    1102     position_ids=position_ids,    1103     head_mask=head_mask,    1104     inputs_embeds=inputs_embeds,    1105     encoder_hidden_states=encoder_hidden_states,    1106     encoder_attention_mask=encoder_attention_mask,    1107     output_attentions=output_attentions,    1108     output_hidden_states=output_hidden_states,    1109     return_dict=return_dict,    1110 )    1111 sequence_output = outputs[0]    1112 prediction_scores = self.lm_head(sequence_output) File ~/anaconda3/lib/python3.9/sitepackages/torch/fx/_symbolic_trace.py:604, in Tracer.trace..module_call_wrapper(mod, *args, **kwargs)     600     return _orig_module_call(mod, *args, **kwargs)     602 _autowrap_check(patcher, getattr(getattr(mod, ""forward"", mod), ""__globals__"", {}),     603                 self._autowrap_function_ids) > 604 return self.call_module(mod, forward, args, kwargs) File ~/anaconda3/lib/python3.9/sitepackages/torch/fx/_symbolic_trace.py:422, in Tracer.call_module(self, m, forward, args, kwargs)     420 module_qualified_name = self.path_of_module(m)     421 if not self.is_leaf_module(m, module_qualified_name): > 422     return forward(*args, **kwargs)     423 return self.create_proxy('call_module', module_qualified_name, args, kwargs) File ~/anaconda3/lib/python3.9/sitepackages/torch/fx/_symbolic_trace.py:600, in Tracer.trace..module_call_wrapper..forward(*args, **kwargs)     599 def forward(*args, **kwargs): > 600     return _orig_module_call(mod, *args, **kwargs) File ~/anaconda3/lib/python3.9/sitepackages/torch/nn/modules/module.py:1102, in Module._call_impl(self, *input, **kwargs)    1098  If we don't have any hooks, we want to skip the rest of the logic in    1099  this function, and just call forward.    1100 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1101         or _global_forward_hooks or _global_forward_pre_hooks): > 1102     return forward_call(*input, **kwargs)    1103  Do not call functions when jit is used    1104 full_backward_hooks, non_full_backward_hooks = [], [] File ~/anaconda3/lib/python3.9/sitepackages/transformers/models/roberta/modeling_roberta.py:824, in RobertaModel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)     820         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)     822  We can provide a selfattention mask of dimensions [batch_size, from_seq_length, to_seq_length]     823  ourselves in which case we just need to make it broadcastable to all heads. > 824 extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)     826  If a 2D or 3D attention mask is provided for the crossattention     827  we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]     828 if self.config.is_decoder and encoder_hidden_states is not None: File ~/anaconda3/lib/python3.9/sitepackages/transformers/modeling_utils.py:559, in ModuleUtilsMixin.get_extended_attention_mask(self, attention_mask, input_shape, device)     543 """"""     544 Makes broadcastable attention and causal masks so that future and masked tokens are ignored.     545     (...)     555     `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.     556 """"""     557  We can provide a selfattention mask of dimensions [batch_size, from_seq_length, to_seq_length]     558  ourselves in which case we just need to make it broadcastable to all heads. > 559 if attention_mask.dim() == 3:     560     extended_attention_mask = attention_mask[:, None, :, :]     561 elif attention_mask.dim() == 2:     562      Provided a padding mask of dimensions [batch_size, seq_length]     563       if the model is a decoder, apply a causal mask in addition to the padding mask     564       if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length] File ~/anaconda3/lib/python3.9/sitepackages/torch/fx/proxy.py:251, in Proxy.__bool__(self)     250 def __bool__(self) > bool: > 251     return self.tracer.to_bool(self) File ~/anaconda3/lib/python3.9/sitepackages/torch/fx/proxy.py:152, in TracerBase.to_bool(self, obj)     145 (is_backward_compatible=True)     146 def to_bool(self, obj: 'Proxy') > bool:     147     """"""Called when a proxy object is being converted to a boolean, such as     148     when used in control flow.  Normally we don't know what to do because     149     we don't know the value of the proxy, but a custom tracer can attach more     150     information to the graph node using create_node and can choose to return a value.     151     """""" > 152     raise TraceError('symbolically traced variables cannot be used as inputs to control flow') TraceError: symbolically traced variables cannot be used as inputs to control flow ```  Versions PyTorch version: 1.10.2 Is debug build: False CUDA used to build PyTorch: Could not collect ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.10.102.1microsoftstandardWSL2x86_64withglibc2.31 Is CUDA available: False CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA T500 Nvidia driver version: 472.91 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.5 [pip3] numpydoc==1.2 [pip3] torch==1.10.2 [pip3] torchvision==0.11.3 [conda] blas                      1.0                         mkl [conda] cudatoolkit               11.3.1               h2bc3f7f_2 [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklservice               2.4.0            py39h7f8727e_0 [conda] mkl_fft                   1.3.1            py39hd3c417c_0 [conda] mkl_random                1.2.2            py39h51133e4_0 [conda] numpy                     1.21.5           py39he7a7128_1 [conda] numpybase                1.21.5           py39hf524024_1 [conda] numpydoc                  1.2                pyhd3eb1b0_0 [conda] pytorch                   1.10.2          cpu_py39hfa7516b_0 [conda] torchvision               0.11.3               py39_cu113    pytorch ",2022-05-27T21:33:23Z,triaged module: fx,open,0,0,https://github.com/pytorch/pytorch/issues/78435
transformer,Torchdynamo for Deepspeed and FSDP,"In AWS we are working with customers to enable gigantic transformer model training on EC2. Furthermore, we attempt to leverage compiler techniques to optimize Pytorch workloads due to its widely adoption. For example, we recently opensourced RAF, a deep learning compiler for training, that shows promising training acceleration for a set of models and verified it works with TorchDynamo. On the other hand, we do see the gap converting Pytorch programs to the IR we are using, especially when it comes to complex strategies such as distributed training. One example is Deepspeed. It implements data parallelism (ZeRO) on top of Pytorch, introducing sharding for optimizer states, gradients and parameters. (FSDP is another approach to do ZeRO). The idea itself is pretty straightforward, but when trying to convert the Deepspeed implementation to RAF via lazytensor, it doesn’t work well. For instance, the NaN check for gradients breaks the graph and result performance degradation; It doesn’t capture CUDA stream usage; etc. In my understanding, those issues could potentially be resolved via TorchＤynamo as it has the capability to extract structures and call information via Python bytecode, apart from the lazy tensor tracing mechanism at higher level. So we’d like to get suggestion from TorchDynamo community, what do you think about supporting such scenarios in TorchDynamo? Specifically, 1) how should TorchDynamo support ZeRO tracing? 2) would that be in the TorchDynamo soon or later? Due to our current roadmap and goals, we at AWS would be interested in collaboration along this direction if possible.  ",2022-05-26T21:46:51Z,oncall: distributed feature low priority triaged module: fsdp oncall: pt2 module: dynamo,open,7,3,https://github.com/pytorch/pytorch/issues/93756,'re looking into this and will post an update when we can.," We have support for FSDP and DDP with graph breaks currently, you may have seen these announced.  However we're also working on traceable collectives as part of an effort to make it possible to do more than just graphbreaks.  CC([RFC] PT2-Friendly Traceable, Functional Collective Communication APIs)","Pull requests are welcome to improve this, but this won't be a short term priority for H1."
transformer,Add nn.module activation support in BetterTransformer,"Summary: textray is using nn.module as activation function params, since functional is not scriptable in textray's module. Therefore we should support this in nn.module as well. Test Plan: CI Differential Revision: D36678078",2022-05-26T21:38:09Z,fb-exported Merged cla signed release notes: nn topic: performance,closed,0,8,https://github.com/pytorch/pytorch/issues/78394,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78394**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures, 1 Base Failures As of commit 65579265f3 (more details on the Dr. CI page): Expand to see more  * **1/2** failures introduced in this PR * **1/2** broken upstream at merge base c88367442d on May 31 from  1:36pm to  4:14pm   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6678786767?check_suite_focus=true) pull / linuxfocalpy3.7gcc7mobilelightweightdispatchbuild / build (1/1) **Step:** ""Build"" (full log  :repeat: rerun)   20220531T21:32:30.3692209Z [error]Process completed with exit code 137.  ``` 20220531T21:27:02.1717370Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/RegisterCodegenUnboxedKernels_2.cpp.o[0m 20220531T21:27:02.3079867Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/RegisterCodegenUnboxedKernels_3.cpp.o[0m 20220531T21:27:08.3710529Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/RegisterCodegenUnboxedKernels_4.cpp.o[0m 20220531T21:27:13.4791834Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/RegisterCodegenUnboxedKernels_5.cpp.o[0m 20220531T21:28:41.4017168Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/RegisterCodegenUnboxedKernels_6.cpp.o[0m 20220531T21:30:03.7010061Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/RegisterCodegenUnboxedKernels_7.cpp.o[0m 20220531T21:30:36.2742877Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/RegisterCodegenUnboxedKernels_8.cpp.o[0m 20220531T21:31:37.3301459Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/RegisterCodegenUnboxedKernels_9.cpp.o[0m 20220531T21:32:03.1415359Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/UnboxingFunctions_0.cpp.o[0m 20220531T21:32:04.2590567Z [ 98%] [32mBuilding CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/UnboxingFunctions_1.cpp.o[0m 20220531T21:32:30.3692209Z [error]Process completed with exit code 137. 20220531T21:32:30.4730951Z Prepare all required actions 20220531T21:32:30.4805237Z [group]Run ./.github/actions/teardownlinux 20220531T21:32:30.4805537Z with: 20220531T21:32:30.4805738Z env: 20220531T21:32:30.4805958Z   IN_CI: 1 20220531T21:32:30.4806188Z   IS_GHA: 1 20220531T21:32:30.4806403Z [endgroup] 20220531T21:32:30.4848657Z [group]Run .github/scripts/wait_for_ssh_to_drain.sh 20220531T21:32:30.4849041Z 36;1m.github/scripts/wait_for_ssh_to_drain.sh[0m 20220531T21:32:30.6088306Z shell: /usr/bin/bash noprofile norc e o pipefail {0} ```    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * [pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) on May 31 from  1:36pm to  4:14pm (96c134854d  d71816a51b)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D36678078,This pull request was **exported** from Phabricator. Differential Revision: D36678078," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D36678078,This pull request was **exported** from Phabricator. Differential Revision: D36678078,  merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[docs] Move a sentence from `nn.Transformer` to `nn.TransformerEncoder`,"`nn.Transformer` is not possible to be used to implement BERT, while `nn.TransformerEncoder` does. So this PR moves the sentence 'Users can build the BERT model with corresponding parameters.' from `nn.Transformer` to `nn.TransformerEncoder`. Fixes CC([docs] `nn.Transformer` is not possible to be used to implement BERT)",2022-05-26T03:18:57Z,open source Merged cla signed release notes: nn topic: docs,closed,0,4,https://github.com/pytorch/pytorch/issues/78337,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78337**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (3 Pending) As of commit e9673b306d (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!, merge this please
rag,Code coverage?," 🚀 The feature, motivation and pitch The code coverage on codecov: https://app.codecov.io/gh/pytorch/pytorch is 8 months old and it doesn't seem to receive new coverage information. Is there a place where we can find test coverage information without having to run any code locally?  Alternatives _No response_  Additional context _No response_",2022-05-26T00:50:00Z,,closed,0,3,https://github.com/pytorch/pytorch/issues/78328,"Hi, The coverage tool was very flaky and no one was maintaining the integration on our end. So it was discontinued I'm afraid.","> Hi, >  > The coverage tool was very flaky and no one was maintaining the integration on our end. So it was discontinued I'm afraid. I see. Thanks for the info",I am closing this Issue for now as the question has been responded. Feel free to reopen it if the question is not resolved.
transformer,[PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path,  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path) Straightforward after previous diffs in stack cleaning up code and adding test coverage. Differential Revision: D36564008,2022-05-25T16:55:15Z,Merged cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/78269,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78269**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9f0925688b (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,XLA is an infra flake, merge f, successfully started a merge job. Check the current status here,"Merge failed due to Command `git C /home/runner/actionsrunner/_work/pytorch/pytorch cherrypick x a51ec04800a4329500682925ce50c06c34e2c33d` returned nonzero exit code 1 ``` Automerging torch/nn/modules/transformer.py CONFLICT (content): Merge conflict in torch/nn/modules/transformer.py error: could not apply a51ec04800... [PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path hint: After resolving the conflicts, mark them with hint: ""git add/rm "", then run hint: ""git cherrypick continue"". hint: You can instead skip this commit with ""git cherrypick skip"". hint: To abort and get back to the state before ""git cherrypick"", hint: run ""git cherrypick abort"". ``` Raised by https://github.com/pytorch/pytorch/actions/runs/2692324992", merge g, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add test_modules test for TransformerEncoderLayer fast path,  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path)  CC([PyTorch] Add test_modules test for TransformerEncoderLayer fast path) Extend the existing TransformerEncoderLayer test to cover the fast path. Differential Revision: D36564009,2022-05-25T16:55:09Z,Merged cla signed topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/78268,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78268**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 789d0a70af (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge, successfully started a merge job. Check the current status here, your PR has been successfully merged.,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add fused kernel for binary_cross_entropy_with_logits,"  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path)  CC([PyTorch] Add test_modules test for TransformerEncoderLayer fast path)  CC([PyTorch] Add fused kernel for binary_cross_entropy_with_logits)  CC([PyTorch][easy] Fix borrowing from optional in binary_cross_entry_with_logits)  CC([PyTorch] Clean up native transformer implementation) I noticed this does a ton of extra dispatches (and, on CUDA, kernel launches), so I'm adding fused kernels. Differential Revision: D36650625 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-05-25T16:55:03Z,cla signed Stale,closed,0,4,https://github.com/pytorch/pytorch/issues/78267,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78267**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 14 New Failures, 1 Flaky Failures As of commit 97d17639f4 (more details on the Dr. CI page): Expand to see more  * **14/15** failures introduced in this PR * **1/15** tentatively recognized as flaky :snowflake:     * Click here to rerun these jobs   :detective: 14 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6675060577?check_suite_focus=true) pull / linuxxenialpy3.7gcc7 / build (1/14) **Step:** ""Build"" (full log  :repeat: rerun) :snowflake:   20220531T18:28:32.0177570Z unknown file: Failure  ``` 20220531T18:28:32.0171725Z W0531 18:26:49.907315 22890 upsampling.h:61] Warning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details.  (function _interp_output_size) 20220531T18:28:32.0173341Z W0531 18:26:49.908704 22890 upsampling.h:61] Warning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details.  (function _interp_output_size) 20220531T18:28:32.0174338Z [       OK ] ModulesTest.Upsampling3D (11 ms) 20220531T18:28:32.0174731Z [ RUN      ] ModulesTest.CTCLoss 20220531T18:28:32.0175076Z [       OK ] ModulesTest.CTCLoss (4 ms) 20220531T18:28:32.0175454Z [ RUN      ] ModulesTest.PoissonNLLLoss 20220531T18:28:32.0175871Z [       OK ] ModulesTest.PoissonNLLLoss (3 ms) 20220531T18:28:32.0176287Z [ RUN      ] ModulesTest.MarginRankingLoss 20220531T18:28:32.0176755Z [       OK ] ModulesTest.MarginRankingLoss (5 ms) 20220531T18:28:32.0177201Z [ RUN      ] ModulesTest.BCEWithLogitsLoss 20220531T18:28:32.0177570Z unknown file: Failure 20220531T18:28:32.0178027Z C++ exception with description ""Tensors of type UndefinedTensorImpl do not have strides 20220531T18:28:32.0178646Z Exception raised from strides_custom at c10/core/TensorImpl.cpp:406 (most recent call first): 20220531T18:28:32.0179137Z (no backtrace available)"" thrown in the test body. 20220531T18:28:32.0179656Z [  FAILED  ] ModulesTest.BCEWithLogitsLoss (17 ms) 20220531T18:28:32.0180096Z [ RUN      ] ModulesTest.MultiheadAttention 20220531T18:28:32.0180550Z [       OK ] ModulesTest.MultiheadAttention (101778 ms) 20220531T18:28:32.0180996Z [ RUN      ] ModulesTest.PrettyPrintIdentity 20220531T18:28:32.0181446Z [       OK ] ModulesTest.PrettyPrintIdentity (0 ms) 20220531T18:28:32.0181921Z [ RUN      ] ModulesTest.PrettyPrintFlatten 20220531T18:28:32.0182353Z [       OK ] ModulesTest.PrettyPrintFlatten (0 ms) ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",> Got any benchmark results? no. what benchmark?,> no. what benchmark? Just curious about general perf on CPU and CUDA before and after the fused kernel.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,[PyTorch][easy] Fix borrowing from optional in binary_cross_entry_with_logits,  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path)  CC([PyTorch] Add test_modules test for TransformerEncoderLayer fast path)  CC([PyTorch] Add fused kernel for binary_cross_entropy_with_logits)  CC([PyTorch][easy] Fix borrowing from optional in binary_cross_entry_with_logits)  CC([PyTorch] Clean up native transformer implementation) Saves a refcount bump. Differential Revision: D36650626,2022-05-25T16:54:58Z,Merged cla signed release notes: nn topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/78266,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78266**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c4aed91562 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Clean up native transformer implementation,  CC([PyTorch] Support norm_first in nn.TransformerEncoderLayer fast path)  CC([PyTorch] Add test_modules test for TransformerEncoderLayer fast path)  CC([PyTorch] Add fused kernel for binary_cross_entropy_with_logits)  CC([PyTorch][easy] Fix borrowing from optional in binary_cross_entry_with_logits)  CC([PyTorch] Clean up native transformer implementation) In preparation for supporting norm_first Differential Revision: D36564011,2022-05-25T16:54:52Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/78265,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78265**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 39921e047b (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
transformer,Any plan to add Noam scheduling?," 🚀 The feature, motivation and pitch I found Noam scheduling(from ""Attention is all you need"" paper) important for training transformers. But it's not included in torch. Do you have plan to add it or want others to add it?  Alternatives _No response_  Additional context _No response_",2022-05-25T12:22:13Z,triaged module: LrScheduler,open,0,2,https://github.com/pytorch/pytorch/issues/78253,I think the idea is that you could use/subclass LambdaLR with ease.,Right. I can implement it with LambdaLR or adopt other implementations. I think it's about policy or plan of this project whether to include Noam scheme. So opened this issue. And you mean it doesn't look good to provide that scheme in pytorch?
rag,Add from_blob with storage_offset arg,  CC(Add from_blob with storage_offset arg) Seperated out from other diff to better comply with github first Differential Revision: D36644764,2022-05-24T21:53:20Z,Merged cla signed,closed,0,6,https://github.com/pytorch/pytorch/issues/78217,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78217**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 08ad746024 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Merge failed due to Refusing to merge as mandatory check(s) linuxdocs / builddocs (cpp), winvs2019cpupy3 / build, winvs2019cuda11.3py3 / build are pending/not yet run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2381160006", merge this please,"It would also be nice to expose `from_blob` to Python:  CC(Support creating a CPU tensor from ctypes pointer in Python / from_blob(ptr, shape, strides, dtype))","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,DOC Corrects default value for storage_offset in as_strided,Fixes CC(Documentation bug for torch.as_strided),2022-05-24T20:48:00Z,module: docs triaged open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/78202,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78202**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c7d83c0a07 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
finetuning,torch.jit.script gives a RuntimeError for a custom MaskRCNN model," 🐛 Describe the bug We complied a MaskRCNN model shown in [TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL] (https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) with torch.jit.script. They define the model as mentioned below: ```python def get_model_instance_segmentation(num_classes):     model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)     in_features = model.roi_heads.box_predictor.cls_score.in_features     model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels     hidden_layer = 256     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,                                                        hidden_layer,                                                        num_classes)     return model ``` torch.jit.script works perfectly for this example. Please find the Google Colab notebook link for the working example here. We then wrapped the ""get_model_instance_segmentation()"" function inside a nn.Module class as shown below: ```python class CustomModel(nn.Module):     def __init__(self, num_classes=2):         super(CustomModel, self).__init__()         self.model = get_instance_segmentation_model(num_classes)     def forward(self, images, targets=None):         if targets is not None:             out = self.model(images, targets)         else:             out = self.model(images)         return out ``` The CustomModel trains and tests correctly. However, it fails when we try to compile it with the torch.jit.script. The runtime error is as follows: ``` forward(__torch__.torchvision.models.detection.mask_rcnn.MaskRCNN` self, Tensor[] images, Dict(str, Tensor)[]? targets=None) > ((Dict(str, Tensor), Dict(str, Tensor)[])): Expected a value of type 'List[Tensor]' for argument 'images' but instead found type 'Tensor (inferred)'. Inferred the value for argument 'images' to be of type 'Tensor' because it was not annotated with an explicit type. :   File """", line 10     def forward(self, images, targets=None):         if targets is not None:             out = self.model(images, targets)                   ~~~~~~~~~~ < HERE         else:             print('target', targets) ``` The link to the Google Colab notebook when CustomModel fails compilation is here. Both notebooks only contain the minimum required code.  Versions PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: version 3.22.4 Libc version: glibc2.26 Python version: 3.7.13 (default, Apr 24 2022, 01:04:09)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.188+x86_64withUbuntu18.04bionic Is CUDA available: True CUDA runtime version: 11.1.105 GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 460.32.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.7.6.5 /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.0.5 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.0.5 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.21.6 [pip3] torch==1.11.0+cu113 [pip3] torchaudio==0.11.0+cu113 [pip3] torchsummary==1.5.1 [pip3] torchtext==0.12.0 [pip3] torchvision==0.12.0+cu113 [conda] Could not collect",2022-05-24T17:49:06Z,oncall: jit,closed,1,3,https://github.com/pytorch/pytorch/issues/78188,"Can you annotate the type of `images` in the forward method? ```python from typing import List ...     def forward(self, images: List[torch.Tensor], targets=None): ``` The error message suggests that `self.model(..)` expects an argument that is a list of tensors. Since `images` is not annotated, torchscript assumes that it is a tensor. If it's not a tensor, it needs to be annotated as a tensor.",Thank you. It works.,Why didn't you use detectron2 instead torchvision?
yi,Allow specifying pickle module for torch.package," 🚀 The feature, motivation and pitch I'm working on a service that is to receive files created with `torch.package` to instantiate `torch.nn.Module`s created by a different service. Some of the modules use, somewhere in their structure, lambda functions and other constructs that are not supported by `pickle`, but are supported by the `dill` package. Seeing as how the `torch.save` and `torch.load` functions both support specifying a `pickle_module` to use, I was wondering if it was possible to support specifying it when instantiating `torch.package.PackageExporter` and `torch.package.PackageImporter`, so I could take advantage of the dependency bundling they provide?  Alternatives I can use `torch.save` and `torch.load` with `dill` as the `pickle_module`, though that in and out of itself doesn't bundle any dependencies needed to instantiate the module later on, and so I'd need both services to have the classes and methods, and updates on one side won't be reflected on the other. I could also copy the source code of `torch.package`, make the necessary adjustments to it so that it uses `dill`, and use that code instead, but I'd have to make sure it is up to date with any changes made in this repository, and again will need this code to exist in both services.  Additional context _No response_",2022-05-24T12:19:47Z,enhancement oncall: package/deploy imported,open,5,0,https://github.com/pytorch/pytorch/issues/78172
transformer,fix typo in docstring of `Transformer.forward()`,"Fixed the word ""decode"" to be ""decoder"".",2022-05-24T07:19:49Z,open source Merged cla signed release notes: nn topic: docs,closed,0,3,https://github.com/pytorch/pytorch/issues/78167,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78167**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit b44ac4b040 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Device MPS: -> RuntimeError Placeholder buffer size is not large enough to contain the Tensor storage of size," 🐛 Describe the bug Hello to everyone, I am trying to use the torch nightly version which supports the M1 drivers. My machine is the M1 Pro 14'' base version (16GB Ram, 512GB SSD, 8 CPU cores). I have tried to install the nightly version in two different ways, but I got the same issue. In both ways, I have created a new conda env. 1) ```conda install pytorch torchvision torchaudio c pytorchnightly``` 2) ```pip3 install pre torch torchvision torchaudio extraindexurl https://download.pytorch.org/whl/nightly/cpu``` The error: ```python3 ❯ python3 test.py Some weights of the model checkpoint at bertbasecased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']  This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).  This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Reusing dataset conll2003 (/Users/andreabacciu/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee) 100% 3/3 [00:00     outputs = bert(**model_inputs)   File ""/opt/homebrew/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/opt/homebrew/lib/python3.10/sitepackages/transformers/models/bert/modeling_bert.py"", line 1009, in forward     pooled_output = self.pooler(sequence_output) if self.pooler is not None else None   File ""/opt/homebrew/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/opt/homebrew/lib/python3.10/sitepackages/transformers/models/bert/modeling_bert.py"", line 637, in forward     pooled_output = self.dense(first_token_tensor)   File ""/opt/homebrew/lib/python3.10/sitepackages/torch/nn/modules/module.py"", line 1130, in _call_impl     return forward_call(*input, **kwargs)   File ""/opt/homebrew/lib/python3.10/sitepackages/torch/nn/modules/linear.py"", line 114, in forward     return F.linear(input, self.weight, self.bias) RuntimeError: Placeholder buffer size (98304) is not large enough to contain the Tensor storage of size 5505024 ``` The code: ```python3 import torch import transformers as tr from datasets import load_dataset from time import time from tqdm import tqdm from torch.utils.data import DataLoader bert = tr.AutoModel.from_pretrained(""bertbasecased"") tokenizer = tr.AutoTokenizer.from_pretrained(""bertbasecased"") datasets = load_dataset(""conll2003"") test_data = datasets[""test""] def collate(batch):     return tokenizer(         [t[""tokens""] for t in batch],         return_tensors=""pt"",         padding=True,         is_split_into_words=True,     ) dataloader = DataLoader(     test_data,     batch_size=32,     collate_fn=collate,     num_workers=0, ) times_mps = [] bert = bert.to(""mps"")  group in batches of 32 for model_inputs in tqdm(dataloader):     model_inputs = {k: v.to(""mps"") for k, v in model_inputs.items()}     t0 = time()     with torch.no_grad():         outputs = bert(**model_inputs)     t1 = time()     times_mps.append(t1  t0) times = [] bert = bert.to(""cpu"")  group in batches of 32 for model_inputs in tqdm(dataloader):     model_inputs = {k: v.to(""cpu"") for k, v in model_inputs.items()}     t0 = time()     with torch.no_grad():         outputs = bert(**model_inputs)     t1 = time()     times.append(t1  t0) print(""Mean time per batch GPU: {:.2f}s"".format(sum(times_mps) / len(times_mps))) print(""Mean time per batch CPU: {:.2f}s"".format(sum(times) / len(times))) ```  Versions Collecting environment information... PyTorch version: 1.13.0.dev20220521 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2) CMake version: version 3.23.0 Libc version: N/A Python version: 3.10.2 (main, Feb  2 2022, 05:51:25) [Clang 13.0.0 (clang1300.0.29.3)] (64bit runtime) Python platform: macOS12.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.6 [pip3] pytorchlightning==1.6.0 [pip3] pytorch3d==0.6.2 [pip3] torch==1.13.0.dev20220521 [pip3] torchaudio==0.11.0 [pip3] torchmetrics==0.8.2 [pip3] torchvision==0.12.0 [conda] pytorch                   1.13.0.dev20220521        py3.10_0    pytorchnightly ",2022-05-21T13:00:53Z,module: macos module: mps,closed,0,7,https://github.com/pytorch/pytorch/issues/78042,I did not experience this on `1.13.0.dev20220522`. Can you update and try again?,"I had this problem with 1.13.0.dev20220521, but it went away with 1.13.0.dev20220522.",seems like the problem no longer exist in the latest nightly. I am closing this issue.  please feel free to reopen if the problem persist,"I saw this error printed only after another error occured (the model wasn't converted to mps, but the input was). Just in case you have the same issue.","Hi, Yes, I confirm the issue is fixed in updating to the version `1.13.0.dev20220522`. Thank you for the support  ,  , . Thank you also to , now the issue is solved on my side and as you can see from the code posted I have moved the model and inputs to the mps device. Kind regards, Andrea Bacciu","Hi, I am too facing the issue. System: Macbook M1 Pro 14'' base model Relevant packages versions: torch                         2.0.0.dev20221213 torchmetrics                  0.11.0 torchvision                   0.13.1a0 The error which I am getting: `RuntimeError: Invalid buffer size: 9.75 GB` For a reason, I am predicting the results using the trained model on the entire test data(test data size ~ 19962, RGB image). Dataset Used  celebA's test data ``` device = torch.device('mps') model = model.to(device) data_s = data_s.to(device) pred = model(data_s) ``` when the third line is executed, the above error is prompted. Is it due to the limitation of my hardware or is it someother issue?","> Hi, I am too facing the issue. >  > System: Macbook M1 Pro 14'' base model >  > Relevant packages versions: torch 2.0.0.dev20221213 torchmetrics 0.11.0 torchvision 0.13.1a0 >  > The error which I am getting: `RuntimeError: Invalid buffer size: 9.75 GB` >  > For a reason, I am predicting the results using the trained model on the entire test data(test data size ~ 19962, RGB image). >  > Dataset Used  celebA's test data >  > ``` > device = torch.device('mps') > model = model.to(device) > data_s = data_s.to(device) >  > pred = model(data_s) > ``` >  > when the third line is executed, the above error is prompted. Is it due to the limitation of my hardware or is it someother issue? I get the same result with any batch_size other than 1"
rag,Move THPStorage definitions out of `torch/csrc/generic`,Fixes CC(Move C++/Python Storage bindings out of `torch/csrc/generic`),2022-05-21T02:18:27Z,module: internals open source Merged cla signed Reverted ciflow/binaries,closed,0,13,https://github.com/pytorch/pytorch/issues/78032,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78032**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: 1 Base Failures As of commit 4530f9b96b (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 5b922c29e9 since May 31   :construction: 1 ongoing upstream failure: These were probably **caused by upstream breakages** that are **not fixed yet**. * pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) since May 31 (a88f155f4b)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",unfortunately bazel error looks real ``` 20220521T03:14:38.5329326Z [31m[1mERROR: [0m/var/lib/jenkins/workspace/BUILD.bazel:1693:11: Compiling torch/csrc/Storage.cpp failed: (Exit 1): gcc failed: error executing command /opt/cache/bin/gcc U_FORTIFY_SOURCE fstackprotector Wall Wunusedbutsetparameter Wnofreenonheapobject fnoomitframepointer 'std=c++11' MD MF ... (remaining 279 argument(s) skipped) 20220521T03:14:38.5331112Z  20220521T03:14:38.5331668Z Use sandbox_debug to see verbose messages from the sandbox 20220521T03:14:38.5332232Z torch/csrc/Storage.cpp:359:10: fatal error: torch/csrc/StorageMethods.cpp: No such file or directory 20220521T03:14:38.5332725Z  include  20220521T03:14:38.5333091Z           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ```, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""This broke windows binary builds, see: https://hud.pytorch.org/pytorch/pytorch/commit/f0121528364f6023c69f49e69fabc00863a5ef57"" c ""nosignal""",Here's the important part of the log of the failure: ``` 20220524T15:59:56.3816356Z [6097/6309] Building CXX object caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\StorageSharing.cpp.obj 20220524T15:59:56.3817015Z FAILED: caffe2/torch/CMakeFiles/torch_python.dir/csrc/StorageSharing.cpp.obj  ... 20220524T15:59:56.3845731Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C2039: 'copysign': is not a member of '`global namespace'' 20220524T15:59:56.3846639Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C3861: 'copysign': identifier not found ... 20220524T16:00:02.2409189Z [6102/6309] Building CXX object caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\StorageMethods.cpp.obj 20220524T16:00:02.2410153Z FAILED: caffe2/torch/CMakeFiles/torch_python.dir/csrc/StorageMethods.cpp.obj  ... 20220524T16:00:02.2438157Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C2039: 'copysign': is not a member of '`global namespace'' 20220524T16:00:02.2447842Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C3861: 'copysign': identifier not found ``` I'm not sure where this `copysign` is coming from. I'm not calling it in `StorageMethods.cpp` or `StorageSharing.cpp`. I guess it's possible that I removed an include file that was required. I'll see what I can figure out,"I added this to `StorageMethods.cpp` and `StorageSharing.cpp`: ``` ifdef _MSC_VER include  endif ``` Not sure if it'll work, but it did jump out to me since it's a windowsspecific include that these files used to have access to, which I had removed","Hmmm, it looks like `wheelpy3_7cuda11_3build` wasn't run, even though we have the `ciflow/all` label. Not sure what to do about that. Maybe I need to make a new PR",I noticed that `c10/util/math_compat.h` has some `std::copysign` definitions. So I've included that file in `StorageMethods.cpp` and `StorageSharing.cpp`. Hopefully that fixes the error,"It's still failing. Since I'm having trouble figuring out which include is missing, I think I should try just copying all of the includes from `torch/csrc/Storage.cpp` into `StorageMethods.cpp` and `StorageSharing.cpp`. Even though it's overkill, I'm pretty sure that would fix the error","The XLA failure appears to be upstream, so I think this should be good to go now", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[flatbuffer] Move saving storage to the last step.,"Summary: Move storage saving to last step, because otherwise tensors saved after storage are already saved will not have storage. Test Plan: Tested by loading the file in `clowder get GLDGLQnKrIsQFg8DAPxq9vg59ZwZbmQwAAAA orig.pt` and converting to flatbuffer and load again Differential Revision: D36552645",2022-05-20T23:08:41Z,oncall: jit fb-exported Merged cla signed release notes: mobile topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/78024,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78024**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 035e45252a (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36552645," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this on green,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Add meta device support to `_UntypedStorage` and `_TypedStorage`,Fixes CC(Add meta device support for Storages),2022-05-20T20:58:48Z,module: internals open source Merged cla signed module: meta tensors,closed,0,5,https://github.com/pytorch/pytorch/issues/78008,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/78008**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 75b566a2a8 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,It's very possible that some of the functions for a meta storage will fail in an ugly way at the moment (like assuming that the data pointer is valid and trying to access it). I will take a look into all of them,Need to disable the test when run on xla, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Add a check to ensure input func to Library.impl is callable,Stack from ghstack:  CC(Add a check to ensure input func to Library.impl is callable),2022-05-20T18:39:16Z,Merged cla signed release notes: composability topic: improvements module: library,closed,0,4,https://github.com/pytorch/pytorch/issues/77990,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77990**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 2 New Failures As of commit d5060040a9 (more details on the Dr. CI page): Expand to see more  * **2/2** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6710704355?check_suite_focus=true) pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) (1/2) **Step:** ""Test"" (full log  :repeat: rerun)   20220602T15:35:23.4814796Z RuntimeError: No such operator custom_namespace::custom_add  ``` 20220602T15:35:23.4078978Z  20220602T15:35:23.4083571Z [gw2] [32mPASSED[0m test/onnx/test_pytorch_onnx_onnxruntime.py [1A 20220602T15:35:23.4808864Z  [36mtest/onnx/test_pytorch_onnx_onnxruntime.py[0m::TestONNXRuntime_opset9.test_div_promotion_trace[0m [32m✓[0m[32m31% [0m[40m[32m█[0m[40m[32m█[0m[40m[32m█[0m[40m[32m▏[0m[40m[32m      [0m[1B 20220602T15:35:23.4809320Z  20220602T15:35:23.4809356Z  20220602T15:35:23.4812039Z ―――――――――――――――――――――――― TestCustomOps.test_custom_add ――――――――――――――――――――――――― 20220602T15:35:23.4812684Z [gw0] linux  Python 3.7.13 /opt/conda/bin/python 20220602T15:35:23.4813152Z Traceback (most recent call last): 20220602T15:35:23.4813785Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/_ops.py"", line 198, in __getattr__ 20220602T15:35:23.4814290Z     op, overload_names = torch._C._jit_get_operation(qualified_op_name) 20220602T15:35:23.4814796Z RuntimeError: No such operator custom_namespace::custom_add 20220602T15:35:23.4815091Z  20220602T15:35:23.4815344Z The above exception was the direct cause of the following exception: 20220602T15:35:23.4815643Z  20220602T15:35:23.4815832Z Traceback (most recent call last): 20220602T15:35:23.4816311Z   File ""/var/lib/jenkins/workspace/test/onnx/test_custom_ops.py"", line 53, in test_custom_add 20220602T15:35:23.4816827Z     onnxir, _ = do_export(model, (x, y), opset_version=11) 20220602T15:35:23.4817359Z   File ""/var/lib/jenkins/workspace/test/onnx/test_pytorch_onnx_caffe2.py"", line 100, in do_export 20220602T15:35:23.4818210Z     out = torch.onnx._export(model, inputs, f, *args, **kwargs) 20220602T15:35:23.4819048Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/onnx/__init__.py"", line 53, in _export 20220602T15:35:23.4819517Z     result = utils._export(*args, **kwargs) ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge this please, merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"expected key in DispatchKeySet(CPU, CUDA, ....) but got: MPS"," 🐛 Describe the bug Error: ```python RuntimeError: new(): expected key in DispatchKeySet(CPU, CUDA, HIP, XLA, IPU, XPU, HPU, Lazy) but got: MPS ``` This error is thrown from hugging face transformers module. https://github.com/huggingface/transformers/blob/54192058f3826eb38f9aaea02961f1304678198f/src/transformers/generation_utils.pyL1658L1659 minimum reproducible would presumably be: ```python  tensor  = torch.LongTensor(...) tensor.new(1, device=torch.device('mps')) ```  Versions ``` PyTorch version: 1.12.0.dev20220520 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.23.1 Libc version: N/A Python version: 3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ] (64bit runtime) Python platform: macOS12.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.12.0.dev20220520 [conda] numpy                     1.22.3                   pypi_0    pypi [conda] torch                     1.12.0.dev20220520          pypi_0    pypi ```",2022-05-20T12:46:18Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/77960,"I'm changing to the following as a workaround right now. ```python unfinished_sequences = torch.ones(input_ids.shape[0], device=input_ids.device) ``` however not sure if dtype is also passed in `torch.LongTensor.new`."
rag,[GHF][BE] Use GraphQL fragments,"Introduce `PRReviews`, `PRCheckSuites` and `CommitAuthors` fragments This avoids code duplication and possibility ones query will look different for paginated subquery vs default one",2022-05-20T05:59:56Z,Merged cla signed topic: not user facing,closed,0,2,https://github.com/pytorch/pytorch/issues/77945,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77945**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 702baf5748 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this
transformer,"Revert ""Revert ""Switch to use nested tensor by-default in Transformer…","…Encoder ( CC(Switch to use nested tensor bydefault in TransformerEncoder))"""" This reverts commit 0d6fa91d1ba93b423a31a7811b679a4770470363. Fixes ISSUE_NUMBER",2022-05-20T00:57:50Z,Merged cla signed,closed,0,19,https://github.com/pytorch/pytorch/issues/77924,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77924**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit cdb0a1f3af (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Not ready yet.," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794,This pull request was **exported** from Phabricator. Differential Revision: D36535794," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","One last test, come on!",All tests passed!, merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Move C++/Python Storage bindings out of `torch/csrc/generic`,"Since CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage) was merged, we now only have one `THPStorage` class, which means that we can move all storage related stuff out of `torch/csrc/generic` to simplify things further ",2022-05-19T22:00:55Z,module: internals triaged,closed,0,0,https://github.com/pytorch/pytorch/issues/77908
transformer,buffer is not large enough when running pytorch on Mac M1 mps," 🐛 Describe the bug The bug seems related to CC(`MPSNDArray` or `MPSGraphTensorData` allocated with wrong size) To reproduce the bug: ```python from transformers import AutoModel from transformers import AutoTokenizer import torch model_ckpt = ""distilbertbaseuncased"" device = torch.device(""mps"") model = AutoModel.from_pretrained(model_ckpt).to(device) model_ckpt = ""distilbertbaseuncased"" tokenizer = AutoTokenizer.from_pretrained(model_ckpt) text = ""this is a test"" inputs = tokenizer(text, return_tensors=""pt"") inputs = {k:v.to(device) for k,v in inputs.items()} with torch.no_grad():     outputs = model(**inputs) ``` The error message: ``` /AppleInternal/Library/BuildRoots/8d3bda538d9c11ecabd7fa6a1964e34e/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 432 bytes ' [1] 75519 abort python 02.py /Users/xiaoou/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown warnings.warn('resource_tracker: There appear to be %d ' ```  Versions 1.12.0.dev20220519",2022-05-19T20:33:25Z,module: memory usage triaged module: mps,closed,9,34,https://github.com/pytorch/pytorch/issues/77886,Same issue with YOLOv5 on MPS noted in  CC(YOLOv5: MPS on Macbook Air M1 NotImplementedError: Could not run 'aten::empty.memory_format' with arguments...)issuecomment1131558211. I see `buffer is not large enough. Must be 25600 bytes`,"Same, likely be related. ```python generator = transformers.pipeline(task=""textgeneration"", model=model.to('mps'), tokenizer=tokenizer, device=torch.device(""mps"")) generator(""This shall brake. "", max_length=200, use_cache=True) ``` gives ```python Setting `pad_token_id` to `eos_token_id`:50256 for openend generation.  RuntimeError                              Traceback (most recent call last) Input In [9], in () > 1 r = generator(""Yes, your highness. "", max_length=200, use_cache=True)       2 print(r[0]['generated_text']) File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/pipelines/text_generation.py:176, in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)     137 def __call__(self, text_inputs, **kwargs):     138     """"""     139     Complete the prompt(s) given as inputs.     140     (...)     174           ids of the generated text.     175     """""" > 176     return super().__call__(text_inputs, **kwargs) File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/pipelines/base.py:1032, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)    1030     return self.iterate(inputs, preprocess_params, forward_params, postprocess_params)    1031 else: > 1032     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params) File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/pipelines/base.py:1039, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)    1037 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):    1038     model_inputs = self.preprocess(inputs, **preprocess_params) > 1039     model_outputs = self.forward(model_inputs, **forward_params)    1040     outputs = self.postprocess(model_outputs, **postprocess_params)    1041     return outputs File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/pipelines/base.py:948, in Pipeline.forward(self, model_inputs, **forward_params)     946     with inference_context():     947         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device) > 948         model_outputs = self._forward(model_inputs, **forward_params)     949         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu""))     950 else: File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/pipelines/text_generation.py:215, in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)     213     in_b = input_ids.shape[0]     214 prompt_text = model_inputs.pop(""prompt_text"") > 215 generated_sequence = self.model.generate(input_ids=input_ids, **generate_kwargs)   BS x SL     216 out_b = generated_sequence.shape[0]     217 if self.framework == ""pt"": File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/torch/autograd/grad_mode.py:27, in _DecoratorContextManager.__call__..decorate_context(*args, **kwargs)      24 .wraps(func)      25 def decorate_context(*args, **kwargs):      26     with self.clone(): > 27         return func(*args, **kwargs) File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/generation_utils.py:1316, in GenerationMixin.generate(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)    1308     input_ids, model_kwargs = self._expand_inputs_for_generation(    1309         input_ids,    1310         expand_size=num_return_sequences,    1311         is_encoder_decoder=self.config.is_encoder_decoder,    1312         **model_kwargs,    1313     )    1315      12. run sample > 1316     return self.sample(    1317         input_ids,    1318         logits_processor=logits_processor,    1319         logits_warper=logits_warper,    1320         stopping_criteria=stopping_criteria,    1321         pad_token_id=pad_token_id,    1322         eos_token_id=eos_token_id,    1323         output_scores=output_scores,    1324         return_dict_in_generate=return_dict_in_generate,    1325         synced_gpus=synced_gpus,    1326         **model_kwargs,    1327     )    1329 elif is_beam_gen_mode:    1330     if num_return_sequences > num_beams: File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/generation_utils.py:1935, in GenerationMixin.sample(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)    1932 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)    1934  forward pass to get next token > 1935 outputs = self(    1936     **model_inputs,    1937     return_dict=True,    1938     output_attentions=output_attentions,    1939     output_hidden_states=output_hidden_states,    1940 )    1942 if synced_gpus and this_peer_finished:    1943     cur_len = cur_len + 1 File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)    1126  If we don't have any hooks, we want to skip the rest of the logic in    1127  this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or _global_forward_hooks or _global_forward_pre_hooks): > 1130     return forward_call(*input, **kwargs)    1131  Do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], [] File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/models/gptj/modeling_gptj.py:815, in GPTJForCausalLM.forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)     807 r""""""     808 labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):     809     Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set     810     `labels = input_ids` Indices are selected in `[100, 0, ..., config.vocab_size]` All labels set to `100`     811     are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`     812 """"""     813 return_dict = return_dict if return_dict is not None else self.config.use_return_dict > 815 transformer_outputs = self.transformer(     816     input_ids,     817     past_key_values=past_key_values,     818     attention_mask=attention_mask,     819     token_type_ids=token_type_ids,     820     position_ids=position_ids,     821     head_mask=head_mask,     822     inputs_embeds=inputs_embeds,     823     use_cache=use_cache,     824     output_attentions=output_attentions,     825     output_hidden_states=output_hidden_states,     826     return_dict=return_dict,     827 )     828 hidden_states = transformer_outputs[0]     830  Set device for model parallelism File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)    1126  If we don't have any hooks, we want to skip the rest of the logic in    1127  this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or _global_forward_hooks or _global_forward_pre_hooks): > 1130     return forward_call(*input, **kwargs)    1131  Do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], [] File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/models/gptj/modeling_gptj.py:670, in GPTJModel.forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)     662     outputs = torch.utils.checkpoint.checkpoint(     663         create_custom_forward(block),     664         hidden_states,    (...)     667         head_mask[i],     668     )     669 else: > 670     outputs = block(     671         hidden_states,     672         layer_past=layer_past,     673         attention_mask=attention_mask,     674         head_mask=head_mask[i],     675         use_cache=use_cache,     676         output_attentions=output_attentions,     677     )     679 hidden_states = outputs[0]     680 if use_cache is True: File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)    1126  If we don't have any hooks, we want to skip the rest of the logic in    1127  this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or _global_forward_hooks or _global_forward_pre_hooks): > 1130     return forward_call(*input, **kwargs)    1131  Do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], [] File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/models/gptj/modeling_gptj.py:305, in GPTJBlock.forward(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)     303 residual = hidden_states     304 hidden_states = self.ln_1(hidden_states) > 305 attn_outputs = self.attn(     306     hidden_states,     307     layer_past=layer_past,     308     attention_mask=attention_mask,     309     head_mask=head_mask,     310     use_cache=use_cache,     311     output_attentions=output_attentions,     312 )     313 attn_output = attn_outputs[0]   output_attn: a, present, (attentions)     314 outputs = attn_outputs[1:] File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)    1126  If we don't have any hooks, we want to skip the rest of the logic in    1127  this function, and just call forward.    1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1129         or _global_forward_hooks or _global_forward_pre_hooks): > 1130     return forward_call(*input, **kwargs)    1131  Do not call functions when jit is used    1132 full_backward_hooks, non_full_backward_hooks = [], [] File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/models/gptj/modeling_gptj.py:229, in GPTJAttention.forward(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)     226 q_pass = query[:, :, :, self.rotary_dim :]     228 sincos = fixed_pos_embedding(k_rot, 1, seq_len=seq_len) > 229 k_rot = apply_rotary_pos_emb(k_rot, sincos, offset=offset)     230 q_rot = apply_rotary_pos_emb(q_rot, sincos, offset=offset)     232 key = torch.cat([k_rot, k_pass], dim=1) File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/models/gptj/modeling_gptj.py:90, in apply_rotary_pos_emb(x, sincos, offset)      88 sin, cos = map(lambda t: duplicate_interleave(t)[None, offset : x.shape[1] + offset, None, :], sincos)      89  einsum notation for lambda t: repeat(t[offset:x.shape[1]+offset,:], ""n d > () n () (d j)"", j=2) > 90 return (x * cos) + (rotate_every_two(x) * sin) File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/sitepackages/transformers/models/gptj/modeling_gptj.py:72, in rotate_every_two(x)      70 x1 = x[:, :, :, ::2]      71 x2 = x[:, :, :, 1::2] > 72 x = torch.stack((x2, x1), axis=1)      73 return x.flatten(2) RuntimeError: Placeholder buffer size (14336) is not large enough to contain the Tensor storage of size 114688 ``` version  `1.12.0a0+git64c741e` with https://github.com/pytorch/pytorch/pull/77966 applied and `PYTORCH_ENABLE_MPS_FALLBACK=1` set",same issue ,This issue is fixed in PR CC(MPS: Fix crashes in view tensors due to buffer size mismatch)  (nightly build 1.13.0.dev20220531 or later).,"Same issue with YOLOv5 on device ""mps"" MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 19200 bytes' CC(MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 20480 bytes ')",> This issue is fixed in PR CC(MPS: Fix crashes in view tensors due to buffer size mismatch) (nightly build 1.13.0.dev20220531 or later). This issue has not been fixed.. ,  can you please reinstall nightly and see if this resolves https://github.com/ultralytics/yolov5/issues/8102,"Hello I am still receiving this error. What do I have to do to resolve this bug/issue? Thanks. pytorch nightly version: torch1.13.0.dev20220607 /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 160000 bytes","I'm also still receiving this error, a fix would be appreciated. PyTorch version 1.13.0.dev20220607 Fusing layers...  YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 25600 bytes","jocher  > can you please reinstall nightly and see if this resolves ultralytics/yolov5 CC(zerodivision crashes on 0.4.0) No it does not ``` python yolov5/detect.py source 0 device='mps'                                        detect: weights=yolov5/yolov5s.pt, source=0, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=mps, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False YOLOv5 🚀 v6.1246g2dd3db0 Python3.8.13 torch1.13.0.dev20220607 MPS Fusing layers...  YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients 1/1: 0...  Success (inf frames 1280x720 at 30.00 FPS) /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 25600 bytes ' zsh: abort      python yolov5/detect.py source 0 device='mps' ``` ``` Collecting environment information... PyTorch version: 1.13.0.dev20220607 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.4 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: Could not collect Libc version: N/A Python version: 3.8.13  (default, Mar 25 2022, 06:04:14)  [Clang 12.0.1 ] (64bit runtime) Python platform: macOS12.4arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.4 [pip3] torch==1.13.0.dev20220607 [pip3] torchvision==0.14.0.dev20220608 [conda] numpy                     1.22.4           py38he1fcd3f_0    condaforge [conda] pytorch                   1.13.0.dev20220607         py3.8_0    pytorchnightly [conda] torchvision               0.14.0.dev20220603          pypi_0    pypi ```","Same issue try running YOLOv5s with mps on M1 Pro YOLOv5 🚀 2022612 Python3.9.12 torch1.13.0.dev20220612 MPS ``` /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion [MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 19200 bytes ```","same issue when running with stable baselines 3 contrib PPO recurrent  > /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 576 bytes '","YOLOv5 🚀 v6.1253g75bbaa8 Python3.10.4 torch1.13.0.dev20220616 MPS !image /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 25600 bytes",Same issue  MacBook Pro M1  ,Same issue  macbook pro m1 !image,Same issue +1 ,"I confirm I'm experiencing the same YOLOv5 Apple MPS bug with torch 1.12 on MacBook M1: `Error: buffer is not large enough. Must be 25600 bytes` ```bash $ glennjocherMacBookAir yolov5 % python detect.py device mps detect: weights=yolov5s.pt, source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=mps, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False YOLOv5 🚀 v6.1386g858a1a3 Python3.9.13 torch1.12.0 MPS Fusing layers...  YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients /AppleInternal/Library/BuildRoots/b6051351c03011ec96e93e7866fcf3a1/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 25600 bytes ```",> This issue is fixed in PR CC(MPS: Fix crashes in view tensors due to buffer size mismatch) (nightly build 1.13.0.dev20220531 or later).   is this error supposed to exist in torch 1.12? I see the fix was was in a 1.13 nightly.,"Hi, I'm not sure if this made it for the release no. If you're using MPS a lot, I would recommend using the nightly though as we did quite a few fixes that didn't make it to 1.12.","> I'm not sure if this made it for the release no. If you're using MPS a lot, I would recommend using the nightly though as we did quite a few fixes that didn't make it to 1.12. Hmm https://github.com/pytorch/pytorch/pull/78496 was picked into release branch as  https://github.com/pytorch/pytorch/commit/e3e753161c7c65532883b991693451849afdd708","Reopening to investigate if it still crashes on trunk, and if it is not, why Pytorch1.12 is still affected",I'm still getting the issue on 1.13.0.dev20220712 Is there any fix?,"jocher,  this should be fixed in the latest PyTorch nightly (1.13.0.dev20220722). Could you please let me know if you are still seeing the issue on your end?",  NotImplementedError: The operator 'aten::index.Tensor_out' is not current implemented for the MPS device. CC(NotImplementedError: The operator 'aten::index.Tensor_out' is not current implemented for the MPS device.)," I confirm that the original `buffer is not large enough` error is now resolved in latest nightly.  YOLOv5 inference still fails on `operator 'aten::index.Tensor_out' is not current implemented for the MPS device` as mentioned by , but that's a separate issue, so I believe this issue can be closed now.",Thanks a lot  and jocher for checking this! `index.Tensor_out` is already part of https://github.com/kulinseth/pytorch and we hope to get it soon in PyTorch master, awesome! Thanks for the update.,"I'm on the latest nightly `torch1.13.0.dev20221003` and still getting this error. Could it be cause `aten::repeat_interleave.self_int` fell back to CPU? ``` /usr/local/opt/miniforge3/lib/python3.9/sitepackages/whisper/decoding.py:628: /UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently /supported on the MPS backend and will fall back to run on the CPU. This may /have performance implications. (Triggered internally at //Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11 /.)   audio_features = audio_features.repeat_interleave(self.n_group, dim=0) /AppleInternal/Library/BuildRoots/a0876c02178811edb9c496898e02b808/Library/ /Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray. /mm:782: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: /buffer is not large enough. Must be 201452 bytes ' ``` **Edited to hardwrap the long lines**"," that's not an error, that's a warning. As the message states not all torch ops are fully converted to MPS yet.",">  that's not an error, that's a warning. As the message states not all torch ops are fully converted to MPS yet. I think you have have to side scroll to see everything jocher, the formatting wasn't great. "" Error: buffer is not large enough. Must be 201452 bytes"" is there on the last line. and then the kernel dies (in Ipython or Jupyter)"
rag,Add meta device support for Storages,"Now that CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage) is merged, it should be relatively straightforward to add support for the meta device in `_UntypedStorage` and `_TypedStorage`. ",2022-05-19T20:07:40Z,module: internals triaged module: meta tensors,closed,1,0,https://github.com/pytorch/pytorch/issues/77885
chat,Make the appropriate backend `DimensionNode` visible to LTC core," 🚀 The feature, motivation and pitch With the current implementation of `DimensionNode` in LTC, each backend has its own implementation (PyTorch/XLA, TorchScript).  At the same time, shape inference builds off of LTC core classes leading to this PR failing to build shape inference implementation for `expand.SymInt`. Please make the appropriate backend `DimensionNode` visible to LTC core. **Solution** alternatives based on offline chat with . * IR core to access the correct backend implementation * Use multiple inheritance CC    Alternatives _No response_  Additional context _No response_",2022-05-19T19:39:03Z,triaged lazy,open,0,1,https://github.com/pytorch/pytorch/issues/77880,"Can we leverage the nonnative IR codegen ( CC(Codegen NonNative IR Nodes)) to solve this, by making DimensionNode one of the generated IRs that backends own? I guess that is orthogonal. That would be equivalent to the status quo, but save some codeduplication. We could expand on the IR Builder interface to let backends build their own DimensionNode through a core/backend API."
rag,Restore old names for private funcs in legacy storages,Followup from CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage),2022-05-19T17:39:33Z,module: internals open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/77861,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77861**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (8 Pending) As of commit 041f6740e2 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Add missing decref to `createStorageGetType`,Followup from PR CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage),2022-05-19T17:09:09Z,module: internals open source Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/77860,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77860**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 1 New Failures As of commit 92dccdcf44 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6511788349?check_suite_focus=true) pull / linuxbionicrocm5.1py3.7 / build (1/1) **Step:** ""Calculate docker image"" (full log  :repeat: rerun)   20220519T17:11:24.2151219Z [error]Process completed with exit code 1.  ``` 20220519T17:11:24.2118408Z + git revparse 0d9e42408b6fcf7540273b74ecdbdf66effc424d:.circleci/docker 20220519T17:11:24.2130236Z 6fc2e0be6a27b6d8230bfa7d47bbb5649a7d1e7e 20220519T17:11:24.2133804Z ++ git revparse 0d9e42408b6fcf7540273b74ecdbdf66effc424d:.circleci/docker 20220519T17:11:24.2145404Z + PREVIOUS_DOCKER_TAG=6fc2e0be6a27b6d8230bfa7d47bbb5649a7d1e7e 20220519T17:11:24.2146056Z + [[ 6fc2e0be6a27b6d8230bfa7d47bbb5649a7d1e7e = \\6\\f\\c\\2\\e\\0\\b\\e\\6\\a\\2\\7\\b\\6\\d\\8\\2\\3\\0\\b\\f\\a\\7\\d\\4\\7\\b\\b\\b\\5\\6\\4\\9\\a\\7\\d\\1\\e\\7\\e ]] 20220519T17:11:24.2146693Z + echo 'ERROR: Something has gone wrong and the previous image isn'\\''t available for the mergebase of your branch' 20220519T17:11:24.2147311Z + echo '       contact the PyTorch team to restore the original images' 20220519T17:11:24.2147544Z + exit 1 20220519T17:11:24.2147971Z ERROR: Something has gone wrong and the previous image isn't available for the mergebase of your branch 20220519T17:11:24.2148298Z        contact the PyTorch team to restore the original images 20220519T17:11:24.2151219Z [error]Process completed with exit code 1. 20220519T17:11:24.2319300Z Prepare all required actions 20220519T17:11:24.2337739Z [group]Run ./.github/actions/teardownlinux 20220519T17:11:24.2337955Z with: 20220519T17:11:24.2338091Z env: 20220519T17:11:24.2338244Z   IN_CI: 1 20220519T17:11:24.2338391Z   IS_GHA: 1 20220519T17:11:24.2338555Z [endgroup] 20220519T17:11:24.2354925Z [group]Run .github/scripts/wait_for_ssh_to_drain.sh 20220519T17:11:24.2355244Z [36;1m.github/scripts/wait_for_ssh_to_drain.sh[0m 20220519T17:11:24.2366478Z shell: /usr/bin/bash noprofile norc e o pipefail {0} ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[PyTorch] Avoid initializing storage for empty Optionals,"  CC([PyTorch] Avoid initializing storage for empty Optionals) We don't need to initialize for the nonconstexpr case ever, or in the constexpr case after C++20. Differential Revision: D36519379",2022-05-19T16:51:28Z,Merged cla signed Reverted ciflow/trunk,closed,0,17,https://github.com/pytorch/pytorch/issues/77858,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77858**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 2 New Failures, 3 Base Failures As of commit 8f5b3ca515 (more details on the Dr. CI page): Expand to see more  * **2/5** failures introduced in this PR * **3/5** broken upstream at merge base 5994f0674a on May 19 from  7:04am to 12:59pm   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6734675298?check_suite_focus=true) pull / linuxxenialpy3.7clang7asan / test (default, 2, 5, linux.2xlarge) (1/2) **Step:** ""Test"" (full log  :repeat: rerun)   20220604T00:36:09.6189356Z [error]Process completed with exit code 1.  ``` 20220604T00:36:09.5973498Z   Array cookie:            [1m[31mac[1m[0m 20220604T00:36:09.5973819Z   Intra object redzone:    [1m[33mbb[1m[0m 20220604T00:36:09.5974076Z   ASan internal:           [1m[33mfe[1m[0m 20220604T00:36:09.5974348Z   Left alloca redzone:     [1m[34mca[1m[0m 20220604T00:36:09.5974623Z   Right alloca redzone:    [1m[34mcb[1m[0m 20220604T00:36:09.5974899Z   Shadow gap:              [1m[0mcc[1m[0m 20220604T00:36:09.5975090Z ==87==ABORTING 20220604T00:36:09.6126576Z + cleanup 20220604T00:36:09.6126791Z + retcode=1 20220604T00:36:09.6126997Z + set +x 20220604T00:36:09.6189356Z [error]Process completed with exit code 1. 20220604T00:36:09.6228557Z [group]Run pytorch/pytorch/.github/actions/getworkflowjobid 20220604T00:36:09.6228850Z with: 20220604T00:36:09.6229328Z   githubtoken: *** 20220604T00:36:09.6229545Z env: 20220604T00:36:09.6229727Z   IN_CI: 1 20220604T00:36:09.6229931Z   IS_GHA: 1 20220604T00:36:09.6230157Z   GIT_DEFAULT_BRANCH: master 20220604T00:36:09.6230372Z [endgroup] 20220604T00:36:09.6259431Z [group]Run nickfields/retry 20220604T00:36:09.6259709Z with: ```    :construction: 3 fixed upstream failures: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * pull / linuxxenialpy3.7clang7asan / test (default, 1, 5, linux.2xlarge) on May 19 from  7:04am to 12:18pm (2d4291fb81  5cdf79fddc)     * :repeat: rerun * pull / linuxxenialpy3.7clang7asan / test (default, 5, 5, linux.2xlarge) on May 19 from  7:17am to 11:58am (ec290949aa  007cc731ce)     * :repeat: rerun * pull / linuxxenialpy3.7clang7asan / test (default, 4, 5, linux.2xlarge) on May 19 from  7:17am to 12:59pm (ec290949aa  5cdf79fddc)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ", merge g,(previous test run was green and all I did was fix indentation for lintrunner),Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2423256541," can you confirm that the 4 failures on this diff aren't blocking? Two of them are initializationorderfiasco and look unrelated, one looks like a timeout while building, and one is clearly unrelated backward compat failure.", merge f, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2437199503, merge f, successfully started a merge job. Check the current status here,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""asan builds failed on both PR and trunk, see https://hud.pytorch.org/pytorch/pytorch/commit/17bd683aad6a8aec54a5604c02fd841320981cbf"" c ignoredsignal", successfully started a revert job. Check the current status here,">  can you confirm that the 4 failures on this diff aren't blocking? Two of them are initializationorderfiasco and look unrelated, one looks like a timeout while building, and one is clearly unrelated backward compat failure. I don't know about the previous failures, but latest ones indeed look related, as one can observe in the HUD history: https://hud.pytorch.org/hud/pytorch/pytorch/master/1?name_filter=asan","> >  can you confirm that the 4 failures on this diff aren't blocking? Two of them are initializationorderfiasco and look unrelated, one looks like a timeout while building, and one is clearly unrelated backward compat failure. >  > I don't know about the previous failures, but latest ones indeed look related, as one can observe in the HUD history: https://hud.pytorch.org/hud/pytorch/pytorch/master/1?name_filter=asan I saw those ASAN failures, and they don't look related to me just based on their contents.","...however, the red/green history is pretty conclusive. sigh",looks likely that the change caused a preexisting bug to start getting caught by ASAN. investigating
rag,Running MPS model crash '_mtlIOGPUCommandBufferStorageRebaseShmemHeader'," 🐛 Describe the bug Running model using 'mps' device resulted in some internal crash `Python[4210:2560233] failed assertion false at line 567 in _mtlIOGPUCommandBufferStorageRebaseShmemHeader` without any additional output/info.  Versions PyTorch version: 1.12.0.dev20220518 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.23.1 Libc version: N/A Python version: 3.9.12 (main, Mar 26 2022, 15:44:31)  [Clang 13.1.6 (clang1316.0.21.2)] (64bit runtime) Python platform: macOS12.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.12.0.dev20220518 [pip3] torchaudio==0.11.0 [pip3] torchvision==0.12.0 [conda] Could not collect",2022-05-19T10:09:39Z,needs reproduction triaged module: mps,closed,0,5,https://github.com/pytorch/pytorch/issues/77843,"Hi, Would you have any small code sample that we can use to reproduce this please?","hey that is a big project so its hard to tell what exactly cause internal issue, after some print and run trails I seem to be pinpoint issue to following layer: `nn.Conv1d(dim_in, dim_out, kernel_size=3, padding=1, padding_mode='circular', bias=False)` what is strange it runs successfully few loop iterations and then crashes with the above error. if i remove `padding_mode` argument it seems to run fine",Ok! Can you share what are the values of `dim_in`/`dim_out` and the size of the Tensors you pass as input?,"Yup! dim_in = 8 dim_out = 128 input tensors: (32, 8, 64) or (4, 8, 64)","Hi  . I tried the Conv1d with the input sizes you provided (test case below), and I'm unable to reproduce the issue. Please try with the latest nightly build and reopen the issue if you still see the crash. Thank you. ``` def test_conv1d_circular_padding(self):         y_cpu = torch.randn(32, 8, 64)         conv_cpu = nn.Conv1d(8, 128, kernel_size=3, padding=1, padding_mode='circular', bias=False)         conv_gpu = copy.deepcopy(conv_cpu).to(device='mps')         x_cpu = conv_cpu(y_cpu)         y_gpu = y_cpu.to(device='mps')         x_gpu = conv_gpu(y_gpu)         self.assertEqual(x_cpu, x_gpu.cpu()) ```"
transformer,MPS device appears much slower than CPU on M1 Mac Pro," 🐛 Describe the bug Using MPS for BERT inference appears to produce about a 2x slowdown compared to the CPU. Here is code to reproduce the issue: ```python  MPS Version from transformers import AutoTokenizer, BertForSequenceClassification import timeit import torch tokenizer = AutoTokenizer.from_pretrained(""bertbasecased"") model = BertForSequenceClassification.from_pretrained(""bertbasecased"").to(torch.device(""mps"")) tokens = tokenizer.tokenize(""Hello world, this is michael!"") tids = tokenizer.convert_tokens_to_ids(tokens) with torch.no_grad():     t_tids = torch.tensor([tids]*64, device=torch.device(""mps""))     res = timeit.timeit(lambda: model(input_ids=t_tids), number=100) print(res) ``` `torch.backends.mps.is_available()` reports `True` as expected.  I chose a batch size of 64, since that was mentioned on the blog post for the ~5x speedup in Huggingface BERT.  Running this benchmark, swapping ""mps"" and ""cpu"" for the devices, I get: CPU 18.38 s MPS 36.00 s And, for comparison, running this same snippet on an A10G using CUDA 11.6 on an AWS g5.xlarge instance (ubuntu 20.04, pytorch 1.11.0 compiled from source), I get: CUDA 0.82 s I used the `no_grad()` construction due to the comment mentioned on CC(Memory usage and epoch iteration time increases indefinitely on M1 pro MPS).  I used today's nightly build. Furthermore, I also built PyTorch from source and observed no differences on the results here. Lastly, I confirmed with Activity Monitor and `powermetrics` that the GPU is in fact being used, seeing GPU usage at 98.8%, GPU power spike to 15711 mW when running the MPS benchmark, and CPU power spiking as expected when running the CPU benchmark.  So it does seem like things are ""hooked up"" right, just not efficient.  As a side note, when building PyTorch from source, I observed in the CMake logs that Accelerate.framework was selected as the BLAS library, but sadly I didn't observe any power consumption by the ANE module, which is too bad.  Thanks for all the hard work on this new feature, and looking forward to any insights and further improvements!   Versions ``` PyTorch version: 1.12.0.dev20220518 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.3) CMake version: version 3.23.1 Libc version: N/A Python version: 3.9.12 (main, Mar 26 2022, 15:44:31)  [Clang 13.1.6 (clang1316.0.21.2)] (64bit runtime) Python platform: macOS12.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.942 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] torch==1.12.0.dev20220518 [pip3] torchaudio==0.11.0 [pip3] torchvision==0.12.0 [conda] Could not collect ``` ",2022-05-18T22:23:55Z,module: performance triaged inference mode module: mps,closed,57,82,https://github.com/pytorch/pytorch/issues/77799,"I'd also add that I saw the same effects when running `timeit` with the toy examples in `docs/source/notes/mps.rst`. (e.g.: `timeit.timeit(lambda: x * 2, number=100000)` on both mps/GPU. ","Example of the toy example run: ```python In [17]:  toy example mps     ...: import timeit     ...: import torch     ...: import random     ...:     ...: x = torch.ones(5000, device=""mps"")     ...: timeit.timeit(lambda: x * random.randint(0,100), number=100000) Out[17]: 4.568202124999971 In [18]:  toy example cpu     ...: import timeit     ...: import torch     ...: import random     ...:     ...: x = torch.ones(5000, device=""cpu"")     ...: timeit.timeit(lambda: x * random.randint(0,100), number=100000) Out[18]: 0.30446054200001527 ```",observed the same behavior on M1 Max,same behavior on M1 Pro,"> but sadly I didn't observe any power consumption by the ANE module, which is too bad. The neural engine can't be used for training anyway. It only supports Float16, Int8, and UInt8, and is only accessible through CoreML and MLCompute. PyTorch uses neither of these APIs for training. Could you check in the activity monitor whether it's using multiple CPU cores? If so, it would just be utilizing the AMX to maximum capacity, reaching multiple TFLOPS of compute power on the CPU. Thus, certain models would close the gap in performance between CPU and GPU, making CPU faster.","> The neural engine can't be used for training anyway. It only supports Float16, Int8, and UInt8, and is only accessible through CoreML and MLCompute. PyTorch uses neither of these APIs for training. Very interesting, thanks for the context, I'm among many who are curious about the ANE. I wonder if it could be useful for quantized models, though I suppose this is what Core ML Tools is for. I have checked the activity monitor with the toy multiplication example above and increasing the number of steps to 1e7. I observed:  Running on the CPU device, the toy example completes in 23s, only saturating one core (CPU usage does not exceed 100)  Running on the MPS device, GPU is saturated at 100%, and CPU usage of the python process reaches 175%. Memory usage of the python process increases without end, similar to what was described in CC(Memory usage and epoch iteration time increases indefinitely on M1 pro MPS). kernel_task CPU usage around 3540%, which was not observed on the CPU device. Benchmark takes 441s. ","From  CC(Memory usage and epoch iteration time increases indefinitely on M1 pro MPS)issuecomment1132230314, the GPU may be slower because it spends more of its time writing to new memory than actually executing. Also, since it continues writing to new memory on every iteration, that might destroy cache coherence which brings down performance further. Although if each tensor is deallocated and reallocated on every loop iteration, then this bottleneck would appear regardless of whether there was a memory leak. Metal forces the memory to be overwritten with zeros when you initialize a `MTLBuffer`  something you can sometimes avoid in CUDA.",Fascinating. That hypothesis might also explain why the delta is so much worse with the toy example compared to the full size BERT. ,"I do have a workaround planned for my own framework (s4tf/s4tf) where I will bypass this restriction. It's described in https://github.com/AnarchoSystems/DeepSwift/issues/1issuecomment1129891796, although please don't comment on that thread.  does your backend use `MTLHeap`'s in the way I described to permit zerocost memory allocation? Note: This is still just a theoretical idea. I don't have a proofofconcept of that in action, but it shouldn't be hard to set up.",As of pytorchnightly build `1.13.0.dev20220522`  CPU `18.835263124999983` s  MPS `18.363079624999955` s,"I have another example where mps is roughly 7x slower than cpu, doing nothing but matrix multiplications. I'm on nightly build `1.13.0.dev20220627`: ```python a_cpu = torch.rand(1000, device='cpu') b_cpu = torch.rand((1000, 1000), device='cpu') a_mps = torch.rand(1000, device='mps') b_mps = torch.rand((1000, 1000), device='mps') print('cpu', timeit.timeit(lambda: a_cpu @ b_cpu, number=100_000)) print('mps', timeit.timeit(lambda: a_mps @ b_mps, number=100_000)) ``` Prints: ``` cpu 3.126431 mps 23.259843875 ```","Interestingly, if I do the same experiment as above using smaller tensors, the disparity grows to mps being roughly 25x slower: ```python a_cpu = torch.rand(250, device='cpu') b_cpu = torch.rand((250, 250), device='cpu') a_mps = torch.rand(250, device='mps') b_mps = torch.rand((250, 250), device='mps') print('cpu', timeit.timeit(lambda: a_cpu @ b_cpu, number=100_000)) print('mps', timeit.timeit(lambda: a_mps @ b_mps, number=100_000)) ``` Prints: ``` cpu 0.8596386669999999 mps 19.975668917 ```","I have observed the same problem, on an AMD GPU, got a 3x slow down compare to CPU. Any fixes available yet?","I wonder if we could convert this to actual MPS code in Swift, then profile how long that code takes. That would determine whether the bottleneck is PyTorch’s fault and something that can be fixed. From a distance, it looks like a fundamental problem of GPU communication overhead. Is your timing mechanism taking the number of seconds for one iteration, then multiplying it by 100,000?","another datapoint here: (nightly build `1.13.0.dev20220704`, i77920HQ / Radeon Pro 560) running the two matrix multiplication tasks aboveissuecomment1168102637:) ``` cpu 4.141497317003086 mps 102.30748211298487 ``` ``` cpu 0.7337763879913837 mps 55.41895890497835 ```","  I noticed this ticket's been subject to triage, and a few other folks have filed issues regarding similar observations.  Do you know how the figure in the press release (about large MPS performance benefits) was computed? ",This might be why it’s so slow. Worth reading if you have the time. https://discuss.pytorch.org/t/sequentialthroughputofgpuexecution/156303,">   I noticed this ticket's been subject to triage, and a few other folks have filed issues regarding similar observations. >  > Do you know how the figure in the press release (about large MPS performance benefits) was computed? Hi  , those numbers were collected on a M1 Ultra system using torchbench with the Batch size as listed in the figure. The command used was: ``` pytest test_bench.py k ""resnet50 or hf_Bert or vgg16"" ignore_machine_config ``` and compare the MPS time with CPU time. ``` Testing conducted by Apple in April 2022 using production Mac Studio systems with Apple M1 Ultra, 20core CPU, 64core GPU 128GB of RAM, and 2TB SSD. Tested with macOS Monterey 12.3, prerelease PyTorch 1.12, ResNet50 (batch size=128), HuggingFace BERT (batch size=64), and VGG16 (batch size=64). Performance tests are conducted using specific computer systems and reflect the approximate performance of Mac Studio. ```"," Thanks for the great read.  thanks for sharing, I'll try running that on my system, and I'm curious to dig in, because when I ran my own benchmarks with HuggingFace's BERT on an MPS device on an M1 Pro it was dramatically slower, so maybe I can dig through the benchmark scripts and see if I've got something wrong. And my M1 Studio is on order 😄 ",">  thanks for sharing, I'll try running that on my system, and I'm curious to dig in, because when I ran my own benchmarks with HuggingFace's BERT on an MPS device on an M1 Pro it was dramatically slower, so maybe I can dig through the benchmark scripts and see if I've got something wrong. >  > And my M1 Studio is on order 😄  can you try with latest nightly how the performance looks like? Also are you running the mentioned network : ```  MPS Version from transformers import AutoTokenizer, BertForSequenceClassification import timeit import torch tokenizer = AutoTokenizer.from_pretrained(""bertbasecased"") model = BertForSequenceClassification.from_pretrained(""bertbasecased"").to(torch.device(""mps"")) tokens = tokenizer.tokenize(""Hello world, this is michael!"") tids = tokenizer.convert_tokens_to_ids(tokens) with torch.no_grad():     t_tids = torch.tensor([tids]*64, device=torch.device(""mps""))     res = timeit.timeit(lambda: model(input_ids=t_tids), number=100) print(res) ``` to do your measurement.?",Device: M1 Pro 10CPU 16GPU Training my model using mps: ``` 0m 2s (Time Remaining: 4167m 10s) (epoch: 1 finish: 0%) 0m 5s (Time Remaining: 4838m 36s) (epoch: 2 finish: 0%) 0m 8s (Time Remaining: 4894m 53s) (epoch: 3 finish: 0%) 0m 13s (Time Remaining: 5462m 44s) (epoch: 4 finish: 0%) ``` Training my model using CPU: ``` 0m 0s (Time Remaining: 945m 23s) (epoch: 1 finish: 0%) 0m 1s (Time Remaining: 879m 47s) (epoch: 2 finish: 0%) 0m 1s (Time Remaining: 933m 46s) (epoch: 3 finish: 0%) 0m 2s (Time Remaining: 1024m 4s) (epoch: 4 finish: 0%) ```,"> Device: M1 Pro 10CPU 16GPU >  > Training my model using mps: >  > ``` > 0m 2s (Time Remaining: 4167m 10s) (epoch: 1 finish: 0%) > 0m 5s (Time Remaining: 4838m 36s) (epoch: 2 finish: 0%) > 0m 8s (Time Remaining: 4894m 53s) (epoch: 3 finish: 0%) > 0m 13s (Time Remaining: 5462m 44s) (epoch: 4 finish: 0%) > ``` >  > Training my model using CPU: >  > ``` > 0m 0s (Time Remaining: 945m 23s) (epoch: 1 finish: 0%) > 0m 1s (Time Remaining: 879m 47s) (epoch: 2 finish: 0%) > 0m 1s (Time Remaining: 933m 46s) (epoch: 3 finish: 0%) > 0m 2s (Time Remaining: 1024m 4s) (epoch: 4 finish: 0%) > ```  , can you please provide a testcase to repro this? Also what torch nightly version you are using for these numbers?",  Repo: https://github.com/realJustinLee/dumbchatbot ``` torch==1.13.0.dev20220715 torchaudio==0.14.0.dev20220603 torchvision==0.14.0.dev20220715 ```,">  MPS Version > from transformers import AutoTokenizer, BertForSequenceClassification > import timeit > import torch >  > tokenizer = AutoTokenizer.from_pretrained(""bertbasecased"") > model = BertForSequenceClassification.from_pretrained(""bertbasecased"").to(torch.device(""mps"")) >  > tokens = tokenizer.tokenize(""Hello world, this is michael!"") > tids = tokenizer.convert_tokens_to_ids(tokens) > with torch.no_grad(): >     t_tids = torch.tensor([tids]*64, device=torch.device(""mps"")) >     res = timeit.timeit(lambda: model(input_ids=t_tids), number=100) > print(res) CPU M1 8Core 16G RAM Using `mps` : `65.45052295800001` Using `cpu` :  `24.685709957999997`",Same here for LSTM examples. I used Torch Profiler to see the performance difference: CPU:  MPS: ,"The spike in microsecondlevel overhead (CPU time avg) was discussed hereissuecomment1204672455). I think I’ve found a solution to it, but haven’t put it into practice with an RNN.","> The spike in microsecondlevel overhead (CPU time avg) was discussed hereissuecomment1204672455). I think I’ve found a solution to it, but haven’t put it into practice with an RNN. Any recent plan to implement it? Looking forward to it.","> Any recent plan to implement it? I’m not planning to implement it in PyTorch; however, it’s open source and I’ve explained it in great depth. Someone else could look at it and reimplement it in PyTorch. If anyone does,  me and I can take a look. I just wanted to showcase the hard work and months of planning that went into these optimizations of driver overhead in my project. They are showing realworld performance gains that could make it significantly faster than PyTorch (at least with the current MPS backend) for certain use cases. Current repo: https://github.com/philipturner/metalexperiment1 Will eventually migrate to: https://github.com/s4tf/metal","Regarding a working RNN implementation, it probably won't happen any time within the next few weeks. I'm juggling a bunch of other projects simultaneously, so things will happen slowly regarding the Metal backend. I need to fix an existing bug in constant folding and support `MPSKernel` caching for an `MPSMatrixMultiplication`. Perhaps I also need to support some NDArray ops, which are only exposed with reasonable overhead in `MPSGraph`.","Device: M1 Pro 8CPU 14GPU Just to add some more data points, using the nightly build 1.13.0.dev20220818. **Experiment 1** Running the following code: ``` a_cpu = torch.rand(1000, device='cpu') b_cpu = torch.rand((1000, 1000), device='cpu') a_mps = torch.rand(1000, device='mps') b_mps = torch.rand((1000, 1000), device='mps') print('cpu', timeit.timeit(lambda: a_cpu @ b_cpu, number=100_000)) print('mps', timeit.timeit(lambda: a_mps @ b_mps, number=100_000)) ``` I get the following results: ``` cpu 2.9181434169999996 mps 13.219588082999998 ``` **Experiment 2** Running the following code: ``` a_cpu = torch.rand(250, device='cpu') b_cpu = torch.rand((250, 250), device='cpu') a_mps = torch.rand(250, device='mps') b_mps = torch.rand((250, 250), device='mps') print('cpu', timeit.timeit(lambda: a_cpu @ b_cpu, number=100_000)) print('mps', timeit.timeit(lambda: a_mps @ b_mps, number=100_000)) ``` I get the following results: ``` cpu 0.7114390410000624 mps 12.975056834000043 ```"
yi,TypeError: Trying to convert Double to the MPS backend but there is no mapping for it.," 🐛 Describe the bug Steps to reproduce: ``` import torch mps_device = torch.device(""mps"") z = torch.ones(5, device=mps_device) z = torch.ones(5, device=mps_device, dtype=float) Traceback (most recent call last):   File """", line 1, in  TypeError: Trying to convert Double to the MPS backend but there is no mapping for it. z = torch.ones(5, device=mps_device, dtype=torch.float32) z = torch.ones(5, device=mps_device, dtype=torch.float64) Traceback (most recent call last):   File """", line 1, in  TypeError: Trying to convert Double to the MPS backend but there is no mapping for it. ```  Versions PyTorch version: 1.12.0.dev20220518 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.5) CMake version: version 3.23.1 Libc version: N/A Python version: 3.9.9  (main, Dec 20 2021, 02:41:06)  [Clang 11.1.0 ] (64bit runtime) Python platform: macOS12.3arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.0 [pip3] torch==1.12.0.dev20220518 [conda] numpy                     1.22.0           py39h61a45d2_0    condaforge [conda] pytorch                   1.12.0.dev20220518         py3.9_0    pytorchnightly",2022-05-18T20:02:58Z,triaged actionable module: mps,closed,0,6,https://github.com/pytorch/pytorch/issues/77781,Hi! I am afraid that the MPS framework does not support double precision numbers. You can use `float32` though. We should improve that error message to clarify this (and potentially link to the metal doc referencing this),ah gotcha  I suppose implicit conversion would result in unexpected behavior.,"I'm getting strange behavior casting to `torch.float32` ``` >>> import torch >>> from torch import tensor >>> import numpy as np >>> a = np.ones(10) ``` Converting to a tensor works as expected ``` >>> tensor(a) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64) ``` And fails as expected without casting ``` >>> tensor(a, device='mps') Traceback (most recent call last):   File """", line 1, in    File ""/Users/normar1/miniconda3/envs/naie/lib/python3.9/sitepackages/torch/_tensor.py"", line 341, in __repr__     return torch._tensor_str._str(self, tensor_contents=tensor_contents)   File ""/Users/normar1/miniconda3/envs/naie/lib/python3.9/sitepackages/torch/_tensor_str.py"", line 477, in _str     return _str_intern(self, tensor_contents=tensor_contents)   File ""/Users/normar1/miniconda3/envs/naie/lib/python3.9/sitepackages/torch/_tensor_str.py"", line 443, in _str_intern     tensor_str = _tensor_str(self, indent)   File ""/Users/normar1/miniconda3/envs/naie/lib/python3.9/sitepackages/torch/_tensor_str.py"", line 270, in _tensor_str     formatter = _Formatter(get_summarized_data(self) if summarize else self)   File ""/Users/normar1/miniconda3/envs/naie/lib/python3.9/sitepackages/torch/_tensor_str.py"", line 103, in __init__     nonzero_finite_vals = torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)) TypeError: Trying to convert Double to the MPS backend but there is no mapping for it. ``` However, this is where it gets weird: if I force the dtype, I get strange values ``` >>> tensor(a, device='mps', dtype=torch.float32) tensor([0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8750e+00,         0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8750e+00], device='mps:0') ``` and if I use another method (`from_numpy` I think is recommended for conversion to tensors) ``` >>> torch.from_numpy(a).to('mps',dtype=torch.float32) tensor([0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8750e+00,         0.0000e+00, 1.8750e+00, 0.0000e+00, 1.8750e+00], device='mps:0') ``` but it works if I cast before converting to tensor ``` >>> torch.from_numpy(a.astype(np.float32)).to('mps') tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00], device='mps:0') ``` Looks like I will be doing the numpy casting beforehand. Am I doing something obviously wrong here? ",This comment might be particularly relevant: https://discuss.pytorch.org/t/codedidntspeedupasexpectedwhenusingmps/152016/6?u=philipturner,"Hi, The PR linked above is going to clarify the double situation. The issue when converting to float via `.to()` is known and is already fixed in master.",The error message is now properly updated in master.
rag,General MPS op coverage tracking issue," This issue is to have a centralized place to list and track work on adding support to new ops for the MPS backend. **PyTorch MPS Ops Project** : Project to track all the ops for MPS backend. There are a very large number of operators in pytorch and so they are not all yet implemented. We will be prioritizing adding new operators based on user feedback. If possible, please also provide link to the network or usecase where this op is getting used. As Ops are requested we will add "" *To Triage*"" pool. If we have 3+ requests for an operation and given its complexity/need the operation will be moved ""*To be implemented*"" pool. If you want to work on adding support for such op, feel free to comment below to get assigned one. Please avoid pickup up an op that is already being worked on tracked in ""*In progress*"" pool.  Link to the wiki for details on how to add these ops and example PRs. **MPS operators coverage matrix**  The matrix covers most of the supported operators but is not exhaustive. **Please look at the `In vx.x.x` column, if the box is green, it means that the op implementation is included in the latest release; on the other hand, if the box is yellow, it means the op implementation is in the nightly and has not yet included in the latest release.** Before you comment below, please take a look at this matrix to make sure the operator you're requesting has not been implemented in nightly.  More details can be found on the readme. ",2022-05-18T18:12:47Z,feature triaged tracker module: mps,open,790,1618,https://github.com/pytorch/pytorch/issues/77764,"Are there any linear algebra ops not implemented in MPS that you have made custom shaders for? Any shaders I could ""borrow"" from your project (with full credit) and use in my own? Specifically, it would be helpful to have SVD and reversemode Cholesky operators.","Hey, There are no custom shaders at the moment as everything we needed for the basic networks we looked at was already provided by MPS (or a set of ops in MPS). Also , required functions that are not in the hot path are simply falling back to CPU for now. It is mentioned here as this is something that is possible to be done easily within the integration. But not something that is used today.","I was testing a bunch of speech synthesis and vocoder models, and found the following operators missing so far:  `aten::flip`  `aten::equal`   `aten::upsample_nearest1d.out`","One vote for a CPU fallback for `torch.bincount`. Is there any reason, given the unified memory architecture, that every op not implemented on Metal cannot just fall back to the CPU implementation without memory copy operations? (Based, of course, on my 10,000ft view of the architecture, which I'm sure is wildly oversimplified.)","Tip for everyone: Run your script with PYTORCH_ENABLE_MPS_FALLBACK=1 which will fallback to the CPU. I'm using a custom build which merges pull request CC(MPS Fixes: copy operations, addmm and baddmm) so am not sure if this is included in the current build (Edit: It's not. You need to build PyTorch yourself with the pull request or trust an online build with it).",Testing with some huggingface transformers code: + 1 vote for `aten::cumsum.out` Tried with the fallback env var but doesn't seem to work for me.,"One missing op I ran into and haven't seen mentioned yet is `aten::_unique2`. Edit: This error goes away when passing `PYTORCH_ENABLE_MPS_FALLBACK=1` when using the current `main` branch build. However, instead I get warnings ``` The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/lukas/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.) ``` then ``` The dst MTL buffer in copy_to_mps is noncontiguous (Triggered internally at  /Users/lukas/pytorch/aten/src/ATen/native/mps/operations/Copy.mm:323.) ``` and finally the forward pass through my model crashes with ``` RuntimeError: Placeholder buffer size (7493632) is not large enough to contain the Tensor storage of size 14986944 ``` On `cpu` it works fine. Could be CC(buffer is not large enough when running pytorch on Mac M1 mps) I suppose.","> Testing with some huggingface transformers code: + 1 vote for `aten::cumsum.out` > Tried with the fallback env var but doesn't seem to work for me. +1  setting `PYTORCH_ENABLE_MPS_FALLBACK=1` still results in: ``` NotImplementedError: Could not run 'aten::cumsum.out' with arguments from the 'MPS' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::cumsum.out' is only available for these backends: [Dense, Conjugate, UNKNOWN_TENSOR_TYPE_ID, QuantizedXPU, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseCPU, SparseCUDA, SparseHIP, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseXPU, UNKNOWN_TENSOR_TYPE_ID, SparseVE, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, NestedTensorCUDA, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID]. CPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:37386 [kernel] Meta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:31637 [kernel] BackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback] Python: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:133 [backend fallback] Named: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel] Conjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback] Negative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback] ZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback] ADInplaceOrView: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:3288 [kernel] AutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] UNKNOWN_TENSOR_TYPE_ID: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] UNKNOWN_TENSOR_TYPE_ID: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] AutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:13238 [autograd kernel] Tracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:12585 [kernel] AutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:481 [backend fallback] Autocast: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback] Batched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback] VmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback] Functionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:12118 [kernel] PythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:137 [backend fallback] ```", could you open a new separate issue for the cpu fallback failing for you? The error seems to hint at the fact that you're doing moving across device noncontiguous Tensor. Making sure they are might help as a workaround. We can continue this discussion in the new issue you will create. Zhang the fallback is ONLY available if you build from source right now. It will be in the nightly build tomorrow (May 21st).,"Would like to add `aten::_local_scalar_dense` to the list. Also, is it possible to link to some examples in the top post on how we can implement these into Pytorch? I'd love to give it a shot if it's not too hard."," Yep, making the Tensors contiguous worked. But yet another issue revealed itself. I created CC(CPU fallback for `aten::index.Tensor` on MPS crashes or gives wrong result) and CC(MPS backend has problems with printing noncontiguous tensors).","I've got a non supported op: `aten::grid_sampler_2d` ``` envs/pytorchenv/lib/python3.9/sitepackages/torch/nn/functional.py:4172: UserWarning: The operator 'aten::grid_sampler_2d' is not currently supported on the MPS backend and will fall back to run on the CPU. This may performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)   return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners) ```","Not supported  `aten::l1_loss_backward.grad_input`  `aten::kl_div_backward` Code ```python X, y = torch.rand(16, 10).to(""mps""), torch.rand(16, 1).to(""mps"") model = nn.Linear(10, 1).to(""mps"") criterion = nn.L1Loss()  nn.KLDivLoss() loss = criterion(model(X), y) loss.backward() ``` Output ``` NotImplementedError: Could not run 'aten::l1_loss_backward.grad_input' with arguments from the 'MPS' backend ```","Trying to use affine crop from torchvision, and found the operator `aten::linspace.out` does not seem to be implemented with the MPS backend","Trying to use MPS backend with pytorch geometric, and found the operator `aten::index.Tensor` is not yet implemented.",Found the operator 'aten::grid_sampler_2d' is not current implemented for the MPS device.,Would be great to add `aten::adaptive_max_pool2d` to the list  seems to be fairly common and for me useful in some point cloud architectures.,I ran into this error with `aten::count_nonzero.dim_IntList` (via `torch.count_nonzero()`). I'll take a look at implementing this op with MPS.,The operator `aten::lgamma.out` is curently not yet implemented either.,"Hello, the operator Linspace is not implemented, for you my error message:  NotImplementedError                       Traceback (most recent call last) Input In [2], in ()       7 device = torch.device(""mps"")       9  Create random input and output data > 10 x = torch.linspace(math.pi, math.pi, 2000, device=device, dtype=dtype)      11 y = torch.sin(x)      13  Randomly initialize weights NotImplementedError: The operator 'aten::linspace.out' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on  CC(General MPS op coverage tracking issue). As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS. Thank you !","I would like to add `aten::linalg_householder_product` Using orthogonal parametrization with `PYTORCH_ENABLE_MPS_FALLBACK=1`. I get:  ``` Q = torch.linalg.householder_product(A, tau) loc(""mps_multiply""(""(mpsFileLoc): /AppleInternal/Library/BuildRoots/560148d7a55911ec8c964add460b61a6/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm"":219:0)):  error: input types 'tensor' and 'tensor' are not broadcast compatible ```","Hi, please consider `aten::avg_pool3d.out`.",The operator `aten::erfinv.out` is not implemented.,The operator `aten::logical_and.out` is not current implemented for the MPS device.,The operator `aten::bitwise_and.Tensor_out` is not yet implemented for the MPS backend.,The operator 'aten::_slow_conv2d_forward' is not currently implemented for the MPS device. Also found this: NotImplementedError: Could not run 'aten::_copy_from_and_resize' with arguments from the 'CPU' backend. after enacting the PYTORCH_ENABLE_MPS_FALLBACK=1 env variable.,Got a message that `aten::softplus.out` is not supported. I'd need that to update OpenPifPaf.,"> Would like to add `aten::_local_scalar_dense` to the list. Also, is it possible to link to some examples in the top post on how we can implement these into Pytorch? I'd love to give it a shot if it's not too hard. You can use this as a guide: https://github.com/pytorch/pytorch/wiki/AddingOpforMPSBackend Please provide feedback if there is anything missing.","> Would like to add `aten::_local_scalar_dense` to the list. Also, is it possible to link to some examples in the top post on how we can implement these into Pytorch? I'd love to give it a shot if it's not too hard. MPS backend already has support for `aten::_local_scalar_dense` (file https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/mps/operations/Scalar.mm). If you are still seeing the issue, could you please share the example you are trying to run? ","The operator 'aten::_index_put_impl_' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature,"
yi,Allow specifying alias analysis while registering new ops,Stack from ghstack:  CC(Allow specifying alias analysis while registering new ops),2022-05-17T22:17:19Z,Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/77690,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77690**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit b7b9e75d19 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6512543659?check_suite_focus=true) pull / linuxbionicrocm5.1py3.7 / test (default, 2, 2, linux.rocm.gpu) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220519T21:12:35.0491658Z RuntimeError: test_sparse_csr failed!  ``` 20220519T21:12:31.5315588Z  20220519T21:12:31.5315895Z Generating XML reports... 20220519T21:12:31.8028373Z Generated XML report: testreports/pythonunittest/test_sparse_csr/TESTTestSparseCSRCUDA20220519211157.xml 20220519T21:12:31.8030663Z Generated XML report: testreports/pythonunittest/test_sparse_csr/TESTTestSparseCSRSampler20220519211157.xml 20220519T21:12:31.8615418Z Generated XML report: testreports/pythonunittest/test_sparse_csr/TESTTestSparseCompressedCUDA20220519211157.xml 20220519T21:12:35.0476802Z Traceback (most recent call last): 20220519T21:12:35.0477780Z   File ""test/run_test.py"", line 1074, in  20220519T21:12:35.0484587Z     main() 20220519T21:12:35.0485293Z   File ""test/run_test.py"", line 1052, in main 20220519T21:12:35.0490816Z     raise RuntimeError(err_message) 20220519T21:12:35.0491658Z RuntimeError: test_sparse_csr failed! 20220519T21:12:37.5408058Z  20220519T21:12:37.5408928Z real	85m30.265s 20220519T21:12:37.5409752Z user	123m19.012s 20220519T21:12:37.5410386Z sys	11m43.870s 20220519T21:12:37.5410996Z + cleanup 20220519T21:12:37.5411547Z + retcode=1 20220519T21:12:37.5412110Z + set +x 20220519T21:12:37.5523168Z [error]Process completed with exit code 1. 20220519T21:12:37.5592565Z [group]Run  copy test results back to the mounted workspace, needed sudo, resulting permissions were correct 20220519T21:12:37.5593568Z [36;1m copy test results back to the mounted workspace, needed sudo, resulting permissions were correct[0m ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",added another decorator to define and impl in the same line since you approved  ,decorator seems fine, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[LT] Add IR resuing support for manually-implemented ops,Stack from ghstack:  CC([LT] Add IR resuing support for manuallyimplemented ops) Summary: Add CanBeReused methods for manuallyimplemented ops and replace MakeNode with ReuseOrMakeNode.,2022-05-17T00:38:22Z,Merged cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/77616,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77616**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 2156113415 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," , this PR only touches ops for ts_backend, but can answer your earlier question about how to handle manuallyimplemented ops.", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Add support for TxT mask layout for masked_softmax in BetterTransformer,Summary: Expand mask to BxHxDxD when mask is DxD layout Test Plan: buck build mode/opt c fbcode.platform=platform009 c fbcode.enable_gpu_sections=true caffe2/test:nn && buckout/opt/gen/caffe2/test/nn\\binary.par r masked_softmax_DxD Differential Revision: D36428170,2022-05-16T23:07:31Z,fb-exported Merged cla signed topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/77607,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77607**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit ddafd20dc1 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36428170,This pull request was **exported** from Phabricator. Differential Revision: D36428170,This pull request was **exported** from Phabricator. Differential Revision: D36428170,This pull request was **exported** from Phabricator. Differential Revision: D36428170, merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Failed to run on iOS - Couldn't find an operator for `aten::conv1d`," 🐛 Describe the bug Hi, I'm testing one of my projects with `libtorch 1.11.0` for iOS simulator and I'm facing the following issue. ```python Caused by:     Internal torch error:     Couldn't find an operator for aten::conv1d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) > Tensor. Do you have to update a set of hardcoded JIT ops?     The above operation failed shape propagation in this context:     /home/aalvarez/Projects/main/apps/lmrescore/tracegpt2.py(32): forward     /home/aalvarez/.virtualenvs/lmrescoreu7Iy2escpy3.10/lib/python3.10/sitepackages/torch/nn/modules/module.py(1092): _slow_forward     /home/aalvarez/.virtualenvs/lmrescoreu7Iy2escpy3.10/lib/python3.10/sitepackages/torch/nn/modules/module.py(1108): _call_impl     /home/aalvarez/.virtualenvs/lmrescoreu7Iy2escpy3.10/lib/python3.10/sitepackages/torch/jit/_trace.py(939): trace_module     /home/aalvarez/.virtualenvs/lmrescoreu7Iy2escpy3.10/lib/python3.10/sitepackages/torch/jit/_trace.py(735): trace     /home/aalvarez/Projects/main/apps/lmrescore/tracegpt2.py(45):      Serialized   File ""code/__torch__.py"", line 17         transformer = model0.transformer         position_ids = self.position_ids         _0 = ops.prim.NumToTensor(torch.size(input_ids, 1))                                   ~~~~~~~~~~  const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:autocast_mode.cpp.o: 00000000000178d6 T at::autocast::WrapFunction_ const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long), &(at::conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long)), at::Tensor, c10::guts::typelist::typelist const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long> >::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:autocast_mode.cpp.o: 0000000000002034 T at::autocast::WrapFunction_ const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long), &(at::conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long)), at::Tensor, c10::guts::typelist::typelist const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long> >::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:Convolution.cpp.o: 0000000000002556 T at::native::conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:Convolution.cpp.o: 0000000000001048 T at::native::conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000010d110 b guard variable for at::_ops::conv1d_padding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long)::op libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000010d0f8 b guard variable for at::_ops::conv1d_padding::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long)::op libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000010d0b0 b guard variable for at::_ops::conv1d::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long)::op libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000010d098 b guard variable for at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long)::op libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 0000000000007c9e T at::_ops::conv1d_padding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 0000000000007b5a T at::_ops::conv1d_padding::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 00000000000076da T at::_ops::conv1d::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000000758c T at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000000767c t at::_ops::create_conv1d_typed_handle() libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 0000000000007c4a t at::_ops::create_conv1d_padding_typed_handle() libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000010d100 b at::_ops::conv1d_padding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long)::op libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000010d0e8 b at::_ops::conv1d_padding::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long)::op libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000010d0a0 b at::_ops::conv1d::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long)::op libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000010d088 b at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long)::op libtorch1.11.0iosfat/lib/libtorch_cpu.a:RegisterCompositeImplicitAutograd.cpp.o: 0000000000001f07 t at::(anonymous namespace)::(anonymous namespace)::wrapper__conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:RegisterCompositeImplicitAutograd.cpp.o: 0000000000002045 t at::(anonymous namespace)::(anonymous namespace)::wrapper_padding_conv1d_padding(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:RegisterCompositeImplicitAutograd.cpp.o: 0000000000002010 T at::compositeimplicitautograd::conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:RegisterCompositeImplicitAutograd.cpp.o: 0000000000001ed2 T at::compositeimplicitautograd::conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:RegisterCompositeImplicitAutograd.cpp.o:                  U at::native::conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::basic_string_view, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:RegisterCompositeImplicitAutograd.cpp.o:                  U at::native::conv1d(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:helper.cpp.o: 0000000000003424 T torch::jit::is_conv1d_module(torch::jit::Match const&, std::__1::unordered_map, std::__1::allocator >, torch::jit::Value*, std::__1::hash, std::__1::allocator > >, std::__1::equal_to, std::__1::allocator > >, std::__1::allocator, std::__1::allocator > const, torch::jit::Value*> > > const&) libtorch1.11.0iosfat/lib/libtorch_cpu.a:insert_observers.cpp.o:                  U torch::jit::is_conv1d_module(torch::jit::Match const&, std::__1::unordered_map, std::__1::allocator >, torch::jit::Value*, std::__1::hash, std::__1::allocator > >, std::__1::equal_to, std::__1::allocator > >, std::__1::allocator, std::__1::allocator > const, torch::jit::Value*> > > const&) libtorch1.11.0iosfat/lib/libtorch_cpu.a:external_functions.cpp.o:                  U at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) libtorch1.11.0iosfat/lib/libtorch_cpu.a:external_functions.cpp.o: 0000000000003f20 T _nnc_aten_conv1d libtorch1.11.0iosfat/lib/libtorch_cpu.a:external_functions.cpp.o: 000000000000226b T _nnc_aten_quantized_conv1d ``` Essentially, `at::_ops::conv1d::call` is found ``` libtorch1.11.0iosfat/lib/libtorch_cpu.a:Operators_3.cpp.o: 000000000000758c T at::_ops::conv1d::call(at::Tensor const&, at::Tensor const&, c10::optional const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, long long) ```  Also in the linking I `force_load` libraries:  ```bash      external/libtorch_cpu_ios_fl/lib/libtorch.a \\     external/libtorch_cpu_ios_fl/lib/libtorch_cpu.a \\     external/libtorch_cpu_ios_fl/lib/libc10.a \\     external/libtorch_cpu_ios_fl/lib/libclog.a \\     external/libtorch_cpu_ios_fl/lib/libcpuinfo.a \\     external/libtorch_cpu_ios_fl/lib/libpthreadpool.a \\     external/libtorch_cpu_ios_fl/lib/libpytorch_qnnpack.a \\     external/libtorch_cpu_ios_fl/lib/libXNNPACK.a \\ ...     force_load \\     external/libtorch_cpu_ios_fl/lib/libtorch.a \\     force_load \\     external/libtorch_cpu_ios_fl/lib/libtorch_cpu.a \\     force_load \\     external/libtorch_cpu_ios_fl/lib/libc10.a \\     force_load \\     external/libtorch_cpu_ios_fl/lib/libpytorch_qnnpack.a \\     force_load \\     external/libtorch_cpu_ios_fl/lib/libXNNPACK.a \\ ``` The compilation was done with the following confguration: ```json     ""cacheVariables"": {         ""CMAKE_BUILD_TYPE"": ""MinSizeRel"",         ""CMAKE_CXX_COMPILER_LAUNCHER"": {           ""type"": ""FILEPATH"",           ""value"": ""/Users/xdev/.cargo/bin/sccache""         },         ""CMAKE_C_COMPILER_LAUNCHER"": {           ""type"": ""FILEPATH"",           ""value"": ""/Users/xdev/.cargo/bin/sccache""         },         ""CMAKE_TOOLCHAIN_FILE"": {           ""type"": ""FILEPATH"",           ""value"": ""${sourceDir}/cmake/iOS.cmake""         },         ""CMAKE_THREAD_LIBS_INIT"": ""lpthread"",         ""CMAKE_HAVE_THREADS_LIBRARY"": ""1"",         ""CMAKE_USE_PTHREADS_INIT"": ""1"",         ""PYTHON_EXECUTABLE"": {           ""type"": ""FILEPATH"",           ""value"": ""python3""         },         ""CMAKE_CXX_FLAGS"": ""fobjcarc"",         ""TRACING_BASED"": false,         ""BUILD_BINARY"": false,         ""BUILD_CUSTOM_PROTOBUF"": false,         ""BUILD_LITE_INTERPRETER"": false,         ""BUILD_PYTHON"": false,         ""BUILD_SHARED_LIBS"": false,         ""BUILD_TEST"": false,         ""USE_CUDA"": false,         ""USE_GFLAGS"": false,         ""USE_LEVELDB"": false,         ""USE_LITE_INTERPRETER_PROFILER"": false,         ""USE_LMDB"": false,         ""USE_MKLDNN"": false,         ""USE_MPI"": false,         ""USE_NNPACK"": false,         ""USE_NUMPY"": false,         ""USE_OPENCV"": false,         ""IOS_PLATFORM"": ""SIMULATOR"",         ""IOS_ARCH"": ""x86_64"",         ""CMAKE_CXX_FLAGS"": ""fobjcarc miossimulatorversionmin=15.0"",         ""CMAKE_C_FLAGS"": ""fobjcarc miossimulatorversionmin=15.0""    } ```  Versions ```zsh Collecting environment information... PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: macOS 12.3 (x86_64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2) CMake version: version 3.23.1 Libc version: N/A Python version: 3.9.12 (main, May  8 2022, 18:05:47)  [Clang 13.1.6 (clang1316.0.21.2)] (64bit runtime) Python platform: macOS12.3x86_64i38664bit Is CUDA available: N/A CUDA runtime version: Could not collect GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: N/A Versions of relevant libraries: [pip3] numpy==1.22.3 [conda] Could not collect ```",2022-05-16T13:04:25Z,triaged module: ios,open,0,3,https://github.com/pytorch/pytorch/issues/77538,On feedback only versi run on iOS,"Sorry, I didn't get it. What do you mean?","Hi again, I still cannot solve this error. Any ideas?"
rag,SegFault after average pooling quantization," 🐛 Describe the bug After adding `ceil_mode=True` in AveragePooling given code return segfault. ```python3 import torch import torch.nn as nn if __name__ == '__main__':     model = nn.Sequential(         torch.quantization.QuantStub(),         nn.Conv1d(40, 40, 5, stride=1, padding=2),         nn.ReLU(),         nn.AvgPool1d(32, ceil_mode=True),         torch.quantization.DeQuantStub()     )     model.eval()     torch.backends.quantized.engine = 'qnnpack'     model.qconfig = torch.quantization.get_default_qconfig('qnnpack')     quant_prepared = torch.quantization.prepare(model)     input_data = torch.randn((32, 40, 32))     quant_prepared(input_data)     quant_int8 = torch.quantization.convert(quant_prepared)     input_data = torch.randn((32, 40, 16))     print(quant_int8.forward(input_data).shape) ```  Versions [pip3] mypy==0.942 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.3 [pip3] torch==1.11.0 [conda] Could not collect ",2022-05-15T20:41:48Z,oncall: quantization,closed,0,6,https://github.com/pytorch/pytorch/issues/77511,MaxPooling in that case works fine,Taking a look,"  I'm able to reproduce your error on a local build. Are you implying `ceil_mode = false` worked for you?  using your script, when `ceil_mode = false`, I get the following error ``` Traceback (most recent call last):   File ""/data/users/dzdang/pytorch/oncall.py"", line 21, in      output = quant_int8.forward(input_data)   File ""/data/users/dzdang/pytorch/torch/nn/modules/container.py"", line 139, in forward     input = module(input)   File ""/data/users/dzdang/pytorch/torch/nn/modules/module.py"", line 1129, in _call_impl     return forward_call(*input, **kwargs)   File ""/data/users/dzdang/pytorch/torch/nn/modules/pooling.py"", line 542, in forward     return F.avg_pool1d( RuntimeError: qnnpack_avg_pool2d(): the resulting output Tensor size should be >= 0 ``` I think the two errors are related, so I can look into this further, but prior to doing so, I'm wondering if your sizes in `input_data = torch.randn((32, 40, 16))` are correct? I noticed you calibrated with `[32, 40, 32]` and you also use `32` for the kernel size, but you're doing inference on a different size (16) for the last dimension.  if you run your script with `valgrind`, you'll see it's segfaulting in `pytorch_qnnp_indirection_init_dwconv` (see below) ``` ==2741746== Process terminating with default action of signal 11 (SIGSEGV): dumping core ==2741746==  Bad permissions for mapped region at address 0x287FD000 ==2741746==    at 0x1F567FF8: pytorch_qnnp_indirection_init_dwconv (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1F561456: pytorch_qnnp_setup_average_pooling2d_nhwc_q8 (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1C9F1DC6: at::native::qnnp_avgpool_helper::qnnpack_avg_pool2d(at::Tensor, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1C9F566A: at::native::avg_pool2d_quantized_cpu(at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1D487DBD: c10::impl::wrap_kernel_functor_unboxed_, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional), &at::(anonymous namespace)::(anonymous namespace)::wrapper__avg_pool2d>, at::Tensor, c10::guts::typelist::typelist, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional > >, at::Tensor (at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1CD65280: at::_ops::avg_pool2d::redispatch(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1DEFBA45: torch::autograd::VariableType::(anonymous namespace)::avg_pool2d(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1DEFC20C: c10::impl::wrap_kernel_functor_unboxed_, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional), &torch::autograd::VariableType::(anonymous namespace)::avg_pool2d>, at::Tensor, c10::guts::typelist::typelist, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional > >, at::Tensor (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1CDCA693: at::_ops::avg_pool2d::call(at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool, c10::optional) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1C753AD8: at::native::avg_pool1d(at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1D185AA5: c10::impl::wrap_kernel_functor_unboxed_, c10::ArrayRef, c10::ArrayRef, bool, bool), &at::(anonymous namespace)::(anonymous namespace)::wrapper__avg_pool1d>, at::Tensor, c10::guts::typelist::typelist, c10::ArrayRef, c10::ArrayRef, bool, bool> >, at::Tensor (at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746==    by 0x1CB9520D: at::_ops::avg_pool1d::call(at::Tensor const&, c10::ArrayRef, c10::ArrayRef, c10::ArrayRef, bool, bool) (in /data/users/dzdang/pytorch/torch/lib/libtorch_cpu.so) ==2741746== Invalid read of size 4 ==2741746==    at 0x55497F8: free_mem (in /usr/lib64/libc2.28.so) ==2741746==    by 0x55492D1: __libc_freeres (in /usr/lib64/libc2.28.so) ==2741746==    by 0x40301A4: _vgnU_freeres (vg_preloaded.c:77) ==2741746==  Address 0x8080808080808090 is not stack'd, malloc'd or (recently) free'd ==2741746==  ``` i'm checking if the qnnpack backend supports this configuration. it looks like we're accessing invalid memory, which we should throw an exception instead of causing UB","> Are you implying ceil_mode = false worked for you? using your script, when ceil_mode = false, I get the following error Sorry for misunderstanding, it also write RuntimeError for me. But i think it's ok, because i have too small last dimension for `ceil_mode=False` :) But is starts to work, when i change AvgPool to MaxPool. > I think the two errors are related, so I can look into this further, but prior to doing so, I'm wondering if your sizes in input_data = torch.randn((32, 40, 16)) are correct? I noticed you calibrated with [32, 40, 32] and you also use 32 for the kernel size, but you're doing inference on a different size (16) for the last dimension. I want to use tensors of different dimension with this model, so i just reproduce this case in my sample.","I think qnnpack may not support the kernel size being bigger than the input. Max pool is also implemented with qnnpack, but when `ceil_mode = true`, it doesn't use qnnpack (see https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/qpool.cppL383). This PR here https://github.com/pytorch/pytorch/pull/77792/files mimics this logic for avgpool, and it seems to resolve the issue when I tried it locally. are you able to build pytorch locally? if not, I think you'll have to keep the input size (+ padding) >= kernel size for now until this change makes its way to a future release",closing due to inactivity
rag,Dot/group_norm/instance_norm/var_mean/index_reduce/matmul/bernoulli/adaptive_avg_pool coverage,  CC(Dot/group_norm/instance_norm/var_mean/index_reduce/matmul/bernoulli/adaptive_avg_pool coverage) Signedoffby: Edward Z. Yang ,2022-05-15T03:08:52Z,Merged cla signed release notes: composability topic: improvements,closed,0,4,https://github.com/pytorch/pytorch/issues/77499,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77499**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 1 New Failures As of commit 52e4243d4e (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6511998349?check_suite_focus=true) pull / linuxbionicrocm5.1py3.7 / build (1/1) **Step:** ""Calculate docker image"" (full log  :repeat: rerun)   20220519T17:26:23.1392064Z [error]Process completed with exit code 1.  ``` 20220519T17:26:23.1357930Z + git revparse a0beb3d75987051c41797524f0d79341f646cbee:.circleci/docker 20220519T17:26:23.1370269Z 6fc2e0be6a27b6d8230bfa7d47bbb5649a7d1e7e 20220519T17:26:23.1373941Z ++ git revparse a0beb3d75987051c41797524f0d79341f646cbee:.circleci/docker 20220519T17:26:23.1385819Z + PREVIOUS_DOCKER_TAG=6fc2e0be6a27b6d8230bfa7d47bbb5649a7d1e7e 20220519T17:26:23.1386457Z + [[ 6fc2e0be6a27b6d8230bfa7d47bbb5649a7d1e7e = \\6\\f\\c\\2\\e\\0\\b\\e\\6\\a\\2\\7\\b\\6\\d\\8\\2\\3\\0\\b\\f\\a\\7\\d\\4\\7\\b\\b\\b\\5\\6\\4\\9\\a\\7\\d\\1\\e\\7\\e ]] 20220519T17:26:23.1387395Z + echo 'ERROR: Something has gone wrong and the previous image isn'\\''t available for the mergebase of your branch' 20220519T17:26:23.1388186Z + echo '       contact the PyTorch team to restore the original images' 20220519T17:26:23.1388424Z + exit 1 20220519T17:26:23.1388814Z ERROR: Something has gone wrong and the previous image isn't available for the mergebase of your branch 20220519T17:26:23.1389143Z        contact the PyTorch team to restore the original images 20220519T17:26:23.1392064Z [error]Process completed with exit code 1. 20220519T17:26:23.1559305Z Prepare all required actions 20220519T17:26:23.1578331Z [group]Run ./.github/actions/teardownlinux 20220519T17:26:23.1578548Z with: 20220519T17:26:23.1578706Z env: 20220519T17:26:23.1578868Z   IN_CI: 1 20220519T17:26:23.1579025Z   IS_GHA: 1 20220519T17:26:23.1579201Z [endgroup] 20220519T17:26:23.1593794Z [group]Run .github/scripts/wait_for_ssh_to_drain.sh 20220519T17:26:23.1594063Z [36;1m.github/scripts/wait_for_ssh_to_drain.sh[0m 20220519T17:26:23.1605208Z shell: /usr/bin/bash noprofile norc e o pipefail {0} ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",logit got moved out of this PR, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
chat,Back out Dispatcher change that makes Messenger Desktop crash on M1 devices,"Summary: This change causes Messenger Dekstop to crash on M1 devices when the user enables background during the call. The change apparently causes the compiler to emit AVX instructions that are not supported by Rosetta. This is a surgical backout that only backs out the changes in C++ side, and not Python bindings which I believe are not shipped with Workplace Chat. Test Plan: Run the application and make sure that it doesn't crash when the background is enabled https://pxl.cl/23VSH Reviewed By: ezyang Differential Revision: D36358832",2022-05-13T13:33:34Z,fb-exported Merged cla signed,closed,0,6,https://github.com/pytorch/pytorch/issues/77414,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77414**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (2 Pending) As of commit 4dc9cc52ef (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,This pull request was **exported** from Phabricator. Differential Revision: D36358832," force merge this (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)","Merge failed due to Matched rule superuser, but PR has not been reviewed yet Raised by https://github.com/pytorch/pytorch/actions/runs/2320784596"," force merge this (Initiating merge automatically since Phabricator Diff has merged, using force because this PR might not pass merge_rules.json but landed internally)","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[distributed] c10d crashing on assert," 🐛 Describe the bug Finally got a simple script that reproduces the pt1.11/c10d crash on assert and or exit  which on JeanZay HPC most of the time leads to core dumps. The script is totally unrelated to BigScience but the C traceback looks similar. ``` $ CUDA_VISIBLE_DEVICES=0 python m torch.distributed.run nproc_per_node=1  master_addr='127.0.0.1' master_port=9901 test.py [...]   File ""/mnt/nvme0/code/github/00optimize/deepspeed/deepspeed/runtime/zero/partitioned_param_coordinator.py"", line 279, in fetch_sub_module     assert param.ds_status == ZeroParamStatus.AVAILABLE, param.ds_summary() AssertionError: {'id': 6, 'status': 'INFLIGHT', 'numel': 0, 'ds_numel': 64, 'shape': (0,), 'ds_shape': (64,), 'requires_grad': True, 'grad_shape': None, 'persist': True, 'active_sub_modules': {116}}  10% 1/10 [00:00 + 0xc9039 (0x7f51ca9cd039 in /mnt/nvme0/anaconda3/envs/py38pt111/bin/../lib/libstdc++.so.6) frame CC(Checklist for Release):  + 0x94947 (0x7f51cdd19947 in /lib/x86_64linuxgnu/libc.so.6) frame CC(Remove dampening from SGD): clone + 0x44 (0x7f51cdda9a44 in /lib/x86_64linuxgnu/libc.so.6) ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 6) local_rank: 0 (pid: 654206) of binary: /home/stas/anaconda3/envs/py38pt111/bin/python Traceback (most recent call last):   File ""/home/stas/anaconda3/envs/py38pt111/lib/python3.8/runpy.py"", line 194, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/stas/anaconda3/envs/py38pt111/lib/python3.8/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/distributed/run.py"", line 728, in      main()   File ""/home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/distributed/run.py"", line 724, in main     run(args)   File ""/home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/distributed/run.py"", line 715, in run     elastic_launch(   File ""/home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/distributed/launcher/api.py"", line 245, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ======================================================= test.py FAILED  Failures:     Root Cause (first observed failure): [0]:   time      : 20220511_18:54:22   host      : localhost   rank      : 0 (local_rank: 0)   exitcode  : 6 (pid: 654206)   error_file:    traceback : Signal 6 (SIGABRT) received by PID 654206 ``` I attached the 2 needed files: test.txt ds_config.txt please rename those upon saving to: ``` mv ds_config.txt ds_config.json mv test.txt test.py ``` Here are the right sha's otherwise the initial problem will be fixed shortly and there will be no assert. Please use the following order: ``` pip install deepspeed transformers  gets the deps right quickly pip install git+https://github.com/microsoft/DeepSpeed.git pip install git+https://github.com/huggingface/transformers.git ``` Then just: ``` $ CUDA_VISIBLE_DEVICES=0 python m torch.distributed.run nproc_per_node=1  master_addr='127.0.0.1' master_port=9901 test.py ``` Clearly there is some bad interaction happening between deepspeed, which uses pytorch CUDA extensions and pytorch c10d.  Versions PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Red Hat Enterprise Linux release 8.4 (Ootpa) (x86_64) GCC version: (GCC) 8.4.1 20200928 (Red Hat 8.4.11) Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.28 Python version: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.18.0305.40.2.el8_4.x86_64x86_64withglibc2.17 Is CUDA available: False CUDA runtime version: 11.4.152 GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.2 [pip3] torch==1.11.0+cu115 [pip3] torchaudio==0.11.0+cu115 [pip3] torchvision==0.12.0+cu115 [conda] blas                      1.0                         mkl [conda] cudatoolkit               11.3.1               h2bc3f7f_2 [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklservice               2.4.0            py38h7f8727e_0 [conda] mkl_fft                   1.3.1            py38hd3c417c_0 [conda] mkl_random                1.2.2            py38h51133e4_0 [conda] mypyextensions           0.4.3                    pypi_0    pypi [conda] numpy                     1.22.2                   pypi_0    pypi [conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch [conda] pytorchmutex             1.0                        cuda    pytorchnightly [conda] torch                     1.11.0+cu115             pypi_0    pypi [conda] torchaudio                0.11.0+cu115             pypi_0    pypi [conda] torchvision               0.12.0+cu115             pypi_0    pypi ",2022-05-12T21:07:39Z,oncall: distributed,closed,0,6,https://github.com/pytorch/pytorch/issues/77374," since there is an assertion error from the application, the application is trying to exit, so the NCCL kernel exits as well. This looks expected to me. Maybe I'm wrong.   would you please help taking a look at it? Thanks","You consider this exiting expected? ``` Exception raised from query at ../aten/src/ATen/cuda/CUDAEvent.h:95 (most recent call first): frame CC(未找到相关数据): c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f51ab1577d2 in /home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/lib/libc10.so) frame CC(Matrix multiplication operator): c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x11a (0x7f518b4d79ea in /home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(Don't support legacy Python): c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x50 (0x7f518b4d9fd0 in /home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(PEP8): c10d::ProcessGroupNCCL::workCleanupLoop() + 0x145 (0x7f518b4db265 in /home/stas/anaconda3/envs/py38pt111/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cpp.so) frame CC(PEP8):  + 0xc9039 (0x7f51ca9cd039 in /mnt/nvme0/anaconda3/envs/py38pt111/bin/../lib/libstdc++.so.6) frame CC(Checklist for Release):  + 0x94947 (0x7f51cdd19947 in /lib/x86_64linuxgnu/libc.so.6) frame CC(Remove dampening from SGD): clone + 0x44 (0x7f51cdda9a44 in /lib/x86_64linuxgnu/libc.so.6) ``` and most of the time in the larger MegatronDeepspeed application this segfaults and dumps corrupted core files. There a similar log plus segfault and core dump happens even if I do `sys.exit(0)` which is not even an assert. Please see more live backtraces and gdb's core dump's bt here:  CC(torch.elastic fails to shutdown despite crashed processes)issuecomment1117938234","`workCleanupLoop` is a busylooping thread that monitors the state of enqueued collectives. This threads calls `isCompleted()` on each collective work item, which then calls `finishedGPUExecutionInternal()`. In `finishedGPUExecutionInternal()`, a query is made on a `cudaEvent` recorded after the collective, to check if the collective is completed. The ""driver shutting down"" error is thrown during the `cudaEventQuery`, which probably indicates that the CUDA context has been corrupted in previous failure.","I met the exactly same error, I'm also using the same version of pytorch and deepspeed, waiting for solution.","Can anyone confirm if this is still happening?  Should be fixed by https://github.com/pytorch/pytorch/pull/106503 (landed in aug, would be part of the latest release)","Will, I have just tried to retest but the current torch doesn't work with those older versions of other packages that were triggering the situation. But I haven't run into this issue again since then, so as you saying it is probably ok."
yi,"outputs_[i]->uses().empty()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/jit/ir/ir.cpp"":1314, please report a bug to PyTorch. "," 🐛 Describe the bug When I use torch.jit.script for the class, in the line  p = pt_2d[b, :, pix2pt] it will pop up the issue descriped in the title. The snippet : ```python import math import torch import torch.nn as nn class DifferentiableRasterizer(nn.Module):     def __init__(self,face,height,width,block_size=32):         super(DifferentiableRasterizer, self).__init__()         self.block_size = block_size         self.width = width         self.height = height         self.width_exp = int(math.ceil(float(width)/float(self.block_size)))*self.block_size         self.height_exp = int(math.ceil(float(height)/float(self.block_size)))*self.block_size         self.face = face         self.index_buf = torch.full((self.height_exp, self.width_exp), face.shape[1], dtype=torch.long).to(face.device)         self.face_index = torch.LongTensor(range(0,self.face.shape[1])).to(face.device)         self.x_grid = torch.tensor(range(0,self.width)).unsqueeze(0).to(face.device)         self.y_grid = torch.tensor(range(0,self.height)).unsqueeze(1).to(face.device)         self.x_grid_block = torch.tensor(range(0,self.block_size)).unsqueeze(0).unsqueeze(2).to(face.device)         self.y_grid_block = torch.tensor(range(0,self.block_size)).unsqueeze(1).unsqueeze(2).to(face.device)     def forward(self,pt_2d,color,pt_3d,normal,R,T):         ftiny = 1.17e35         inf_value = 3.40e+35         lower_inf_value = 3.40e+34         batch, vnum, pnum = pt_2d.shape         cnum = color.shape[1]         image = torch.zeros(batch,cnum,self.height,self.width,device=pt_2d.device)         mask = torch.zeros(batch,self.height,self.width,device=pt_2d.device)         for b in range(batch):             with torch.no_grad():                 norm_cul = torch.sum((pt_3d[b,:,self.face[0, :]] + (R[b,:,:].t()[b,:, :])) * normal[b,:,:],0)  0                 if torch.sum(norm_cul * depth_cul).item()==0:                     continue                 face_red = self.face[:, norm_cul * depth_cul]                 face_index_red = self.face_index[norm_cul * depth_cul]                 num = face_red.shape[1]                 self.index_buf[:] = num                 p = pt_2d[b, :, face_red]                 pz_min,_ = torch.min(p[2,:,:],0)                 px_min,_ = torch.min(p[0,:,:].int(),0)                 px_max,_ = torch.max(p[0,:,:].int(),0)                 py_min,_= torch.min(p[1, :, :].int(), 0)                 py_max,_ = torch.max(p[1,:,:].int(),0)                 x_min,_ = torch.min(px_min,0)                 x_max,_ = torch.max(px_max,0)                 y_min,_ = torch.min(py_min,0)                 y_max,_ = torch.max(py_max,0)                 range_x_min = max(x_min.item()x_min.item()%self.block_size,0)                 range_y_min = max(y_min.item()  y_min.item() % self.block_size, 0)                 range_x_max = min(x_max.item(), self.width_exp)                 range_y_max = min(y_max.item(), self.height_exp)                 det = ((p[1, 1, :]  p[1, 2, :]) * (p[0, 0, :]  p[0, 2, :]) + (p[0, 2, :]  p[0, 1, :]) * (p[1, 0, :]  p[1, 2, :])).unsqueeze(0).unsqueeze(0)                 det = det.sign()*torch.clamp(det.abs(),min=ftiny)                 inv_det = 1/det                 l0_x = (p[1, 1, :]  p[1, 2, :]) * inv_det                 l0_y = (p[0, 2, :]  p[0, 1, :]) * inv_det                 l0_c = l0_x*p[0, 2, :]  l0_y *p[1, 2, :]                 l1_x = (p[1, 2, :]  p[1, 0, :]) * inv_det                 l1_y = (p[0, 0, :]  p[0, 2, :]) * inv_det                 l1_c = l1_x*p[0, 2, :]  l1_y *p[1, 2, :]                 l2_x = l0_x  l1_x                 l2_y = l0_y  l1_y                 l2_c = 1l0_cl1_c                 p = p.unsqueeze(1).unsqueeze(1)                 D_x = p[2, :, :, 0, :] * l0_x + p[2, :, :, 1, :] * l1_x + p[2, :, :, 2, :] * l2_x                 D_y = p[2, :, :, 0, :] * l0_y + p[2, :, :, 1, :] * l1_y + p[2, :, :, 2, :] * l2_y                 D_c = (p[2,:,:,0,:]*l0_c + p[2,:,:,1,:]*l1_c + p[2,:,:,2,:]*l2_c)                 for i in range(int(range_y_min),int(range_y_max),int(self.block_size)):                     D_yc = D_y * (float(i)+self.y_grid_block) + D_c                     l0_yc = l0_y * (float(i) + self.y_grid_block) + l0_c                     l1_yc = l1_y * (float(i) + self.y_grid_block) + l1_c                     l2_yc = l2_y * (float(i) + self.y_grid_block) + l2_c                     for k in range(int(range_x_min),int(range_x_max),int(self.block_size)):                         target = (px_max>=k)*(px_min=i)*(py_min= (l0_x[:,:,target]* kxg)) * (l1_yc[:,:,target]  >= (l1_x[:,:,target]* kxg)) * (l2_yc[:,:,target] >= (l2_x[:,:,target]* kxg))                         vis_ct = torch.max(torch.sum(M, 2)).item()                         if vis_ct==1:                             vis, idx = torch.max(M, 2)                             self.index_buf[i:i + self.block_size, k:k + self.block_size][vis] = (self.face_index[0:target.shape[0]][target])[idx[vis]]                         elif vis_ct>1:                             D = M.bitwise_not().float() * inf_value + D_x[:,:,target] * (float(k)+self.x_grid_block) + D_yc[:,:,target]                             D[D!=D]=inf_value                             depth, idx = torch.min(D,2)                             vis = depth< lower_inf_value                             self.index_buf[i:i+self.block_size, k:k+self.block_size][vis] = (self.face_index[0:target.shape[0]][target])[idx[vis]]                 mask_ = (self.index_buf[0:self.height,0:self.width]!=num).float()                 index_buf_tmp = self.index_buf[0:self.height,0:self.width]                 index_buf_tmp[index_buf_tmp==num] = 0                 pix2pt = self.face[:,face_index_red[index_buf_tmp]]             p = pt_2d[b, :, pix2pt]  face = torch.randint(1,100, (3, 20)) traced_script_module = torch.jit.script(DifferentiableRasterizer(face,224,224,32)) ```  the error occur in the last line     p = pt_2d[b, :, pix2pt]   Versions Versions of relevant libraries: [pip3] facenetpytorch==2.5.2 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.20.1 [pip3] numpydoc==1.1.0 [pip3] pytorch3d==0.6.1 [pip3] torch==1.11.0 [pip3] torchaudio==0.11.0 [pip3] torchvision==0.12.0 [conda] blas                      1.0                         mkl [conda] cudatoolkit               10.2.89              hfd86e86_1 [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2021.2.0           h06a4308_296 [conda] mklservice               2.3.0            py38h27cfd23_1 [conda] mkl_fft                   1.3.0            py38h42c9631_2 [conda] mkl_random                1.2.1            py38ha9443f7_2 [conda] numpy                     1.20.1           py38h93e21f0_0 [conda] numpybase                1.20.1           py38h7d8b39e_0 [conda] numpydoc                  1.1.0              pyhd3eb1b0_1 [conda] pytorch                   1.11.0          py3.8_cuda10.2_cudnn7.6.5_0                                                                                                                 pytorch [conda] pytorchmutex             1.0                        cuda    pytorch [conda] pytorch3d                 0.6.1           py38_cu102_pyt1110    pytorch3                                                                                                             dnightly [conda] torchaudio                0.11.0               py38_cu102    pytorch [conda] torchvision               0.12.0               py38_cu102    pytorch",2022-05-12T15:49:38Z,oncall: jit,open,0,0,https://github.com/pytorch/pytorch/issues/77354
gpt,Add device_id support to FSDP,"  CC(Add device_id support to FSDP) Adds `device_id` argument to FSDP. If this is specified, and module is on CPU, we move module to this device. In addition, even if device_Id is not specified, we move module to this device to do flattening, sharding, etc but then move it back to CPU before returning to user. This PR also moves module inputs to the `compute_device` during forward of root FSDP instance. Note that we guarantee `compute_device == device_id` if `device_id` is specified. This PR also detects if user passed in multidevice module, which FSDP does not support, and throws an error. Benchmarking on single host 8 GPUs:  GPT 700m parameter model FSDP init on CPU: ~28s, moving to GPU speeds it up to 2s, a 14x win  GPT 6B parameter model FSDP init on CPU: ~47s, moving it to GPU speeds it up to 17s, 2.7x win",2022-05-12T01:53:42Z,oncall: distributed Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/77321,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77321**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit fdf0b665e8 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Hey varma. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","Hi varma , I keep running into an error when I use FSDP in my system. I have 4 GPU's, in my node, all of which I use while running my bash script which is as follows :  ``` export CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch num_processes=1 train.py  \\ do_train per_device_train_batch_size 1 \\ do_eval per_device_eval_batch_size 1 \\ eval_steps 2000 \\ num_train_epochs 5 \\ save_strategy ""epoch"" \\ learning_rate 5e5 \\ logging_steps 1 \\ gradient_checkpointing true \\ fsdp ""full_shard auto_wrap"" \\ fsdp_transformer_layer_cls_to_wrap ""LlamaDecoderLayer""  ``` The error that I get is   `ValueError: Inconsistent compute device and `device_id` on rank 0: cuda:1 vs cuda:0` While debugging I noticed that the error comes from _get_compute_device() method. I logged compute_device and device_from_device_id, and noticed that whenever the compute_device moves to cuda:1 the error occurs, as shown in the  following   ``` cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:0 cuda:1 cuda:0 Traceback (most recent call last): ``` Would you happen to know the possible reason?", What exactly are you logging? When does the first value become `cuda:1`? Are you calling `torch.cuda.set_device()` on each rank?
yi,"Autogen Tags enum, and allow specifying tags while defining an op","Stack from ghstack:  CC(Autogen Tags enum, and allow specifying tags while defining an op) 1. Autogenerates Enum Tag (containing all valid tags from tags.yaml). 2. Adds an unordered set of tags to OperatorEntry class. 3. Updates Library def to optionally take tags. 4. Updates codegen for registering ops to pass tags to def. 5. Autogenerates python bindings for Tag Enum. 6. Expose tags in `torch.ops` API 7. New test to validate correctness of torch.Tags.inplace_view TODO: Add docs for Tags https://github.com/pytorch/pytorch/runs/6600629359?check_suite_focus=true Differential Revision: D36912769 Internal Changes will be coming soon",2022-05-11T23:50:19Z,Merged cla signed ciflow/trunk,closed,0,22,https://github.com/pytorch/pytorch/issues/77313,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77313**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 55a7a502ad (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,  added a new test in `test_ops.py` to verify correctness for `torch.Tags.inplace_view`. Eventually we should expand this test to add tests for every newly added tag., merge,"Merge failed due to Refusing to merge as mandatory check(s) linuxdocs / builddocs (cpp), linuxdocs / builddocs (python), winvs2019cpupy3 / build, winvs2019cuda11.3py3 / build, linuxxenialpy3.7gcc7noops / build, linuxbionicpy3.7clang9 / build, linuxxenialpy3.7clang7onnx / build, linuxxenialpy3.7gcc7 / build, linuxxenialpy3.7clang7asan / build, linuxvulkanbionicpy3.7clang9 / build, linuxxenialcuda11.3py3.7gcc7 / build, linuxbioniccuda11.3py3.7clang9 / build, linuxxenialpy3clang5mobilebuild / build, linuxxenialpy3clang5mobilecustombuildstatic / build, pytorchxlalinuxbionicpy3.7clang8 / build, deploylinuxxenialcuda11.3py3.7gcc7 / build are pending/not yet run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2431233135", merge,Merge failed due to Refusing to merge as mandatory check(s) linuxdocs / builddocs (cpp) are pending/not yet run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2431583740, merge,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."," revert m ""Broke OSS buck builds, see https://hud.pytorch.org/pytorch/pytorch/commit/9476a78f3754aa122323b431c59360b254559d16"" c missedsignal"," revert m ""Broke OSS buck builds, see https://hud.pytorch.org/pytorch/pytorch/commit/9476a78f3754aa122323b431c59360b254559d16"" c nosignal"," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", help, merge g, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2476967592, merge, merge, successfully started a merge job. Check the current status here,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2477512496
transformer,Switch to use nested tensor by-default in TransformerEncoder,Summary: Switch to use nested tensor as by default setting in TransformerEncoderLayer. Test Plan: CI Torchtext buck test mode/opt pytorch/text/test:integration_tests_test_models  test_xlmr_base_model Reviewed By: frankwei Differential Revision: D36153335,2022-05-11T01:16:44Z,fb-exported Merged cla signed Reverted,closed,0,15,https://github.com/pytorch/pytorch/issues/77217,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77217**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 13 New Failures As of commit 49e9a81ed5 (more details on the Dr. CI page): Expand to see more  * **13/13** failures introduced in this PR   :detective: 13 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6514105745?check_suite_focus=true) pull / winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) (1/13) **Step:** ""Test"" (full log  :repeat: rerun)   20220519T22:12:48.6865198Z RuntimeError: test_meta failed!  ``` 20220519T22:12:44.1824776Z FAILED (errors=21, skipped=130, expected failures=63) 20220519T22:12:44.1824964Z  20220519T22:12:44.1825076Z Generating XML reports... 20220519T22:12:45.2528843Z Generated XML report: testreports/pythonunittest/test_meta/TESTTestMetaCUDA20220519215439.xml 20220519T22:12:45.2538832Z Generated XML report: testreports/pythonunittest/test_meta/TESTTestMetaConverter20220519215439.xml 20220519T22:12:48.6850166Z Traceback (most recent call last): 20220519T22:12:48.6851351Z   File ""test/run_test.py"", line 1074, in  20220519T22:12:48.6857922Z     main() 20220519T22:12:48.6858845Z   File ""test/run_test.py"", line 1052, in main 20220519T22:12:48.6863660Z     raise RuntimeError(err_message) 20220519T22:12:48.6865198Z RuntimeError: test_meta failed! 20220519T22:12:50.7013962Z  20220519T22:12:50.7014413Z real	18m18.353s 20220519T22:12:50.7015239Z user	18m8.850s 20220519T22:12:50.7015996Z sys	0m28.765s 20220519T22:12:50.7016969Z + cleanup 20220519T22:12:50.7017531Z + retcode=1 20220519T22:12:50.7018086Z + set +x 20220519T22:12:50.7145249Z [error]Process completed with exit code 1. 20220519T22:12:50.7227217Z [group]Run  copy test results back to the mounted workspace, needed sudo, resulting permissions were correct 20220519T22:12:50.7227878Z [36;1m copy test results back to the mounted workspace, needed sudo, resulting permissions were correct[0m ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335, ,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335,This pull request was **exported** from Phabricator. Differential Revision: D36153335, merge (Initiating merge automatically since Phabricator Diff has merged), revert this please as it broke many tests: https://github.com/pytorch/pytorch/runs/6515962633?check_suite_focus=true, also the PR signal is mostly red. So we should avoid merging PRs with broken CI.,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PT-D][Sharding] Enable more ops needed in the transformer model training,"  CC([PTD][Sharding] Enable more ops needed in the transformer model training) From the code base of MetaSeq Model, we have found that loads of ops are not supported by sharded tensor. In https://github.com/pytorch/pytorch/pull/75374, we have enabled most of ops already and this PR/diff aims at enabling the rest of them. Fix some unit test errors. Differential Revision: D36302780",2022-05-11T01:10:03Z,oncall: distributed Merged cla signed sharded_tensor release notes: distributed (sharded) topic: new features,closed,0,4,https://github.com/pytorch/pytorch/issues/77214,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77214**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 2 New Failures, 1 Base Failures As of commit fd787ad0d1 (more details on the Dr. CI page): Expand to see more  * **2/3** failures introduced in this PR * **1/3** broken upstream at merge base 8b93abd082 on May 10 from  5:00pm to  7:23pm   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6432733993?check_suite_focus=true) pull / linuxxenialpy3.7clang7onnx / test (default, 1, 2, linux.2xlarge) (1/2) **Step:** ""Test"" (full log  :repeat: rerun)   20220514T06:25:00.6765626Z AssertionError: 1 unit test(s) failed:  ``` 20220514T06:25:00.4707112Z  20220514T06:25:00.4707192Z OK 20220514T06:25:00.4707287Z  20220514T06:25:00.4707388Z Generating XML reports... 20220514T06:25:00.4739820Z Generated XML report: testreports/pythonunittest/distributed.rpc.test_tensorpipe_agent/TESTTensorPipeThreeWorkersRemoteModuleTest20220514062458.xml 20220514T06:25:00.6762346Z Traceback (most recent call last): 20220514T06:25:00.6762702Z   File ""distributed/rpc/test_tensorpipe_agent.py"", line 38, in  20220514T06:25:00.6763317Z     run_tests() 20220514T06:25:00.6764135Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 635, in run_tests 20220514T06:25:00.6764990Z     len(failed_tests), '\\n\\t'.join(failed_tests)) 20220514T06:25:00.6765626Z AssertionError: 1 unit test(s) failed: 20220514T06:25:00.6766079Z 	TensorPipeDistAutogradTest.test_backward_multiple_output_tensors 20220514T06:25:00.8070485Z Traceback (most recent call last): 20220514T06:25:00.8070890Z   File ""test/run_test.py"", line 1072, in  20220514T06:25:00.8072383Z     main() 20220514T06:25:00.8072739Z   File ""test/run_test.py"", line 1050, in main 20220514T06:25:00.8074436Z     raise RuntimeError(err_message) 20220514T06:25:00.8074898Z RuntimeError: distributed/rpc/test_tensorpipe_agent failed! 20220514T06:25:01.0548062Z + cleanup 20220514T06:25:01.0548298Z + retcode=1 20220514T06:25:01.0548458Z + set +x ```    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) on May 10 from  5:00pm to  7:23pm (81528d4b21  e832ff58bf)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",Rebase and addressed the comment from reviewers., merge this (Initiating merge automatically since Phabricator Diff has merged),"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Set block dim and grid dim macros within NestedTensorTransformerFunctions.cu,Follow up to https://github.com/pytorch/pytorch/pull/76157discussion_r856668878,2022-05-10T21:12:24Z,Merged cla signed topic: not user facing,closed,0,3,https://github.com/pytorch/pytorch/issues/77199,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77199**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 14695bfbae (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,nn.functional.pad accepts bool values but raises internal assert when converted to JIT," 🐛 Describe the bug While working on Longformer in `transformers`, I came across this comment: https://github.com/huggingface/transformers/issues/13126issuecomment993645323. An incorrect implementation passes a bool as `value` into `nn.functional.pad`, which normally works. However, it raises an internal assert when used with `torch.jit`: Example: ```python import torch from torch import nn class MyModule(nn.Module):     def forward(self, inputs):         return nn.functional.pad(             inputs, (0, inputs.size(1) + 1), value=False  works if value=0         ) ``` `torch.jit.trace_module(MyModule(), {""forward"": torch.zeros(3, 4)})` results in ```  RuntimeError                              Traceback (most recent call last) /Users/patrick/Projects/opensource/transformers/notebooks/torchjitpad.ipynb Cell 3' in () > 1 torch.jit.trace_module(MyModule(), {""forward"": torch.zeros(3, 4)}) File ~/.pyenvx86/versions/transformersx86/lib/python3.9/sitepackages/torch/jit/_trace.py:958, in trace_module(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)     954     argument_names = get_callable_argument_names(func)     956 example_inputs = make_tuple(example_inputs) > 958 module._c._create_method_from_trace(     959     method_name,     960     func,     961     example_inputs,     962     var_lookup_fn,     963     strict,     964     _force_outplace,     965     argument_names,     966 )     967 check_trace_method = module._c._get_method(method_name)     969  Check the trace against new traces created from userspecified inputs RuntimeError: 0INTERNAL ASSERT FAILED at ""/Users/distiller/project/pytorch/torch/csrc/jit/ir/alias_analysis.cpp"":607, please report a bug to PyTorch. We don't have an op for aten::constant_pad_nd but it isn't a special case.  Argument types: Tensor, int[], bool,  Candidates: 	aten::constant_pad_nd(Tensor self, int[] pad, Scalar value=0) > (Tensor) ``` `torch.jit.script(MyModule())` results in ```  RuntimeError                              Traceback (most recent call last) /Users/patrick/Projects/opensource/transformers/notebooks/torchjitpad.ipynb Cell 3' in () > 1 torch.jit.script(MyModule()) File ~/.pyenv/versions/torch/lib/python3.9/sitepackages/torch/jit/_script.py:1265, in script(obj, optimize, _frames_up, _rcb, example_inputs)    1263 if isinstance(obj, torch.nn.Module):    1264     obj = call_prepare_scriptable_func(obj) > 1265     return torch.jit._recursive.create_script_module(    1266         obj, torch.jit._recursive.infer_methods_to_compile    1267     )    1269 if isinstance(obj, dict):    1270     return create_script_dict(obj) File ~/.pyenv/versions/torch/lib/python3.9/sitepackages/torch/jit/_recursive.py:454, in create_script_module(nn_module, stubs_fn, share_types, is_tracing)     452 if not is_tracing:     453     AttributeTypeIsSupportedChecker().check(nn_module) > 454 return create_script_module_impl(nn_module, concrete_type, stubs_fn) File ~/.pyenv/versions/torch/lib/python3.9/sitepackages/torch/jit/_recursive.py:520, in create_script_module_impl(nn_module, concrete_type, stubs_fn)     518  Compile methods if necessary     519 if concrete_type not in concrete_type_store.methods_compiled: > 520     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)     521      Create hooks after methods to ensure no name collisions between hooks and methods.     522      If done before, hooks can overshadow methods that aren't exported.     523     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs) File ~/.pyenv/versions/torch/lib/python3.9/sitepackages/torch/jit/_recursive.py:371, in create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)     368 property_defs = [p.def_ for p in property_stubs]     369 property_rcbs = [p.resolution_callback for p in property_stubs] > 371 concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults) RuntimeError:  _pad(Tensor input, int[] pad, str mode=""constant"", float value=0.) > (Tensor): Expected a value of type 'float' for argument 'value' but instead found type 'bool'. :   File ""/var/folders/qm/r50_76fn105g_18z8w81058c0000gn/T/ipykernel_98988/2039179918.py"", line 3     def forward(self, inputs):         return nn.functional.pad(                ~~~~~~~~~~~~~~~~~ < HERE             inputs, (0, inputs.size(1) + 1), value=False  works if value=0         ) ``` Is this considered a user error, or should it be addressed? Maybe we could at least improve the error message for `torch.jit.trace_module`. Best, Patrick  Versions Collecting environment information... PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.3) CMake version: version 3.23.1 Libc version: N/A Python version: 3.9.11 (main, May  4 2022, 09:48:34)  [Clang 13.1.6 (clang1316.0.21.2.3)] (64bit runtime) Python platform: macOS12.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.11.0 [pip3] torchaudio==0.11.0 [pip3] torchvision==0.12.0 [conda] Could not collect",2022-05-10T15:11:31Z,oncall: jit,open,0,1,https://github.com/pytorch/pytorch/issues/77167,"I met the same issue, I think it would be better if it could mention which exact layer is raising this?"
yi,"torch.cholesky has been deprecated in favour of torch.linalg.cholesky. However, torch.cholesky_inverse remains as is. It should also be moved to torch.linalg"," 🐛 Describe the bug Not sure where this should be filed under. But I noticed that, in the most recent version of pytorch `1.11.0+cu102`, when using `torch.cholesky`, it says it's deprecated in favour of `torch.linalg.cholesky`, and will getting phased out in future versions. However, the complementary function `torch.cholesky_inverse` still runs with no deprecation warning. Indeed, there's currently no `torch.linalg.cholesky_inverse`, nor does it seem like there are any such plans. If you saw it fit to move `cholesky` to `torch.linalg`, then it seems only right that `cholesky_inverse` should join it there.  Versions Running PyTorch 1.11.0+cu102 on WSL. ",2022-05-10T14:31:13Z,triaged module: linear algebra,open,1,3,https://github.com/pytorch/pytorch/issues/77166,"Note that `cholesky_solve` is simply implemented as ```python torch.cholesky_solve(L, B) == linalg.solve_triangular(L.mH, linalg.solve_triangular(L, B, upper=False), upper=True) ``` as such, you also have that ```python Id = torch.eye(L.size(1), device=L.device) torch.cholesky_inverse(L) == cholesky_solve(L, Id) ``` Given that these operations are so easy to implement, we have not prioritised moving them to linalg. In fact, as we are doing with `triangular`, we will likely just move `cholesky_solve`, and expect that the user uses the construction above if they want to materialise the inverse. This is already tracked in  CC(Move `torch.cholesky_solve` into `torch.linalg`.)"," Personally, I think it's absolutely worth keeping around `cholesky_inverse`. Just like how `torch.cat`, `torch.concat`, and `torch.stack`, `torch.hstack`, and `torch.vstack` can be used to implement each other in straightforward ways, it's still absolutely worth keeping them all around to fulfill their common usecases. Similarly, `cholesky_inverse` is such a straightforward description of a commonly desired function. Even if it's not much work to implement it in terms of `cholesky_solve` or `solve_triangular`, I still think getting rid of `cholesky_inverse` is the wrong move here. And if you're going to keep `cholesky_inverse` around, it should be moved to `torch.linalg` along with the other related functions. It's definitely not a priority, but I'd imagine it shouldn't be tremendous work to move `cholesky_inverse` wholesale into the the `torch.linalg` module....","One of the rules we follow to choose what goes in and what doesn't is whether NumPy / SciPy have these functions. In this case, they just have `cho_solve`. As such, you could consider pitching adding this function to SciPy. If they add it, we could then consider adding it."
rag,"Support Tensor source for x.set_(storage, offset, size, strides)","Stack from ghstack:  CC(Add OpInfo based meta tensor tests [RELAND])  CC(Support Tensor source for x.set_(storage, offset, size, strides)) This is convenient for cases where we don't have Storage bound correctly (e.g., meta tensors).  It is also consistent with a universe where we get rid of storages, although arguably this is never gonna happen. Signedoffby: Edward Z. Yang ",2022-05-06T23:50:28Z,Merged cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/77007,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/77007**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit c174dbea2c (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Disable transformer/MHA fast path when autocast is enabled,  CC([PyTorch] Disable transformer/MHA fast path when autocast is enabled) We need predictable Tensor dtypes because we call data_ptr. Autocast messes with this Differential Revision: D36190109,2022-05-06T01:48:07Z,cla signed Stale,closed,0,8,https://github.com/pytorch/pytorch/issues/76934,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76934**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :x: 10 New Failures, 1 Base Failures As of commit 68221cfcc1 (more details on the Dr. CI page): Expand to see more  * **10/11** failures introduced in this PR * **1/11** broken upstream at merge base b109658649 on May 05 from  4:13pm to  6:52pm   :detective: 10 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6326488492?check_suite_focus=true) pull / linuxxenialpy3.7gcc7 / test (default, 2, 2, linux.2xlarge) (1/10) **Step:** ""Test"" (full log  :repeat: rerun)   20220506T18:13:05.2186076Z RuntimeError: test_jit failed!  ``` 20220506T18:13:04.8441357Z Generated XML report: testreports\\pythonunittest\\test_jit\\TESTjit.test_ignore_context_manager.TestIgnoreContextManager20220506181117.xml 20220506T18:13:04.8441992Z Generated XML report: testreports\\pythonunittest\\test_jit\\TESTjit.test_legacy_upgraders.TestLegacyUpgraders20220506181117.xml 20220506T18:13:04.8442556Z Generated XML report: testreports\\pythonunittest\\test_jit\\TESTjit.test_backend_nnapi.TestNnapiBackend20220506181117.xml 20220506T18:13:04.8443081Z Generated XML report: testreports\\pythonunittest\\test_jit\\TESTjit.test_save_load.TestSaveLoadFlatbuffer20220506181117.xml 20220506T18:13:04.8443605Z Generated XML report: testreports\\pythonunittest\\test_jit\\TESTjit.test_torchbind.TestTorchbind20220506181117.xml 20220506T18:13:05.2184769Z Traceback (most recent call last): 20220506T18:13:05.2185163Z   File ""run_test.py"", line 1070, in  20220506T18:13:05.2185383Z     main() 20220506T18:13:05.2185614Z   File ""run_test.py"", line 1048, in main 20220506T18:13:05.2185851Z     raise RuntimeError(err_message) 20220506T18:13:05.2186076Z RuntimeError: test_jit failed! 20220506T18:13:05.4890865Z  20220506T18:13:05.4891608Z (base) C:\\actionsrunner\\_work\\pytorch\\pytorch\\test>popd 20220506T18:13:05.4896069Z  20220506T18:13:05.4896318Z (base) C:\\actionsrunner\\_work\\pytorch\\pytorch>if ERRORLEVEL 1 exit /b 1  20220506T18:13:05.4920349Z + cleanup 20220506T18:13:05.4920648Z + retcode=1 20220506T18:13:05.4920807Z + set +x 20220506T18:13:05.4953170Z [error]Process completed with exit code 1. 20220506T18:13:05.5495679Z [group]Run pytorch/pytorch/.github/actions/getworkflowjobid 20220506T18:13:05.5496013Z with: ```    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * pull / linuxbionicrocm5.1py3.7 / test (default, 1, 2, linux.rocm.gpu) on May 05 from  4:13pm to  6:52pm (8ac6b0a010  a0ebf1d386)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","argh, `torch.is_autocast_enabled` does not work with JIT, how is that even possible",,"Errr, that's unfortunate. We don't have `autocastEnabled` exposed... :cry:  https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/autocast.hL12 You can add an entry here for that and query that one for jit instead. https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/python/init.cppL225 These are not thread safe...  https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/autocast.cppL469L477 IIRC, autocast is enabled per thread, maybe we can change `autocast_enabled` to thread_local.",> query that one for jit instead. am I supposed to branch on some sort of torch.is_jit thing? I'm missing a piece I think, ```torch.jit.is_scripting()``` might work,"> is_scripting that's not going to help. I need to make the exact same check (is autocast enabled) regardless of whether the JIT is involved or not. need the JIT folks to fix this, and ideally need to not have to do extra stuff to make perfectly normal functions work in TorchScript.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,improve index_select performance on CPU,"Stack from ghstack:  CC(improve cat performance on CPU)  CC(improve index_select performance on CPU) Improve some hotspot operators in transformer models. This PR is targeting at further improving `torch.index_select` performance on CPU. `index_select` is a critical hotspot in decoderbased Transformer variants with TopK sampling approach, also it is widely used in other scenarios like `nn.Embedding`. The existing implementation on `index_select` has a fast path when `dim == 1`, this PR parallels it. The existing implementation is generally OK when used in embedding,  e.g. with input like '[50000, 128]', while the index_size (50000) is big enough. it will parallel on index_size. But for transformer shapes, the current implementation is not very efficient: The transformer shape is like [5, 8, N, 64] and N ranges from 2 to 114 and dim = 2. * when N is very small: e.g. N=2, the input shape is [5, 8, 2, 64], it is a sequential run, actually most of time is spent is on constructing TensorIterator itself since the problem size is very small. * when N is median size: e.g. N=50, slice_size  grain_size, it will parallel on index_size (e.g. 5). The magic number 5 is topk, it is not very friendly for parallelization: a) if you have 20 cores, 3/4 the CPU will be idle; b) when you have 4 cores, it will go 221 and still 1 core is idle. * when N is large: e.g. N = 114, slice_size > grain_size: it will parallel on slice_size. it will launch 5 omp sessions which will increase omp parallel overhead. The kernels in this PR is explained in the notes, and briefly: * when outer_size == 1 (dim=0): parallel on index_size; if slice_size is very big, do blocking to increase parallelism; * when outer_size != 1 (dim !=0): parallel on outer_size; if the inner_size is very small, use vectorized gather instead of scalar logic. The following is benchmark result from torch operator_benchmark, Xeon 6248, dual sockets, 20 cores per socket, 2.5GHz. ``` numactl physcpubind=0 membind=0 python m pt.index_select_test omp_num_threads 1 mkl_num_threads 1 tag_filter ${tag_name} ``` Unit: us per iteration.  index_select_test tag ""short"" Name  341.45% (*) for shape [50000, 256], prefetch will be helpful.",2022-05-05T02:47:48Z,open source cla signed Stale intel,closed,0,5,https://github.com/pytorch/pytorch/issues/76868,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76868**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9746998bdc (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,">  can you please rebase, as import/land attempt fail (build_variables.bzl were moved to top level folder) Probably still need to update this one since we recently started the optimization project on PyG. Need to make sure the current optimization is able to cover the scenarios from PyG models...","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: mingfeima / name: Ma Mingfei  (721c2663124470d436c9bcc6a65882a292ccce0c, d08de41ef603919a932549746c72a02189c7a865, 9746998bdcc51df5248208f7af0beeb6a35e4156)"
yi,Support indexing of the underlying tensors for nested tensors," 🚀 The feature, motivation and pitch Nested tensors were recently added to PyTorch core, but operator support is still fairly sparse. Notably, it's not yet possible to index the underlying tensors: ```python import torch x = torch.nested_tensor((torch.randn(3, 4), torch.randn(3, 5))) print(x[0])   RuntimeError: Internal error: NestedTensorImpl doesn't support sizes. Please file an issue on https://github.com/pytorch/nestedtensor ``` It'd be great to have simple indexing work to get the underlying tensors from a nested tensor. To start, this could support only indexing on the implicit ""batch dimension"" that nested tensor provides & error for anything else.  Alternatives None  Additional context _No response_ ",2022-05-04T20:39:11Z,triaged module: nestedtensor,closed,0,1,https://github.com/pytorch/pytorch/issues/76843,"Here's some useful code pointers for adding support. On the python side, the function called for indexing is `torch._C._TensorBase.__getitem__`, implementing the `Mapping` interface. This is defined for the `_TensorBase` class on the C++ side here: https://github.com/pytorch/pytorch/blob/55f55a4cf6cc50cde9e8e8369d92847ca85b23da/torch/csrc/autograd/python_variable.cppL1393 https://github.com/pytorch/pytorch/blob/55f55a4cf6cc50cde9e8e8369d92847ca85b23da/torch/csrc/autograd/python_variable.cppL1301L1305 https://github.com/pytorch/pytorch/blob/465e0ae2665b5474edf494247c9c809fbaac5210/torch/csrc/autograd/python_variable_indexing.cppL265L341 https://github.com/pytorch/pytorch/blob/465e0ae2665b5474edf494247c9c809fbaac5210/aten/src/ATen/TensorIndexing.hL469L540 This calls `applySelect()` for the 1D integer index case: https://github.com/pytorch/pytorch/blob/52af4fc5baee363bb9c3170017d46d4fe0069c56/aten/src/ATen/TensorIndexing.hL215L236 which dispatches to: https://github.com/pytorch/pytorch/blob/710246ea99478c011c7dff5690d1a3470d9b3aaa/aten/src/ATen/native/native_functions.yamlL3897L3903 The `select.int` function should be implemented for the `NestedTensorCPU` / `NestedTensorCUDA` dispatch keys."
rag,[PT-D][Sharding] Clean up sharded tensor code by leverage handle_torch_function,"  CC([PTD][Sharding] Clean up sharded tensor code by leverage handle_torch_function) In https://github.com/pytorch/pytorch/pull/75374, we have added some ops for sharded tensor and there are some duplicate code which can be removed by `leveraging handle_torch_function` Differential Revision: D36135157",2022-05-04T16:47:25Z,oncall: distributed cla signed sharded_tensor release notes: distributed (sharded) topic: new features,closed,0,2,https://github.com/pytorch/pytorch/issues/76824,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76824**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit d3b1d275c3 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[checkpoint] Handle overlapping storage during save and load," 🚀 The feature, motivation and pitch When loading a checkpoint, we should check whether any of the tensors have overlapping storage and, if so, we're loading data from different storage_keys. We should eliminate the superfluous load is such cases. A similar check should be performed when saving a checkpoint and use that to reduce the amount of IO done.  Alternatives _No response_  Additional context _No response_ ",2022-05-03T16:40:09Z,oncall: distributed triaged sharded_tensor,open,0,0,https://github.com/pytorch/pytorch/issues/76745
finetuning,RuntimeError while optimizing a TorchScript model," 🐛 Describe the bug I run into a peculiar issue when trying to carry out finetuning on a TorchScript model: during the **second** `backward()` call it fails with a `RuntimeError`. The same PyTorch model can be optimized without problems and the issue also disappears if lines 1720 are wrapped into a `torch.no_grad()` block and / or strangely if the `print` statement on line 22 gets uncommented.  Code ```python import torch class Model(torch.nn.Module):     def __init__(self):         super().__init__()         self.embed = torch.nn.Embedding(1000, 512)         self.lin = torch.nn.Linear(512, 1024)         self.loss = torch.nn.CTCLoss(0, reduction='sum')     def make_pad_mask(self, ylen, y):         r = torch.arange(0, y.size(1), dtype=ylen.dtype).unsqueeze(0)         u = r.expand(ylen.size(0), r.size(1))         return u >= ylen.unsqueeze(1)     def forward(self, x, xlen, y, ylen):          with torch.no_grad():         m = self.make_pad_mask(ylen, y)         y.masked_fill_(m, 0)         c = y.new_full((y.size(0), 1), 0)         z = torch.cat([c, y], dim=1)          print('uncommenting this print fixes the issue')         z = self.embed(z)         z = self.lin(z)         logits = x + z         logits = logits.transpose(0, 1)         return self.loss(logits, y, xlen, ylen) model = Model() scripted = torch.jit.script(model) torch.jit.save(scripted, 'export.pt') x = torch.rand(1, 15, 1024) xlen = torch.tensor([15], dtype=torch.int) y = torch.tensor([[33, 50, 49, 42, 326, 32, 172, 205, 242, 66, 162, 151, 51, 504]], dtype=torch.int) ylen = torch.tensor([y.size(1)], dtype=torch.int) model = torch.jit.load('export.pt')  commenting this line out fixes the issue (ie. using the PyTorch model instead of the TorschScript one) optimizer = torch.optim.SGD(model.parameters(), lr=1e5) for i in range(5):     loss = model(x, xlen, y, ylen)     optimizer.zero_grad()     loss.backward()     optimizer.step() ```  Error ``` Traceback (most recent call last):   File ""bug.py"", line 46, in      loss.backward()   File ""/home/User/Documents/env/lib/python3.8/sitepackages/torch/_tensor.py"", line 363, in backward     torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)   File ""/home/Usere/Documents/env/lib/python3.8/sitepackages/torch/autograd/__init__.py"", line 173, in backward     Variable._execution_engine.run_backward(   Calls into the C++ engine to run the backward pass RuntimeError: Function torch::jit::(anonymous namespace)::DifferentiableGraphBackward returned an invalid number of gradients  expected 6, but got 5 ```  Versions PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.22.3 Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.13.040genericx86_64withglibc2.29 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] torch==1.11.0 [conda] Could not collect",2022-05-03T15:02:40Z,oncall: jit,closed,0,1,https://github.com/pytorch/pytorch/issues/76739,This doesn't repro for me on master any more. Please feel free to reopen if you see an issue on master.
rag,[Model Averaging] Support disabling post-local gradient sync,"I find that sometimes disabling intrasubgroup gradient allreduce can still give a satisfying accuracy for some cases, so better to make such gradient averaging configurable. This does not take into account the saving in the communication of allreducing gradients.",2022-05-03T07:35:41Z,oncall: distributed open source Merged cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/76723,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76723**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: 1 Base Failures As of commit e832ff58bf (more details on the Dr. CI page): Expand to see more  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 81528d4b21 on May 10 from  4:33pm to  9:34pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * pull / linuxbionicrocm5.1py3.7 / test (default, 1, 2, linux.rocm.gpu) on May 10 from  4:33pm to  9:34pm (02713221e3  3a68155ce0)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","> Sounds good, but this is no longer local SGD then? Curious why disabling intranode allreduce can help. This allows disabling gradient averaging. It still keeps the concurrent parameter/model averaging, which is the crux of local SGD.","> Looks like a test is failing, can you take a look? test_ddp_hook_parity_post_localSGD: https://github.com/pytorch/pytorch/runs/6285865880?check_suite_focus=true Should be fixed now.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Support no sharding config,  CC(Support no sharding config)  CC(Provide an auto wrap policy for common transformer models) supporting no sharding config to make it similar to DDP algorithm Differential Revision: D36050353,2022-05-01T06:42:13Z,oncall: distributed cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/76628,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76628**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 7554f35024 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6245169194?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220501T06:55:17.4357861Z The PR is introduc...m to confirm whether this change is wanted or not.  ``` 20220501T06:55:17.4345222Z processing existing schema:  text(__torch__.torch.classes.profiling.SourceRef _0) > (str _0) 20220501T06:55:17.4346412Z processing existing schema:  count(__torch__.torch.classes.profiling.InstructionStats _0) > (int _0) 20220501T06:55:17.4347600Z processing existing schema:  duration_ns(__torch__.torch.classes.profiling.InstructionStats _0) > (int _0) 20220501T06:55:17.4349072Z processing existing schema:  source(__torch__.torch.classes.profiling.SourceStats _0) > (__torch__.torch.classes.profiling.SourceRef _0) 20220501T06:55:17.4351027Z processing existing schema:  line_map(__torch__.torch.classes.profiling.SourceStats _0) > (Dict(int, __torch__.torch.classes.profiling.InstructionStats) _0) 20220501T06:55:17.4351946Z processing existing schema:  __init__(__torch__.torch.classes.profiling._ScriptProfile _0) > (NoneType _0) 20220501T06:55:17.4353448Z processing existing schema:  enable(__torch__.torch.classes.profiling._ScriptProfile _0) > (NoneType _0) 20220501T06:55:17.4354301Z processing existing schema:  disable(__torch__.torch.classes.profiling._ScriptProfile _0) > (NoneType _0) 20220501T06:55:17.4356040Z processing existing schema:  _dump_stats(__torch__.torch.classes.profiling._ScriptProfile _0) > (__torch__.torch.classes.profiling.SourceStats[] _0) 20220501T06:55:17.4357605Z processing existing schema:  __init__(__torch__.torch.classes.dist_rpc.WorkerInfo _0, str _1, int _2) > (NoneType _0) 20220501T06:55:17.4357861Z The PR is introducing backward incompatible changes to the operator library. Please contact PyTorch team to confirm whether this change is wanted or not.  20220501T06:55:17.4357875Z  20220501T06:55:17.4357946Z Broken ops: [ 20220501T06:55:17.4358479Z 	aten::_sparse_compressed_tensor_unsafe(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, *, int? dtype=None, int? layout=None, Device? device=None, bool? pin_memory=None) > (Tensor) 20220501T06:55:17.4358637Z 	prim::CudaFusionIvalGuard(...) > (bool) 20220501T06:55:17.4358923Z 	aten::linalg_ldl_factor_ex(Tensor self, *, bool hermitian=False, bool check_errors=False) > (Tensor LD, Tensor pivots, Tensor info) 20220501T06:55:17.4359319Z 	aten::linalg_ldl_factor_ex.out(Tensor self, *, bool hermitian=False, bool check_errors=False, Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info) > (Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info) 20220501T06:55:17.4359553Z 	aten::linalg_ldl_solve(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False) > (Tensor) 20220501T06:55:17.4359822Z 	aten::linalg_ldl_solve.out(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False, Tensor(a!) out) > (Tensor(a!)) 20220501T06:55:17.4360053Z 	aten::linalg_ldl_factor(Tensor self, *, bool hermitian=False) > (Tensor LD, Tensor pivots) 20220501T06:55:17.4360351Z 	aten::linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) > (Tensor(a!) LD, Tensor(b!) pivots) ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," yes, I've been benchmarking this version of ddp vs PyTorch C++ ddp, the perf gap is small and depends on the wrapping strategy. For 1GB bert model on 32 gpus with slow network connected.   1. if the bucket size is small like 25MB for PT DDP and wrapping min size is like 28MB for this version of ddp, this version of ddp has around 6% perf regression. The difference is because this version of ddp wrapping needs one more all reduce, not due to the python context switch;    2. if the bucket size is large like 40MB for PT DDP and wrapping min size is like 40MB for this version of ddp, this version of ddp has around 17% perf regression. because the last FSDP unit in this version of DDP wrapping has large delay to kick off the first all reduce; The difference is not due to the python context switch   3. if the bucket size larger than the model size, both PT DDP and this version of DDP will have single all reduce, they have the similar performance. That means python context switch is not a big concern again.   Overall, I think if the wrapping can be done well in this version of DDP and aligned with PT DDP bucketing orders, then the performance will be comparable. As you can see, we still need to improve our auto wrapping policy for both no_shard and fsdp strategy.   Once the auto wrapping issues is resolved for this API overall, it is promising to make this API back compatible with PT C++ DDP and merge them in the long run. ","closing this, in favor of https://github.com/pytorch/pytorch/pull/76736"
rag,Expand pow and float_pow sampling function for more coverage," 🐛 Describe the bug This issue is linked to:  CC(xlogy, xlog1py are all skipping TestGradients) with matching PR: CC(Remove pow and float_power TestGradient Skips) After looking into gradient computations for the pow and float pow the nans arise when lhs or base tensor has values that are negative.  Since the exponent or rhs tensor has fractional values for the opinfo test the power function can and does map floats > complex  values in the analytical case. An example of nans in the function. ``` import torch def rand_lh(shape, low=0, high=1):     rand_val = torch.rand(shape)     result = high * rand_val + low * (1  rand_val)     return result a = rand_lh((1,2,3,4), low = 1 ) power = torch.rand((1,2,3,4)) print(torch.pow(a, power)) ```  In order to unskip TestGradients the lhs low value can be set to 0 so that this tensor doesn't include any negative values.   Follow up work would be maybe adding documentation that includes this note. Or updating pow to promote the output tensor to complex type.   Versions Collecting environment information... PyTorch version: 1.12.0a0+git2469525 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 12.3.1 (arm64) GCC version: Could not collect Clang version: 13.1.6 (clang1316.0.21.2.3) CMake version: version 3.22.1 Libc version: N/A Python version: 3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ] (64bit runtime) Python platform: macOS12.3.1arm64arm64bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.942 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.2 [pip3] torch==1.12.0a0+git2469525 [conda] mypy                      0.942                    pypi_0    pypi [conda] mypy_extensions           0.4.3           py310hca03da5_1 [conda] nomkl                     3.0                           0 [conda] numpy                     1.21.2          py310hb38b75b_0 [conda] numpybase                1.21.2          py310h6269429_0 [conda] torch                     1.12.0a0+git2469525           dev_0     ",2022-04-27T21:05:57Z,module: tests triaged,open,1,7,https://github.com/pytorch/pytorch/issues/76483,Sounds like forward tests should fail also? Why were only gradient tests failing previously?,https://github.com/pytorch/pytorch/blob/a997046017aa9f97213f6bd50bc152a6445e8086/test/test_binary_ufuncs.pyL1048 I am not sure if this is the only place that the forward is tested but reading this looks like the base is bounded below by 1 or 0 . Also running np.power() on tensors with negative elements produces the same NaN behavior ,Also if i change the the low=1 on line 1048 and run the binary_ufunc_tests: `FAILED test/test_binary_ufuncs.py::TestBinaryUfuncsCPU::test_pow_cpu_bfloat16  ValueError: math domain error`,Nice analysis! Once the PR lands let's update the original issue to reflect it no longer applies to the pow and float_power operators,> I am not sure if this is the only place that the forward is tested but reading this looks like the base is bounded below by 1 or 0 . Maybe `test_reference_numerics` would have caught if the OpInfo for pow had a `.ref` field. Would it be `np.power` here? , Holy cow I can't believe that pow doesn't have a reference implementation! We should really prioritize adding them. (Maybe we could sneak pow's reference into this PR?),"If I add `ref=np.power` get a number of errors: `ValueError: Integers to negative integer powers are not allowed.` ``` Mismatched elements: 331 / 50625 (0.7%) Greatest absolute difference: nan at index (0,) (up to 1e05 allowed) Greatest relative difference: nan at index (0,) (up to 1.3e06 allowed) ``` ``` TypeError: pow() received an invalid combination of arguments  got (int, int), but expected one of:  * (Tensor input, Tensor exponent, *, Tensor out)  * (Number self, Tensor exponent, *, Tensor out)  * (Tensor input, Number exponent, *, Tensor out)  ```"
rag,Add from_blob variation with storage_offset argument,  CC(Add from_blob variation with storage_offset argument) Adding a from_blob that lets me set storage_offset. Differential Revision: D35979729,2022-04-27T20:46:56Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/76478,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76478**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9dca745331 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
yi,Fix GenLazyIR.node_base_ctor_call,"  CC(Make lazy tensor ptr class customizable)  CC(Make lazy tensor creation and value strings customizable)  CC(Fix GenLazyIR.node_base_ctor_call) Make node_base_ctor_call produce the entire node_bace_ctor_call. Previously it was only producing the beginning of the call, which was unintended. Addresses part of https://github.com/pytorch/xla/issues/3472 Differential Revision: D35980436",2022-04-27T19:57:42Z,cla signed topic: not user facing release notes: lazy,closed,0,2,https://github.com/pytorch/pytorch/issues/76471,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76471**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit ffe9782372 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
yi,[old]Fix GenLazyIR.node_base_ctor_call,old stack.. replaced with  https://github.com/pytorch/pytorch/pull/76471,2022-04-27T18:44:40Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/76458,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76458**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 1a6ffa40a5 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,oops i messed up my stack so i had to make a new one https://github.com/pytorch/pytorch/pull/76471
yi,"Revert ""Revert ""Allow specifying tags for aten operators in native_functions.yaml""""","Stack from ghstack:  CC(Revert ""Revert ""Allow specifying tags for aten operators in native_functions.yaml"""") This reverts commit ea44645c9a682a4e212e64b94a86383c3388ed6b. reland of https://github.com/pytorch/pytorch/pull/72549",2022-04-27T18:15:48Z,oncall: jit cla signed,closed,0,7,https://github.com/pytorch/pytorch/issues/76456,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76456**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 28512057ec (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6203870990?check_suite_focus=true) pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220428T02:10:40.8676526Z python: can't open...un_server.py': [Errno 2] No such file or directory  ``` 20220428T02:10:40.8436755Z + XLA_DIR=/var/lib/jenkins/workspace/xla 20220428T02:10:40.8439913Z ++ command v nvidiasmi 20220428T02:10:40.8483433Z + '[' x '' ']' 20220428T02:10:40.8483783Z + export 'XRT_DEVICE_MAP=CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0' 20220428T02:10:40.8484178Z + XRT_DEVICE_MAP='CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0' 20220428T02:10:40.8487715Z ++ shuf i 4070140999 n 1 20220428T02:10:40.8525081Z + XLA_PORT=40709 20220428T02:10:40.8525738Z + export 'XRT_WORKERS=localservice:0;grpc://localhost:40709' 20220428T02:10:40.8526190Z + XRT_WORKERS='localservice:0;grpc://localhost:40709' 20220428T02:10:40.8526539Z + python torch_xla/core/xrt_run_server.py port 40709 restart 20220428T02:10:40.8676526Z python: can't open file 'torch_xla/core/xrt_run_server.py': [Errno 2] No such file or directory 20220428T02:10:40.8694067Z + cleanup 20220428T02:10:40.8694251Z + retcode=2 20220428T02:10:40.8694488Z + set +x 20220428T02:10:40.8736338Z [error]Process completed with exit code 2. 20220428T02:10:40.8870261Z [group]Run pytorch/pytorch/.github/actions/getworkflowjobid 20220428T02:10:40.8870500Z with: 20220428T02:10:40.8870829Z   githubtoken: *** 20220428T02:10:40.8871000Z env: 20220428T02:10:40.8871139Z   IN_CI: 1 20220428T02:10:40.8871299Z   IS_GHA: 1 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","~Is there a reason for this revert? Can we include that in the PR description?~ Nevermind I see this is a reland, can we include a link to the original PR?","> ~Is there a reason for this revert? Can we include that in the PR description?~ >  > Nevermind I see this is a reland, can we include a link to the original PR? added in the description", merge this please,"Merge failed due to Command `git C /home/runner/work/pytorch/pytorch cherrypick x edafdc7562939df7586c72910067bccdde6e3831` returned nonzero exit code 1 ``` Automerging .github/workflows/lint.yml Automerging BUILD.bazel Automerging caffe2/CMakeLists.txt Automerging tools/autograd/gen_inplace_or_view_type.py Automerging tools/autograd/gen_python_functions.py Automerging tools/setup_helpers/generate_code.py CONFLICT (content): Merge conflict in tools/setup_helpers/generate_code.py Automerging torch/CMakeLists.txt Automerging torchgen/gen.py Automerging torchgen/gen_functionalization_type.py Automerging torchgen/model.py error: could not apply edafdc7562... Revert ""Revert ""Allow specifying tags for aten operators in native_functions.yaml"""" hint: After resolving the conflicts, mark them with hint: ""git add/rm "", then run hint: ""git cherrypick continue"". hint: You can instead skip this commit with ""git cherrypick skip"". hint: To abort and get back to the state before ""git cherrypick"", hint: run ""git cherrypick abort"". ``` Raised by https://github.com/pytorch/pytorch/actions/runs/2236559551", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Provide an auto wrap policy for common transformer models,  CC(Provide an auto wrap policy for common transformer models) Provide an auto wrap policy for common transformer models Differential Revision: D35972488,2022-04-27T17:19:06Z,oncall: distributed cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/76455,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76455**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 38295bbab0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"ci: Unblock syncbranches, add a58c6ae and 7106d21 to block list","Adds a58c6aea5a0c9f8759a4154e46f544c8b03b8db1 and 7106d216c29ca16a3504aa2bedad948ebcf4abc2 to the list of excluded commits since this was landed through phab and cherry picked to master directly Similar to https://github.com/pytorch/pytorch/pull/76231 In both cases the original `syncbranches.yml` run came up with `Nothing to do`: * Back out ""record_function: update to use custom_class API"" (a58c6aea5a0c9f8759a4154e46f544c8b03b8db1)   * https://github.com/pytorch/pytorch/runs/6153245247   * Originally landed through GHF, landed internally, reverted using GHF, relanded using GHF, reverted in Phab, cherry picked onto master * [PyTorch] Add native fast path for transformer encoder inference (7106d216c29ca16a3504aa2bedad948ebcf4abc2)   * https://github.com/pytorch/pytorch/actions/runs/2223597371   * Originally landed through GHF, reverted using GHF, relanded using Phab Signedoffby: Eli Uriegas ",2022-04-26T23:15:19Z,cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/76417,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76417**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f92eac5718 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,rocm averages 20m to pull docker image,"For the past week, rocm (specifically linuxbionicrocm5.0py3.7 / test) averages about 20m to pull the docker image, while the other linux jobs generally take about 2m. Example GHA runs: https://github.com/pytorch/pytorch/runs/6178824044?check_suite_focus=true, https://github.com/pytorch/pytorch/runs/6178824044?check_suite_focus=true, https://github.com/pytorch/pytorch/runs/6132356305?check_suite_focus=true ",2022-04-26T22:50:26Z,module: rocm,closed,1,2,https://github.com/pytorch/pytorch/issues/76413,"We discussed this with  yesterday.  Proposing CC([ROCm][GHA] keep docker images for at most 1 day) to help.  ROCm runners shouldn't need to completely wipe out all docker images, just the containers.  This should hopefully speed up docker image pulls. Followup: Is there a way to generate this timetopull metric rather than looking at individual job logs?  ","> Followup: Is there a way to generate this timetopull metric rather than looking at individual job logs? I look at rockset to get averages, but outside of that, I'm not sure"
yi,404 when trying to get pytorch-mutex-1.0-cuda.tar.bz2 from Conda," 🐛 Describe the bug As part of a Dockerfile, we have a line that runs the following: `conda install pytorch torchvision torchaudio torchtext cudatoolkit channel pytorch && pip install torchinfo torchsummary` This has been working fine, but it stopped working today with the error ""404"" NOT FOUND for url https://conda.anaconda.org/pytorch/noarch/pytorchmutex1.0cuda.tar.bz2 If you go to the page https://conda.anaconda.org/pytorch/noarch/, then you do see pytorchmutex1.0cuda.tar.bz2 listed there, but indeed, going to that file opens up a 404 page instead of downloading the file. What could be going on here? Why was that file removed from the conda channel?  Versions N/A ",2022-04-26T21:01:23Z,high priority triage review module: binaries,closed,2,9,https://github.com/pytorch/pytorch/issues/76401,I can reproduce this,Myself and  are currently looking into this,Pinning this issue since this will affect all conda users until it gets resolved,Issue created for anaconda repo here,"Same here. I used the simplest install command `conda install pytorch torchvision torchaudio cudatoolkit=11.3 c pytorch`, and it complains that pytorchmutex1.0cuda.tar.bz2 does not exist when downloaded.","Update to this, this is an outage from anaconda directly. They have an incident page up here: https://anaconda.statuspage.io/incidents/t1xrstmkf13x","Thanks, all. This is fixed.","> Thanks, all. This is fixed.   what is the solution, and why dont you write the solution?"," This was an issue from April, and I didn't solve it. In the link that  shared you can see the updates from the Anaconda team about that issue. (""Our engineering team has traced the issue back to a recent update. We’ve reverted this change, resolving the issue for all impacted users."" Posted Apr 27, 2022  00:09 UTC in Anaconda status page)"
yi,Whether 'targetSize' in inferExpandGeometryImpl needs to be checked when it is less than 0," 🐛 Describe the bug when I run below code ``` import torch a = torch.randn(1,3) a.expand([2, 2, 1, 3]) ``` I got error message as below: > Traceback (most recent call last): >   File """", line 1, in  >   File ""/home/pytorch/torch/_tensor.py"", line 341, in __repr__ >     return torch._tensor_str._str(self) >   File ""/home/pytorch/torch/_tensor_str.py"", line 439, in _str >     return _str_intern(self) >   File ""/home/pytorch/torch/_tensor_str.py"", line 414, in _str_intern >     tensor_str = _tensor_str(self, indent) >   File ""/home/pytorch/torch/_tensor_str.py"", line 264, in _tensor_str >     formatter = _Formatter(get_summarized_data(self) if summarize else self) >   File ""/home/pytorch/torch/_tensor_str.py"", line 92, in __init__ >     tensor_view = tensor.reshape(1) > RuntimeError: Trying to create tensor with negative dimension 2: [2, 2, 1, 3] It seems to report 'reshape' has invalid parameter， actually expand op does.  It causes confuse sometime. Maybe torch should check 'targerSize' in inferExpandGeometryImpl to make sure it is greater than or equal to 0.  Versions '1.12.0a0+git0aa3c39'",2022-04-26T03:37:57Z,module: error checking triaged,open,0,1,https://github.com/pytorch/pytorch/issues/76362,Agree that the error message could be clearer
rag,[ONNX] Check tensor has storage before refer to tensor data ptr,"In the exporter dedupe initializers passes, check the tensor has storage before reference to tensor's data_ptr, otherwise it will result in a crash.",2022-04-25T21:32:40Z,oncall: jit module: onnx open source cla signed release notes: onnx topic: bug fixes,closed,0,3,https://github.com/pytorch/pytorch/issues/76342,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76342**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit dd3a100496 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :female_detective: 1 failure *not* recognized by patterns: The following CI failures may be due to changes from the PR    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add native fast path for transformer encoder inference (re-land),"  CC([PyTorch] Add native fast path for transformer encoder inference (reland)) The current PyTorch multihead attention and transformer implementations are slow. This should speed them up for inference. This was previously erroneously landed via GitHub, but needs to land via fbcode. Differential Revision: D35239925 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-25T19:13:27Z,cla signed release notes: nn release notes: sparse topic: performance,closed,0,1,https://github.com/pytorch/pytorch/issues/76333,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76333**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 98c64024f0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  
rag,[checkpoint] Use fsspec to support object storage," 🚀 The feature, motivation and pitch Extends FileSystemStorageReader / FileSystemStorageWrite to work with fsspec so they can be used with object storage.  Alternatives _No response_  Additional context _No response_",2022-04-25T18:12:36Z,triaged sharded_tensor,closed,0,1,https://github.com/pytorch/pytorch/issues/76325,Closing as we now have https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/_fsspec_filesystem.py
transformer,improve log_softmax multi-core performance,"Stack from ghstack:  CC(improve log_softmax multicore performance)  CC(add BFloat16 support on CPU for cumsum, cumprod and logcumsumexp and adjust grain size)  CC(improve sort cpu bfloat16 perf by directly comparing on acc_type)  CC(improve sort multicore perf by adjusting grain_size w.r.t. dim_size) Differential Revision: D36337195 This PR is to improve `logsoftmax` multicore performance by decoupling `grain_size` and `chunk_size`. This original implementation has a minimum `grain_size` of `chunk_size` so that log can use vectorized implementation. This limits the parallelism for most of the input shapes in transformers. For example, the shape `[512, 28996]` would use only use 2 cores at most. We can decouple `grain_size` and `chunk_size`, the vec::map will still ensure to use vectorized implementation (it will add padding if necessary and do a vectorized log). Benchmark data on Xeon 6248, 20 cores per socket, dual socket, 2.50GHz:  cpu performance on single socket (20 cores): ```  before  torch.Size([512, 28996]) torch.float32 time: 14.025 ms  torch.Size([511, 250002]) torch.float32 time: 119.716 ms  after  torch.Size([512, 28996]) torch.float32 time: 2.715 ms  torch.Size([511, 250002]) torch.float32 time: 26.520 ms ```  cpu performance on 4 cores run ```  before  torch.Size([512, 28996]) torch.float32 time: 13.166 ms  torch.Size([511, 250002]) torch.float32 time: 119.698 ms  after  torch.Size([512, 28996]) torch.float32 time: 6.760 ms  torch.Size([511, 250002]) torch.float32 time: 62.041 ms ```  cpu performance on 1 core run ```  before  torch.Size([512, 28996]) torch.float32 time: 25.599 ms  torch.Size([511, 250002]) torch.float32 time: 232.332 ms  after  torch.Size([512, 28996]) torch.float32 time: 25.764 ms  torch.Size([511, 250002]) torch.float32 time: 232.528 ms ``` ",2022-04-24T06:43:43Z,module: cpu open source cla signed topic: not user facing intel,closed,0,10,https://github.com/pytorch/pytorch/issues/76279,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76279**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 4 New Failures As of commit 258a41f4b3 (more details on the Dr. CI page): Expand to see more  * **4/4** failures introduced in this PR   :detective: 4 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6749307404?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7 / test (default, 1, 4, linux.4xlarge.nvidia.gpu) (1/4) **Step:** ""Test"" (full log  :repeat: rerun)   20220606T03:07:47.7424398Z RuntimeError: CUDA error: an illegal memory access was encountered  ``` 20220606T03:07:47.7419988Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 1168, in wrapper 20220606T03:07:47.7420357Z     fn(*args, **kwargs) 20220606T03:07:47.7420652Z   File ""test_meta.py"", line 908, in test_dispatch_meta 20220606T03:07:47.7420947Z     for sample_input in samples: 20220606T03:07:47.7421443Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/autograd/grad_mode.py"", line 43, in generator_context 20220606T03:07:47.7421822Z     response = gen.send(None) 20220606T03:07:47.7422393Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_methods_invocations.py"", line 6454, in sample_inputs_linalg_cholesky_inverse 20220606T03:07:47.7422947Z     batch_well_conditioned_matrices = random_well_conditioned_matrix(2, S, S, dtype=dtype, device=device) 20220606T03:07:47.7423592Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 2659, in random_well_conditioned_matrix 20220606T03:07:47.7424044Z     u, _, vh = torch.linalg.svd(x, full_matrices=False) 20220606T03:07:47.7424398Z RuntimeError: CUDA error: an illegal memory access was encountered 20220606T03:07:47.7424873Z CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. 20220606T03:07:47.7425319Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1. 20220606T03:07:47.7425535Z  20220606T03:07:47.7425785Z  20220606T03:07:47.7426113Z Ran 837 tests in 309.077s 20220606T03:07:47.7426279Z  20220606T03:07:47.7426422Z FAILED (errors=10, expected failures=30) 20220606T03:07:47.7426612Z  20220606T03:07:47.7426734Z Generating XML reports... 20220606T03:07:47.8215850Z Generated XML report: testreports/pythonunittest/test_meta/TESTTestMetaCUDA20220606030238.xml ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ","TODO:  [ ] experiment with implementation with parallel on inner dimension of logsoftmax. With commonly used shapes in logsoftmax from Transformer models, the lastdim is quite large (20k~300k). And the kernel of logsoftmax requires to read the input for 3 times, so it is possibly to be swapped out of cache. The cuda kernel utilized different block setup rules for different input shapes, which will go parallel on lastdim in case it is big enough so as to improve cache hit rate for the input data. Try to implement similar kernel for CPU (need to plant it into 3rd party, e.g. fbgemm, since the kernel requires sync inside omp loops and currently aten won't compile it)","wei has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","wei has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.",Pls check failed test cases.,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","/easycla As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA. See CC(Contributor License Agreement Updates) for additional details. This comment will trigger a new check of this PR. If you are already covered, you will simply see a new ""EasyCLA"" check that passes. If you are not covered, a bot will leave a new comment with a link to sign.","The committers listed above are authorized under a signed CLA.:white_check_mark: login: mingfeima / name: Ma Mingfei  (80e1817b47425326a99e72291faaff1701108c78, 97a7e49db6d523ec90b811d7773ddcd43157ecbd, dbfe108107a605874c950a28608ddb19b4ed2d77, d60774c3abfe4082bcbdc295b2cb8f45cd59ce65, 0623b4ad4d290e90818fadcb161978bb1df5bdf2, 455fd15d21dcbbac938f4c7bc96814862eebd6fb, b902e1990c78e2fe256adb88b87a3f4fc6fc0eef, e2ff07fb39fda6507eb2ef3d2f73e22fca893e4b, 258a41f4b399865ca4b7d959158dd41f0bac0cb5)","Hi , can we add this change in CC(Fix threadallocation in `_vec_log_softmax_lastdim`)? CC(Fix threadallocation in `_vec_log_softmax_lastdim`)'s merging had failed earlier but I made more changes &  approved it y'day. Thanks!","> Hi , can we add this change in CC(Fix threadallocation in `_vec_log_softmax_lastdim`)? CC(Fix threadallocation in `_vec_log_softmax_lastdim`)'s merging had failed earlier but I made more changes &  approved it y'day. Thanks! Oh, cool. I will close this one!"
rag,[Model Averaging] Make the error message more informative in hierarchical_model_averager.py,As title,2022-04-24T05:09:16Z,oncall: distributed open source cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/76277,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76277**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit cf444eae15 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,The document's representation of the underlying document," 📚 The doc issue Hello,  Thanks you all! I am looking for a function of torch.stft today. But I only found that it ended up returning ._VF.stft.  I didn't find any description of the underlying implementation.  I have some doubts about this.   Suggest a potential alternative/fix _No response_",2022-04-24T03:41:31Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/76276,The documentation for torch.stft can be found here
transformer,deploy: add dummy metadata for builtin packages,"This adds dummy metadata for frozen builtin packages when using `torch::deploy`. This is a bit hacky but unblocks allows Huggingface transformers library to be used within `torch::deploy` which depends on `importlib.metadata.version` to detect whether torch is installed or not. https://github.com/huggingface/transformers/blob/main/src/transformers/utils/import_utils.pyL49 Test plan: Added `importlib.metadata.version(""torch"")` unit test",2022-04-22T00:56:05Z,cla signed release notes: package/deploy topic: bug fixes,closed,0,6,https://github.com/pytorch/pytorch/issues/76211,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76211**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit cee8c44fbd (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,Elastic agents cannot properly shutdown/restart after failure," 🐛 Describe the bug I am trying to run a distributed training using the Elastic Job Controller (from here: https://github.com/pytorch/elastic/tree/master/kubernetes) and it seems as though that the distributed agenst is  be able to shutdown/restart themselves properly. Here is a stacktrace of what happens, eventually, when one of the pods running the agent, dies: I know the underlying issue might be coming from anywhere but I wanted to reach out to the broader community just to see if anyone ran into a similar issue. I also asked the same question here ``` [E ProcessGroupGloo.cpp:136] Rank 1 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank. Error executing job with overrides: ['learning_rate=single', 'model.architecture=efflitenet40', 'dataset.val.dataset_dpath=/data/wild_v4_tiny_fr_val', 'optimization.batch_size=256', 'backend.num_nodes=4', 'experiment_id=experiment01', 'logging.ckpt_nsteps=500', 'logging.run_id=c61a45bf6a054435944becc895b75a19', 'backend.num_nodes=4', 'resume_training.auto_resume=True', 'backend.wandb_group=multinodec61a45bf6a054435944becc895b75a19', 'backend.num_cores=1'] Traceback (most recent call last):   File ""pl_trainer.py"", line 422, in main     trainer.fit(pl_model, pl_data)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/trainer/trainer.py"", line 768, in fit     self._call_and_handle_interrupt(   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/trainer/trainer.py"", line 721, in _call_and_handle_interrupt     return trainer_fn(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/trainer/trainer.py"", line 809, in _fit_impl     results = self._run(model, ckpt_path=self.ckpt_path)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/trainer/trainer.py"", line 1234, in _run     results = self._run_stage()   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/trainer/trainer.py"", line 1321, in _run_stage     return self._run_train()   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/trainer/trainer.py"", line 1351, in _run_train     self.fit_loop.run()   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/base.py"", line 204, in run     self.advance(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/fit_loop.py"", line 269, in advance     self._outputs = self.epoch_loop.run(self._data_fetcher)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/base.py"", line 204, in run     self.advance(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/epoch/training_epoch_loop.py"", line 208, in advance     batch_output = self.batch_loop.run(batch, batch_idx)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/base.py"", line 204, in run     self.advance(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/batch/training_batch_loop.py"", line 88, in advance     outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/base.py"", line 204, in run     self.advance(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/optimization/optimizer_loop.py"", line 203, in advance     result = self._run_optimization(   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/optimization/optimizer_loop.py"", line 256, in _run_optimization     self._optimizer_step(optimizer, opt_idx, batch_idx, closure)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/optimization/optimizer_loop.py"", line 369, in _optimizer_step     self.trainer._call_lightning_module_hook(   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/trainer/trainer.py"", line 1593, in _call_lightning_module_hook     output = fn(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/core/lightning.py"", line 1625, in optimizer_step     optimizer.step(closure=optimizer_closure)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/core/optimizer.py"", line 168, in step     step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/strategies/ddp.py"", line 278, in optimizer_step     optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/strategies/strategy.py"", line 193, in optimizer_step     return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/plugins/precision/native_amp.py"", line 85, in optimizer_step     closure_result = closure()   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/optimization/optimizer_loop.py"", line 148, in __call__     self._result = self.closure(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/optimization/optimizer_loop.py"", line 134, in closure     step_output = self._step_fn()   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/loops/optimization/optimizer_loop.py"", line 427, in _training_step     training_step_output = self.trainer._call_strategy_hook(""training_step"", *step_kwargs.values())   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/trainer/trainer.py"", line 1763, in _call_strategy_hook     output = fn(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/pytorch_lightning/strategies/ddp.py"", line 341, in training_step     return self.model(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/nn/modules/module.py"", line 1110, in _call_impl     return forward_call(*input, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/nn/parallel/distributed.py"", line 955, in forward     self._sync_buffers()   File ""/usr/local/lib/python3.8/distpackages/torch/nn/parallel/distributed.py"", line 1602, in _sync_buffers     self._sync_module_buffers(authoritative_rank)   File ""/usr/local/lib/python3.8/distpackages/torch/nn/parallel/distributed.py"", line 1606, in _sync_module_buffers     self._default_broadcast_coalesced(authoritative_rank=authoritative_rank)   File ""/usr/local/lib/python3.8/distpackages/torch/nn/parallel/distributed.py"", line 1627, in _default_broadcast_coalesced     self._distributed_broadcast_coalesced(   File ""/usr/local/lib/python3.8/distpackages/torch/nn/parallel/distributed.py"", line 1543, in _distributed_broadcast_coalesced     dist._broadcast_coalesced( RuntimeError: Rank 1 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank.  Original exception: [../third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [10.116.5.70]:40678 Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace. wandb: Waiting for W&B process to finish... (failed 1). Press ControlC to abort syncing. wandb: wandb: Synced playfuljazz804: https://wandb.ai/teamquentin/face_recognitionface_recognition_train/runs/ny9ee8dv wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s) wandb: Find logs at: ./wandb/run20220421_193319ny9ee8dv/logs ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 421) of binary: /usr/bin/python INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish ERROR:torch.distributed.elastic.agent.server.api:Error waiting on exit barrier. Elapsed: 300.1109097003937 seconds Traceback (most recent call last):   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/elastic/agent/server/api.py"", line 906, in _exit_barrier     store_util.barrier(   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/elastic/utils/store.py"", line 67, in barrier     synchronize(store, data, rank, world_size, key_prefix, barrier_timeout)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/elastic/utils/store.py"", line 53, in synchronize     agent_data = get_all(store, key_prefix, world_size)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/elastic/utils/store.py"", line 31, in get_all     data = store.get(f""{prefix}{idx}"")   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/elastic/rendezvous/etcd_store.py"", line 75, in get     raise LookupError(f""Key {key} not found in EtcdStore"") LookupError: Key torchelastic/agent/terminal_state3 not found in EtcdStore INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 1 FAILED with no error file. Decorate your entrypoint fn with  for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html Traceback (most recent call last):   File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/run.py"", line 728, in      main()   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper     return f(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/run.py"", line 724, in main     run(args)   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/run.py"", line 715, in run     elastic_launch(   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/usr/local/lib/python3.8/distpackages/torch/distributed/launcher/api.py"", line 245, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ============================================================ pl_trainer.py FAILED  Failures:     Root Cause (first observed failure): [0]:   time      : 20220421_19:48:31   host      : jobc61a45bf6a054435944becc895b75a19worker0   rank      : 1 (local_rank: 0)   exitcode  : 1 (pid: 421)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ```  Versions PyTorch version: 1.11.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.1 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.144+x86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: Tesla V100SXM216GB GPU 1: Tesla V100SXM216GB GPU 2: Tesla V100SXM216GB GPU 3: Tesla V100SXM216GB Nvidia driver version: 450.119.04 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] pytorchlightning==1.6.1 [pip3] torch==1.11.0+cu113 [pip3] torchmetrics==0.4.1 [pip3] torchvision==0.11.0+cu113 [conda] Could not collect ",2022-04-21T21:23:08Z,oncall: distributed module: elastic,closed,0,2,https://github.com/pytorch/pytorch/issues/76198,"Hi I see this error message: ``` Rank 1 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank. ``` Did you get a chance to see what happened on rank 0? As suggested by the message, rank 1 failing may be a side effect of that.","Yes. Actually, I realized that I was not setting `maxrestarts` to a value greater than 0. Once I started passing `maxrestarts=5` to `python m torch.distributed.run` everything started working as expected."
agent,Add optional timeout argument for RpcAgent join(),Differential Revision: D35825382 Changes:  Adds timeout argument to RpcAgent.join()  Update API.py to also include fix bug (missing timeout for signal)  Change default shutdown timeout to 0 (no timeout). Existing functionality in _all_gather will remain the same and wait indefinitely for signal if no timeout is set for the function. New functionality has user specify timeout for both the signal and rpc calls.,2022-04-21T20:23:39Z,oncall: distributed fb-exported cla signed ciflow/trunk release notes: distributed (rpc),closed,0,9,https://github.com/pytorch/pytorch/issues/76194,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76194**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 40c3c13fe9 (more details on the Dr. CI page): Expand to see more  * **1/2** failures introduced in this PR * **1/2** broken upstream at merge base 9bf2c87e2b from Apr 27 until Apr 28   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6214025523?check_suite_focus=true) pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220428T15:49:52.0871716Z python: can't open...un_server.py': [Errno 2] No such file or directory  ``` 20220428T15:49:52.0686304Z + XLA_DIR=/var/lib/jenkins/workspace/xla 20220428T15:49:52.0689149Z ++ command v nvidiasmi 20220428T15:49:52.0692218Z + '[' x '' ']' 20220428T15:49:52.0692782Z + export 'XRT_DEVICE_MAP=CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0' 20220428T15:49:52.0693309Z + XRT_DEVICE_MAP='CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0' 20220428T15:49:52.0696872Z ++ shuf i 4070140999 n 1 20220428T15:49:52.0722285Z + XLA_PORT=40785 20220428T15:49:52.0722734Z + export 'XRT_WORKERS=localservice:0;grpc://localhost:40785' 20220428T15:49:52.0723083Z + XRT_WORKERS='localservice:0;grpc://localhost:40785' 20220428T15:49:52.0723415Z + python torch_xla/core/xrt_run_server.py port 40785 restart 20220428T15:49:52.0871716Z python: can't open file 'torch_xla/core/xrt_run_server.py': [Errno 2] No such file or directory 20220428T15:49:52.0889153Z + cleanup 20220428T15:49:52.0889356Z + retcode=2 20220428T15:49:52.0889541Z + set +x 20220428T15:49:52.1082783Z [error]Process completed with exit code 2. 20220428T15:49:52.1152441Z [group]Run pytorch/pytorch/.github/actions/getworkflowjobid 20220428T15:49:52.1152678Z with: 20220428T15:49:52.1153004Z   githubtoken: *** 20220428T15:49:52.1153174Z env: 20220428T15:49:52.1153329Z   IN_CI: 1 20220428T15:49:52.1153473Z   IS_GHA: 1 ```    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * trunk / linuxbionicrocm5.1py3.7distributed / test (distributed, 1, 1, linux.rocm.gpu) from Apr 27 until Apr 28 (d0cb31d5bc  833d65aecb)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,This pull request was **exported** from Phabricator. Differential Revision: D35825382,"Hey Huang. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[quant][core][gpu][feature] Added support for float->quantized cuda tensor copying,"  CC([quant][core][gpu][improvement] Added support for padding quantized cudnn conv2d operator)  CC([quant][core][gpu][feature] Added support for float>quantized cuda tensor copying) Summary: Previously, support for copying a fp tensor to a quantized tensor was limited to CPU tensors. This PR extends the support to GPU tensors. A corresponding test was added to test_qtensor_float_assignment for cuda tensors Test plan: ``` python test/test_quantization.py k test_qtensor_float_assignment ``` Differential Revision: D35817832",2022-04-21T15:55:00Z,cla signed release notes: quantization topic: new features,closed,0,7,https://github.com/pytorch/pytorch/issues/76177,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76177**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 152bb6dd06 (more details on the Dr. CI page): Expand to see more  * **5/5** failures introduced in this PR   :detective: 5 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6194983288?check_suite_focus=true) pull / winvs2019cuda11.3py3 / test (default, 2, 2, windows.8xlarge.nvidia.gpu) (1/5) **Step:** ""Test"" (full log  :repeat: rerun)   20220427T15:10:55.9497321Z Build left local git repository checkout dirty  ``` 20220427T15:10:54.9942734Z + assert_git_not_dirty 20220427T15:10:54.9943032Z + [[ winvs2019cpupy3 != *rocm* ]] 20220427T15:10:54.9943278Z + [[ winvs2019cpupy3 != *xla* ]] 20220427T15:10:55.0022031Z ++ git status porcelain 20220427T15:10:55.9495881Z + git_status='?? quantized' 20220427T15:10:55.9496210Z + [[ n ?? quantized ]] 20220427T15:10:55.9496470Z + echo 'Build left local git repository checkout dirty' 20220427T15:10:55.9496717Z + echo 'git status porcelain:' 20220427T15:10:55.9496924Z + echo '?? quantized' 20220427T15:10:55.9497113Z + exit 1 20220427T15:10:55.9497321Z Build left local git repository checkout dirty 20220427T15:10:55.9497565Z git status porcelain: 20220427T15:10:55.9497752Z ?? quantized 20220427T15:10:55.9501275Z + cleanup 20220427T15:10:55.9501449Z + retcode=1 20220427T15:10:55.9501611Z + set +x 20220427T15:10:55.9556183Z [error]Process completed with exit code 1. 20220427T15:10:55.9891823Z [group]Run pytorch/pytorch/.github/actions/getworkflowjobid 20220427T15:10:55.9892147Z with: 20220427T15:10:55.9893078Z   githubtoken: *** 20220427T15:10:55.9893381Z env: ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
transformer,[PyTorch] Fix bulleted lists in transformer fast path docstrings,  CC([PyTorch] Fix bulleted lists in transformer fast path docstrings)  CC([PyTorch] Add native fast path for transformer encoder inference) The formatting was off. Differential Revision: D35797735,2022-04-20T23:00:11Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/76154,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76154**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit b45fbec0e2 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :female_detective: 1 failure *not* recognized by patterns: The following CI failures may be due to changes from the PR    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"was hoping to land previous PR without another update, but internal CI is failing; rolling this in there"
rag,Coverage test is only checking packages and not all submodules,The check for `ispkg` should be removed from this check: https://github.com/pytorch/pytorch/blob/81722f66306aef05334c27bb2a64137d8fa6493a/docs/source/conf.pyL469 The allowlist need to be updated to reflect that. ,2022-04-20T20:42:44Z,module: docs triaged module: python frontend,open,0,0,https://github.com/pytorch/pytorch/issues/76138
rag,Calling storage() on lazy tensor does not work," 🐛 Describe the bug ``` >>> import torch._lazy.ts_backend >>> torch._lazy.ts_backend.init() >>> torch.empty(0, device='lazy') tensor([], device='lazy:0') >>> torch.empty(0, device='lazy').storage() Traceback (most recent call last):   File """", line 1, in    File ""/data/users/ezyang/pytorchtmp/torch/_tensor.py"", line 206, in storage     return torch._TypedStorage(wrap_storage=self._storage(), dtype=self.dtype) RuntimeError: it != attype_to_py_storage_type.end() INTERNAL ASSERT FAILED at ""/data/users/ezyang/pytorchtmp/torch/csrc/DynamicTypes.cpp"":69, please report a bug to PyTorch. Failed to get the Python type of `_UntypedStorage`. ```    Versions master",2022-04-20T18:22:04Z,triaged module: lazy,closed,1,2,https://github.com/pytorch/pytorch/issues/76124,"This error no longer happens, probably after CC(Merge torch.cuda._UntypedStorage into torch._UntypedStorage), or maybe CC(Add meta device support to `_UntypedStorage` and `_TypedStorage`): ```python >>> import torch >>> import torch._lazy.ts_backend >>> torch._lazy.ts_backend.init() >>> torch.empty(0, device='lazy') tensor([], device='lazy:0') >>> torch.empty(0, device='lazy').storage() [torch.storage._TypedStorage(dtype=torch.float32, device=lazy:0) of size 0] ``` However, this doesn't seem right: ```python >>> torch.empty(10, device='lazy').storage() [torch.storage._TypedStorage(dtype=torch.float32, device=lazy:0) of size 0] ``` I don't know very much at all about lazy tensors, but I assume the storage size is supposed to correspond with the tensor's size","This is LTC's problem,  functionalize storages should fix it, so we can close this."
rag,Allow any operation that takes a Storage to also take a contiguous Tensor instead," 🐛 Describe the bug We don't like storages, right? So we should make it possible to do stuff in Python userland without having to explicitly refer to storage. Allowing all operations on storages to instead accept contiguous tensors is a good start; e.g., `set_(..., storage_offset, size, strides)` should take a contiguous tensor. One fly in the ointment is that even if a tensor is contiguous, it could refer to the inside of a storage (nonzero storage offset).  This should probably also be disallowed when using the API in this way, or accounted for (in the case of `set_`, you can just implicitly adjust `storage_offset` and it will work out)  Versions master",2022-04-20T13:48:10Z,feature triaged module: python frontend,open,0,0,https://github.com/pytorch/pytorch/issues/76106
rag,[quant][core][gpu][feature] Implemented quantized cuda adaptive average pool2d op,"  CC([quant][core][gpu][feature] Implemented quantized cuda adaptive average pool2d op) Summary: The current implementation of quantized cuda adaptive average pooling uses the following: dequant > fp32 adaptive average pooling > quant. This is the same numerically as quantized adaptive average pooling. This is not the ideal implementation, as we desire to operate on the quantized values directly. However, we are currently blocked on this as we are waiting for cudnn's 8.5.0 release, which is anticipated to support adaptive average pooling. When that support is made available, we will use it directly. Test plan: ``` python test/test_quantization.py TestQuantizedOps.test_adaptive_avg_pool ``` Differential Revision: D35768751",2022-04-20T01:14:27Z,cla signed release notes: quantization topic: new features,closed,0,10,https://github.com/pytorch/pytorch/issues/76081,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76081**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit 9c052d3e4a (more details on the Dr. CI page): Expand to see more  * **3/3** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6204335663?check_suite_focus=true) pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) (1/3) **Step:** ""Test"" (full log  :repeat: rerun)   20220428T08:48:45.7766630Z [error]The reque...igured HttpClient.Timeout of 100 seconds elapsing.  ``` 20220428T08:43:02.0399749Z [endgroup] 20220428T08:43:02.0403110Z Secret source: Actions 20220428T08:43:02.0403727Z Prepare workflow directory 20220428T08:43:02.1539110Z Prepare all required actions 20220428T08:43:02.1784034Z Getting action download info 20220428T08:43:02.7115125Z Download action repository 'pytorch/pytorch' (SHA:f51516df1c977921207a01d601b6915d18bb1286) 20220428T08:44:42.7450076Z [warning]Failed to download action 'https://api.github.com/repos/pytorch/pytorch/tarball/f51516df1c977921207a01d601b6915d18bb1286'. Error: The request was canceled due to the configured HttpClient.Timeout of 100 seconds elapsing. 20220428T08:44:42.7468190Z [warning]Back off 26.723 seconds before retry. 20220428T08:46:49.4831410Z [warning]Failed to download action 'https://api.github.com/repos/pytorch/pytorch/tarball/f51516df1c977921207a01d601b6915d18bb1286'. Error: The request was canceled due to the configured HttpClient.Timeout of 100 seconds elapsing. 20220428T08:46:49.4834225Z [warning]Back off 16.277 seconds before retry. 20220428T08:48:45.7766630Z [error]The request was canceled due to the configured HttpClient.Timeout of 100 seconds elapsing. ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.",land the max pool 2d PR first: https://github.com/pytorch/pytorch/pull/76081 and then come back and fix the macros here,rebase on master after https://github.com/pytorch/pytorch/pull/76129 is merged with master. should fix the current CI errors," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
transformer,Can 'aten::to_dense' be suppored by 'CUDA' backend," 🚀 The feature, motivation and pitch I have run my code on CUDA 11.3 with torch '1.10.1+cu113': sp_attn = torch.sparse_coo_tensor(i, V, (Nt, Ns) ) out =torch.sparse.mm(sp_attn, V).to_dense() however, I have got a CUDA error like this: NotImplementedError: Could not run 'aten::to_dense' with arguments from 'CUDA' backend.  Motivation I think the sparse matrix is more and more useful, especially with the transformer and its selfattention mechanisms explosion trending. The 'aten::to_dense' should not be available for 'SparseCUDA',  Can it be available for 'CUDA' backend? Thank you.  Alternatives _No response_  Additional context _No response_ ",2022-04-19T17:15:40Z,module: sparse triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/76051,"I don't whether it is a bug caused by torch.sparse.mm(). Because when I use the to_dense() directly after constructing a sparse tensor, it is ok. However, when I use to_dense() after torch.sparse.mm(), it will not be ok.",Hello yiqun  Could please try again with a nightly? If it succeeds then it'll have been resolved in the upcoming release.,"Hi,   I have tried it again with the nightly. It succeeds. However, unfortunately,  some other functions such as ""torch.inverse"" are not stable in the nightly version. Thus, I hope the upcoming release can come out soon. Thank you very much"
transformer,Experimental MetaTensorTracer,"Stack from ghstack:  CC(Fix unnecessary recursion in GraphModule.__call__)  CC(Experimental MetaTensorTracer) Experimental `Tracer` implementation that carries a meta tensor for each value and allows access to the concrete shape/dtype/etc values. Confirmed to work with HF models (except for `*ForQuestionAnswering`). See https://github.com/pytorch/PiPPy/pull/138 Example: ``` import inspect import torch import transformers.utils.fx as fx from transformers import * device = torch.device('meta') config = BertConfig() model = BertModel(config) model.to(device)  hf_tracer = HFBertTracer() input_names = model.dummy_inputs.keys() sig = inspect.signature(model.forward) concrete_args = {p.name: p.default for p in sig.parameters.values() if p.name not in input_names} from torch.fx.experimental.meta_tracer import MetaTracer tracer = MetaTracer() T, B = 50, 50 concrete_metas = [None, torch.zeros(T, B, dtype=torch.long, device='meta')] graph = tracer.trace(model, concrete_metas, concrete_args) gm = torch.fx.GraphModule(model, graph) print(gm.code) """""" def forward(self, input_ids : typing_Union[torch.Tensor,NoneType] = None, attention_mask_1 = None, token_type_ids_1 = None, position_ids_1 = None, head_mask_1 = None, inputs_embeds_1 = None, encoder_hidden_states_1 = None, encoder_attention_mask_1 = None, past_key_values_1 = None, use_cache_1 = None, output_attentions_1 = None, output_hidden_states_1 = None, return_dict_1 = None) > typing_Union[typing_Tuple[torch.Tensor],transformers_modeling_outputs_BaseModelOutputWithPoolingAndCrossAttentions]:     _assert_is_none = torch.fx._symbolic_trace._assert_is_none(attention_mask_1, 'attention_mask has been specialized to have value None but got another value');  attention_mask_1 = None     _assert_is_none_1 = torch.fx._symbolic_trace._assert_is_none(token_type_ids_1, 'token_type_ids has been specialized to have value None but got another value');  token_type_ids_1 = None     _assert_is_none_2 = torch.fx._symbolic_trace._assert_is_none(position_ids_1, 'position_ids has been specialized to have value None but got another value');  position_ids_1 = None     _assert_is_none_3 = torch.fx._symbolic_trace._assert_is_none(head_mask_1, 'head_mask has been specialized to have value None but got another value');  head_mask_1 = None     _assert_is_none_4 = torch.fx._symbolic_trace._assert_is_none(inputs_embeds_1, 'inputs_embeds has been specialized to have value None but got another value');  inputs_embeds_1 = None     _assert_is_none_5 = torch.fx._symbolic_trace._assert_is_none(encoder_hidden_states_1, 'encoder_hidden_states has been specialized to have value None but got another value');  encoder_hidden_states_1 = None     _assert_is_none_6 = torch.fx._symbolic_trace._assert_is_none(encoder_attention_mask_1, 'encoder_attention_mask has been specialized to have value None but got another value');  encoder_attention_mask_1 = None     _assert_is_none_7 = torch.fx._symbolic_trace._assert_is_none(past_key_values_1, 'past_key_values has been specialized to have value None but got another value');  past_key_values_1 = None     _assert_is_none_8 = torch.fx._symbolic_trace._assert_is_none(use_cache_1, 'use_cache has been specialized to have value None but got another value');  use_cache_1 = None     _assert_is_none_9 = torch.fx._symbolic_trace._assert_is_none(output_attentions_1, 'output_attentions has been specialized to have value None but got another value');  output_attentions_1 = None     _assert_is_none_10 = torch.fx._symbolic_trace._assert_is_none(output_hidden_states_1, 'output_hidden_states has been specialized to have value None but got another value');  output_hidden_states_1 = None     _assert_is_none_11 = torch.fx._symbolic_trace._assert_is_none(return_dict_1, 'return_dict has been specialized to have value None but got another value');  return_dict_1 = None     getattr_1 = input_ids.device     ones = torch.ones((50, 50), device = getattr_1);  getattr_1 = None     getitem = ones[(slice(None, None, None), None, None, slice(None, None, None))];  ones = None     to = getitem.to(dtype = torch.float32);  getitem = None     sub = 1.0  to;  to = None     mul = sub * 10000.0;  sub = None     embeddings_word_embeddings = self.embeddings.word_embeddings(input_ids);  input_ids = None     _tensor_constant0 = self._tensor_constant0     embeddings_token_type_embeddings = self.embeddings.token_type_embeddings(_tensor_constant0);  _tensor_constant0 = None     add = embeddings_word_embeddings + embeddings_token_type_embeddings;  embeddings_word_embeddings = embeddings_token_type_embeddings = None     _tensor_constant1 = self._tensor_constant1     embeddings_position_embeddings = self.embeddings.position_embeddings(_tensor_constant1);  _tensor_constant1 = None     add_1 = add + embeddings_position_embeddings;  add = embeddings_position_embeddings = None     embeddings_layer_norm = self.embeddings.LayerNorm(add_1);  add_1 = None     embeddings_dropout = self.embeddings.dropout(embeddings_layer_norm);  embeddings_layer_norm = None  """""" ```",2022-04-18T23:13:12Z,cla signed module: fx release notes: fx topic: new features,closed,0,3,https://github.com/pytorch/pytorch/issues/76003,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/76003**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit a2a175f4b3 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. , merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Disallow calling tolist on tensors with nullptr storage,"Stack from ghstack:  CC(Disallow calling tolist on tensors with nullptr storage) In response to D35571561 ``` >>> torch._efficientzerotensor(2).tolist() Traceback (most recent call last):   File """", line 1, in  RuntimeError: tolist() shouldn't be called on a tensor with unallocated storage ``` This code used to segfault before. Not adding this as a test because we should just return a list of zeros in this case. will fix it in a follow up PR. ",2022-04-18T20:01:31Z,cla signed topic: bug fixes module: python frontend,closed,0,3,https://github.com/pytorch/pytorch/issues/75990,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75990**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 212baf01f0 (more details on the Dr. CI page):  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6068514265?check_suite_focus=true) pull / winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) (1/12) **Step:** ""Test"" (full log  :repeat: rerun)   RuntimeError: tolist() shouldn't be called on a tensor with unallocated storage  ``` 20220418T21:05:45.9830666Z   SHARD_NUMBER: 2 20220418T21:05:45.9830919Z   NUM_TEST_SHARDS: 2 20220418T21:05:45.9832864Z   PR_BODY: Stack from ghstack:  CC(Disallow calling tolist on tensors with nullptr storage) In response to D35571561 ``` >>> torch._efficientzerotensor(2).tolist() Traceback (most recent call last):   File """", line 1, in  RuntimeError: tolist() shouldn't be called on a tensor with unallocated storage ``` This code used to segfault before. Not adding this as a test because we should just return a list of zeros in this case. will fix it in a follow up PR.  20220418T21:05:45.9834758Z   SCCACHE_BUCKET: osscicompilercachecircleciv2 20220418T21:05:45.9835083Z   SHM_SIZE: 2g 20220418T21:05:45.9835558Z   DOCKER_IMAGE: 308535385114.dkr.ecr.useast1.amazonaws.com/pytorch/pytorchlinuxxenialcuda11.3cudnn8py3gcc7:f829b04e9d449a9c1e1071e1c2341d894467d898 20220418T21:05:45.9836045Z   XLA_CUDA:  20220418T21:05:45.9836420Z   XLA_CLANG_CACHE_S3_BUCKET_NAME: osscicompilerclangcachecirclecixla 20220418T21:05:45.9836838Z [endgroup] 20220418T21:05:45.9865653Z + [[ default == \\m\\u\\l\\t\\i\\g\\p\\u ]] 20220418T21:05:45.9866612Z + [[ linuxxenialcuda11.3py3.7gcc7 == *onnx* ]] ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Model Averaging] Make an error message more clear in hierarchical_model_averager.py,As title,2022-04-14T21:07:01Z,oncall: distributed triaged open source cla signed,closed,0,7,https://github.com/pytorch/pytorch/issues/75832,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75832**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f0080849b0 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  , merge this please,Merge failed due to Refusing to merge as mandatory check Lint has not been run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2227368077, merge this please,Merge failed due to Refusing to merge as mandatory check Lint failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2227386373, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add native fast path for transformer encoder inference,"  CC([PyTorch] Add native fast path for transformer encoder inference) The current PyTorch multihead attention and transformer implementations are slow. This should speed them up for inference. **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-14T18:06:13Z,cla signed release notes: nn release notes: sparse topic: performance,closed,0,22,https://github.com/pytorch/pytorch/issues/75809,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75809**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 4731e208ea (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"> would be simpler to review if it was splitted more 1) just the files for MHA are about 1200 lines, so more splitting is not going to get you below that number 2) This code was already reviewed internally (though there are changes in this PR to deal with PyTorch core integration); if you use Phabricator, you can see which lines haven't changed and focus on ""is this acceptable in PyTorch core"" vs ""is this good software"" for those > there seems to be API added both via native_function and torch.ops. They seems independent? Also we should justify why we're doing this one way or the other? ~The torch.ops stuff is hit only in tests. I'm happy to move it to native_functions.yaml if you don't mind further clutter there.~ will fix. it's an artifact of being initially developed out of core. > Testing should be done via OpInfo for the most part, I am surprised to see 100s of lines of test with hardcoded values and no OpInfo? This is an extension to an existing feature, so I have mostly extended the existing tests, which evidently did not use OpInfo. Converting existing nonOpInfo tests to OpInfo should be out of scope, but I can look into OpInfo for the new tests in test_native_mha.py ~; can you point me to the doc on how to use it?~ > The doc refers to NestedTensor but that should point to something more specific so that users can use that information I don't know how to do that. Can you be more specific? Are you asking me to add a hyperlink in the docstring? How does that work? > torch.ops is known to be ludicrously slow ( CC(Calling `torch.ops.aten.add_` is ludicrously slow)) so most likely should not be used for anything perf sensitive? ~used only in tests.~ will fix.","> I can look into OpInfo for the new tests I'm not adding new public operators here, just an is_inference_mode_enabled fast path, so I don't see how OpInfo applies.","> just the files for MHA are about 1200 lines, so more splitting is not going to get you below that number Removing 30% of the PR is already a very nice improvement! For the MHA files, Isn't it already split into 3 different native functions?  Also they have independent implementations for CPU/CUDA? We already did the hard work of nicely splitting this up in multiple functions (that have their own bindings and tests). So we might as well make them different PR. > I don't know how to do that. Can you be more specific? Are you asking me to add a hyperlink in the docstring? How does that work? Sure, you can do ```:class:`torch.Tensor` ``` for example to link to `torch.Tensor`. Not sure where NestedTensor lives on the pytorch namespace, but similar thing would work. If you don't have a class to link to and just want to link to a page, the syntax is for example: ``` `Module `_  ``` where the first part is the text that will appear and between the `` the link and the `_` at the end is important.","> So we might as well make them different PR. I would agree with you if this was new code, but this has already been developed in a bunch of separate diffs internally and is logically an `hg mv` into PyTorch core. The kernels do not require detailed rereview from scratch.","Some things making this PR difficult to split:  900+ lines just for the MHA kernels, so it will never be small  test_nn.py is huge; splitting out the (comparatively small and, per Phabricator, mostly unchanged) TransformerEncoderLayer fast path would entail a semimanual edit in `hg split` that would need to be performed exactly the same way twice thanks to dirsync  Replicating changes to diff structure across the 3+ different build systems involved is errorprone In short, the cost in my time is high and the perceived benefit given that the code is not new and Phabricator will show what has been changed is not high.",`torch.is_inference_mode_enabled` seems to not work in TorchScript. Not sure why or what to do about it.,> torch.is_inference_mode_enabled seems to not work in TorchScript. Not sure why or what to do about it. I guess you could add support for it in TS... personally I'd probably see if the requires_grad tests were valid TS and do that instead ,"> > torch.is_inference_mode_enabled seems to not work in TorchScript. Not sure why or what to do about it. >  > I guess you could add support for it in TS... personally I'd probably see if the requires_grad tests were valid TS and do that instead I ran into a surprising behavior here: we are somehow getting `requires_grad=True` on some Tensors even though we are running test_transformerencoder_layer in test_nn.py in inference_mode. In other words, testing requires_grad means that inference_mode is not sufficient to hit the fast path!",Talked to  offline; decided to check requires_grad and torch.is_grad_enabled rather than torch.is_inference_mode_enabled. Remaining problems seem to be:  docs build  CUDA files can't see NestedTensor_get_max_size and this is persisting even after I put `TORCH_API` on it for some reason,will send a followup PR to fix formatting of bulleted list in fast path docstrings.,"linuxbionicpy3.7clang9 test failure is puzzling (gating for the fast path somehow failed only on one platform?), and Dr. CI didn't call it out in the comment, confused about why.","""MultiheadAttention does not support NestedTensor outside of its fast path. The fast path was not hit because some Tensor argument has_torch_function"" on linuxbionicpy3.7clang . Why the heck are there platformspecific torch_functions? I guess I will patch the test to check itself if there are torch_functions set up for some reason and do the right thing.","That must be the crossref config; it runs all the tests with a noop torch function to check if our implementations invariant to a torch function mode. In your case, you should skip the fast path tests here.", merge this please,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2218269607, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.", revert this,"(made a mistake attempting to merge, going to rediff and land from fbcode)",Reverting PR 75809 failed due to Can't revert PR that was landed via phabricator as D35239925 Raised by https://github.com/pytorch/pytorch/actions/runs/2221162526,Manually reverted with my admin powers: https://github.com/pytorch/pytorch/commit/2387efd35601205eabeb0dfe90731aa1dbf0eabb
transformer,[PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys,"  CC([PyTorch] Add native fast path for transformer encoder inference)  CC([PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys) Just as it is often difficult to write a single kernel that can handle both CPU and CUDA, so can it be difficult to do the same for NestedTensor. Differential Revision: D35603836 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-14T18:03:42Z,cla signed topic: not user facing,closed,0,5,https://github.com/pytorch/pytorch/issues/75808,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75808**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8cb23aaa26 (more details on the Dr. CI page):  * **3/3** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6069885628?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ", merge this, merge this,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2170625831,Merge failed due to This PR has internal changes and must be landed via Phabricator Raised by https://github.com/pytorch/pytorch/actions/runs/2170625826
transformer,[PyTorch] Add test for all-masked case for native softmax,  CC([PyTorch] Add native fast path for transformer encoder inference)  CC([PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys)  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-14T16:44:37Z,cla signed topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/75803,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75803**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 832cfa548b (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"previously commented on as https://github.com/pytorch/pytorch/pull/75493, had to send a new PR due to tooling issues", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","  please start adding these labels, we all do it","I do add them, but I don't check github notifications particularly often and you keep beating me to it :)"
llm,[PyTorch] Add test for all-masked case for native softmax, * (to be filled) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-14T16:43:45Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75802,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75802**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 75cc3e0ae7 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
yi,Large numerical error when applying nn.Linear in RTX A6000 with cuda>=11.1," 🐛 Describe the bug  Applying a simple Linear layer (Y = aX) is generating incorrect output with A6000 GPUs using cuda >=11.1  Report:  1.In the code snippet, we have a simple linear layer (without bias). First we use nn.Linear to compute Y=aX. Next we explicitly multiply the two tensors, We expect the norm of the difference to be 0 in all cases (can be confirmed by uncommenting the line setting device to ""cpu""). However a significant error is seen with A6000 GPUs using cuda >= 11.1 (Please see output table). 2. No error observed for CPU, and other GPUs at our disposal (NVIDIA RTX, TITAN X ) 3. With A6000, **no error for cuda 11.0**. 4. With A6000, **error confirmed for cuda 11.1 and  cuda 11.3**.  5. This error is deadly serious since it is an insidious corruption, without any obvious outward symptoms. In fact, if we uncomment the other line of code, it can be seen that a significant error is introduced even when multiplying with 1.  Code Snippet: ```python import torch d = ""cuda:0"" d = ""cpu"" x = [pow(10,x) for x in list(range(10))] for size in x:     data = torch.rand(size,1)     lin = torch.nn.Linear(1,1,bias=False).to(d)     lin.weight.data = torch.tensor([[1.0]])     print (size,torch.norm(data.to(d)*lin.weight.to(d)  lin.to(d)(data.to(d))).item()) ```  Output    Versions Collecting environment information... PyTorch version: 1.10.1+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.094genericx86_64withglibc2.29 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: NVIDIA RTX A6000 GPU 1: NVIDIA RTX A6000 GPU 2: NVIDIA RTX A6000 GPU 3: NVIDIA RTX A6000 Nvidia driver version: 470.86 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.22.0 [pip3] torch==1.10.1+cu113 [pip3] torchcluster==1.5.9 [pip3] torchgeometric==2.0.3 [pip3] torchscatter==2.0.9 [pip3] torchsparse==0.6.12 [pip3] torchsplineconv==1.2.1 [pip3] torchaudio==0.10.1+cu113 [pip3] torchvision==0.11.2+cu113 [conda] blas                      1.0                         mkl [conda] mkl                       2021.2.0           h06a4308_296 [conda] mklservice               2.3.0            py38h27cfd23_1 [conda] mkl_fft                   1.3.0            py38h42c9631_2 [conda] mkl_random                1.2.1            py38ha9443f7_2 [conda] mypy_extensions           0.4.3                    py38_0 [conda] numpy                     1.20.1           py38h93e21f0_0 [conda] numpybase                1.20.1           py38h7d8b39e_0 [conda] numpydoc                  1.1.0              pyhd3eb1b0_1 ",2022-04-13T16:26:38Z,high priority module: cuda triaged module: tf32,open,2,3,https://github.com/pytorch/pytorch/issues/75740,"Marking hipri for potentially silently incorrect behavior. I don't know if this is just a numerical stability problem, though","This is likely related to tf32, if this error is affecting your results turn it off by `torch.backends.cuda.matmul.allow_tf32 = True` https://pytorch.org/docs/master/notes/cuda.htmltf32onampere","> This is likely related to tf32, if this error is affecting your results turn it off by `torch.backends.cuda.matmul.allow_tf32 = True` https://pytorch.org/docs/master/notes/cuda.htmltf32onampere Yes, error disappears when setting the flag to False on A6000 with torch versions: 1.10.1+cu113, 1.9.1+cu111, 1.7.1+cu110"
rag,expand the coverage of conv folding,"Expand the coverage of conv folding, such as conv>mul>add>bn etc.",2022-04-13T08:05:11Z,oncall: jit triaged open source Merged cla signed intel,closed,0,8,https://github.com/pytorch/pytorch/issues/75724,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75724**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :x: 1 New Failures As of commit 005fcc7c76 (more details on the Dr. CI page): Expand to see more  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6384864317?check_suite_focus=true) pull / linuxxenialpy3.7clang7asan / test (default, 2, 5, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220511T09:00:40.9078653Z SUMMARY: Undefined.../jenkins/workspace/aten/src/ATen/Utils.cpp:20:3 in  ``` 20220511T09:00:40.8529459Z CC(Tensors don't print sometimes) 0x562344ff7c81 in run_mod /home/builder/tkoch/workspace/python_1648536129212/work/Python/pythonrun.c:1037 20220511T09:00:40.8529988Z CC(add multiprocessing unit tests, working filedescriptor based solution and OSX) 0x562345002c69 in PyRun_StringFlags /home/builder/tkoch/workspace/python_1648536129212/work/Python/pythonrun.c:961 20220511T09:00:40.8530625Z CC(Initial utils implementation + bug fixes) 0x562345002ccb in PyRun_SimpleStringFlags /home/builder/tkoch/workspace/python_1648536129212/work/Python/pythonrun.c:455 20220511T09:00:40.8531874Z CC(Infinite recursion when indexing Variable) 0x562345002dc8 in pymain_run_command /home/builder/tkoch/workspace/python_1648536129212/work/Modules/main.c:420 20220511T09:00:40.8532423Z CC(Clean up Module forward and __call__) 0x562345002dc8 in pymain_run_python /home/builder/tkoch/workspace/python_1648536129212/work/Modules/main.c:2907 20220511T09:00:40.8532846Z CC(Use chainerstyle constructor for Conv2d) 0x562345002dc8 in pymain_main /home/builder/tkoch/workspace/python_1648536129212/work/Modules/main.c:3460 20220511T09:00:40.8533561Z CC(Error on legacy.nn serialization) 0x56234500318b in _Py_UnixMain /home/builder/tkoch/workspace/python_1648536129212/work/Modules/main.c:3495 20220511T09:00:40.9076593Z CC(Error on printing nn.ConcatTable) 0x7fc0904a683f in __libc_start_main /build/glibcS7Ft5T/glibc2.23/csu/../csu/libcstart.c:291 20220511T09:00:40.9077193Z CC(OS X build issue in THP_decodeInt64Buffer) 0x562344fa8039 in _start (/opt/conda/bin/python3.7+0x1d8039) 20220511T09:00:40.9077932Z  20220511T09:00:40.9078653Z SUMMARY: UndefinedBehaviorSanitizer: undefinedbehavior /var/lib/jenkins/workspace/aten/src/ATen/Utils.cpp:20:3 in  20220511T09:00:40.9340997Z + retcode=1 20220511T09:00:40.9341485Z + set e 20220511T09:00:40.9341725Z + return 1 20220511T09:00:40.9344167Z + [[ linuxxenialpy3.7clang7asandefault == *NO_AVX* ]] 20220511T09:00:40.9344679Z + [[ default == \\n\\o\\g\\p\\u\\_\\N\\O\\_\\A\\V\\X ]] 20220511T09:00:40.9345297Z + [[ linuxxenialpy3.7clang7asandefault == *NO_AVX2* ]] 20220511T09:00:40.9345773Z + [[ default == \\n\\o\\g\\p\\u\\_\\N\\O\\_\\A\\V\\X\\2 ]] 20220511T09:00:40.9346123Z + [[ linuxxenialpy3.7clang7asandefault == *NO_AVX512* ]] 20220511T09:00:40.9346399Z + [[ default == \\n\\o\\g\\p\\u\\_\\N\\O\\_\\A\\V\\X\\5\\1\\2 ]] 20220511T09:00:40.9348943Z + [[ linuxxenialpy3.7clang7asandefault == *tbb* ]] ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  "," , I have modified the code to solve the problem of UT failure, could you please review it again?", , rebase this please, do you mind rebasing ? sorry,">  do you mind rebasing ? sorry Done, sorry for late reply.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Added functorch to functional_autograd_benchmark,"Description:  Following https://github.com/pytorch/functorch/issues/497 adding an option to run benchmarks with functorch and compare to original functional autograd results. Running the benchmark we get below table:   Table  ```  ```    Stdout  ``` Found functorch: 0.2.0a0+386a541 Results for model resnet18 on task vjp: 0.03826599195599556s (var: 4.3332115637895186e06) Results for model resnet18 on task vjp using Functorch: 0.037201929837465286s (var: 6.139693198292662e09) Results for model resnet18 on task vhp: 0.2202976644039154s (var: 2.8687209052691287e08) Results for model resnet18 on task vhp using Functorch: 0.22117868065834045s (var: 4.108771278765744e08) Results for model resnet18 on task jvp: 0.18679651618003845s (var: 1.832455254202614e08) Results for model resnet18 on task jvp using Functorch: 0.05305683612823486s (var: 1.6690266946284282e08) Results for model fcn_resnet on task vjp: 0.6071907877922058s (var: 7.436695454998699e07) Results for model fcn_resnet on task vjp using Functorch: 0.6115708947181702s (var: 1.121692207561864e06) Results for model fcn_resnet on task vhp: 3.419469118118286s (var: 0.020633839070796967) Failed model using Functorch: fcn_resnet, task: vhp, Error message:  	 CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 47.46 GiB total capacity; 45.62 GiB already allocated; 5.31 MiB free; 46.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF Results for model fcn_resnet on task jvp: 2.5421929359436035s (var: 3.1765587209520163e06) Results for model fcn_resnet on task jvp using Functorch: 0.7628333568572998s (var: 1.4555752159139956e07) Results for model detr on task vjp: 0.19494840502738953s (var: 1.9122715457342565e05) Failed model using Functorch: detr, task: vjp, Error message:  	 Cannot access data pointer of Tensor that doesn't have storage Results for model detr on task vhp: 1.1664292812347412s (var: 0.000948643428273499) Failed model using Functorch: detr, task: vhp, Error message:  	 Cannot access data pointer of Tensor that doesn't have storage Results for model detr on task jvp: 0.9990308880805969s (var: 1.0214127541985363e05) Failed model using Functorch: detr, task: jvp, Error message:  	 Trying to use forward AD with _cdist_forward that does not support it because it has not been implemented yet. Please file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=featurerequest.yml so that we can prioritize its implementation. Results for model ppl_simple_reg on task vjp: 0.0007535457843914628s (var: 6.024204690646684e09) Results for model ppl_simple_reg on task vjp using Functorch: 0.0016954183811321855s (var: 1.160151974488599e08) Results for model ppl_simple_reg on task vhp: 0.0011888503795489669s (var: 5.93119386937957e10) Results for model ppl_simple_reg on task vhp using Functorch: 0.0026826143730431795s (var: 1.6787025103326414e08) Results for model ppl_simple_reg on task jvp: 0.001067900680936873s (var: 7.409912128331086e10) Results for model ppl_simple_reg on task jvp using Functorch: 0.002065300941467285s (var: 9.710328185974504e08) Results for model ppl_simple_reg on task hvp: 0.001212477684020996s (var: 1.974137298077494e09) Results for model ppl_simple_reg on task hvp using Functorch: 0.00482442369684577s (var: 2.327668653379078e07) Results for model ppl_simple_reg on task jacobian: 0.0009108781814575195s (var: 3.489469158068914e09) Results for model ppl_simple_reg on task jacobian using Functorch: 0.0019866942893713713s (var: 1.938326299466553e08) Results for model ppl_simple_reg on task hessian: 0.005053090862929821s (var: 3.370298600202659e07) Results for model ppl_simple_reg on task hessian using Functorch: 0.006374978926032782s (var: 7.556796077778927e08) Results for model ppl_simple_reg on task hessian_fwdrev: 0.0036706924438476562s (var: 1.996075527088692e09) Results for model ppl_simple_reg on task hessian_fwdrev using Functorch: 0.0058908225037157536s (var: 7.548283775804521e08) Results for model ppl_simple_reg on task hessian_revrev: 0.0015769004821777344s (var: 1.5754418214442012e08) Results for model ppl_simple_reg on task hessian_revrev using Functorch: 0.0041002752259373665s (var: 6.713568723171193e08) Results for model ppl_simple_reg on task jacfwd: 0.0018048763740807772s (var: 2.7375660849315864e08) Results for model ppl_simple_reg on task jacfwd using Functorch: 0.002047991845756769s (var: 2.432247070416338e09) Results for model ppl_simple_reg on task jacrev: 0.0009733677143231034s (var: 1.0078769818733235e08) Results for model ppl_simple_reg on task jacrev using Functorch: 0.0021971464157104492s (var: 1.2729884701911942e08) Results for model ppl_robust_reg on task vjp: 0.005820560269057751s (var: 8.582588151284654e08) Results for model ppl_robust_reg on task vjp using Functorch: 0.00796132069081068s (var: 9.663100541956737e09) Results for model ppl_robust_reg on task vhp: 0.009825301356613636s (var: 2.0081762386325863e07) Results for model ppl_robust_reg on task vhp using Functorch: 0.014890861697494984s (var: 4.558066279969353e07) Results for model ppl_robust_reg on task jvp: 0.008297419175505638s (var: 2.9454400873873965e07) Results for model ppl_robust_reg on task jvp using Functorch: 0.008052706718444824s (var: 7.120377176761394e08) Results for model ppl_robust_reg on task hvp: 0.015414690598845482s (var: 7.42123745567369e07) Results for model ppl_robust_reg on task hvp using Functorch: 0.02699306048452854s (var: 1.4650488537881756e06) Results for model ppl_robust_reg on task jacobian: 0.006207776255905628s (var: 1.7068457225377642e07) Results for model ppl_robust_reg on task jacobian using Functorch: 0.009173822589218616s (var: 1.2214455580306094e07) Results for model ppl_robust_reg on task hessian: 0.04670915752649307s (var: 1.4299343092716299e05) Results for model ppl_robust_reg on task hessian using Functorch: 0.02337808534502983s (var: 3.0397418413485866e06) Results for model ppl_robust_reg on task hessian_fwdrev: 0.024229884147644043s (var: 2.0425247839739313e06) Results for model ppl_robust_reg on task hessian_fwdrev using Functorch: 0.022021746262907982s (var: 3.512146236062108e07) Results for model ppl_robust_reg on task hessian_revrev: 0.012355780228972435s (var: 7.090877147675201e07) Results for model ppl_robust_reg on task hessian_revrev using Functorch: 0.013960313983261585s (var: 6.326549737423193e07) Results for model ppl_robust_reg on task jacfwd: 0.008112502284348011s (var: 2.88503088086145e08) Results for model ppl_robust_reg on task jacfwd using Functorch: 0.008947920985519886s (var: 4.2070990247111695e08) Results for model ppl_robust_reg on task jacrev: 0.00635871896520257s (var: 1.3403841592207755e07) Results for model ppl_robust_reg on task jacrev using Functorch: 0.009123563766479492s (var: 2.677554675756255e07) Results for model wav2letter on task vjp: 0.02078995667397976s (var: 2.1110793113621185e06) Results for model wav2letter on task vjp using Functorch: 0.019202351570129395s (var: 9.210506135559626e09) Results for model wav2letter on task vhp: 0.05997290462255478s (var: 8.558587616391833e09) Results for model wav2letter on task vhp using Functorch: 0.06035261228680611s (var: 1.6448565842708263e09) Results for model wav2letter on task jvp: 0.04507789760828018s (var: 1.5771547401399744e09) Results for model wav2letter on task jvp using Functorch: 0.013057494536042213s (var: 3.804750292601966e09) Results for model deepspeech on task vjp: 0.3648746609687805s (var: 1.5359055396402255e05) Failed model using Functorch: deepspeech, task: vjp, Error message:  	 Cannot access storage of TensorWrapper Results for model transformer on task vjp: 0.05496881157159805s (var: 1.242562319703211e08) Results for model transformer on task vjp using Functorch: 0.057835936546325684s (var: 2.6113376350167528e08) Results for model transformer on task vhp: 0.18313491344451904s (var: 7.226336151688884e08) Failed model using Functorch: transformer, task: vhp, Error message:  	 bad optional access Results for model transformer on task jvp: 0.13924935460090637s (var: 1.6989159234981344e07) Failed model using Functorch: transformer, task: jvp, Error message:  	 Trying to use forward AD with embedding that does not support it because it has not been implemented yet. Please file an issue to PyTorch at https://github.com/pytorch/pytorch/issues/new?template=featurerequest.yml so that we can prioritize its implementation. Results for model multiheadattn on task vjp: 0.0014708995586261153s (var: 3.710916729460223e08) Results for model multiheadattn on task vjp using Functorch: 0.002404856728389859s (var: 2.1910574687922235e08) Results for model multiheadattn on task vhp: 0.003382015274837613s (var: 5.3098595742540056e08) Results for model multiheadattn on task vhp using Functorch: 0.005340623669326305s (var: 5.897558708056749e08) Results for model multiheadattn on task jvp: 0.0027526854537427425s (var: 3.508620949332908e08) Results for model multiheadattn on task jvp using Functorch: 0.0022981404326856136s (var: 1.327894807445773e07) ```  All functorch errors are reported in its repository.   ",2022-04-12T19:32:21Z,triaged open source cla signed topic: not user facing,closed,0,8,https://github.com/pytorch/pytorch/issues/75689,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75689**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 7e86a3b696 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,",  for autograd as well if you guys have any comments (I'll take a pass through this later today, sorry for the delay!)", merge this please,Merge failed due to Refusing to merge as mandatory check Lint has not been run for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2189993744,"5, I guess we're not allowed to skip CI :). Can you push a new empty commit (or rebase this) when you get a chance?", sure. I skipped CI to avoid unnecessary CI run if we had to update the the comment., merge this please,"Hey 5. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[FSDP] [Mixed Precision] using param_dtype breaks transformers ( in attention_probs matmul)," 🐛 Describe the bug Using PyTorch nightly, enable mixed precision support parameters for fsdp. When running with a transformer, you will hit the following error  float expected but received half or bfloat (depending on mp config):  Note that comm mp and buffer mp all work as expected.  You can use partial mixed precision by setting the param_dtype to fp32:  The issue is specific to the attention_probs matmul:  and previous matmuls work as expected:  Enabling stack trace yields: File ""/home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/transformers/models/bert/modeling_bert.py"", line 340, in forward     context_layer = torch.matmul(attention_probs, value_layer) RuntimeError: expected scalar type BFloat16 but found Float Exception raised from data_ptr at aten/src/ATen/core/TensorMethods.cpp:18 (most recent call first): frame CC(未找到相关数据): c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fa8dadacb82 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libc10.so) frame CC(Matrix multiplication operator): c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x5b (0x7fa8dada921b in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libc10.so) frame CC(Don't support legacy Python): c10::BFloat16* at::TensorBase::data_ptr() const + 0xde (0x7fa91c90d1ae in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(PEP8):  + 0x479b0a1 (0x7fa8df7920a1 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cu.so) frame CC(PEP8): at::native::structured_bmm_out_cuda::impl(at::Tensor const&, at::Tensor const&, at::Tensor const&) + 0x66 (0x7fa8df793356 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cu.so) frame CC(Checklist for Release):  + 0x45c94dc (0x7fa8df5c04dc in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cu.so) frame CC(Remove dampening from SGD):  + 0x45c956f (0x7fa8df5c056f in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cuda_cu.so) frame CC(ImportError: No module named _C): at::_ops::bmm::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&) + 0x74 (0x7fa91c31f064 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(fake commit):  + 0x2bc7f33 (0x7fa91d348f33 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Add Storage.from_buffer):  + 0x2bc87c2 (0x7fa91d3497c2 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Tensors don't print sometimes): at::_ops::bmm::call(at::Tensor const&, at::Tensor const&) + 0xb2 (0x7fa91c358a62 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(add multiprocessing unit tests, working filedescriptor based solution and OSX):  + 0x144bdd0 (0x7fa91bbccdd0 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Initial utils implementation + bug fixes): at::native::matmul(at::Tensor const&, at::Tensor const&) + 0x3a (0x7fa91bbcdaca in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Infinite recursion when indexing Variable):  + 0x1e9db5f (0x7fa91c61eb5f in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Clean up Module forward and __call__): at::_ops::matmul::call(at::Tensor const&, at::Tensor const&) + 0xb2 (0x7fa91c404e82 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_cpu.so) frame CC(Use chainerstyle constructor for Conv2d):  + 0x345de9 (0x7fa942354de9 in /home/ubuntu/anaconda3/envs/pytorch_p38/lib/python3.8/sitepackages/torch/lib/libtorch_python.so) frame CC(Error on legacy.nn serialization): PyCFunction_Call + 0x54 (0x55a44eee9f44 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Error on printing nn.ConcatTable): _PyObject_MakeTpCall + 0x31e (0x55a44eef930e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(OS X build issue in THP_decodeInt64Buffer): _PyEval_EvalFrameDefault + 0x53cf (0x55a44ef8f6ff in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Add missing PyBuffer_Release calls): _PyEval_EvalCodeWithName + 0x2c3 (0x55a44ef6cdb3 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Figure out and fix Tensor(Storage) constructor): _PyFunction_Vectorcall + 0x378 (0x55a44ef6e198 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(import torch works in ipython but not in python (_THRefcountedMapAllocator)):  + 0x1b0dfc (0x55a44ef6edfc in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(adding cuda driver check functions for runtime checking): PyObject_Call + 0x5e (0x55a44eee316e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(OSX Multiprocessing errors out): _PyEval_EvalFrameDefault + 0x21bf (0x55a44ef8c4ef in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(More modules for nn + improvements in CUDA tests): _PyEval_EvalCodeWithName + 0x2c3 (0x55a44ef6cdb3 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Convert between CUDA types): _PyObject_FastCallDict + 0x2c1 (0x55a44eedde11 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Fix error message for wrong type (THTensor* > THFloatTensor*)): _PyObject_Call_Prepend + 0xce (0x55a44eee8abe in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Can't print tensors with inf or nan):  + 0x17f3f9 (0x55a44ef3d3f9 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Support multitype operations where reasonable): _PyObject_MakeTpCall + 0x31e (0x55a44eef930e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(module.parameters() should only return unique parameters ?): _PyEval_EvalFrameDefault + 0x53cf (0x55a44ef8f6ff in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(save/load CUDA tensor always puts it on device 0): _PyEval_EvalCodeWithName + 0x2c3 (0x55a44ef6cdb3 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(nn.Container.add_module: should return self, and name optional): _PyFunction_Vectorcall + 0x378 (0x55a44ef6e198 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Add information about nondifferentiable points to grad tests):  + 0x1b0dfc (0x55a44ef6edfc in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Add more functions to autograd): PyObject_Call + 0x5e (0x55a44eee316e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Various improvements and fixes): _PyEval_EvalFrameDefault + 0x21bf (0x55a44ef8c4ef in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(nn API Docs): _PyEval_EvalCodeWithName + 0x2c3 (0x55a44ef6cdb3 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Add cuDNN support for convolutions): _PyFunction_Vectorcall + 0x378 (0x55a44ef6e198 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Add more functions to autograd): _PyObject_FastCallDict + 0x2fd (0x55a44eedde4d in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Fix multiprocessing on OS X): _PyObject_Call_Prepend + 0x63 (0x55a44eee8a53 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Error messages to improve):  + 0x17f3f9 (0x55a44ef3d3f9 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Codemod to remove camel case method naming): _PyObject_MakeTpCall + 0x31e (0x55a44eef930e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(loss functions targets shouldn't have to declare requires_grad = False): _PyEval_EvalFrameDefault + 0x56cb (0x55a44ef8f9fb in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Rename CELoss to CrossEntropyLoss): _PyEval_EvalCodeWithName + 0x2c3 (0x55a44ef6cdb3 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(fix bad error message): _PyFunction_Vectorcall + 0x378 (0x55a44ef6e198 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(dataset/dataloader, multiprocess):  + 0x1b0dfc (0x55a44ef6edfc in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Refactor _C extension to export some utilities): PyObject_Call + 0x5e (0x55a44eee316e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Multiprocessing doesn't preserve data sharing of storage slices): _PyEval_EvalFrameDefault + 0x21bf (0x55a44ef8c4ef in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Improve storage, tensor and module C error messages + fix for dl flags in nightly python): _PyEval_EvalCodeWithName + 0x2c3 (0x55a44ef6cdb3 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(full half tensor support for nn and autograd): _PyObject_FastCallDict + 0x2c1 (0x55a44eedde11 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(optim API to incorporate not just a rigid model): _PyObject_Call_Prepend + 0xce (0x55a44eee8abe in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Improvements in torch.nn):  + 0x17f3f9 (0x55a44ef3d3f9 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Various improvements): _PyObject_MakeTpCall + 0x31e (0x55a44eef930e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Combine autograd.Leaf and autograd.Variable): _PyEval_EvalFrameDefault + 0x5026 (0x55a44ef8f356 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(OSX + Python 2 build fixes): _PyEval_EvalCodeWithName + 0x2c3 (0x55a44ef6cdb3 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(improve parallel performance for RNN models): _PyFunction_Vectorcall + 0x378 (0x55a44ef6e198 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Add back support for child=None in Container constructor):  + 0x1b0dfc (0x55a44ef6edfc in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Update THC and use CUDA caching allocator): PyObject_Call + 0x5e (0x55a44eee316e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Update THC and use CUDA caching allocator): _PyEval_EvalFrameDefault + 0x21bf (0x55a44ef8c4ef in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Add data parallel functions to nn): _PyEval_EvalCodeWithName + 0x2c3 (0x55a44ef6cdb3 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Fix NLLLoss to take Long targets on both CPU and GPU): _PyFunction_Vectorcall + 0x378 (0x55a44ef6e198 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(cpu builds broken due to cudnn and dataparallel): _PyObject_FastCallDict + 0x2fd (0x55a44eedde4d in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(fixing CPU builds by making cuda imports optional): _PyObject_Call_Prepend + 0x63 (0x55a44eee8a53 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Add ffi utils for user C extensions):  + 0x17f3f9 (0x55a44ef3d3f9 in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) frame CC(Install Error, OSX 10.11.6, fresh miniconda install): _PyObject_MakeTpCall + 0x31e (0x55a44eef930e in /home/ubuntu/anaconda3/envs/pytorch_p38/bin/python) Repro code is here: https://github.com/lessw2020/fsdp_review/blob/main/bert/fsdp_bert_fix.py It does expect the imdb dataset to be present, so if needed I can setup modular repro setup.    Versions Pytorch Nightly 4/08, with YanLi's bert ordered dict fix in fdsp/utils. *I'll run the env collect if needed once I'm on my server.  ",2022-04-12T17:58:38Z,triaged module: fsdp,closed,3,4,https://github.com/pytorch/pytorch/issues/75676,"Small repo to quickly reproduce: https://github.com/lessw2020/mixprecision HF transformer dependency  you may need to **""pip install transformers""** if transformer import fails.  git clone and run ""python fsdp_mp.py"" should put you right into stack trace. adjust active policy to relevant configs to toggle between working (bfSixteen_working) and not (bfSixteen).  ","Discussed with Less offline, pasting result from discussion here:  From some debugging, the issue is in this line: https://github.com/huggingface/transformers/blob/db9f189121b6bf06f8f0825fa6e0f051c8e46b27/src/transformers/models/bert/modeling_bert.pyL985 because the `extended_attention_mask` is computed in a helper function that is unaware of FSDP / mixed precision. There is a hack we can add in the BERT code we can do which enables the model to train, but we're not sure if FSDP can have a fix for this or if we need to fix it in the BERT code somehow. Essentially we need to figure out whether FSDP can somehow inform functions that generate tensors such as these to move the input to fp16 or not. ","Hi, is there a decision made on whether this will be fixed at the FSDP layer or at the HuggingFace layer? ",It is unlikely that we will fix this in the FSDP layer. I will mark this as not planned for now.
yi,Misleading documentation for cholesky_inverse," 📚 The doc issue https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html The documentation suggests that the inputted matrix should be the original positivedefinite matrix A, where as the examples show that the user must first compute the cholesky u and input that into the cholesky_inverse function.  Suggest a potential alternative/fix Change the parameters section so that it is clear that the input must be the cholesky decomposition. ",2022-04-12T11:14:35Z,module: docs triaged module: linear algebra,open,1,0,https://github.com/pytorch/pytorch/issues/75659
yi,jit fails when trying to assign values to model via hook," 🐛 Describe the bug I'm using hooks to assign new values to the model on forward passes. Jit script fails with: ``` RuntimeError:  attribute assignment is not defined on python value of type 'Model Name': ``` For Example: ```python from typing import Iterable, Callable, Tuple from torch import Tensor, nn, jit from torchvision.models import resnet50 class FeatureExtractor(nn.Module):     def __init__(self, model: nn.Module, layers: Iterable[str], num):         super().__init__()         self.model = model         self.num = num         for layer_id in layers:             layer = dict([*self.model.named_modules()])[layer_id]             layer.register_forward_hook(self.save_outputs_hook(layer_id))     def save_outputs_hook(self, layer_id: str) > Callable:         fe = self         def fn(_, input: Tuple[Tensor], output):             fe.num = 3         return fn     def forward(self, x: Tensor):         _ = self.model(x) if __name__ == '__main__':     resnet_features = FeatureExtractor(resnet50(), layers=[""layer4"", ""avgpool""], num=0)     script = jit.script(resnet_features) ``` Fails with  ``` RuntimeError:  attribute assignment is not defined on python value of type 'FeatureExtractor':   File """", line 20         def fn(_, input: Tuple[Tensor], output):             fe.num = 3             ~~~~~~~~~~ < HERE ```  Versions PyTorch version: 1.11.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 11.6 (x86_64) GCC version: Could not collect Clang version: 13.0.0 (clang1300.0.29.30) CMake version: Could not collect Libc version: N/A Python version: 3.9.8 (main, Nov 10 2021, 09:21:22)  [Clang 13.0.0 (clang1300.0.29.3)] (64bit runtime) Python platform: macOS11.6x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] numpy==1.22.3 [pip3] pytorchlightning==1.6.0 [pip3] torch==1.11.0 [pip3] torchmetrics==0.7.3 [pip3] torchvision==0.12.0 [conda] Could not collect",2022-04-12T08:50:35Z,oncall: jit,open,1,1,https://github.com/pytorch/pytorch/issues/75654,I managed to circumvent this issue by using jit.trace instead of script.
yi,"[quant][core][improvement] Added support for data_ptr<T> for quantized tensors to return pointer to underlying int type (e.g., int8* instead of qint*)","  CC([quant][bcbreaking][improvements] Removed quantized_max_pool1d registration)  CC([Quant][core][improvements] Combined dispatch registration for max_pool1d & quantized_max_pool1d)  CC([quant][bcbreaking][improvements] Removed quantized_max_pool2d registration)  CC([Quant][core][improvements] Combined dispatch registration for max_pool2d & quantized_max_pool2d and implemented max_pool2d_with_indices_out_quantized_cpu)  CC([quant][core][improvement] Added support for data_ptr for quantized tensors to return pointer to underlying int type (e.g., int8* instead of qint*)) Summary: Previously, when using `data_ptr` on a quantized tensor, it did not provide support for returning a pointer to the underlying int tensor. Instead, it returned a pointer to a quantized tensor (e.g., qint*), and backend users had to call, e.g., reinterpret_cast(quantized_tensor.data_ptr()) to cast it to an int* pointer. This PR enables direct support for returning the underlying pointer without need for casting. Differential Revision: D35569200",2022-04-12T00:45:03Z,cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/75643,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75643**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit b0e87eb14f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", merge this (Initiating merge automatically since Phabricator Diff has merged),"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Docs: Detail 3D tensor shape for transformer masks,Fixes CC(mask parameter in Transformer support 3D tensor but not mentioned).,2022-04-09T02:45:30Z,triaged open source cla signed release notes: nn topic: docs,closed,0,3,https://github.com/pytorch/pytorch/issues/75552,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75552**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 40f4ae3a5e (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5951820182?check_suite_focus=true) pull / linuxxenialpy3.7clang7asan / test (default, 2, 3, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220409T03:25:05.4712079Z RuntimeError: test_cpp_extensions_aot_no_ninja failed!  ``` 20220409T03:24:31.1103973Z [01;32m[K ^[m[K 20220409T03:25:05.1287605Z g++ pthread shared B /opt/conda/compiler_compat L/opt/conda/lib Wl,rpath=/opt/conda/lib Wl,noasneeded Wl,sysroot=/ build/temp.linuxx86_643.7/rng_extension.o L/opt/conda/lib/python3.7/sitepackages/torch/lib lc10 ltorch ltorch_cpu ltorch_python o build/lib.linuxx86_643.7/torch_test_cpp_extension/rng.cpython37mx86_64linuxgnu.so 20220409T03:25:05.1366993Z OMP: Error CC(Use chainerstyle constructor for Conv2d): Initializing libiomp5.so, but found unknown library already initialized. 20220409T03:25:05.1368177Z OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/. 20220409T03:25:05.1449671Z error: command 'g++' terminated by signal 6 20220409T03:25:05.4704817Z Traceback (most recent call last): 20220409T03:25:05.4705101Z   File ""test/run_test.py"", line 1056, in  20220409T03:25:05.4708534Z     main() 20220409T03:25:05.4708742Z   File ""test/run_test.py"", line 1034, in main 20220409T03:25:05.4711823Z     raise RuntimeError(err_message) 20220409T03:25:05.4712079Z RuntimeError: test_cpp_extensions_aot_no_ninja failed! 20220409T03:25:05.8523176Z + cleanup 20220409T03:25:05.8523530Z + retcode=1 20220409T03:25:05.8523825Z + set +x 20220409T03:25:05.8564956Z [error]Process completed with exit code 1. 20220409T03:25:05.8618828Z [group]Run pytorch/pytorch/.github/actions/getworkflowjobid 20220409T03:25:05.8619077Z with: 20220409T03:25:05.8619733Z   githubtoken: *** 20220409T03:25:05.8619908Z env: 20220409T03:25:05.8620064Z   IN_CI: 1 20220409T03:25:05.8620211Z   IS_GHA: 1 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add test for all-masked case for native softmax,  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-08T03:57:46Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75493,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75493**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 5395edc79f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"ghexport stopped wanting to update this, so I've had to send a new PR  https://github.com/pytorch/pytorch/pull/75803"
transformer,[PyTorch] Add NestedTensor support functions for transformers,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:44:05Z,cla signed release notes: sparse topic: new features,closed,0,3,https://github.com/pytorch/pytorch/issues/75491,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75491**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8a7da49dd1 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. , merge this (Initiating merge automatically since Phabricator Diff has merged),"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,[PyTorch] Add test for all-masked case for native softmax, * (to be filled) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-08T03:38:18Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75490,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75490**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 4bc1f6f82c (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 10 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879643784?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (default, 1, 2, linux.2xlarge) (1/10) **Step:** ""Test"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
transformer,[PyTorch] Add NestedTensor support functions for transformers," * (to be filled) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:38:09Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75489,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75489**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 3b1cec5fb0 (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879665674?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (noarch, 1, 1, linux.2xlarge) (1/11) **Step:** ""Test"" (full log  :repeat: rerun)   20220408T04:46:01.8541854Z AssertionError: Th...eturned by torch._overrides.get_ignored_functions.  ``` 20220408T04:46:00.1073556Z  20220408T04:46:00.1073647Z Generating XML reports... 20220408T04:46:00.1126805Z Generated XML report: testreports/pythonunittest/test_logging/TESTLoggingTest20220408044557.xml 20220408T04:46:00.4685150Z Running test_overrides ... [20220408 04:46:00.468135] 20220408T04:46:00.4685702Z Executing ['/opt/conda/bin/python', 'test_overrides.py', 'v', 'importslowtests', 'importdisabledtests'] ... [20220408 04:46:00.468226] 20220408T04:46:01.8538492Z Traceback (most recent call last): 20220408T04:46:01.8538995Z   File ""test_overrides.py"", line 356, in  20220408T04:46:01.8539421Z     generate_tensor_like_torch_implementations() 20220408T04:46:01.8539875Z   File ""test_overrides.py"", line 344, in generate_tensor_like_torch_implementations 20220408T04:46:01.8540809Z     assert len(untested_funcs) == 0, msg.format(pprint.pformat(untested_funcs)) 20220408T04:46:01.8541854Z AssertionError: The following functions are not tested for __torch_function__ support, please ensure there is an entry in the dict returned by torch._overrides.get_testing_overrides for this function or if a __torch_function__ override does not make sense, add an entry to the tuple returned by torch._overrides.get_ignored_functions. 20220408T04:46:01.8542664Z  20220408T04:46:01.8543079Z [""._nested_tensor_layer_norm"", 20220408T04:46:01.8543606Z  "".to_padded_tensor""] 20220408T04:46:02.1150222Z Traceback (most recent call last): 20220408T04:46:02.1150506Z   File ""test/run_test.py"", line 1052, in  20220408T04:46:02.1153460Z     main() 20220408T04:46:02.1153780Z   File ""test/run_test.py"", line 1030, in main 20220408T04:46:02.1156285Z     raise RuntimeError(err_message) 20220408T04:46:02.1156695Z RuntimeError: test_overrides failed! 20220408T04:46:02.5574615Z + cleanup ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
llm,[PyTorch] Add test for all-masked case for native softmax, * (to be filled) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-08T03:37:08Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75488,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75488**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit d39ced1870 (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879389195?check_suite_focus=true) pull / deploylinuxxenialcuda11.3py3.7gcc7 / build (1/11) **Step:** ""Build"" (full log  :repeat: rerun)   20220408T03:53:42.2858252Z C:\\actionsrunner\\...Tensor_embedding': redefinition; different linkage  ``` 20220408T03:53:40.8314034Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220408T03:53:40.8314343Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220408T03:53:40.8314521Z  20220408T03:53:41.1771000Z [4740/5834] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DADD_BREAKPAD_SIGNAL_HANDLER DAT_PER_OPERATOR_HEADERS DCPUINFO_SUPPORTED_PLATFORM=1 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cpu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party\\breakpad\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\miniz2.0.8 IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\pthreadpool\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\cpuinfo\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\third_party\\asmjit\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\FP16\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\src\\..\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2 /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DCAFFE2_BUILD_MAIN_LIB DONNX_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cpu.dir\\__\\aten\\src\\ATen\\native\\quantized\\cpu\\q_avgpool.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cpu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\q_avgpool.cpp 20220408T03:53:41.1777089Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220408T03:53:41.1777393Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220408T03:53:41.1777566Z  20220408T03:53:42.2826863Z [4741/5834] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DADD_BREAKPAD_SIGNAL_HANDLER DAT_PER_OPERATOR_HEADERS DCPUINFO_SUPPORTED_PLATFORM=1 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cpu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party\\breakpad\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\miniz2.0.8 IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\pthreadpool\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\cpuinfo\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\third_party\\asmjit\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\FP16\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\src\\..\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2 /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DCAFFE2_BUILD_MAIN_LIB DONNX_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cpu.dir\\__\\aten\\src\\ATen\\native\\nested\\NestedTensorTransformerFunctions.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cpu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\nested\\NestedTensorTransformerFunctions.cpp 20220408T03:53:42.2837101Z FAILED: caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp.obj  20220408T03:53:42.2847538Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DADD_BREAKPAD_SIGNAL_HANDLER DAT_PER_OPERATOR_HEADERS DCPUINFO_SUPPORTED_PLATFORM=1 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cpu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party\\breakpad\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\miniz2.0.8 IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\pthreadpool\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\cpuinfo\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\third_party\\asmjit\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\FP16\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\src\\..\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2 /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DCAFFE2_BUILD_MAIN_LIB DONNX_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cpu.dir\\__\\aten\\src\\ATen\\native\\nested\\NestedTensorTransformerFunctions.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cpu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\nested\\NestedTensorTransformerFunctions.cpp 20220408T03:53:42.2858252Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src\\ATen/ops/embedding_native.h(21): error C2375: 'at::native::NestedTensor_embedding': redefinition; different linkage 20220408T03:53:42.2859111Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen/native/nested/NestedTensorTransformerFunctions.h(75): note: see declaration of 'at::native::NestedTensor_embedding' 20220408T03:53:42.2859811Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220408T03:53:42.2860326Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220408T03:53:42.2860615Z  20220408T03:53:51.5982241Z [4742/5834] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DADD_BREAKPAD_SIGNAL_HANDLER DAT_PER_OPERATOR_HEADERS DCPUINFO_SUPPORTED_PLATFORM=1 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cpu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party\\breakpad\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\miniz2.0.8 IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\pthreadpool\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\cpuinfo\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\third_party\\asmjit\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\FP16\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\src\\..\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2 /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DCAFFE2_BUILD_MAIN_LIB DONNX_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cpu.dir\\__\\aten\\src\\ATen\\native\\quantized\\cpu\\qadd.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cpu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\qadd.cpp 20220408T03:53:51.5993714Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220408T03:53:51.7494164Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220408T03:53:51.8963623Z  20220408T03:53:52.1349395Z [4743/5834] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DADD_BREAKPAD_SIGNAL_HANDLER DAT_PER_OPERATOR_HEADERS DCPUINFO_SUPPORTED_PLATFORM=1 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cpu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2\\..\\third_party\\breakpad\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\miniz2.0.8 IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\kineto\\libkineto\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\pthreadpool\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\cpuinfo\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fbgemm\\third_party\\asmjit\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\FP16\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\src\\..\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\caffe2 /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DCAFFE2_BUILD_MAIN_LIB DONNX_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cpu.dir\\__\\aten\\src\\ATen\\native\\quantized\\cpu\\qchannel_shuffle.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cpu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\qchannel_shuffle.cpp 20220408T03:53:52.3014466Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
transformer,[PyTorch] Add NestedTensor support functions for transformers," * (to be filled) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:36:59Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75487,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75487**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit ea3beb7cfc (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879386218?check_suite_focus=true) Lint / clangtidy (1/11) **Step:** ""Generate build files"" (full log  :repeat: rerun)   20220408T03:39:26.5908352Z [36;1m  echo ""ERR...t available for the mergebase of your branch""[0m  ``` 20220408T03:39:26.5905494Z [36;1mfi[0m 20220408T03:39:26.5905728Z [36;1m Covers the case where a previous tag doesn't exist for the tree[0m 20220408T03:39:26.5906071Z [36;1m this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly[0m 20220408T03:39:26.5906402Z [36;1mif ! git revparse ""$MERGE_BASE:.circleci/docker""; then[0m 20220408T03:39:26.5906732Z [36;1m  echo ""Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit""[0m 20220408T03:39:26.5907012Z [36;1m  exit 1[0m 20220408T03:39:26.5907176Z [36;1mfi[0m 20220408T03:39:26.5907397Z [36;1mPREVIOUS_DOCKER_TAG=$(git revparse ""$MERGE_BASE:.circleci/docker"")[0m 20220408T03:39:26.5907723Z [36;1m If no image exists but the hash is the same as the previous hash then we should error out here[0m 20220408T03:39:26.5908024Z [36;1mif [[ ""${PREVIOUS_DOCKER_TAG}"" = ""${DOCKER_TAG}"" ]]; then[0m 20220408T03:39:26.5908352Z [36;1m  echo ""ERROR: Something has gone wrong and the previous image isn't available for the mergebase of your branch""[0m 20220408T03:39:26.5908680Z [36;1m  echo ""       contact the PyTorch team to restore the original images""[0m 20220408T03:39:26.5908908Z [36;1m  exit 1[0m 20220408T03:39:26.5909137Z [36;1mfi[0m 20220408T03:39:26.5909325Z [36;1mecho ::setoutput name=rebuild::yes[0m 20220408T03:39:26.5920146Z shell: /usr/bin/bash noprofile norc e o pipefail {0} 20220408T03:39:26.5920371Z env: 20220408T03:39:26.5920518Z   IN_CI: 1 20220408T03:39:26.5920683Z   IS_GHA: 1 20220408T03:39:26.5920896Z   BASE_REVISION: c23e1926fa1535ce76227749e134ffc81f981f48 20220408T03:39:26.5921278Z   DOCKER_IMAGE: 308535385114.dkr.ecr.useast1.amazonaws.com/pytorch/pytorchlinuxbionicrocm5.0py3.7:b0a888fe39b5e7f3543e198ce8875507d832dae0 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
transformer,[PyTorch] Add NestedTensor support functions for transformers," * (to be filled) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:35:27Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75486,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75486**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit eb7bb376e7 (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879376584?check_suite_focus=true) pull / winvs2019cuda11.3py3 / build (1/11) **Step:** ""Build"" (full log  :repeat: rerun)   20220408T03:37:29.5326646Z [36;1m  echo ""ERR...t available for the mergebase of your branch""[0m  ``` 20220408T03:37:29.5323740Z [36;1mfi[0m 20220408T03:37:29.5324040Z [36;1m Covers the case where a previous tag doesn't exist for the tree[0m 20220408T03:37:29.5324381Z [36;1m this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly[0m 20220408T03:37:29.5324691Z [36;1mif ! git revparse ""$MERGE_BASE:.circleci/docker""; then[0m 20220408T03:37:29.5325030Z [36;1m  echo ""Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit""[0m 20220408T03:37:29.5325312Z [36;1m  exit 1[0m 20220408T03:37:29.5325474Z [36;1mfi[0m 20220408T03:37:29.5325696Z [36;1mPREVIOUS_DOCKER_TAG=$(git revparse ""$MERGE_BASE:.circleci/docker"")[0m 20220408T03:37:29.5326021Z [36;1m If no image exists but the hash is the same as the previous hash then we should error out here[0m 20220408T03:37:29.5326322Z [36;1mif [[ ""${PREVIOUS_DOCKER_TAG}"" = ""${DOCKER_TAG}"" ]]; then[0m 20220408T03:37:29.5326646Z [36;1m  echo ""ERROR: Something has gone wrong and the previous image isn't available for the mergebase of your branch""[0m 20220408T03:37:29.5326975Z [36;1m  echo ""       contact the PyTorch team to restore the original images""[0m 20220408T03:37:29.5327258Z [36;1m  exit 1[0m 20220408T03:37:29.5327420Z [36;1mfi[0m 20220408T03:37:29.5327603Z [36;1mecho ::setoutput name=rebuild::yes[0m 20220408T03:37:29.5338787Z shell: /usr/bin/bash noprofile norc e o pipefail {0} 20220408T03:37:29.5339006Z env: 20220408T03:37:29.5339150Z   IN_CI: 1 20220408T03:37:29.5339309Z   IS_GHA: 1 20220408T03:37:29.5339529Z   BASE_REVISION: a9e0d4f9bdc9ba53c061649e20914d7b5e03fdf2 20220408T03:37:29.5339930Z   DOCKER_IMAGE: 308535385114.dkr.ecr.useast1.amazonaws.com/pytorch/pytorchlinuxxenialcuda11.3cudnn8py3gcc7:b0a888fe39b5e7f3543e198ce8875507d832dae0 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
transformer,[PyTorch] Add NestedTensor support functions for transformers," * (to be filled) Here are the NestedTensor kernels we'll need for the improved transformer implementation. Differential Revision: D35409275 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-08T03:32:14Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/75485,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75485**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 9f66a1044b (more details on the Dr. CI page):  * **11/11** failures introduced in this PR   :detective: 11 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879352686?check_suite_focus=true) pull / winvs2019cuda11.3py3 / build (1/11) **Step:** ""Build"" (full log  :repeat: rerun)   20220408T03:36:02.0126496Z CMake Error at caffe2/CMakeLists.txt:909 (add_library):  ``` 20220408T03:36:02.0125090Z  20220408T03:36:02.0125194Z CMake Error at caffe2/CMakeLists.txt:909 (add_library): 20220408T03:36:02.0125416Z   Cannot find source file: 20220408T03:36:02.0125535Z  20220408T03:36:02.0125718Z     /__w/pytorch/pytorch/aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp 20220408T03:36:02.0125928Z  20220408T03:36:02.0126058Z   Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm 20220408T03:36:02.0126269Z   .hpp .hxx .in .txx 20220408T03:36:02.0126378Z  20220408T03:36:02.0126382Z  20220408T03:36:02.0126496Z CMake Error at caffe2/CMakeLists.txt:909 (add_library): 20220408T03:36:02.0126733Z   No SOURCES given to target: torch_cuda 20220408T03:36:02.0126863Z  20220408T03:36:02.0126909Z  20220408T03:36:02.0127037Z CMake Generate step failed.  Build files cannot be regenerated correctly. 20220408T03:36:02.0127379Z Command exited with nonzero status 1 20220408T03:36:02.0127643Z 20.47user 4.83system 0:25.18elapsed 100%CPU (0avgtext+0avgdata 144344maxresident)k 20220408T03:36:02.0127931Z 0inputs+122840outputs (0major+1819075minor)pagefaults 0swaps 20220408T03:36:02.0128273Z Failed to run ['time', '/usr/bin/python3', 'setup.py', 'cmakeonly', 'build'] 20220408T03:36:02.0137457Z [error]Process completed with exit code 1. 20220408T03:36:02.0188927Z [group]Run pytorch/addannotationsgithubaction ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","having issues with tooling, spammed multiple PRs"
rag,Merge torch.cuda._UntypedStorage into torch._UntypedStorage,Fixes CC(Combine `torch._UntypedStorage` and `torch.cuda._UntypedStorage`),2022-04-07T21:08:15Z,module: internals triaged open source Merged cla signed,closed,0,12,https://github.com/pytorch/pytorch/issues/75459,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75459**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :white_check_mark: No Failures (0 Pending) As of commit 9095936ce3 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,"There are a lot of issues with this PR at the moment. I'm getting CUDA memory leaks that I haven't figured out yet. Also, now that THPStorage is being used underneath all storages, we're getting some failures related to the fact that THCPStorage used to do some things differently than THPStorage","I'm having some trouble with a segfault at the moment. It happens when I run `python test/test_serialization.py k serialization_map_location`, and it seems like the exact spot where it happens is inconsistent. I'll update when I know more","I found another serialization failure. The old serialization format (with `_use_new_zipfile_serialization=False`) doesn't work correctly for CUDA tensors. For instance:  click to expand ```python import torch file_path = 'tmp0.pt' device = 'cuda' new_zip = False a0 = torch.arange(15, device=device).reshape((3, 5))  torch.save(a0, file_path, _use_new_zipfile_serialization=new_zip) a1 = torch.load(file_path) print(a0) print(a1) assert a0.eq(a1).all() ```  When I run this, it gives:  click to expand ``` tensor([[ 0,  1,  2,  3,  4],         [ 5,  6,  7,  8,  9],         [10, 11, 12, 13, 14]], device='cuda:0') tensor([[94041261833792, 94040423850000,              2,              3,                       4],         [             5,              6,              7,              8,                       9],         [            10,             11,             12,             13,                      14]], device='cuda:0') Traceback (most recent call last):   File ""/work2/kurtamohler/development/pytorchstoragevirtualization/../pytorchperftestscripts/untypedStorage/error_serialization.py"", line 14, in      assert a0.eq(a1).all() AssertionError ```  Where the first two elements of the loaded tensor are always different garbage values. I don't get this problem with CPU tensors or if I'm using the new serialization format. It looks like the problem starts in the `torch.save` call. If I load the generated file using a different build of pytorch that doesn't have this PR, the loaded tensor has the same exact garbage values, and they're the same every time I load them. The garbage values change if I save again using a build with this PR. I'll figure out what's going on and fix it. The segfault mentioned in my previous comment probably doesn't have the same exact root cause, since that test is only loading an existing file, not saving one. But it is at least similar in that it's also loading a CUDA tensor with the old serialization format, so there is a chance that the failures are related somehow. To give an update about the segfault issue, I found that it's happening on a `std::vector::push_back()` call within `compute_sizes` in `torch/csrc/utils/tensor_new.cpp`. The vector needs to be resized during that call, so it tries to allocate more memory, and the malloc call segfaults for some reason. It's almost definitely not an issue with the vector itself. Something must have messed up the heap before this pointmaybe something wrote to a location it wasn't supposed to. I briefly tried to work around the vector resize by figuring out how large it needs to be and initializing it to the correct size, avoiding the `push_back` call. That prevented the segfault here, but later on after the `compute_sizes` call, something else tried allocating memory and that also caused a segfault. The `torch.save()` issue with old serialization format for CUDA tensors seems much easier to diagnose, so I'll focus on that. With some luck, once I fix out that issue, maybe the segfault issue will reveal itself to be related",I fixed the serialization issues mentioned above. Still more problems to fix though,"Hey , there are still some loose ends here, but I think it's now in a state where it would be helpful to get some feedback, when you have some time. Also, I'm thinking that it might be good to separate cleaning up the generic storage stuff in `torch/csrc/generic` to a followup PR","I've finished up all my TODOs here so far, CI is passing (the one failure is just a failed network connection), and I think all the unresolved threads above are separable. Let me know if there's anything else I should do"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", rebase this please,"Successfully rebased `combinestoragecpucuda` onto `master`, please pull locally before adding more changes (for example, via `git checkout combinestoragecpucuda && git pull rebase`)", merge g,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PT-D][Sharding] Enable ops needed in the transformer model training,"  CC([PTD][Sharding] Enable ops needed in the transformer model training) From the code base of FairSeq and MetaSeq codebase (which is essentially a transformer model), we have found that loads of ops are not supported by sharded tensor. So we now implement a simple version so that we can at least run a transformer example: Ops include: chuck, transpose, view, mask_fill, dropout, softmax and type_as. Isolate the common logic of registering simple ops into a function and for future register, we just need to implement at most three functions for a new op. Differential Revision: D35123021 **NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on Phabricator!",2022-04-06T22:29:13Z,oncall: distributed cla signed sharded_tensor release notes: distributed (sharded) topic: new features,closed,0,2,https://github.com/pytorch/pytorch/issues/75374,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75374**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 179766d971 (more details on the Dr. CI page): Expand to see more  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ,Addressed comments from reviewers and fix linter errors.
transformer,"[PyTorch] NestedTensor kernels for {r,g}elu{,_}","  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) These are simple elementwise ops it's convenient to be able to use with NestedTensor. Differential Revision: D35448205",2022-04-06T22:01:13Z,cla signed release notes: sparse topic: new features,closed,0,6,https://github.com/pytorch/pytorch/issues/75370,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75370**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 45d19b94dc (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879892464?check_suite_focus=true) pull / winvs2019cuda11.3py3 / test (default, 2, 2, windows.8xlarge.nvidia.gpu) (1/1) **Step:** ""Setup Windows"" (full log  :repeat: rerun)   20220408T04:59:53.2550461Z C:\\actionsrunner\\...5158d0e9b7c.sh: line 2: python3: command not found  ``` 20220408T04:59:52.9063159Z   JOB_BASE_NAME: winvs2019cuda11.3py3test 20220408T04:59:52.9063453Z   BUILD_ENVIRONMENT: winvs2019cuda11.3py3 20220408T04:59:52.9063719Z   PR_NUMBER: 75370 20220408T04:59:52.9064045Z   SHA1: 45d19b94dca5cb03e422b1a00ebbc17cf4d9c585 20220408T04:59:52.9064283Z   TAG:  20220408T04:59:52.9064488Z   WORKFLOW_ID: 2112940302 20220408T04:59:52.9065498Z   GITHUB_TOKEN: *** 20220408T04:59:52.9065738Z   GHA_WORKFLOW_JOB_ID:  20220408T04:59:52.9065967Z [endgroup] 20220408T04:59:53.2435942Z + python3 m pip install r requirements.txt 20220408T04:59:53.2550461Z C:\\actionsrunner\\_work\\_temp\\049d70acdc6c4a4fbcee75158d0e9b7c.sh: line 2: python3: command not found 20220408T04:59:53.2580124Z [error]Process completed with exit code 127. 20220408T04:59:53.2720631Z Prepare all required actions 20220408T04:59:53.2766087Z [group]Run ./.github/actions/teardownwin 20220408T04:59:53.2766355Z with: 20220408T04:59:53.2766527Z env: 20220408T04:59:53.2766719Z   IN_CI: 1 20220408T04:59:53.2766915Z   IS_GHA: 1 20220408T04:59:53.2767119Z   GIT_DEFAULT_BRANCH: master 20220408T04:59:53.2767349Z [endgroup] 20220408T04:59:53.2908799Z [group]Run .github\\scripts\\wait_for_ssh_to_drain.ps1 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","FWIW I ended up not actually needing these for transformers, but I wrote them so here they are.",Followup work is adding documentation for this,hey  can you GH1 land this stack of 3 PRs?, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,[PyTorch] Add test for all-masked case for native softmax,  CC([PyTorch] Add test for allmasked case for native softmax) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-06T18:30:11Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75348,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75348**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8246877e0c (more details on the Dr. CI page):  * **3/3** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5859309035?check_suite_focus=true) pull / linuxbionicrocm5.0py3.7 / test (default, 2, 2, linux.rocm.gpu) (1/3) **Step:** ""Upload test artifacts"" (full log  :repeat: rerun)   20220406T22:33:31.8675019Z RuntimeError: test_mobile_optimizer failed!  ``` 20220406T22:33:31.8565142Z CC(fixing CPU builds by making cuda imports optional) 0x55704c4fd039 in _start (/opt/conda/bin/python3.7+0x1d8039) 20220406T22:33:31.8565435Z  20220406T22:33:31.8565664Z AddressSanitizer can not provide additional info. 20220406T22:33:31.8566824Z SUMMARY: AddressSanitizer: BUS /build/llvmtoolchain77.1.0~svn353565/projects/compilerrt/lib/sanitizer_common/sanitizer_libc.cc:153  20220406T22:33:31.8567308Z ==595==ABORTING 20220406T22:33:31.8664885Z Traceback (most recent call last): 20220406T22:33:31.8665294Z   File ""test/run_test.py"", line 1052, in  20220406T22:33:31.8670272Z     main() 20220406T22:33:31.8670612Z   File ""test/run_test.py"", line 1030, in main 20220406T22:33:31.8674621Z     raise RuntimeError(err_message) 20220406T22:33:31.8675019Z RuntimeError: test_mobile_optimizer failed! 20220406T22:33:32.2399793Z + cleanup 20220406T22:33:32.2400166Z + retcode=1 20220406T22:33:32.2400370Z + set +x 20220406T22:33:32.2448382Z [error]Process completed with exit code 1. 20220406T22:33:32.2519182Z Prepare all required actions 20220406T22:33:32.2519519Z Getting action download info 20220406T22:33:32.4174059Z Download action repository 'actions/uploadartifact' (SHA:82c141cc518b40d92cc801eee768e7aafc9c2fa2) 20220406T22:33:32.6060409Z [group]Run ./.github/actions/uploadtestartifacts 20220406T22:33:32.6060632Z with: 20220406T22:33:32.6060839Z   filesuffix: testdefault33linux.2xlarge ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
transformer,[PyTorch] Run test_transformerencoderlayer_gelu on CUDA,  CC([PyTorch] Add native fast path for transformer encoder inference)  CC([PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys)  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA) Preparing to add native fast path; need to test on CUDA! Differential Revision: D35327729,2022-04-06T18:28:34Z,cla signed topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/75347,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75347**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 6c57de8ee6 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
transformer,[PyTorch] Run test_transformerencoderlayer on CUDA,  CC([PyTorch] Add native fast path for transformer encoder inference)  CC([PyTorch] Add NestedTensorCPU and NestedTensorCUDA dispatch keys)  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA) Preparing to add native fast path; need to test on CUDA! Differential Revision: D35327731,2022-04-06T18:27:22Z,cla signed topic: not user facing,closed,0,1,https://github.com/pytorch/pytorch/issues/75346,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75346**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 3d23bf83c3 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
yi,ProcessGroupNCCL is relying on UB to support bool data type,"In https://github.com/pytorch/pytorch/blob/a90bcd2066389595cd2d1302648dc9232dbc4fe8/torch/csrc/distributed/c10d/ProcessGroupNCCL.cppL79 we can see that ProcessGroupNCCL views a tensor of type `bool` as dtype `uint8_t`. Unfortunately, the C++ standard does not specify any detail about how compilers implements `bool`: https://stackoverflow.com/questions/4897844/issizeofbooldefinedintheclanguagestandard https://stackoverflow.com/questions/37418412/usingreinterpretcastwithbool So we are actually relying on UB for this support. Related discussion: https://github.com/facebookresearch/torch_ucc/pull/64 https://github.com/pytorch/pytorch/pull/50250 cc:  Lebedev ",2022-04-06T17:06:04Z,oncall: distributed module: nccl,open,0,1,https://github.com/pytorch/pytorch/issues/75334,"AFAICT, tensor dtypes have well defined representations that are not tied to C datatypes. For example, `torch.long` is always 64bits."
yi,Make LazyIr.h use provided backend namespace,  CC(Make forced eager fallback optional in codegen)  CC(Make default codegen behavior skip Lower function)  CC(Make LazyIr.h use provided backend namespace) Fixes one of the issues in https://github.com/pytorch/xla/issues/3472 Differential Revision: D35411210,2022-04-05T17:53:41Z,cla signed topic: not user facing release notes: lazy,closed,0,3,https://github.com/pytorch/pytorch/issues/75264,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75264**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 79d341a9e8 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5838584970?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7bazeltest / buildandtest (1/1) **Step:** ""Build"" (full log  :repeat: rerun)   20220405T17:56:02.2489774Z [36;1m  echo ""ERR...t available for the mergebase of your branch""[0m  ``` 20220405T17:56:02.2486917Z [36;1mfi[0m 20220405T17:56:02.2487149Z [36;1m Covers the case where a previous tag doesn't exist for the tree[0m 20220405T17:56:02.2487488Z [36;1m this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly[0m 20220405T17:56:02.2487801Z [36;1mif ! git revparse ""$MERGE_BASE:.circleci/docker""; then[0m 20220405T17:56:02.2488142Z [36;1m  echo ""Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit""[0m 20220405T17:56:02.2488426Z [36;1m  exit 1[0m 20220405T17:56:02.2488588Z [36;1mfi[0m 20220405T17:56:02.2488811Z [36;1mPREVIOUS_DOCKER_TAG=$(git revparse ""$MERGE_BASE:.circleci/docker"")[0m 20220405T17:56:02.2489139Z [36;1m If no image exists but the hash is the same as the previous hash then we should error out here[0m 20220405T17:56:02.2489444Z [36;1mif [[ ""${PREVIOUS_DOCKER_TAG}"" = ""${DOCKER_TAG}"" ]]; then[0m 20220405T17:56:02.2489774Z [36;1m  echo ""ERROR: Something has gone wrong and the previous image isn't available for the mergebase of your branch""[0m 20220405T17:56:02.2490171Z [36;1m  echo ""       contact the PyTorch team to restore the original images""[0m 20220405T17:56:02.2490410Z [36;1m  exit 1[0m 20220405T17:56:02.2490572Z [36;1mfi[0m 20220405T17:56:02.2490757Z [36;1mecho ::setoutput name=rebuild::yes[0m 20220405T17:56:02.2502185Z shell: /usr/bin/bash noprofile norc e o pipefail {0} 20220405T17:56:02.2502420Z env: 20220405T17:56:02.2502565Z   IN_CI: 1 20220405T17:56:02.2502724Z   IS_GHA: 1 20220405T17:56:02.2502907Z   GIT_DEFAULT_BRANCH: master 20220405T17:56:02.2503131Z   BASE_REVISION: b64e7dee512c5696457500b3de7c60ecb0e93870 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"large model, low memory: need `torch.load` that loads one submodule at a time"," 🚀 The feature, motivation and pitch Here is a puzzle for you: Say, you have 24GB of GPU RAM and 32GB of CPU RAM and a pretrained fp32 pytorch model checkpoint that is 40GB big. Say, you want to inference or finetune in fp16 or bf16 and you have enough CPU and GPU memory to handle a 20GB model. It should be possible, but you can't load the model in half precision because `torch.load` is inflexible and requires that all 40GB be loaded first. The main issue here is that the user has enough memory to start training in halfprecision, but once they saved the checkpoint, they can't resume since they won't have enough memory to allocate the model and load the checkpoint. Are there any plans to make `torch.load` more flexible and not load the whole thing at once but do it one submodule (or even param) at a time and bonus for converting to the target `torch.dtype` on the fly. In other words the hardware requirements should be close to the final model size and not 2x or 3x. Currently it's 3x when the original is in the higher dtype. Here is the breakdown: ``` 1x allocate the model in fp16 2x load the fp32 model  3x Total peak memory ``` With flexible `torch.load`: ``` 1x allocate the model in fp16 0.05x allocate the largest layer in fp32  1.05x Total peak memory ``` (for demonstration 0.01 is an example of a model with 20 equal layers) At HF Transformers we already use the hack of allocating the model on the meta device and then materializing the model from the `state_dict`, but this is still not enough as the full fp32 model model has to be loaded first.  Thank you! ",2022-04-05T01:05:43Z,module: serialization triaged,open,8,4,https://github.com/pytorch/pytorch/issues/75242, ,"oh, this is old  this has already been solved nicely with https://github.com/huggingface/safetensors/ and it provides lots of other goodies, e.g. also allows saving/updating one weight a time. just need to integrate it into pytorch? But from what I remember  has been working on something similar already.","Hey, noticed that this issue was still open but should have been addressed by `torch.load(mmap=True)`, does this work for the mentioned use case?","I'm not sure this the same functionality, won't mmap create an unnecessary load on the system creating a lot of IO? But as I said `safetensors` solves this neatly by loading only the desired tensors in the first place. And I mentioned you since you said you were working on a similar functionality in `torch.load`. Was I wrong?"
gemma,"PyTorch source code compile fail after ""Built target fbgemm_avx2"""," 🐛 Describe the bug configuration:  Will link against OpenMP libraries: /opt/rh/devtoolset7/root/usr/lib/gcc/x86_64redhatlinux/7/libgomp.so;/lib64/libpthread.so  Found CUDA: /usr/local/cuda (found version ""10.2"")   Caffe2: CUDA detected: 10.2  Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc  Caffe2: CUDA toolkit directory: /usr/local/cuda  Caffe2: Header version is: 10.2  Found CUDNN: /usr/lib64/libcudnn.so    Found cuDNN: v7.6.5  (include: /usr/include, library: /usr/lib64/libcudnn.so) OS: centos7.6 **command to compile:** PYTORCH_BUILD_VERSION=1.6.0+cu102 \\ 	       PYTORCH_BUILD_NUMBER=0 \\ 	       USE_CUDA=1 \\ 	       USE_CUDNN=1 \\ 	       CUDA_HOME=/usr/local/cuda \\ 	       CUDNN_LIB_DIR=/usr/local/cuda \\ 	       USE_SYSTEM_NCCL=1 \\ 	       NCCL_INCLUDE_DIR=/usr/include \\ 	       NCCL_INCLUDE=/usr/include  \\ 	       NCCL_LIB=/usr/lib64 \\ 	       NCCL_LIB_DIR=/usr/lib64 \\ 	       TORCH_CUDA_ARCH_LIST=""3.5;3.7;5.2;6.0;6.1;7.0;7.5+PTX"" \\ 	       CFLAGS=""Wnoerror=deprecateddeclarations"" \\        python3.6 setup.py bdist_wheel [  9%] Generating python/models/seq2seq/__init__.py gmake[2]: *** [third_party/fbgemm/CMakeFiles/fbgemm_generic.dir/src/EmbeddingSpMDM.cc.o] Error 1 [  9%] Building C object confudeps/pytorch_qnnpack/CMakeFiles/pytorch_qnnpack.dir/src/clamp.c.o [  9%] Building CXX object third_party/fbgemm/asmjit/CMakeFiles/asmjit.dir/src/asmjit/core/zonestack.cpp.o [  9%] Building CXX object confudeps/pytorch_qnnpack/CMakeFiles/pytorch_qnnpack.dir/src/convprepack.cc.o [  9%] Generating python/models/seq2seq/beam_search.py [  9%] Building CXX object c10/CMakeFiles/c10.dir/util/SmallVector.cpp.o [  9%] Building C object confudeps/QNNPACK/CMakeFiles/qnnpack.dir/src/convolution.c.o [  9%] Generating python/models/seq2seq/seq2seq_beam_search_test.py [  9%] Building CXX object third_party/ideep/mkldnn/src/CMakeFiles/mkldnn.dir/cpu/gemm/gemm_utils.cpp.o gmake[2]: *** [third_party/fbgemm/CMakeFiles/fbgemm_generic.dir/src/FbgemmI64.cc.o] Error 1 [  9%] Building C object confudeps/pytorch_qnnpack/CMakeFiles/pytorch_qnnpack.dir/src/deconvolution.c.o [  9%] Building C object confudeps/pytorch_qnnpack/CMakeFiles/pytorch_qnnpack.dir/src/convolution.c.o [  9%] Generating python/models/seq2seq/seq2seq_model_helper.py [  9%] Building CXX object confudeps/pytorch_qnnpack/CMakeFiles/pytorch_qnnpack.dir/src/fcprepack.cc.o [  9%] Building CXX object third_party/ideep/mkldnn/src/CMakeFiles/mkldnn.dir/cpu/gemm/jit_avx2_gemm_f32.cpp.o [  9%] Building C object confudeps/pytorch_qnnpack/CMakeFiles/pytorch_qnnpack.dir/src/fullyconnected.c.o [  9%] Generating python/models/seq2seq/seq2seq_model_helper_test.py [  9%] Built target fmt [  9%] Building CXX object third_party/fbgemm/asmjit/CMakeFiles/asmjit.dir/src/asmjit/core/zonetree.cpp.o [  9%] Building CXX object third_party/fbgemm/asmjit/CMakeFiles/asmjit.dir/src/asmjit/core/zonevector.cpp.o [  9%] Generating python/models/seq2seq/seq2seq_util.py [  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuflite.dir/__/src/google/protobuf/stubs/stringpiece.cc.o [  9%] Generating src/x86_64fma/2dfourier8x8.py.o [  9%] Building CXX object c10/CMakeFiles/c10.dir/util/StringUtil.cpp.o [  9%] Generating python/models/seq2seq/train.py [  9%] Building C object confudeps/pytorch_qnnpack/CMakeFiles/pytorch_qnnpack.dir/src/globalaveragepooling.c.o [  9%] Generating python/models/seq2seq/translate.py Scanning dependencies of target nnpack_reference_layers Scanning dependencies of target cache [  9%] Building CXX object third_party/fbgemm/asmjit/CMakeFiles/asmjit.dir/src/asmjit/arm/a64assembler.cpp.o [  9%] Building CXX object third_party/fbgemm/asmjit/CMakeFiles/asmjit.dir/src/asmjit/arm/armformatter.cpp.o /usr/local/bin/python3.6: Error while finding module specification for 'peachpy.x86_64' (ModuleNotFoundError: No module named 'peachpy') [  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuflite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o [  9%] Generating python/models/shufflenet.py gmake[2]: *** [confudeps/NNPACK/src/x86_64fma/2dfourier8x8.py.o] Error 1 [  9%] Building C object confudeps/pytorch_qnnpack/CMakeFiles/pytorch_qnnpack.dir/src/hardsigmoid.c.o gmake[1]: *** [confudeps/NNPACK/CMakeFiles/nnpack.dir/all] Error 2 gmake[1]: *** Waiting for unfinished jobs.... [ 17%] Built target fbgemm_avx2 gmake: *** [all] Error 2 Traceback (most recent call last):   File ""setup.py"", line 732, in      build_deps()   File ""setup.py"", line 316, in build_deps     cmake=cmake)   File ""/root/pytorch/pytorch/tools/build_pytorch_libs.py"", line 62, in build_caffe2     cmake.build(my_env)   File ""/root/pytorch/pytorch/tools/setup_helpers/cmake.py"", line 345, in build     self.run(build_args, my_env)   File ""/root/pytorch/pytorch/tools/setup_helpers/cmake.py"", line 141, in run     check_call(command, cwd=self.build_dir, env=env)   File ""/usr/local/lib/python3.6/subprocess.py"", line 311, in check_call     raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command '['cmake', 'build', '.', 'target', 'install', 'config', 'Release', '', 'j', '72']' returned nonzero exit status 2.  Versions torch = v1.6.0 ",2022-04-04T13:40:52Z,module: build triaged,open,2,0,https://github.com/pytorch/pytorch/issues/75184
transformer,[PyTorch] Add native fast path for transformer encoder inference,Summary: The current PyTorch multihead attention and transformer implementations are slow. This should speed them up for inference. Test Plan: CI Differential Revision: D35239925,2022-04-03T23:09:13Z,fb-exported cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/75163,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75163**  * :heavy_multiplication_x: Python docs build was skipped  * :heavy_multiplication_x: C++ docs build was skipped  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 4de0b1cd9e (more details on the Dr. CI page):  * **20/20** failures introduced in this PR   :detective: 18 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5879608236?check_suite_focus=true) Lint / quickchecks (1/18) **Step:** ""Ensure all test files have header containing ownership information"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D35239925,  can we break this up into more pieces so that it's not a 3000 line PR?,This pull request was **exported** from Phabricator. Differential Revision: D35239925,sent new PR CC([PyTorch] Add native fast path for transformer encoder inference) because apparently I exported this incorrectly once
llm,[PyTorch] Add test for all-masked case for native softmax, * (to be filled) It returns all NaNs. CUDA implementation required a fix for this. Differential Revision: D35327730,2022-04-02T21:50:41Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75152,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75152**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 3abfe61b67 (more details on the Dr. CI page):  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5802100822?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7bazeltest / buildandtest (1/12) **Step:** ""Build"" (full log  :repeat: rerun)   20220402T22:37:03.4570428Z RuntimeError: test_linalg failed!  ``` 20220402T22:37:03.1212337Z  20220402T22:37:03.1212519Z FAILED (errors=3, skipped=58, expected failures=9) 20220402T22:37:03.1212802Z  20220402T22:37:03.1212966Z Generating XML reports... 20220402T22:37:03.1849019Z Generated XML report: testreports/pythonunittest/test_linalg/TESTTestLinalgCPU20220402223501.xml 20220402T22:37:03.4565857Z Traceback (most recent call last): 20220402T22:37:03.4566322Z   File ""test/run_test.py"", line 1053, in  20220402T22:37:03.4567838Z     main() 20220402T22:37:03.4568286Z   File ""test/run_test.py"", line 1031, in main 20220402T22:37:03.4569906Z     raise RuntimeError(err_message) 20220402T22:37:03.4570428Z RuntimeError: test_linalg failed! 20220402T22:37:03.7298682Z + cleanup 20220402T22:37:03.7334012Z + retcode=1 20220402T22:37:03.7334197Z + set +x 20220402T22:37:03.7345104Z [error]Process completed with exit code 1. 20220402T22:37:03.7401901Z Prepare all required actions 20220402T22:37:03.7402235Z Getting action download info 20220402T22:37:03.9141616Z Download action repository 'actions/uploadartifact' (SHA:82c141cc518b40d92cc801eee768e7aafc9c2fa2) 20220402T22:37:04.0368580Z [group]Run ./.github/actions/uploadtestartifacts 20220402T22:37:04.0368899Z with: 20220402T22:37:04.0369102Z   filesuffix: testdefault12linux.2xlarge ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
transformer,[PyTorch] Run test_transformerencoderlayer_gelu on CUDA, * (to be filled) Preparing to add native fast path; need to test on CUDA! Differential Revision: D35327729,2022-04-02T21:50:35Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75151,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75151**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit a718dadd3e (more details on the Dr. CI page):  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5802274711?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (noarch, 1, 1, linux.2xlarge) (1/12) **Step:** ""Test"" (full log  :repeat: rerun)   20220402T22:45:58.8783046Z C:\\actionsrunner\\...vation_out_cuda': is not a class or namespace name  ``` 20220402T22:45:58.8750813Z affine_quantizer.cu 20220402T22:45:58.8756453Z [5696/6271] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILD_SPLIT_CUDA DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTORCH_CUDA_CU_BUILD_MAIN_LIB DUSE_C10D_GLOO DUSE_CUDA DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cuda_cu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\cudnn_frontend\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\THC IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\cuda IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\cuda\\..\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen I""C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\include"" IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include I""C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\include"" IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\magma\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 DTORCH_CUDA_CU_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cuda_cu.dir\\__\\aten\\src\\ATen\\native\\cuda\\Blas.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cuda_cu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp 20220402T22:45:58.8765642Z FAILED: caffe2/CMakeFiles/torch_cuda_cu.dir/__/aten/src/ATen/native/cuda/Blas.cpp.obj  20220402T22:45:58.8773616Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILD_SPLIT_CUDA DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTORCH_CUDA_CU_BUILD_MAIN_LIB DUSE_C10D_GLOO DUSE_CUDA DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cuda_cu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\cudnn_frontend\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\THC IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\cuda IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\cuda\\..\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen I""C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\include"" IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include I""C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\include"" IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\magma\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 DTORCH_CUDA_CU_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cuda_cu.dir\\__\\aten\\src\\ATen\\native\\cuda\\Blas.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cuda_cu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp 20220402T22:45:58.8780343Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp(296): error C2039: 'relu_': is not a member of 'at' 20220402T22:45:58.8780798Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp(30): note: see declaration of 'at' 20220402T22:45:58.8781251Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp(296): error C3861: 'relu_': identifier not found 20220402T22:45:58.8781681Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp(299): error C2039: 'gelu_': is not a member of 'at' 20220402T22:45:58.8782116Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp(30): note: see declaration of 'at' 20220402T22:45:58.8782555Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp(299): error C3861: 'gelu_': identifier not found 20220402T22:45:58.8783046Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\Blas.cpp(394): error C2653: 'structured_addmm_activation_out_cuda': is not a class or namespace name 20220402T22:45:58.8783454Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220402T22:45:58.8783729Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220402T22:45:58.8783892Z  20220402T22:46:00.6078078Z [5697/6271] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILD_SPLIT_CUDA DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTORCH_CUDA_CU_BUILD_MAIN_LIB DUSE_C10D_GLOO DUSE_CUDA DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cuda_cu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\cudnn_frontend\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\THC IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\cuda IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\cuda\\..\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen I""C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\include"" IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include I""C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\include"" IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\magma\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 DTORCH_CUDA_CU_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cuda_cu.dir\\__\\aten\\src\\ATen\\native\\cuda\\IndexKernel.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cuda_cu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\IndexKernel.cpp 20220402T22:46:00.6086893Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220402T22:46:00.6087385Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220402T22:46:00.6087664Z  20220402T22:46:01.1890626Z [5698/6271] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILD_SPLIT_CUDA DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTORCH_CUDA_CU_BUILD_MAIN_LIB DUSE_C10D_GLOO DUSE_CUDA DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_cuda_cu_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\cudnn_frontend\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\THC IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\cuda IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\..\\third_party\\catch\\single_include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\cuda\\..\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen I""C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\include"" IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include I""C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\include"" IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\magma\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj O2 DTORCH_CUDA_CU_BUILD_MAIN_LIB std:c++14 /showIncludes /Focaffe2\\CMakeFiles\\torch_cuda_cu.dir\\__\\aten\\src\\ATen\\native\\cuda\\GridSampler.cpp.obj /Fdcaffe2\\CMakeFiles\\torch_cuda_cu.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src\\ATen\\native\\cuda\\GridSampler.cpp 20220402T22:46:01.1899642Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220402T22:46:01.1900149Z Copyright (C) Microsoft Corporation.  All rights reserved. ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
transformer,[PyTorch] Run test_transformerencoderlayer on CUDA, * (to be filled) Preparing to add native fast path; need to test on CUDA! Differential Revision: D35327731,2022-04-02T21:50:29Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75150,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75150**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 39206dc561 (more details on the Dr. CI page):  * **12/12** failures introduced in this PR   :detective: 12 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5802099746?check_suite_focus=true) pull / linuxxenialcuda11.3py3.7gcc7 / build (1/12) **Step:** ""Build"" (full log  :repeat: rerun)   20220402T23:05:35.6226674Z RuntimeError: test_linalg failed!  ``` 20220402T23:05:34.9568295Z  20220402T23:05:34.9568405Z FAILED (errors=3, skipped=62, expected failures=9) 20220402T23:05:34.9568536Z  20220402T23:05:34.9568620Z Generating XML reports... 20220402T23:05:35.0417099Z Generated XML report: testreports/pythonunittest/test_linalg/TESTTestLinalgCPU20220402225939.xml 20220402T23:05:35.6218623Z Traceback (most recent call last): 20220402T23:05:35.6218895Z   File ""test/run_test.py"", line 1053, in  20220402T23:05:35.6222402Z     main() 20220402T23:05:35.6222605Z   File ""test/run_test.py"", line 1031, in main 20220402T23:05:35.6226464Z     raise RuntimeError(err_message) 20220402T23:05:35.6226674Z RuntimeError: test_linalg failed! 20220402T23:05:36.0468593Z + cleanup 20220402T23:05:36.0468816Z + retcode=1 20220402T23:05:36.0468984Z + set +x 20220402T23:05:36.0515356Z [error]Process completed with exit code 1. 20220402T23:05:36.0562985Z Prepare all required actions 20220402T23:05:36.0563375Z Getting action download info 20220402T23:05:36.2436495Z Download action repository 'actions/uploadartifact' (SHA:82c141cc518b40d92cc801eee768e7aafc9c2fa2) 20220402T23:05:36.3716872Z [group]Run ./.github/actions/uploadtestartifacts 20220402T23:05:36.3717098Z with: 20220402T23:05:36.3717294Z   filesuffix: testdefault33linux.2xlarge ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
yi,Split PyInterpreter into its own file.,"  CC(Disable torch function inside __torch_function__)  CC(__torch_function__ mode)  CC(Dedupe no parsing __torch_function__ handler)  CC(Introduce SafePyObject, make TorchDispatchTypeObject use it)  CC(Split PyInterpreter into its own file.) I also took the opportunity to update the documentation a little for clarity. Signedoffby: Edward Z. Yang ",2022-04-02T03:00:38Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/75141,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75141**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 166c8e1b18 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
yi,[Codegen] pyi files are not included in the wheel," 🐛 Describe the bug We generate pyi file after `setup` function. This is not useful if users doing `python setup.py install` And, for the precompiled library, the pyi file would never be packaged into the wheel/conda.  Versions main branch",2022-04-01T20:00:54Z,,closed,0,0,https://github.com/pytorch/pytorch/issues/75131
rag,fix 'pytorch/tools/code_coverage/README.md' for renamed options,README Instructions of the coverage tool should be fixed. 1. Some CMAKE options are not consistent with 'pytorch/CmakeLists.txt'.  'CODE_COVERAGE' should be 'USE_CPP_CODE_COVERAGE'.  'CMAKE_BUILD_CONFIG' should be 'CMAKE_BUILD_TYPE'. 2. Some arguments of 'oss_coverage.py' are incorrect.  Both 'interestedonly' and 'interestedfolder' doesn't work. I guess both of them were meant to be 'interestonly',2022-04-01T07:04:30Z,open source cla signed release notes: releng topic: bug fixes,closed,0,6,https://github.com/pytorch/pytorch/issues/75091,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75091**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit c4801c23bb (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!, do you know maintains the code coverage tool?, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,"`cholesky_inverse`: complex autograd, forward AD and correct tests.",As per title.,2022-03-31T19:16:31Z,module: autograd open source module: linear algebra complex_autograd cla signed release notes: autograd topic: improvements module: forward ad,closed,0,3,https://github.com/pytorch/pytorch/issues/75033,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75033**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit eeda6766fb (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5790725320?check_suite_focus=true) pull / linuxbionicrocm5.0py3.7 / test (default, 2, 2, linux.rocm.gpu) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220401T17:35:48.8879746Z FAIL [0.023s]: tes...id_sampler_cuda (__main__.TestTorchDeviceTypeCUDA)  ``` 20220401T17:35:48.8634793Z   test_where_scalar_valid_combination_cuda_int32 (__main__.TestTorchDeviceTypeCUDA) ... ok (0.005s) 20220401T17:35:48.8760678Z   test_where_scalar_valid_combination_cuda_int64 (__main__.TestTorchDeviceTypeCUDA) ... ok (0.012s) 20220401T17:35:48.8812223Z   test_where_scalar_valid_combination_cuda_int8 (__main__.TestTorchDeviceTypeCUDA) ... ok (0.005s) 20220401T17:35:48.8866430Z   test_where_scalar_valid_combination_cuda_uint8 (__main__.TestTorchDeviceTypeCUDA) ... ok (0.005s) 20220401T17:35:48.8875396Z   test_cuda_vitals_gpu_only_cuda (__main__.TestVitalSignsCudaCUDA) ... [TORCH_VITAL] Dataloader.enabled		 True 20220401T17:35:48.8876614Z [TORCH_VITAL] Dataloader.basic_unit_test		 TEST_VALUE_STRING 20220401T17:35:48.8877465Z [TORCH_VITAL] CUDA.used		 true 20220401T17:35:48.8878091Z ok (0.001s) 20220401T17:35:48.8878485Z  20220401T17:35:48.8878800Z ====================================================================== 20220401T17:35:48.8879746Z FAIL [0.023s]: test_invalid_shapes_grid_sampler_cuda (__main__.TestTorchDeviceTypeCUDA) 20220401T17:35:48.8881221Z  20220401T17:35:48.8882306Z RuntimeError: cudnn_grid_sampler_forward: ATen not compiled with cuDNN support 20220401T17:35:48.8882940Z  20220401T17:35:48.8886858Z During handling of the above exception, another exception occurred: 20220401T17:35:48.8887610Z  20220401T17:35:48.8887946Z Traceback (most recent call last): 20220401T17:35:48.8889685Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_utils.py"", line 1780, in wrapper 20220401T17:35:48.8891265Z     method(*args, **kwargs) 20220401T17:35:48.8893152Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_device_type.py"", line 376, in instantiated_test 20220401T17:35:48.8894404Z     result = test(self, **param_kwargs) ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
chat,Add forward AD for torch.atan2,This PR adds a formula for the total differential of the atan2 function. Ref.  CC(Rollup: forward-mode AD operator coverage),2022-03-31T17:24:58Z,open source Merged cla signed release notes: autograd topic: improvements module: forward ad,closed,0,6,https://github.com/pytorch/pytorch/issues/75027,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75027**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8425ce373f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"Sorry, I haven't run `test_autograd.py` locally to verify (didn't know it has anything forward ADrelated), I run only `test_ops_gradients.py`.","Ok, I see 😄  https://github.com/pytorch/pytorch/blob/6905feea1acaa6a7bf673c70c5858553a02c5867/test/test_autograd.pyL392",", the test failure should go away now. ", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Model Averaging] Code simplification for _find_process_group function,"Previously the highestlevel process group in `period_process_group_dict` could be `None`, indicating the global group. Now `period_process_group_dict` cannot contain `None` as a process group, so the function `_find_process_group` can just return a process group instead of a tuple  when not found, just return `None`, because now the returned process group cannot be `None`. Proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD))",2022-03-31T07:25:36Z,oncall: distributed triaged open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/75007,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/75007**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 03447c394a (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5782746146?check_suite_focus=true) pull / linuxbionicpy3.7clang9 / test (default, 2, 2, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220401T05:51:02.2186690Z .jenkins/pytorch/t...ped) python backend.py exportmoduleto=model.pt  ``` 20220401T05:51:01.9230438Z  20220401T05:51:01.9230988Z Ran 2 tests in 0.066s 20220401T05:51:01.9231197Z  20220401T05:51:01.9231290Z OK 20220401T05:51:01.9231386Z  20220401T05:51:01.9231483Z Generating XML reports... 20220401T05:51:01.9260577Z Generated XML report: testreports/pythonunittest/test_custom_backend/TESTTestCustomBackend20220401055101.xml 20220401T05:51:02.0233058Z + python backend.py exportmoduleto=model.pt 20220401T05:51:02.1943252Z OMP: Error CC(Use chainerstyle constructor for Conv2d): Initializing libiomp5.so, but found unknown library already initialized. 20220401T05:51:02.1944371Z OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/. 20220401T05:51:02.2186690Z .jenkins/pytorch/test.sh: line 366:  6441 Aborted                 (core dumped) python backend.py exportmoduleto=model.pt 20220401T05:51:02.2187059Z + cleanup 20220401T05:51:02.2187210Z + retcode=134 20220401T05:51:02.2187557Z + set +x 20220401T05:51:02.2228359Z [error]Process completed with exit code 134. 20220401T05:51:02.2253616Z Prepare all required actions 20220401T05:51:02.2278085Z [group]Run ./.github/actions/chownworkspace 20220401T05:51:02.2278556Z env: 20220401T05:51:02.2278714Z   IN_CI: 1 20220401T05:51:02.2278863Z   IS_GHA: 1 20220401T05:51:02.2279042Z   GIT_DEFAULT_BRANCH: master ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","CI error is unrelated  `OMP: Error CC(Use chainerstyle constructor for Conv2d): Initializing libiomp5.so, but found unknown library already initialized.`","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Make all `.pyi.in` files exportable from torch/_C/ folder,Summary: This would eliminate the need for build system changes when new .pyi.in files is added Test Plan: CI Reviewed By: seemethere Differential Revision: D35255502,2022-03-30T17:53:08Z,fb-exported cla signed,closed,0,6,https://github.com/pytorch/pytorch/issues/74962,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74962**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit b80bc112cb (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D35255502,This pull request was **exported** from Phabricator. Differential Revision: D35255502,This pull request was **exported** from Phabricator. Differential Revision: D35255502,This pull request was **exported** from Phabricator. Differential Revision: D35255502,Should the OSS bazel build also rely on this file?
rag,Combine `torch._UntypedStorage` and `torch.cuda._UntypedStorage`,"Follow up from CC(Virtualize `FloatStorage` and other `Storage` classes). Currently, `torch._UntypedStorage` and `torch.cuda._UntypedStorage` are different classes. They should be combined into one class that encapsulates the functionality of both. `torch._TypedStorage` is already set up this way. Once this is done, `getPyTypeObject()` and all the stuff related to registering different storage types in `torch/csrc/DynamicTypes.cpp` can be removed. ",2022-03-29T23:32:24Z,module: internals triaged,closed,1,0,https://github.com/pytorch/pytorch/issues/74933
agent,[torch][elastic] Make final agent barrier to shutdown properly,"Summary: When workers finish their work TE agent will start `synchronize_barrier` procedure. The barrier will wait for other agents at the end of the execution. There is a race condition may happen: The barrier uses TCPStore which is located on Rank0. When Rank0 finishes the work, other ranks may still be in a process of executing `get_all` method. This means that some of them will fail because the TCPStore will be destroyed. The fix adds additional check on Rank0 process: Rank0 process now waits for all other ranks to finish before terminating the process. Test Plan: unit tests Differential Revision: D35227180",2022-03-29T22:56:33Z,oncall: distributed open source fb-exported cla signed,closed,0,10,https://github.com/pytorch/pytorch/issues/74931,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74931**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 8b0a3a6701 (more details on the Dr. CI page):  * **4/4** failures introduced in this PR   :detective: 4 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6029247124?check_suite_focus=true) pull / winvs2019cuda11.3py3 / test (default, 2, 2, windows.8xlarge.nvidia.gpu) (1/4) **Step:** ""Setup Windows"" (full log  :repeat: rerun)   20220414T18:12:30.4608770Z ERROR: Something h... isn't available for the mergebase of your branch  ``` 20220414T18:12:30.4564342Z ++ git mergebase HEAD 14ae8784f3eab92c1429337a45bc6dab21cee7f2 20220414T18:12:30.4577988Z + MERGE_BASE=14ae8784f3eab92c1429337a45bc6dab21cee7f2 20220414T18:12:30.4578842Z + git revparse 14ae8784f3eab92c1429337a45bc6dab21cee7f2:.circleci/docker 20220414T18:12:30.4590635Z b0a888fe39b5e7f3543e198ce8875507d832dae0 20220414T18:12:30.4594623Z ++ git revparse 14ae8784f3eab92c1429337a45bc6dab21cee7f2:.circleci/docker 20220414T18:12:30.4606685Z + PREVIOUS_DOCKER_TAG=b0a888fe39b5e7f3543e198ce8875507d832dae0 20220414T18:12:30.4607119Z + [[ b0a888fe39b5e7f3543e198ce8875507d832dae0 = \\b\\0\\a\\8\\8\\8\\f\\e\\3\\9\\b\\5\\e\\7\\f\\3\\5\\4\\3\\e\\1\\9\\8\\c\\e\\8\\8\\7\\5\\5\\0\\7\\d\\8\\3\\2\\d\\a\\e\\0 ]] 20220414T18:12:30.4607654Z + echo 'ERROR: Something has gone wrong and the previous image isn'\\''t available for the mergebase of your branch' 20220414T18:12:30.4608050Z + echo '       contact the PyTorch team to restore the original images' 20220414T18:12:30.4608273Z + exit 1 20220414T18:12:30.4608770Z ERROR: Something has gone wrong and the previous image isn't available for the mergebase of your branch 20220414T18:12:30.4609161Z        contact the PyTorch team to restore the original images 20220414T18:12:30.4619267Z [error]Process completed with exit code 1. 20220414T18:12:30.4791765Z Prepare all required actions 20220414T18:12:30.4811194Z [group]Run ./.github/actions/teardownlinux 20220414T18:12:30.4811389Z with: 20220414T18:12:30.4811543Z env: 20220414T18:12:30.4811698Z   IN_CI: 1 20220414T18:12:30.4811847Z   IS_GHA: 1 20220414T18:12:30.4812014Z [endgroup] 20220414T18:12:30.4826535Z [group]Run .github/scripts/wait_for_ssh_to_drain.sh ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D35227180,This pull request was **exported** from Phabricator. Differential Revision: D35227180,This pull request was **exported** from Phabricator. Differential Revision: D35227180, merge this (Initiating merge automatically since Phabricator Diff has merged),Merge failed due to Refusing to merge as mandatory check Lint failed for rule superuser Raised by https://github.com/pytorch/pytorch/actions/runs/2171018932,"Hi !  Thank you for your pull request.  We **require** contributors to sign our **Contributor License Agreement**, and yours needs attention. You currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",Let's retry the CLA check., force merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Dynamo+LTC: force lazy device for tensors created without specifying device,"In PR https://github.com/pytorch/pytorch/pull/72936 we already force tensors to be on lazy device if there is a device argument being specified in a call to aten methods. But it turns out that for some benchmarks (yolov3, hf_Bart), dynamo may generate Fx graphs that create eager tensors on the default device without specifying a device argument.   graph for yolov3: https://gist.github.com/shunting314/eabdf6c769c59bc384469717b8f9bb7f  graph for hf_Bart: https://gist.github.com/shunting314/8d5e2d9348a3258959d3954186c48814 Ideally lazy mode should solve the issue here, but before that, this PR just add an explicit lazy device argument for a list of tensor factory methods. This makes sure we create lazy tensors rather than eager tensors on the default device. Test plan: test thru dynamo ``` LTC_TS_CUDA=1 gpui time python torchbench.py speedupltc dcuda randomizeinput only yolov3 LTC_TS_CUDA=1 gpui time python torchbench.py speedupltc dcuda randomizeinput only hf_Bart ```",2022-03-29T22:48:06Z,cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/74930,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74930**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit f6e5902e38 (more details on the Dr. CI page):  * **9/10** failures introduced in this PR * **1/10** tentatively recognized as flaky :snowflake:     * Click here to rerun these jobs   :detective: 8 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5762223533?check_suite_focus=true) linuxdocs / builddocs (cpp) (1/8) **Step:** ""Unknown"" (full log  :repeat: rerun) :snowflake:   20220330T22:19:46.7966842Z E           hypoth... on the first call but did not on a subsequent one  ``` 20220330T22:19:46.7963315Z /opt/conda/lib/python3.7/sitepackages/hypothesis/core.py:606: in execute 20220330T22:19:46.7963587Z     ) % (test.__name__, text_repr[0],)) 20220330T22:19:46.7964028Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  20220330T22:19:46.7964167Z  20220330T22:19:46.7964336Z self =  20220330T22:19:46.7965100Z message = 'Hypothesis test_bmuf_distributed(self=           raise Flaky(message) 20220330T22:19:46.7966842Z E           hypothesis.errors.Flaky: Hypothesis test_bmuf_distributed(self=, cpu_device=False, nesterov=False) produces unreliable results: Falsified on the first call but did not on a subsequent one 20220330T22:19:46.7967250Z  20220330T22:19:46.7967460Z /opt/conda/lib/python3.7/sitepackages/hypothesis/core.py:877: Flaky 20220330T22:19:46.7967738Z ✅ 2622 Passed 20220330T22:19:46.7967972Z 💨 69 Skipped 20220330T22:19:46.7968241Z 🚨 1 Failed 20220330T22:19:46.8160655Z [group]Run  Remove any previous test jsons if they exist 20220330T22:19:46.8160959Z [36;1m Remove any previous test jsons if they exist[0m 20220330T22:19:46.8161194Z [36;1mrm f testjsons*.zip[0m 20220330T22:19:46.8161428Z [36;1mzip r ""testjsons${FILE_SUFFIX}.zip"" test i '*.json'[0m 20220330T22:19:46.8173142Z shell: /usr/bin/bash e {0} ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",I added a list for the tensor factory functions in `lazy_tensor_core/lazy_tensor_core/core/tensor_factory_functions.py ` by searching thru native_functions.yaml with regular expression as   pointed above.  
finetuning,Adding novel 'AdaFamily' optimizer ," 🚀 The feature, motivation and pitch It could be nice to add the novel 'AdaFamily' optimizer (see [1]) to the already available optimizers under 'torch.optim'. Disclaimer: I am the author of this paper, so I might be a bit biased  :) I know the evaluation in the paper is limited, but it seems to give benefits for both normalsize models (ResNet etc.) and also for compact models (MobileNet etc.). I did also an evaluation with NLP finetuning task which is not in the paper, using the code at [2], where it brings an accuracy improvement of ~ 0.5 percent compared to using 'AdamW'. Furthermore, it is actually not a single method, but a 'family' of algorithms, parametrized by parameter 'myu'. It's implementation is quite simple (there are just a few lines of code changes compared to code for Adam/AdaBelief/AdaMomentum).  The code for 'AdaFamily' optimizer can be found at [4]  If something is unclear, just email me (email: hannes.fassold(at)joanneum.at).  [1] ""AdaFamily: A family of Adamlike adaptive gradient methods"", H. Fassold, ISPR, 2022      https://arxiv.org/abs/2203.01603 [2] https://github.com/hfassold/nlp_finetuning_adafamily [3] https://github.com/hfassold/omni_optimizer/blob/main/my_adafamily.py       or https://github.com/hfassold/omni_optimizer/blob/main/my_adafamily.py  Alternatives _No response_  Additional context _No response_ ",2022-03-29T10:08:43Z,module: optimizer triaged needs research function request,open,0,2,https://github.com/pytorch/pytorch/issues/74904,"Hi, Thanks for opening this issue. We do have some pretty strict rules related to adding new Optimizer/Modules: https://github.com/pytorch/pytorch/wiki/DeveloperFAQihaveanewfunctionorfeatureidliketoaddtopytorchshoulditbeinpytorchcoreoralibraryliketorchvision In particular, this work might be a bit too new to be considered to be added into core just now. And a separate library sounds like the best place for it. Don't hesitate to let us know if there is any low level feature that you're missing to implement this though!","Alright, thanks for the feedback !"
rag,fix PostLocalSGDOptimizer and ModelAverager average bug,Fixes CC(post local sgd  decreases accuracy),2022-03-29T05:07:57Z,oncall: distributed triaged open source cla signed ciflow/trunk,closed,0,38,https://github.com/pytorch/pytorch/issues/74894,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74894**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit a1a94d1ee1 (more details on the Dr. CI page):  * **3/3** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5984418874?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/3) **Step:** ""Test"" (full log  :repeat: rerun)   20220412T05:17:34.8364946Z ERROR: Something h... isn't available for the mergebase of your branch  ``` 20220412T05:17:34.8277530Z ++ git mergebase HEAD f9407fdb8647a5ad89aa3194e6825b17d9d25e56 20220412T05:17:34.8334548Z + MERGE_BASE=f9407fdb8647a5ad89aa3194e6825b17d9d25e56 20220412T05:17:34.8334993Z + git revparse f9407fdb8647a5ad89aa3194e6825b17d9d25e56:.circleci/docker 20220412T05:17:34.8346956Z b0a888fe39b5e7f3543e198ce8875507d832dae0 20220412T05:17:34.8350588Z ++ git revparse f9407fdb8647a5ad89aa3194e6825b17d9d25e56:.circleci/docker 20220412T05:17:34.8361967Z + PREVIOUS_DOCKER_TAG=b0a888fe39b5e7f3543e198ce8875507d832dae0 20220412T05:17:34.8362672Z + [[ b0a888fe39b5e7f3543e198ce8875507d832dae0 = \\b\\0\\a\\8\\8\\8\\f\\e\\3\\9\\b\\5\\e\\7\\f\\3\\5\\4\\3\\e\\1\\9\\8\\c\\e\\8\\8\\7\\5\\5\\0\\7\\d\\8\\3\\2\\d\\a\\e\\0 ]] 20220412T05:17:34.8363619Z + echo 'ERROR: Something has gone wrong and the previous image isn'\\''t available for the mergebase of your branch' 20220412T05:17:34.8364365Z + echo '       contact the PyTorch team to restore the original images' 20220412T05:17:34.8364600Z + exit 1 20220412T05:17:34.8364946Z ERROR: Something has gone wrong and the previous image isn't available for the mergebase of your branch 20220412T05:17:34.8365259Z        contact the PyTorch team to restore the original images 20220412T05:17:34.8373682Z [error]Process completed with exit code 1. 20220412T05:17:34.8560185Z Prepare all required actions 20220412T05:17:34.8579069Z [group]Run ./.github/actions/teardownlinux 20220412T05:17:34.8579278Z with: 20220412T05:17:34.8579430Z env: 20220412T05:17:34.8579569Z   IN_CI: 1 20220412T05:17:34.8579728Z   IS_GHA: 1 20220412T05:17:34.8579895Z [endgroup] 20220412T05:17:34.8594794Z [group]Run .github/scripts/wait_for_ssh_to_drain.sh ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","Thanks for the fix ! I can understand that the performance can be improved by flattening all the tensors into a single buffer, and I assume this won't affect the accuracy. What's the fix for the convergence inefficiency reported in  CC(post local sgd  decreases accuracy)? Is it because previously the averaging function can only support the parameters of `types.GeneratorType`?","> Thanks for the fix ! I can understand that the performance can be improved by flattening all the tensors into a single buffer, and I assume this won't affect the accuracy. >  > What's the fix for the convergence inefficiency reported in CC(post local sgd  decreases accuracy)? `utils.average_parameters(iter(params), self.process_group)   ` iter params will get a new copy of params, so it will not execute all redcue params op. i have read your review, and will optimize code as your comment later","i am struggling with the Lint/qucikchecks now the error is following ``` Run (! git nopager grep Il ''  . ':(exclude)**/contrib/**' ':(exclude)third_party' ':(exclude)**.expect' ':(exclude)**.ipynb' ':(exclude)tools/clang_format_hash'  (echo ""The above files do not have correct trailing newlines; please normalize them""; false)) torch/distributed/algorithms/model_averaging/averagers.py torch/distributed/algorithms/model_averaging/utils.py The above files do not have correct trailing newlines; please normalize them  ``` but i can not get more detail error about this two scripts. I tried to use my local pylint, but not find any error like  trailing newlines. Is there any solution to get more detail error?",Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,> iter params will get a new copy of params Can you give a simple example for the proof of concept?,"> > iter params will get a new copy of params >  > Can you give a simple example for the proof of concept? try the unit test below, the main reason is iter a torch.nn.parameter.Paramete will return a new copy so the current code shoud  from  https://github.com/pytorch/pytorch/blob/master/torch/distributed/optim/post_localSGD_optimizer.pyL82 to   self.averager.average_parameters([params]) ```         (2)         def test_periodic_model_averager_param_group_mock_iter(self):             rank = dist.get_rank()             world_size = dist.get_world_size()             rank_to_GPU = init_multigpu_helper(world_size, BACKEND)             device_id = rank_to_GPU[rank][0]             model = nn.Linear(1, 5, bias=False).cuda(device_id)             param = next(model.parameters())             opt = torch.optim.SGD(model.parameters(), lr=0.1)             period = 4             for warmup_steps in [12, 13, 14, 15]:                 averager = averagers.PeriodicModelAverager(period=period, warmup_steps=warmup_steps)                 for step in range(0, 20):                      Reset the parameters at every step.                     for param_group in opt.param_groups:                         for params in param_group[""params""]:                              mock grad                             params.grad = torch.ones_like(param.data) * rank                             params.data = torch.ones_like(param.data) * rank                     for param_group in opt.param_groups:                         for params in param_group[""params""]:                             if params.grad is None:                                 continue                              iter a torch.nn.parameter.Paramete will return a new copy                              wrong case                             averager.average_parameters(iter(params))                              right case                             averager.average_parameters([params])                     if step >= warmup_steps and (step  warmup_steps) % period == 0:                         for param_group in opt.param_groups:                             for params in param_group[""params""]:                                 if params.grad is None:                                     continue                                 self.assertEqual(param.data, torch.ones_like(param.data) * sum(range(world_size)) / world_size)                     else:                          No model averaging, so the parameters are not updated.                         for param_group in opt.param_groups:                             for params in param_group[""params""]:                                 if params.grad is None:                                     continue                                 self.assertEqual(param.data, torch.ones_like(param.data) * rank) ```"," i donot know how to handle the error  due to the first pr to pytorch ""The above files do not have correct trailing newlines; please normalize them"" should someone give some solutions? "," since i donot familar with github commit merge function, so i have to submmit multi cr. [optimize api of ModelAverager]  (https://github.com/pytorch/pytorch/pull/74894/commits/0a4561d6d28c1674a8a95d1fadb3544eb2bed402)    is the major optimzer cr after wayi1 review. other cr is just for code formatting.",i have fixed all checks i can fix. other failed checks seems faild beyond my code. so i think my commit is finished.   hope someone to review my code again.     can you review again if free?,"varma  Can you help review this PR? I am on travel, so probably won't be available until one week later. I will see if I can get a chance to take a quick look in the meanwhile.",new commit has been updated. include following major change: 1. PeriodicModelAverager() and HierarchicalModelAverager() support model.parameters() input and optimzer.param_groups input. 2. add unit test to validate optimzer.param_groups average parameter behaviour. please cr again if you are free   varma ,"> LGTM, thanks for working on this! >  >  It also looks like there is a merge conflict and for some reason CI didn't trigger on your latest update. Do you mind rebasing your PR and resubmitting, and then we should be able to land this change. Thank you! merge conflict has been solved.","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",awesome! I'll get this landed  Could you run some benchmarks on your end that validate the accuracy improvement given by this fix?,"> awesome! I'll get this landed >  >  Could you run some benchmarks on your end that validate the accuracy improvement given by this fix? ok, i will run one resnet50 on imagenet benchmark.   this may take a few days. when finish, i will report."," awesome, thank you! If you also get a chance and can, could you also post the benchmark code so we can help verify on our end? Thank you!  Regardless, I'll also try to run some benchmarks internally as part of landing this to get an idea of the improvement.",">  awesome, thank you! If you also get a chance and can, could you also post the benchmark code so we can help verify on our end? Thank you! >  > Regardless, I'll also try to run some benchmarks internally as part of landing this to get an idea of the improvement. sorry, due to company policy, i can not post current code.  But i will try to use local sgd based on pytorch example when i am free.    https://github.com/pytorch/examples/tree/main/imagenet and actually local sgd and Hierarchical sgd will occur some convergency problem when used in object detection  task, especially faster rcnn with more than 16 subgroups and 16 comn_period_iter.   test code is based mmdetection. this problem can leave for future. https://github.com/openmmlab/mmdetection/blob/master/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py Totally i have tried image classification and object detection tasks.","> sorry, due to company policy, i can not post current code. > But i will try to use local sgd based on pytorch example when i am free. I am particularly interested in whether the current PR can fix  CC(post local sgd  decreases accuracy). Can you confirm this? > especially faster rcnn with more than 16 subgroups and 16 comn_period_iter. I haven't tried this scale with local SGD, and 16 comn_period_iter is often too large even with less than 16 subgroups. If you only have 2 processes per subgroup, I guess you may want to create 4GPU subgroups instead of 2GPU subgroups.","> I am particularly interested in whether the current PR can fix CC(post local sgd  decreases accuracy). Can you confirm this? i will try resnet on imagenet to confirm CC(post local sgd  decreases accuracy) accuracy has been fixed in a few days. > > especially faster rcnn with more than 16 subgroups and 16 comn_period_iter. >  > I haven't tried this scale with local SGD, and 16 comn_period_iter is often too large even with less than 16 subgroups. If you only have 2 processes per subgroup, I guess you may want to create 4GPU subgroups instead of 2GPU subgroups. i suppose there is one situation 16nodes, each nodes has 1 card in low bandwidth.   So we have to create 16 subgroups. then using with Hierarchical sgd, like period_group_size_dict=""[(2,2),(4,4),(8, 8),(16, 16)]"".    comn_period_iter = 16 to make sure low bandwidth and latency has been covered. in this situation, faster rcnn with batch norm still can not converge,  loss will nan quickly. this problem may related to specific model architecture convergency.  if we use faster rcnn with group norm, model can converge although with a few accuracy decrease. if you are interesting, i will report this issue later with reproducible code. from my view, this issue reflects local sgd can not handle with any model architecture due to convergency problem.","> > I am particularly interested in whether the current PR can fix CC(post local sgd  decreases accuracy). Can you confirm this? i will try resnet on imagenet to confirm CC(post local sgd  decreases accuracy) accuracy has been fixed in a few days. > > > especially faster rcnn with more than 16 subgroups and 16 comn_period_iter. > >  > >  > > I haven't tried this scale with local SGD, and 16 comn_period_iter is often too large even with less than 16 subgroups. If you only have 2 processes per subgroup, I guess you may want to create 4GPU subgroups instead of 2GPU subgroups. i suppose there is one situation 16nodes, each nodes has 1 card in low bandwidth.   So we have to create 16 subgroups.  then using with Hierarchical sgd, like period_group_size_dict=""[(2,2),(4,4),(8, 8),(16, 16)]"". comn_period_iter = 16 to make sure low bandwidth and latency has been covered.  in this situation, faster rcnn with batch norm still can not converge,  loss will nan quickly.  this problem may related to specific model architecture convergency. if we use faster rcnn with group norm, model can converge although with a few accuracy decrease. if you are interesting, i will report this issue later with reproducible code. from my view, this issue reflects local sgd can not handle with any model architecture due to convergency problem.","> then using with Hierarchical sgd, like period_group_size_dict=""[(2,2),(4,4),(8, 8),(16, 16)]"". Interesting. Actually I haven't even tested hierarchical SGD on my own use case yet due to some blockers on my side, so I am not sure if there is any convergency bug. Could you please also try it on  CC(post local sgd  decreases accuracy) by setting `period_group_size_dict=""[(, )]""`, which will be equivalent to local SGD? I guess for your 16GPU use case, you can try a conservative config first like `period_group_size_dict=""[(1,4), (2, 8), (4, 16)]""`, so the global averaging frequency will be 4 in this case. > if we use faster rcnn with group norm, model can converge although with a few accuracy decrease. Do you mean in your use case group norm + local SGD can coverage, but batch norm + local SGD cannot converge? Or it's irrelevant to local SGD at all?",">> Could you please also try it on  CC(post local sgd  decreases accuracy) by setting period_group_size_dict=""[(, )]"", which will be equivalent to local SGD i have tested  hierarchical SGD. it can be equivalent to local sgd. >> I guess for your 16GPU use case, you can try a conservative config first like period_group_size_dict=""[(1,4), (2, 8), (4, 16)]"", so the global averaging frequency will be 4 in this case. in low bandwidth, inter node is very slow. so in 16nodes, 1gpu per node case,  we have to set (1, 1) first to decrease internode communicate frequency. on the other hand in period_group_size_dict=[(1,8), (16, 16) ] for 16 nodes ,8 gpu per node case,  faster rcnn model can converge. so i guess the problem due to specific model convergency. >> Do you mean in your use case group norm + local SGD can coverage, but batch norm + local SGD cannot converge? Or it's irrelevant to local SGD at all? yes. faster rcnn batch norm +  local SGD cannot converge may be relevant to the specific model architecture.  but it can reflect some limits to local sgd.  Because without local sgd,  faster rcnn batch norm training can converge very well. relevant paper does not discuss local sgd with object detection model, so the limit may not be found. And on the other hand  i found  faster rcnn batch norm  with gradient accumulation >=4  will also not converge, but the normal train with gradient accumulation=1 converge  very well. So i think we may use local sgd with some specific model or there should be some regularization method to help model converge with local sgd. This can be investigated in the future. Now i have to use other gradient compression method like powersgd or dgc to help object detection model converge.", merge this (Initiating merge automatically since Phabricator Diff has merged),"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","> > awesome! I'll get this landed > >  Could you run some benchmarks on your end that validate the accuracy improvement given by this fix? >  > ok, i will run one resnet50 on imagenet benchmark. this may take a few days. when finish, i will report. i have tested in resnet50 imagenet benchmark. common params: 1. input size 224 2. batch_size_per_card : 256 3. lr  2.048 4. 8 v100 card 5. epoch=90 post local sgd params: 1. ""comn_period_iter"": 16 2. start_localSGD_iter:  0 3. group_size=1，8 subgroups hierarchical sgd params: 1. ""period_group_size_dict"": [(16,8)] 2. group_size=1， 8 subgroups result: baseline:  val.top1 : 77.04 % val.top5 : 93.38 % post_local_sgd: val.top1 : 76.36 % val.top5 : 92.92 %  hierarchical sgd: val.top1 : 76.60 % val.top5 : 93.07 % this results show the effect of lcoal sgd and hierarchical sgd. and the drop of 0.64% top1 may be related to the high comn_period_iter and high subgroups and low start_localSGD_iter.  this can be further optimized by some method like slow momentum or tune the parms","Thanks for the update !  1. It's not very clear to me if this PR has fixed  CC(post local sgd  decreases accuracy), because you mentioned that ""i compare the PostLocalSGDOptimizer and naive PeriodicModelAverager version. PostLocalSGDOptimizer version can keep accuracy and naive PeriodicModelAverager version will drop accuray."" in  CC(post local sgd  decreases accuracy)issuecomment1067534648. Does this PR make PeriodicModelAverager version on par with PostLocalSGDOptimizer version? 2. In this threadissuecomment1067617987), you mentioned model parity is reached in the following scenario: 8cards 1subgroup(equivelent to mini batch sgd): naive PeriodicModelAverager version: accuracy drops PostLocalSGDOptimizer version: accuracy keeps, top1 75.97%(epoch50) How is the above scenario different from the one you just tested for this PR? 3. > this results show the effect of lcoal sgd and hierarchical sgd. and the drop of 0.64% top1 may be related to the high comn_period_iter and high subgroups and low start_localSGD_iter. this can be further optimized by some method like slow momentum or tune the parms You are right. Additionally, increasing subgroup size to probably 2 or 4, in combination of `post_localSGD_hook` may help a lot here, at the cost of extra communication. Note that even if you try gradient compression, you may still have a lower accuracy with the same  of steps, w/o proper tuning."," Did you get a chance to confirm 1) in the above thread? ""Does this PR make PeriodicModelAverager version on par with PostLocalSGDOptimizer version?"""
agent,Gradient w.r.t. parameters or modules," 🚀 The feature, motivation and pitch Often you have multiple models and multiple losses in a system. GAN for example typically has a generator and a discriminator and uses a discriminator loss and a generator loss.  In these cases, it's often the case that you want to compute gradient of a specific loss w.r.t. a specific model. Again in GAN, you want to compute discriminator loss w.r.t. the discriminator and generator loss w.r.t. the generator. Right now, this is not possible. Let's say you have generator G, discriminator D, and noise z vector z. Example from official pytorch DCGAN Tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.htmltraining ``` fake = G(z) discriminator_signal = D(fake.detach()) discriminator_loss = loss1(discriminator_signal) discriminator_loss.backward // Optimise discriminator discriminator_signal_for_generator = D(fake) generator_loss = loss2(discriminator_signal_for_generator) generator_loss.backward // Optimise generator ``` In this case you have to do a discriminator forward pass twice on the same data which is completely unnecessary. This is because there are 2 different losses targeting at 2 different modules. Tensor::backward cannot specify the modules for which gradient should be computed. If omit the `detach` and use `zero_grad` to avoid 2 passes of the discriminator, then we would compute generator grad for `discriminator_loss` for no reason. Instead, we could have the following ``` discriminator_signal = D(G(z)) discriminator_loss = loss1(discrminator_signal) generator_loss = loss2(discriminator_signal) discriminator_loss.backward(D)   or discriminator_loss.backward(D.parameters) generator_loss.backward(G)   or generator_loss.backward(G.parameters) ``` This is the simplest example. In bigger systems such as RL agents with 5 or 6 losses (including adversarial components like MGAIL), we end up doing many unnecessary passes. This feature can be implemented as simple as detaching everything else while computing gradient. It would be great if this can also be implemented in libtorch. Please let me know what you think!  Alternatives _No response_  Additional context _No response_ ",2022-03-28T15:24:43Z,module: autograd triaged,closed,0,2,https://github.com/pytorch/pytorch/issues/74832,"Hi, Thanks for the details. Isn't the `inputs` argument to `.backward()` doing exactly what you want? (doc here: https://pytorch.org/docs/stable/generated/torch.autograd.backward.html?highlight=backwardtorch.autograd.backward)","Yes thank you, I completely missed it!"
rag,[Model Averaging] Remove unused variable world_size in post_localSGD_hook.py,As title,2022-03-26T18:09:13Z,oncall: distributed open source cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/74803,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74803**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit fb23ce848f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Trying to remove a Python header,  CC(Trying to remove a Python header),2022-03-25T18:03:53Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/74762,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74762**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit ea224cdfa5 (more details on the Dr. CI page):  * **14/14** failures introduced in this PR   :detective: 14 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5696218682?check_suite_focus=true) pull / linuxxenialpy3.7gcc7noops / build (1/14) **Step:** ""Build"" (full log  :repeat: rerun)   20220325T18:24:46.5905728Z FAILED: caffe2/tor...rc/autograd/generated/python_fft_functions.cpp.obj  ``` 20220325T18:24:46.3571614Z  20220325T18:24:46.4305140Z [5636/5824] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_functions_3.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_functions_3.cpp 20220325T18:24:46.4322669Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220325T18:24:46.4324599Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220325T18:24:46.4325370Z  20220325T18:24:46.5265503Z [5637/5824] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_nn_functions.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_nn_functions.cpp 20220325T18:24:46.5276923Z FAILED: caffe2/torch/CMakeFiles/torch_python.dir/csrc/autograd/generated/python_nn_functions.cpp.obj  20220325T18:24:46.5288832Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_nn_functions.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_nn_functions.cpp 20220325T18:24:46.5300148Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch/csrc/python_headers.h(17): fatal error C1189: error:  ""Python 2 has reached endoflife and is no longer supported by PyTorch."" 20220325T18:24:46.5894362Z [5638/5824] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_fft_functions.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_fft_functions.cpp 20220325T18:24:46.5905728Z FAILED: caffe2/torch/CMakeFiles/torch_python.dir/csrc/autograd/generated/python_fft_functions.cpp.obj  20220325T18:24:46.5917230Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_fft_functions.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_fft_functions.cpp 20220325T18:24:46.5928537Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch/csrc/python_headers.h(17): fatal error C1189: error:  ""Python 2 has reached endoflife and is no longer supported by PyTorch."" 20220325T18:24:46.7143100Z [5639/5824] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_functions_2.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_functions_2.cpp 20220325T18:24:46.7154230Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220325T18:24:46.7154825Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220325T18:24:46.7155180Z  20220325T18:24:47.1128156Z [5640/5824] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_torch_functions_0.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_torch_functions_0.cpp 20220325T18:24:47.1139285Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220325T18:24:47.1139982Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220325T18:24:47.1140334Z  ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
gpt,[FSDP] How to use fsdp in GPT model in Megatron-LM," 🚀 The feature, motivation and pitch Are there any examples similar to DeepSpeed ​​that can experience the fsdp function of pytorch. It would be nice to provide the GPT model in MegatronLM.  Alternatives I hope to provide examples of benchmarking DeepSpeed ​​to facilitate the indepth use of the fsdp function.  Additional context _No response_",2022-03-25T08:30:05Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74741,"Hello! It is better to ask such kinds of questions at https://discuss.pytorch.org/ I am going to close this issue. Feel free to reopen it, if you think that you need to report a bug or make a feature proposal. Thanks."
rag,[Model Averaging] Fix post_localSGD_optimizer,"I find that the original implementation of `post_localSGD_optimizer.step()` is incorrect: Whenever `averager.average_parameters()` is called, the builtin step counter will be increased. Therefore, this should only be called exactly once per `optimizer.step()`. However, if a model has multiple param groups or params, the current implementation will call `averager.average_parameters()` multiple times and overincrease the step counter. Relevant proposals since hierarchical SGD can be supported on `post_localSGD_optimizer`:  CC(Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer),  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD))",2022-03-25T06:05:33Z,oncall: distributed triaged open source cla signed,closed,0,9,https://github.com/pytorch/pytorch/issues/74737,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74737**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 3aa7a56032 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"> Test failure is real >  > ``` >   test_post_localSGD_optimizer_parity (__main__.TestDistBackendWithSpawn) ... INFO:numba.cuda.cudadrv.driver:init > INFO:torch.testing._internal.common_distributed:Started process 0 with pid 127986 > INFO:torch.testing._internal.common_distributed:Started process 1 with pid 127987 > INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0 > INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1 > INFO:torch.distributed.distributed_c10d:Rank 0: Completed storebased barrier for key:store_based_barrier_key:1 with 2 nodes. > INFO:torch.distributed.distributed_c10d:Rank 1: Completed storebased barrier for key:store_based_barrier_key:1 with 2 nodes. > INFO:torch.testing._internal.common_distributed:Starting event listener thread for rank 1 > INFO:torch.testing._internal.common_distributed:Starting event listener thread for rank 0 > ERROR:torch.testing._internal.common_distributed:Caught exception:  > Traceback (most recent call last): >   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_distributed.py"", line 601, in run_test >     getattr(self, test_name)() >   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_distributed.py"", line 486, in wrapper >     fn() >   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_distributed.py"", line 131, in wrapper >     return func(*args, **kwargs) >   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/distributed/distributed_test.py"", line 4810, in test_post_localSGD_optimizer_parity >     averager2 = copy.deepcopy(averager) >   File ""/opt/conda/lib/python3.7/copy.py"", line 180, in deepcopy >     y = _reconstruct(x, memo, *rv) >   File ""/opt/conda/lib/python3.7/copy.py"", line 281, in _reconstruct >     state = deepcopy(state, memo) >   File ""/opt/conda/lib/python3.7/copy.py"", line 150, in deepcopy >     y = copier(x, memo) >   File ""/opt/conda/lib/python3.7/copy.py"", line 241, in _deepcopy_dict >     y[deepcopy(key, memo)] = deepcopy(value, memo) >   File ""/opt/conda/lib/python3.7/copy.py"", line 169, in deepcopy >     rv = reductor(4) > TypeError: can't pickle torch._C._distributed_c10d.ProcessGroupNCCL objects >  exiting process 1 with exit code: 10 > ERROR:torch.testing._internal.common_distributed:Caught exception:  > Traceback (most recent call last): >   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_distributed.py"", line 601, in run_test >     getattr(self, test_name)() >   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_distributed.py"", line 486, in wrapper >     fn() >   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_distributed.py"", line 131, in wrapper >     return func(*args, **kwargs) >   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/distributed/distributed_test.py"", line 4810, in test_post_localSGD_optimizer_parity >     averager2 = copy.deepcopy(averager) >   File ""/opt/conda/lib/python3.7/copy.py"", line 180, in deepcopy >     y = _reconstruct(x, memo, *rv) >   File ""/opt/conda/lib/python3.7/copy.py"", line 281, in _reconstruct >     state = deepcopy(state, memo) >   File ""/opt/conda/lib/python3.7/copy.py"", line 150, in deepcopy >     y = copier(x, memo) >   File ""/opt/conda/lib/python3.7/copy.py"", line 241, in _deepcopy_dict >     y[deepcopy(key, memo)] = deepcopy(value, memo) >   File ""/opt/conda/lib/python3.7/copy.py"", line 169, in deepcopy >     rv = reductor(4) > TypeError: can't pickle torch._C._distributed_c10d.ProcessGroupNCCL objects >  exiting process 0 with exit code: 10 > Process 0 terminated with exit code 10, terminating remaining processes. > ERROR (4.301s) >     test_post_localSGD_optimizer_parity errored  num_retries_left: 3 > ``` Thanks  ! The deepcopy of an averager now is avoided, and this should be able to fix the error."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey , could you please rebase and address the merge conflict? Thanks!","> Hey , could you please rebase and address the merge conflict? Thanks!  should be fixed."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,[PyTorch Distributed][improvement] User configured sigterm escalation time in Rdzv_conf,"Based off issue timeout)) I've come across, ""sigterm_timeout"" parameter has been added to LaunchConfig and propagated to start_processes() during an Elastic Launch. This allows users a customiseable grace period from when they recieve a sigterm to do any clean up in their own program. So now this can be defined with `rdzv_conf sigterm_timeout=300`. Unit testing is included to check the sigterm escalation timeout behaviour, but not the rdzv_conf propagation. For example, if my kubernetes pod is evicted through preemption or general killing, a pod first recieves a sigterm, which is later escalated to sigkill after a grace period. I can easly change this grace period in the pod to give enough time to send checkpoints to a remote machine, but now the elastic agent killing the process after recieving a sigterm is a limiting factor.",2022-03-25T04:18:32Z,oncall: distributed triaged open source cla signed Stale module: elastic oncall: r2p,closed,0,6,https://github.com/pytorch/pytorch/issues/74735,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74735**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit ef1f1b5171 (more details on the Dr. CI page):  * **4/4** failures introduced in this PR   :detective: 3 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5687015918?check_suite_focus=true) pull / linuxdocs / builddocs (python) (1/3) **Step:** ""Build python docs"" (full log   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.",Did you actually want me to make the proposed changes?,I would really like this merged rather than patching this all the time. What can I do to get this moving?
transformer,[PyTorch Edge] Make contexts thread local for quantized matmul,"Summary: We don't want to create and destroy a new context with each multiplication Test Plan: From fbcode: ```buck test caffe2/test:quantization  test_qmatmul```  Performance Improvement *Benchmarking done by on a model which performs matmuls of the same shapes and counts as Transformer Model, as determined in D30901505* *Notebook in which Benchmarking was performed: https://www.internalfb.com/intern/anp/view/?id=1582075&revision_id=1891629751047842* **Improvement from this diff alone** ~9.71% Reduction in Latency  Non Thread Local Contexts (before this diff, D35087184 v2): 8.5410ms  Thread Local Contexts (this diff, v12): 7.7113ms **FP32 Matmul vs Quantized Matmul, Overall Improvement from this diff stack** 56% reduction in latency compared to FP32 Matmul, 71% reduction in latency compared to Naive QMatmul  FP32 Matmul: 17.4910ms  Quantized Matmul (after this diff): 7.7113ms  Naive Quantized Matmul (dequantize → fp32matmul → quantize): 26.8639ms Reviewed By: kimishpatel Differential Revision: D34756288",2022-03-24T14:42:07Z,fb-exported cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/74676,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74676**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit cf49eb1f8d (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34756288,This pull request was **exported** from Phabricator. Differential Revision: D34756288,This pull request was **exported** from Phabricator. Differential Revision: D34756288,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Model Averaging] Add a unit test that launches hierarchical SGD by PostLocalSGDOptimizer,As title. The added unit test requires 4 GPUs. Please add `ciflow/all` to enable this test. Proposal:  CC(Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer) Parent proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD)),2022-03-24T08:45:00Z,oncall: distributed open source cla signed,closed,0,10,https://github.com/pytorch/pytorch/issues/74668,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74668**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit 4fe68da91a (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,The failure on Windows seems to be irrelevant.," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.", revert this,Reverting PR 74668 failed due to Can't revert PR that was landed via phabricator as D35173938 Raised by https://github.com/pytorch/pytorch/actions/runs/2068592927," No need to revert PR https://github.com/pytorch/pytorch/pull/74668. Just adding `` to these tests should work. These tests require 4 processes, but even with 4 ROCm GPUs, the test environment somehow has only provided 3 processes. Curious why these failure were not detected when the PR was submitted. The PR has already labeled ""ci/master"" and ""ci/all"". cc:   varma ","Reverting PR 74668 failed due to Comment  No need to revert PR CC([Model Averaging] Add a unit test that launches hierarchical SGD by PostLocalSGDOptimizer). Just add  to these tests should work. Curious why these failure were not detected when the PR was submitted. The PR has already labeled ""ci/master"" and ""ci/all"". cc:   varma does not seem to be a valid revert command Raised by https://github.com/pytorch/pytorch/actions/runs/2075308562","  Oh hmm. I was looking at https://hud.pytorch.org/pytorch/pytorch/pull/74668?sha=3491f4c36f63b174a768354af4f1edb8f66f4d38 which had some failures (my mistake). I don't think it got reverted anyways.  As for why they didn't run, I think it did run, since it had trunk workflows."
transformer,Support channel first(or any dim) LayerNorm," 🚀 The feature, motivation and pitch LayerNorm is widely used in Transformer and recently some CNN model(or Transformer + CNN model)  is trying to use this. But CNN has [B, C, H, W] output, that can not use LayerNorm which only support norm last N channel.  Alternatives Support specific dim of mean, not last N dim.  Additional context _No response_",2022-03-24T03:34:30Z,,closed,0,4,https://github.com/pytorch/pytorch/issues/74661,"related:  CC([docs] Improve documentation for LayerNorm, GroupNorm, etc (+ add python reference impl)),  CC([proposal] Parameter dim for F.linear (and maybe nn.Linear)), ",duplicate of  CC(torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application)?,> duplicate of  CC(torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application)? yes,Closing as a duplicate
yi,Allow torch/csrc/deploy/interpreter/Optional.hpp to be allowed into the wheel distribution,"  CC(Allow torch/csrc/deploy/interpreter/Optional.hpp to be allowed into the wheel distribution) Previously `torch/csrc/deploy/interpreter/Optional.hpp` wasn't getting included in the wheel distribution created by `USE_DEPLOY=1 python setup.py bdist_wheel`, this pr fixes that Differential Revision: D35094459",2022-03-23T21:44:15Z,cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/74643,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74643**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit 4a1496825c (more details on the Dr. CI page):  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/2** tentatively recognized as flaky :snowflake:     * Click here to rerun these jobs * **1/2** broken upstream at merge base c5c2d5a9b8 on Mar 23 from  2:42pm to  4:31pm   :snowflake: 1 failure **tentatively classified as flaky** but reruns have not yet been triggered to confirm:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5668227379?check_suite_focus=true) pull / linuxxenialpy3.7clang7onnx / test (default, 1, 2, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun) :snowflake:   20220323T22:58:13.1470182Z [1m[31mE        ...the first call but did not on a subsequent one[0m  ``` 20220323T22:58:13.1464681Z [1m[31m/opt/conda/lib/python3.7/sitepackages/hypothesis/core.py[0m:606: in execute 20220323T22:58:13.1465693Z     ) % (test.__name__, text_repr[0],)) 20220323T22:58:13.1466065Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  20220323T22:58:13.1466274Z  20220323T22:58:13.1466601Z self =  20220323T22:58:13.1467538Z message = 'Hypothesis test_allcompare(self=, d=2, n=5, num_procs=6) produces unreliable results: Falsified on the first call but did not on a subsequent one' 20220323T22:58:13.1468143Z  20220323T22:58:13.1468359Z     def __flaky(self, message): 20220323T22:58:13.1468690Z         if len(self.falsifying_examples)            raise Flaky(message) 20220323T22:58:13.1470182Z 1m[31mE           hypothesis.errors.Flaky: Hypothesis test_allcompare(self=, d=2, n=5, num_procs=6) produces unreliable results: Falsified on the first call but did not on a subsequent one[0m 20220323T22:58:13.1471019Z  20220323T22:58:13.1471392Z [1m[31m/opt/conda/lib/python3.7/sitepackages/hypothesis/core.py[0m:877: Flaky 20220323T22:58:13.1471967Z  Captured stdout call  20220323T22:58:13.1472421Z Ignoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file. 20220323T22:58:13.1472952Z Ignoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file. 20220323T22:58:13.1473370Z Ignoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file. 20220323T22:58:13.1473809Z Ignoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file. 20220323T22:58:13.1474293Z Ignoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file. 20220323T22:58:13.1474685Z Ignoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file. 20220323T22:58:13.1475037Z Ignoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file. ```    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * [pull / pytorchxlalinuxbionicpy3.7clang8 / test (xla, 1, 1, linux.2xlarge) on Mar 23 from  2:42pm to  4:31pm (f7ee308dfb  d583f9c9d2)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Skip specifying rcond for gelsy driver in tests,Fixes  CC(DISABLED test_linalg_lstsq_cpu_float64 (__main__.TestLinalgCPU)),2022-03-23T21:00:59Z,module: flaky-tests open source module: linear algebra cla signed topic: not user facing,closed,0,4,https://github.com/pytorch/pytorch/issues/74638,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74638**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit b3dd740f53 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"For reference (we can discuss this in the next linalg meting), perhaps a more solid way of checking that the solution of lstsq is correct is via the first order optimality conditions of the problem, i.e. KKT.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,[PyTorch] Add & use in-place gelu,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) It seems silly that we don't have this, so add it. Differential Revision: D35087899",2022-03-23T19:59:46Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/74629,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74629**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 68b2cfc0fb (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. 
transformer,"Training, Forward / backward pass with _different_ batch-size, no speedup observed when backward pass has smaller batch-size"," 🚀 The feature, motivation and pitch Currently, it seems that in training the Pytorch autograd engine is 'optimized' only for the case where the forward and backward pass are done with tensors with identical batchsize (= last dimension of tensor). If the backward propagation operates on a tensor corresponding to a _smaller_ batch size (e.g. because some samples have been discarded from the batch after the forward pass according to some criterion  e.g. loss value), no speedup is observed, although it _should_ be faster theoretically (because of the smaller batchsize in the backward pass). This is also mentioned in the NVIDIA GTC 2022 talk 'Faster Neural Network Training, Algorithmically' [1], minute 20:0021:00 So, although technically it is possible to have a smaller batch size for backward step than in the forward size, currently in Pytorch unfortunately it does not bring a speedup.  It would be great if pytorch autograd engine could be extended, so that using a smaller batch size in the backward pass leads to a speedup in training. There are several curriculumlearninglike methods which could be utilized then successfully in order to speedup the training, like minibatch trimming [2] and selective backprop [3]. The basic principle of these methods is to focus more on the 'hard' examples in the batch (the ones with a higher loss  persample loss is calculated in forward pass) and discard the 'easier' examples prior to running the backward pass. Important: A speedup would be observable only for models _without_ batchnorm layers (e.g. transformer models), see [4] for a discussion.  [1] NVIDIA GTC 2022, S41421, Faster Neural Network Training, Algorithmically https://reg.rainfocus.com/flow/nvidia/gtcspring2022/aplive/page/ap?search=S41421 [2] ""Some like it tough: Improving model generalization via progressively increasing the training difficulty"", Fassold et al, ISPR, 2022 https://arxiv.org/abs/2110.13058 https://github.com/hfassold/mbtrim [3] ""Accelerating deep learning by focusing on the biggest loser"", Jiang et al, 2019 https://arxiv.org/pdf/1910.00762.pdf [4] https://stackoverflow.com/questions/68920059/pytorchnospeedupwhendoingbackwardpassonlyforapartofthesamplesinm  Alternatives _No response_  Additional context _No response_ ",2022-03-23T08:36:15Z,feature module: autograd triaged,open,0,5,https://github.com/pytorch/pytorch/issues/74604,I do not see apriori why wouldn't you see a speedup in this case. Can you provide a small selfcontained script that tests this?,"Apparently, it is about propagating in forward and then in backward through full batches in the computational graph even if some subsets are not used and could be pruned... I wonder if Masked Tensors could provide any benefits in such a case, CC  .",maskedtensor could indeed be used if we provide sufficient operator coverage and specialized kernels that take advantage of the absence. Thanks for pointing this out !  do you have a specific model in mind you'd want to see support for first?,"Good question :) Actually, both methods (papers [1] and [2], see my original post) are designed to be quite general, could be applied to _any_ model. So a 'general' solution would be great, but from your comments I sense that this is not doable. As mentioned in my original post, an actual _speedup_ with these methods (note the method [1] provides additionally also a better generalization capability) can be only expected  for models _without_ batchnormalization layers (see [4]) Most CNNs are using batchnormalization layers. I think these could be replaced, without too much harm, by a combination of group normalization and weight standardization, but on the other hand that makes problem when you want to use a pretrained CNN model. In contrast, transformer architectures used in NLP domain usually do not use batchnormalization, they use layer normalization instead. So I think it makes sense to go for transformer architectures instead in NLP domain. E.g the 'DistillBERT' model is widely used. For this model, I have also a selfcontained test code sample, which is not too complicated, at [6]  . So I think it could make sense to use the **DistilLBERT** model, using my code at [6]. Contact my if you need help (email: hannes.fassold(at)joanneum.at). [5] https://stackoverflow.com/questions/68920059/pytorchnospeedupwhendoingbackwardpassonlyforapartofthesamplesinm  [6] https://github.com/hfassold/nlp_finetuning_adafamily","> I do not see apriori why wouldn't you see a speedup in this case. I think a simple example that shows this is: ```python out = f(x)  out is very big! loss = out[0]  My loss only depends on a single entry in out loss.backward()  This will actually be as expensive as if I used all the elements in out ``` And it is indeed expected and not something that we can easily change. The autograd works at the Tensor level (the whole Tensor) and so we won't be able to see if only a subset of it is being used. Masked Tensor does sound like the most promising approach here to be able to skip unnecessary operations. But as Christian pointed out, this will potentially require a large number if custom kernels to get to the performance that you expect."
rag,Thoroughly document `index_put_` and add more test coverage,"While working on CC(index_put : INTERNAL ASSERT FAILED), I found it very difficult to figure out what the behavior of `index_put_` is supposed to be. The documentation didn't really explain it. I had to read the implementation code and try running it with various different arguments to figure out how the `indices` and `values` arguments work. In my opinion, the behavior is a bit complicated, so I figured that the documentation should explain it thoroughly with some good examples. I also added a unit test that covers the broadcasting and expansion cases that I mention in the documentation, to verify that my understanding is correct and because I didn't see existing tests for all of these cases.",2022-03-22T19:33:13Z,module: docs triaged module: advanced indexing open source cla signed Stale,closed,0,5,https://github.com/pytorch/pytorch/issues/74573,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74573**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit ab62d33fcd (more details on the Dr. CI page):  * **2/2** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5651058834?check_suite_focus=true) pull / winvs2019cuda11.3py3 / build (1/2) **Step:** ""Build"" (full log  :repeat: rerun)   20220322T21:11:50.9138304Z C:\\actionsrunner\\...ult': no appropriate default constructor available  ``` 20220322T21:11:50.9124006Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\exception(267): note: or       'bool std::operator ==(const std::exception_ptr &,const std::exception_ptr &) noexcept' [found using argumentdependent lookup] 20220322T21:11:50.9125516Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\exception(271): note: or       'bool std::operator ==(std::nullptr_t,const std::exception_ptr &) noexcept' [found using argumentdependent lookup] 20220322T21:11:50.9126992Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\exception(275): note: or       'bool std::operator ==(const std::exception_ptr &,std::nullptr_t) noexcept' [found using argumentdependent lookup] 20220322T21:11:50.9128480Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\system_error(168): note: or       'bool std::operator ==(const std::error_code &,const std::error_code &) noexcept' [found using argumentdependent lookup] 20220322T21:11:50.9129970Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\system_error(172): note: or       'bool std::operator ==(const std::error_code &,const std::error_condition &) noexcept' [found using argumentdependent lookup] 20220322T21:11:50.9131473Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\system_error(176): note: or       'bool std::operator ==(const std::error_condition &,const std::error_code &) noexcept' [found using argumentdependent lookup] 20220322T21:11:50.9132989Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\system_error(248): note: or       'bool std::operator ==(const std::error_condition &,const std::error_condition &) noexcept' [found using argumentdependent lookup] 20220322T21:11:50.9134451Z C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29333\\include\\thread(204): note: or       'bool std::operator ==(std::thread::id,std::thread::id) noexcept' [found using argumentdependent lookup] 20220322T21:11:50.9135736Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\test\\cpp\\jit\\test_lite_interpreter.cpp(1010): note: or       'builtin C++ operator==(std::streamoff, std::_Iosb::_Seekdir)' 20220322T21:11:50.9137008Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\test\\cpp\\jit\\test_lite_interpreter.cpp(1010): note: while trying to match the argument list '(std::fpos, const std::_Iosb::_Seekdir)' 20220322T21:11:50.9138304Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\test\\cpp\\jit\\test_lite_interpreter.cpp(1010): error C2512: 'testing::AssertionResult': no appropriate default constructor available 20220322T21:11:50.9139659Z C:\\actionsrunner\\_work\\pytorch\\pytorch\\test\\cpp\\jit\\test_lite_interpreter.cpp(1010): note: No constructor could take the source type, or constructor overload resolution was ambiguous 20220322T21:11:50.9140589Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220322T21:11:50.9141175Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220322T21:11:50.9141528Z  20220322T21:11:51.0308445Z [5637/5809] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_linalg_functions.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_linalg_functions.cpp 20220322T21:11:51.0320038Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 20220322T21:11:51.0320637Z Copyright (C) Microsoft Corporation.  All rights reserved. 20220322T21:11:51.0320991Z  20220322T21:11:51.1826124Z [5638/5809] C:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\bin\\sccachecl.exe   /TP DAT_PER_OPERATOR_HEADERS DBUILDING_TESTS DBUILD_CAFFE2 DFMT_HEADER_ONLY=1 DIDEEP_USE_MKL DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS DONNXIFI_ENABLE_EXT=1 DONNX_ML=1 DONNX_NAMESPACE=onnx_torch DTHP_BUILD_MAIN_LIB DUSE_C10D DUSE_C10D_GLOO DUSE_DISTRIBUTED DUSE_EXTERNAL_MZCRC DUSE_NUMPY DWIN32_LEAN_AND_MEAN D_CRT_SECURE_NO_DEPRECATE=1 D_OPENMP_NOFORCE_MANIFEST Dtorch_python_EXPORTS IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build IC:\\actionsrunner\\_work\\pytorch\\pytorch IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\benchmark\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\foxi IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\aten\\src\\TH IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\caffe2\\aten\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\valgrindheaders IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\onnx IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\..\\third_party\\flatbuffers\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\lib\\libshm_windows IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\distributed IC:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\api IC:\\actionsrunner\\_work\\pytorch\\pytorch\\c10\\.. IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\fmt\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\gloo IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googlemock\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\googletest\\googletest\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\protobuf\\src IC:\\actionsrunner\\_work\\pytorch\\pytorch\\build\\win_tmp\\mkl\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\XNNPACK\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\eigen IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\mkldnn\\third_party\\oneDNN\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\third_party\\ideep\\include IC:\\Jenkins\\Miniconda3\\include IC:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numpy\\core\\include IC:\\actionsrunner\\_work\\pytorch\\pytorch\\cmake\\..\\third_party\\pybind11\\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj DUSE_PTHREADPOOL openmp:experimental IC:/actionsrunner/_work/pytorch/pytorch/build/win_tmp/mkl/include DNDEBUG DUSE_KINETO DLIBKINETO_NOCUPTI DUSE_FBGEMM DUSE_XNNPACK DSYMBOLICATE_MOBILE_DEBUG_HANDLE DEDGE_PROFILER_USE_KINETO DHAVE_AVX512_CPU_DEFINITION DHAVE_AVX2_CPU_DEFINITION /MD /O2 /Ob2 /DNDEBUG /w /bigobj DNDEBUG DCAFFE2_USE_GLOO DTH_HAVE_THREAD /EHsc /DNOMINMAX /wd4267 /wd4251 /wd4522 /wd4838 /wd4305 /wd4244 /wd4190 /wd4101 /wd4996 /wd4275 /bigobj std:c++14 /showIncludes /Focaffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_return_types.cpp.obj /Fdcaffe2\\torch\\CMakeFiles\\torch_python.dir\\ /FS c C:\\actionsrunner\\_work\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_return_types.cpp 20220322T21:11:51.1837643Z Microsoft (R) C/C++ Optimizing Compiler Version 19.28.29337 for x64 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",I suspect my test is failing in CI because some of the cases can have duplicate indices. I'll fix that.,Related:  CC([docs] Unclear description of indices arg in torch.index_put_),"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
rag,[Model Averaging] Make HierarchicalModelAverager a subclass of averagers.ModelAverager,Make `HierarchicalModelAverager` a subclass of `averagers.ModelAverager` is a preparation step for incorporating hierarchical SGD into `PostLocalSGDOptimizer`. Proposal:  CC(Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer) Parent proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD)),2022-03-22T18:13:51Z,oncall: distributed open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/74564,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74564**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit b34c58af61 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,[Dynamic RPC] Add graceful shutdown for dynamic RPC members,Stack from ghstack:  CC([RPC small change] Improve logging from 'unknown destination worker') [RPC small change] Improve logging from 'unknown destination worker' * * CC([Dynamic RPC] Add graceful shutdown for dynamic RPC members) [Dynamic RPC] Add graceful shutdown for dynamic RPC members**  CC([Dynamic RPC] Allow existing ranks to communicate with newly joined ranks) [Dynamic RPC] Allow existing ranks to communicate with newly joined ranks Changes:  Refactor token implementation that was used in initialization to be a context manager that is also used in shutdown  Graceful shutdown when the local active calls on tensorpipe agent is 0  Expose rpc agent store instance as a readonly property.,2022-03-22T17:47:51Z,oncall: distributed cla signed release notes: distributed (rpc) topic: new features,closed,0,4,https://github.com/pytorch/pytorch/issues/74561,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74561**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit c711103367 (more details on the Dr. CI page): Expand to see more  * **5/5** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6162032014?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/2) **Step:** ""Test"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",CI failures are unrelated, merge this,This introduced a serious bug:  CC(Use after free in TensorPipeAgent)
yi,Specifying left-right-padding as tuple for asymmetric padding," 🚀 The feature, motivation and pitch Hello,  would it be reasonable to allow padding along a single dimension to be specified as a tuple (left padding, right padding) for asymmetric padding?  It should be clear, but as an example, if I would like to pad this input sequence ``` A B C  ``` like this  ``` P P A B C P ``` I would specify padding = ((2,1))  In 2D:  ``` A B C  D E F ``` ```     P P P P P A B C P  P P D E F P      P P P      P P P  ``` would result in padding = ((2,1),(1,2)).  Does it make sense, or would you recommend to go with F.pad for asymmetric padding?  Thanks! Best, JZ   Alternatives _No response_  Additional context _No response_ ",2022-03-22T17:06:28Z,feature module: nn triaged module: padding,open,0,1,https://github.com/pytorch/pytorch/issues/74554,"Hey , thanks for the request! Which modules or functions in particular are you referring to (convolution, others)? `F.pad` is generally the way to go for maximum flexibility, but please let us know if it's too slow for your use case."
rag,ComplexHalf Coverage Tracker," 🚀 The feature, motivation and pitch There have been multiple requests and issues (see  CC(ComplexHalf support)) to have better operator support for `complex32`. Currently it is almost impossible to do anything with a `Tensor` of `complex32` dtype (even printing doesn't work). All operators which support complex dtypes should ideally support `complex32`. Computation will occur in `complex64` similar to `Half` wherein computation occurs in `float`. **NOTE** : Math op support will mostly be enabled only on CUDA Following is the operator coverage: Fundamental Ops (expected 1.12) * [x] copy_ (to support casting to and from `complex32`) (PR: https://github.com/pytorch/pytorch/pull/73847) * [x] print support (Ref:  CC(Cannot print 32-bit complex tensors)) * [ ] storage support  CC(`storage` does support `complex32` tensor) * [x] type promotion **NOTE**: List will be updated if supporting above list requires supporting another operator  HighPriority Ops (based on user demand) * [x] Support for fft module (expected 1.12)     * [x] fft.fft     * [x] fft.fft2     * [x] fft.fftn     * [x] fft.hfft     * [x] fft.hfft2     * [x] fft.hfftn     * [x] fft.rfft     * [x] fft.rfft2     * [x] fft.rfftn     * [x] fft.ifft     * [x] fft.ifft2     * [x] fft.ifftn     * [x] fft.irfft     * [x] fft.irfft2     * [x] fft.irfftn     * [x] fft.ihfft     * [x] fft.ihfft2     * [x] fft.ihfftn * [ ] Support for stft (see CC(torch.stft  fill_cuda not implemented for ComplexHalf)) (expected 1.12)     * [x] fill_     * [ ] pad * [ ] Support for convolution (see also CC(Convolution for complex tensors)) HighPriority Ops (expected 1.12) * [x] add * [x] sub * [x] mul * [x] div * [x] sum * [ ] mean * [ ] equal * [x] imag * [x] real * [x] H * [x] T * [x] \\_\\_getitem\\_\\_ * [x] Support testing with OpInfo infra     * [x] testing.make_tensor     * [x] complex (required for make_tensor) (PR: https://github.com/pytorch/pytorch/pull/74667) Tensor Creation Ops: * [x] randn (supported, needs tests) * [x] rand (supported, needs tests) * [x] empty (supported, needs tests) * [x] full (supported, needs tests) * [x] ones (supported, needs tests) * [x] zeros (supported, needs tests) * [ ] linspace * [ ] logspace * [ ] arange * [ ] eye Remaining Operators which support complex type (generated from OpInfo) * [ ] \\_\\_radd\\_\\_ * [ ] \\_\\_rdiv\\_\\_ * [ ] \\_\\_rmatmul\\_\\_ * [ ] \\_\\_rmul\\_\\_ * [ ] \\_\\_rpow\\_\\_ * [ ] \\_\\_rsub\\_\\_ * [ ] _masked.mean * [ ] _masked.prod * [ ] _masked.sum * [ ] _masked.var * [x] abs * [ ] acos * [ ] acosh * [ ] addbmm * [ ] addcdiv * [ ] addcmul * [ ] addmm * [ ] addmv * [ ] addr * [ ] allclose * [x] angle * [ ] any * [ ] argwhere * [x] as_strided * [x] asin * [ ] asinh * [x] atan * [ ] atanh * [x] atleast_1d * [x] atleast_2d * [x] atleast_3d * [ ] baddbmm * [x] bfloat16 * [x] block_diag * [ ] bmm * [x] bool * [ ] broadcast_tensors * [ ] broadcast_to * [ ] byte * [ ] cartesian_prod * [x] cat * [x] char * [ ] cholesky * [ ] cholesky_inverse * [ ] cholesky_solve * [x] chunk * [x] clone * [x] column_stack * [ ] combinations * [x] conj * [x] conj_physical * [x] contiguous * [ ] corrcoef * [ ] cos * [x] cosh * [ ] count_nonzero * [ ] cov * [ ] cross * [ ] cumprod * [ ] cumsum * [ ] cumulative_trapezoid * [x] diag * [x] diag_embed * [ ] diagflat * [x] diagonal * [ ] diff * [ ] dist * [ ] dot * [x] double * [x] dsplit * [ ] dstack * [ ] eig * [ ] einsum * [x] empty_like * [ ] eq * [x] exp * [ ] expand * [ ] expand_as * [x] flatten * [ ] flip * [ ] fliplr * [ ] flipud * [x] float * [ ] float_power * [ ] full_like * [ ] gather * [ ] geqrf * [ ] gradient * [ ] half * [x] hsplit * [ ] hstack * [ ] index_add * [ ] index_copy * [ ] index_fill * [ ] index_put * [ ] index_select * [ ] inner * [ ] int * [ ] inverse * [ ] isclose * [x] isfinite * [x] isinf * [ ] isnan * [x] isreal * [ ] istft * [ ] kron * [ ] ldexp * [ ] lerp * linalg.cholesky (won't support) * linalg.cholesky_ex (won't support) * linalg.cond (won't support) * linalg.cross (won't support) * linalg.det (won't support) * linalg.eig (won't support) * linalg.eigh (won't support) * linalg.eigvals (won't support) * linalg.eigvalsh (won't support) * linalg.householder_product (won't support) * linalg.inv (won't support) * linalg.inv_ex (won't support) * linalg.lstsq (won't support) * linalg.lu_factor (won't support) * linalg.lu_factor_ex (won't support) * linalg.matrix_norm (won't support) * linalg.matrix_power (won't support) * linalg.matrix_rank (won't support) * linalg.multi_dot (won't support) * linalg.norm (won't support) * linalg.pinv (won't support) * linalg.qr (won't support) * linalg.slogdet (won't support) * linalg.solve (won't support) * linalg.solve_triangular (won't support) * linalg.svd (won't support) * linalg.svdvals (won't support) * linalg.tensorinv (won't support) * linalg.tensorsolve (won't support) * linalg.vector_norm (won't support) * [x] log * [ ] log10 * [ ] log2 * [x] log_softmax * [ ] logical_and * [ ] logical_not * [ ] logical_or * [ ] logical_xor * [x] long * [ ] lu * [ ] lu_solve * [ ] lu_unpack * [x] mH * [x] mT * [ ] masked_fill * [ ] masked_scatter * [ ] masked_select * [ ] matmul * [ ] matrix_exp * [ ] meshgrid * [ ] mm * [x] movedim * [ ] mv * [x] narrow * [ ] ne * [x] neg * [x] new_empty * [x] new_full * [x] new_ones * [x] new_zeros * [ ] nn.functional.feature_alpha_dropout * [ ] nn.functional.linear * [ ] nn.functional.normalize * [ ] nn.functional.pad * [ ] nn.functional.pairwise_distance * [ ] nn.functional.pixel_shuffle * [ ] nn.functional.pixel_unshuffle * [ ] nn.functional.silu * [ ] nn.functional.softmin * [ ] nn.functional.softsign * [ ] nn.functional.tanhshrink * [ ] nn.functional.unfold * [ ] nonzero * [ ] norm * [x] ones_like * [ ] ormqr * [ ] outer * [x] permute * [ ] pinverse * [x] positive * [ ] pow * [x] prod * [ ] put * [ ] qr * [x] rand_like * [x] randn_like * [x] ravel * [ ] reciprocal * [ ] renorm * [ ] repeat * [ ] repeat_interleave * [x] reshape * [x] reshape_as * [ ] resize_ * [ ] resize_as_ * [ ] resolve_conj * [ ] resolve_neg * [ ] roll * [ ] rot90 * [ ] rsqrt * [ ] rsub * [ ] scatter * [ ] scatter_add * [x] select * [ ] sgn * [ ] short * [ ] sigmoid * [x] sin * [ ] sinc * [x] sinh * [ ] softmax * [ ] solve * [x] split * [x] split_with_sizes * [ ] sqrt * [ ] square * [x] squeeze * [ ] stack * [ ] std * [ ] std_mean * [ ] sum_to_size * [ ] svd * [ ] symeig * [ ] t * [ ] take * [ ] take_along_dim * [x] tan * [x] tanh * [ ] tensor_split * [ ] tensordot * [ ] tile * [ ] to_sparse * [x] trace * [x] transpose * [ ] trapezoid * [ ] trapz * [ ] triangular_solve * [x] tril * [x] triu * [ ] true_divide * [x] unfold * [x] unsqueeze * [ ] var * [ ] var_mean * [ ] vdot * [ ] view * [ ] view_as * [ ] view_as_real * [x] vsplit * [ ] vstack * [ ] where * [ ] zero_ * [x] zeros_like Misc: * Fix tests in `test_torch.py`     * [ ] test_copy_all_dtypes_and_devices > Fails due to copy(x) (unsupported storage type )     * [ ] test_copy_math_view > Uses make_tensor     * [ ] test_copy_transpose_math_view > Uses make_tensor     * [ ] (maybe?)test_copy_noncontig > Tests only 1 dtype     * [ ] (maybe?)test_copy_broadcast > Tests only 1 dtype cc:    Alternatives _No response_  Additional context _No response_ ",2022-03-22T13:35:08Z,feature triaged module: complex module: half,open,3,4,https://github.com/pytorch/pytorch/issues/74537,Probably prime candidate for being JITerated by default.,Is this still updated?,"Unfortunately, I don't think so.","So, can we close it?"
,RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .," 🐛 Describe the bug I am fine tuning masked language model from XLM Roberta large on google machine specs. I made couple of experiments and was strange to see few results. `I think Pytorch is not functioning properly. ` I am using pretrained Hugging face model.  I am using  https://huggingface.co/xlmrobertalarge I am not using fairseq or anything. ``` tokenizer = tr.XLMRobertaTokenizer.from_pretrained(""xlmrobertalarge"",local_files_only=True) model = tr.XLMRobertaForMaskedLM.from_pretrained(""xlmrobertalarge"", return_dict=True,local_files_only=True) model.gradient_checkpointing_enable() included as new line ``` Here is `NvidiaSMI` ``` b'Tue Mar 22 05:06:40 2022 \\n++\\n  ``` **Training Code** ``` training_args = tr.TrainingArguments(      output_dir='****'     ,logging_dir='****'         directory for storing logs     ,save_strategy=""epoch""     ,run_name=""****""     ,learning_rate=2e5     ,logging_steps=1000     ,overwrite_output_dir=True     ,num_train_epochs=10     ,per_device_train_batch_size=8     ,prediction_loss_only=True     ,gradient_accumulation_steps=4      ,gradient_checkpointing=True     ,bf16=True CC(Can users move generated computation  graph manually in the forward pass from GPU to CPU? And then back to GPU for backward)  ,optim=""adafactor"" ) trainer = tr.Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=train_data ) ``` Also `gradient_checkpointing` never works. Strange.  **Also, is it using all 8 GPU's?**  Versions Versions torch==1.11.0+cu113  torchvision==0.12.0+cu113   torchaudio==0.11.0+cu113  transformers==4.17.0",2022-03-22T05:26:15Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74522,"I think it's possible that you are not using the config/setup properly when training the model given that even batch_size=4 caused OOM, could you submit issue directly to the HuggingFace repo https://github.com/huggingface/transformers? We don't have too much context on model specific issue tbh. They might have a better idea what's wrong with your issue. There's possibility that there's fragmented memory that is not being released properly, although I doubted about it as it seems you might not using all GPUs. But if you think there's fragmented memory, one suggestion is to use `torch.cuda.empty_cache()` to release some memory, you can refer to this post on some previous discussions https://discuss.pytorch.org/t/abouttorchcudaemptycache/34232/20"
transformer,[PyTorch] _addm_activation native function for matmul/bias/activation fusion,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) Here's an extended version of addmm that takes advantage of cublasLt's fused addmm + relu/gelu support. Differential Revision: D35019612",2022-03-21T19:45:29Z,cla signed,closed,0,13,https://github.com/pytorch/pytorch/issues/74490,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74490**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f2448c136c (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,,"> Do we want to expose them in native_functions.yaml if they are private and intended to be called only from some other publicly exposed functions? Don't we need to do that for testing? Also, do we want to keep this private? Fused matmulbias{r,g}elu  seems like something people might want (but what I do I know about AI research).","Yeah, we need it to be able to test from python, and yes, fused matmulbiasgelu/relu is a useful op, but requirements are higher for publicly available op, such as you are expected to provide a gradient computation for it, and documentation. We still can have them in native_functions as private ops starting with `_`, we have a few of those already. ","hey  this is looking green, can I have a review?",". ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it?","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I'll see what I can do.","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively).","> > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? >  > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning.","> > > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? > >  > >  > > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). >  > Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning. OK I've verified that `test_addmm_repro_possible_cublas_gelu_bug_cuda_float32` (and bfloat16, and float64) will fail as advertised if and only if the following patch is applied: ```  a/fbcode/caffe2/aten/src/ATen/native/cuda/Blas.cpp +++ b/fbcode/caffe2/aten/src/ATen/native/cuda/Blas.cpp @@ 257,7 +257,7 @@                self.data_ptr(),                result_>data_ptr(),                result_ld, if 0 +if !0                activation_to_gemm_and_blas_arg(activation)  else                // GELU is not supported (and does not compile!) prior @@ 315,7 +315,7 @@  // gating activation_to_gemm_and_blas_arg above; here we are manually  // performing a postGELU because we weren't able to use the GELU  // epilogue above. if !0 +if !!0    if (useLtInterface && activation == Activation::GELU) {      result_ = c10::MaybeOwned::owned(at::gelu(*result_));    } ``` That said, I agree it is suspicious that it doesn't fail when you try via cublas and it does fail when I try via my new codepath, so I am going to look through this diff again to make sure I haven't done something silly like doublegelu.","> I am going to look through this diff again to make sure I haven't done something silly like doublegelu. Nope, didn't find anything silly. We know relu is working, and we're functioning correctly for gelu and set up to do the fusion when we figure out correctness, so I'm going to go ahead and land this.","https://gist.github.com/swolchok/f550a9dfbab879cc514912d5f5ab59af has the version of test_linalg.py with the repro test for gelu epilogue not working properly, in case old versions of this PR aren't accessible after I update it.",Thanks for the repro; I've isolated it to cublaslt and have passed the repro on.
rag,[Model Averaging] Make HierarchicalModelAverager a subclass of averagers.ModelAverager,Make `HierarchicalModelAverager` a subclass of `averagers.ModelAverager` is a preparation step for incorporating hierarchical SGD into `PostLocalSGDOptimizer`. Proposal:  CC(Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer) Parent proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD)),2022-03-22T18:13:51Z,oncall: distributed open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/74564,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74564**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit b34c58af61 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
agent,[Dynamic RPC] Add graceful shutdown for dynamic RPC members,Stack from ghstack:  CC([RPC small change] Improve logging from 'unknown destination worker') [RPC small change] Improve logging from 'unknown destination worker' * * CC([Dynamic RPC] Add graceful shutdown for dynamic RPC members) [Dynamic RPC] Add graceful shutdown for dynamic RPC members**  CC([Dynamic RPC] Allow existing ranks to communicate with newly joined ranks) [Dynamic RPC] Allow existing ranks to communicate with newly joined ranks Changes:  Refactor token implementation that was used in initialization to be a context manager that is also used in shutdown  Graceful shutdown when the local active calls on tensorpipe agent is 0  Expose rpc agent store instance as a readonly property.,2022-03-22T17:47:51Z,oncall: distributed cla signed release notes: distributed (rpc) topic: new features,closed,0,4,https://github.com/pytorch/pytorch/issues/74561,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74561**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  :pill: CI failures summary and remediations As of commit c711103367 (more details on the Dr. CI page): Expand to see more  * **5/5** failures introduced in this PR   :detective: 2 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/6162032014?check_suite_focus=true) pull / linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) (1/2) **Step:** ""Test"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment.  ",CI failures are unrelated, merge this,This introduced a serious bug:  CC(Use after free in TensorPipeAgent)
yi,Specifying left-right-padding as tuple for asymmetric padding," 🚀 The feature, motivation and pitch Hello,  would it be reasonable to allow padding along a single dimension to be specified as a tuple (left padding, right padding) for asymmetric padding?  It should be clear, but as an example, if I would like to pad this input sequence ``` A B C  ``` like this  ``` P P A B C P ``` I would specify padding = ((2,1))  In 2D:  ``` A B C  D E F ``` ```     P P P P P A B C P  P P D E F P      P P P      P P P  ``` would result in padding = ((2,1),(1,2)).  Does it make sense, or would you recommend to go with F.pad for asymmetric padding?  Thanks! Best, JZ   Alternatives _No response_  Additional context _No response_ ",2022-03-22T17:06:28Z,feature module: nn triaged module: padding,open,0,1,https://github.com/pytorch/pytorch/issues/74554,"Hey , thanks for the request! Which modules or functions in particular are you referring to (convolution, others)? `F.pad` is generally the way to go for maximum flexibility, but please let us know if it's too slow for your use case."
rag,ComplexHalf Coverage Tracker," 🚀 The feature, motivation and pitch There have been multiple requests and issues (see  CC(ComplexHalf support)) to have better operator support for `complex32`. Currently it is almost impossible to do anything with a `Tensor` of `complex32` dtype (even printing doesn't work). All operators which support complex dtypes should ideally support `complex32`. Computation will occur in `complex64` similar to `Half` wherein computation occurs in `float`. **NOTE** : Math op support will mostly be enabled only on CUDA Following is the operator coverage: Fundamental Ops (expected 1.12) * [x] copy_ (to support casting to and from `complex32`) (PR: https://github.com/pytorch/pytorch/pull/73847) * [x] print support (Ref:  CC(Cannot print 32-bit complex tensors)) * [ ] storage support  CC(`storage` does support `complex32` tensor) * [x] type promotion **NOTE**: List will be updated if supporting above list requires supporting another operator  HighPriority Ops (based on user demand) * [x] Support for fft module (expected 1.12)     * [x] fft.fft     * [x] fft.fft2     * [x] fft.fftn     * [x] fft.hfft     * [x] fft.hfft2     * [x] fft.hfftn     * [x] fft.rfft     * [x] fft.rfft2     * [x] fft.rfftn     * [x] fft.ifft     * [x] fft.ifft2     * [x] fft.ifftn     * [x] fft.irfft     * [x] fft.irfft2     * [x] fft.irfftn     * [x] fft.ihfft     * [x] fft.ihfft2     * [x] fft.ihfftn * [ ] Support for stft (see CC(torch.stft  fill_cuda not implemented for ComplexHalf)) (expected 1.12)     * [x] fill_     * [ ] pad * [ ] Support for convolution (see also CC(Convolution for complex tensors)) HighPriority Ops (expected 1.12) * [x] add * [x] sub * [x] mul * [x] div * [x] sum * [ ] mean * [ ] equal * [x] imag * [x] real * [x] H * [x] T * [x] \\_\\_getitem\\_\\_ * [x] Support testing with OpInfo infra     * [x] testing.make_tensor     * [x] complex (required for make_tensor) (PR: https://github.com/pytorch/pytorch/pull/74667) Tensor Creation Ops: * [x] randn (supported, needs tests) * [x] rand (supported, needs tests) * [x] empty (supported, needs tests) * [x] full (supported, needs tests) * [x] ones (supported, needs tests) * [x] zeros (supported, needs tests) * [ ] linspace * [ ] logspace * [ ] arange * [ ] eye Remaining Operators which support complex type (generated from OpInfo) * [ ] \\_\\_radd\\_\\_ * [ ] \\_\\_rdiv\\_\\_ * [ ] \\_\\_rmatmul\\_\\_ * [ ] \\_\\_rmul\\_\\_ * [ ] \\_\\_rpow\\_\\_ * [ ] \\_\\_rsub\\_\\_ * [ ] _masked.mean * [ ] _masked.prod * [ ] _masked.sum * [ ] _masked.var * [x] abs * [ ] acos * [ ] acosh * [ ] addbmm * [ ] addcdiv * [ ] addcmul * [ ] addmm * [ ] addmv * [ ] addr * [ ] allclose * [x] angle * [ ] any * [ ] argwhere * [x] as_strided * [x] asin * [ ] asinh * [x] atan * [ ] atanh * [x] atleast_1d * [x] atleast_2d * [x] atleast_3d * [ ] baddbmm * [x] bfloat16 * [x] block_diag * [ ] bmm * [x] bool * [ ] broadcast_tensors * [ ] broadcast_to * [ ] byte * [ ] cartesian_prod * [x] cat * [x] char * [ ] cholesky * [ ] cholesky_inverse * [ ] cholesky_solve * [x] chunk * [x] clone * [x] column_stack * [ ] combinations * [x] conj * [x] conj_physical * [x] contiguous * [ ] corrcoef * [ ] cos * [x] cosh * [ ] count_nonzero * [ ] cov * [ ] cross * [ ] cumprod * [ ] cumsum * [ ] cumulative_trapezoid * [x] diag * [x] diag_embed * [ ] diagflat * [x] diagonal * [ ] diff * [ ] dist * [ ] dot * [x] double * [x] dsplit * [ ] dstack * [ ] eig * [ ] einsum * [x] empty_like * [ ] eq * [x] exp * [ ] expand * [ ] expand_as * [x] flatten * [ ] flip * [ ] fliplr * [ ] flipud * [x] float * [ ] float_power * [ ] full_like * [ ] gather * [ ] geqrf * [ ] gradient * [ ] half * [x] hsplit * [ ] hstack * [ ] index_add * [ ] index_copy * [ ] index_fill * [ ] index_put * [ ] index_select * [ ] inner * [ ] int * [ ] inverse * [ ] isclose * [x] isfinite * [x] isinf * [ ] isnan * [x] isreal * [ ] istft * [ ] kron * [ ] ldexp * [ ] lerp * linalg.cholesky (won't support) * linalg.cholesky_ex (won't support) * linalg.cond (won't support) * linalg.cross (won't support) * linalg.det (won't support) * linalg.eig (won't support) * linalg.eigh (won't support) * linalg.eigvals (won't support) * linalg.eigvalsh (won't support) * linalg.householder_product (won't support) * linalg.inv (won't support) * linalg.inv_ex (won't support) * linalg.lstsq (won't support) * linalg.lu_factor (won't support) * linalg.lu_factor_ex (won't support) * linalg.matrix_norm (won't support) * linalg.matrix_power (won't support) * linalg.matrix_rank (won't support) * linalg.multi_dot (won't support) * linalg.norm (won't support) * linalg.pinv (won't support) * linalg.qr (won't support) * linalg.slogdet (won't support) * linalg.solve (won't support) * linalg.solve_triangular (won't support) * linalg.svd (won't support) * linalg.svdvals (won't support) * linalg.tensorinv (won't support) * linalg.tensorsolve (won't support) * linalg.vector_norm (won't support) * [x] log * [ ] log10 * [ ] log2 * [x] log_softmax * [ ] logical_and * [ ] logical_not * [ ] logical_or * [ ] logical_xor * [x] long * [ ] lu * [ ] lu_solve * [ ] lu_unpack * [x] mH * [x] mT * [ ] masked_fill * [ ] masked_scatter * [ ] masked_select * [ ] matmul * [ ] matrix_exp * [ ] meshgrid * [ ] mm * [x] movedim * [ ] mv * [x] narrow * [ ] ne * [x] neg * [x] new_empty * [x] new_full * [x] new_ones * [x] new_zeros * [ ] nn.functional.feature_alpha_dropout * [ ] nn.functional.linear * [ ] nn.functional.normalize * [ ] nn.functional.pad * [ ] nn.functional.pairwise_distance * [ ] nn.functional.pixel_shuffle * [ ] nn.functional.pixel_unshuffle * [ ] nn.functional.silu * [ ] nn.functional.softmin * [ ] nn.functional.softsign * [ ] nn.functional.tanhshrink * [ ] nn.functional.unfold * [ ] nonzero * [ ] norm * [x] ones_like * [ ] ormqr * [ ] outer * [x] permute * [ ] pinverse * [x] positive * [ ] pow * [x] prod * [ ] put * [ ] qr * [x] rand_like * [x] randn_like * [x] ravel * [ ] reciprocal * [ ] renorm * [ ] repeat * [ ] repeat_interleave * [x] reshape * [x] reshape_as * [ ] resize_ * [ ] resize_as_ * [ ] resolve_conj * [ ] resolve_neg * [ ] roll * [ ] rot90 * [ ] rsqrt * [ ] rsub * [ ] scatter * [ ] scatter_add * [x] select * [ ] sgn * [ ] short * [ ] sigmoid * [x] sin * [ ] sinc * [x] sinh * [ ] softmax * [ ] solve * [x] split * [x] split_with_sizes * [ ] sqrt * [ ] square * [x] squeeze * [ ] stack * [ ] std * [ ] std_mean * [ ] sum_to_size * [ ] svd * [ ] symeig * [ ] t * [ ] take * [ ] take_along_dim * [x] tan * [x] tanh * [ ] tensor_split * [ ] tensordot * [ ] tile * [ ] to_sparse * [x] trace * [x] transpose * [ ] trapezoid * [ ] trapz * [ ] triangular_solve * [x] tril * [x] triu * [ ] true_divide * [x] unfold * [x] unsqueeze * [ ] var * [ ] var_mean * [ ] vdot * [ ] view * [ ] view_as * [ ] view_as_real * [x] vsplit * [ ] vstack * [ ] where * [ ] zero_ * [x] zeros_like Misc: * Fix tests in `test_torch.py`     * [ ] test_copy_all_dtypes_and_devices > Fails due to copy(x) (unsupported storage type )     * [ ] test_copy_math_view > Uses make_tensor     * [ ] test_copy_transpose_math_view > Uses make_tensor     * [ ] (maybe?)test_copy_noncontig > Tests only 1 dtype     * [ ] (maybe?)test_copy_broadcast > Tests only 1 dtype cc:    Alternatives _No response_  Additional context _No response_ ",2022-03-22T13:35:08Z,feature triaged module: complex module: half,open,3,4,https://github.com/pytorch/pytorch/issues/74537,Probably prime candidate for being JITerated by default.,Is this still updated?,"Unfortunately, I don't think so.","So, can we close it?"
,RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .," 🐛 Describe the bug I am fine tuning masked language model from XLM Roberta large on google machine specs. I made couple of experiments and was strange to see few results. `I think Pytorch is not functioning properly. ` I am using pretrained Hugging face model.  I am using  https://huggingface.co/xlmrobertalarge I am not using fairseq or anything. ``` tokenizer = tr.XLMRobertaTokenizer.from_pretrained(""xlmrobertalarge"",local_files_only=True) model = tr.XLMRobertaForMaskedLM.from_pretrained(""xlmrobertalarge"", return_dict=True,local_files_only=True) model.gradient_checkpointing_enable() included as new line ``` Here is `NvidiaSMI` ``` b'Tue Mar 22 05:06:40 2022 \\n++\\n  ``` **Training Code** ``` training_args = tr.TrainingArguments(      output_dir='****'     ,logging_dir='****'         directory for storing logs     ,save_strategy=""epoch""     ,run_name=""****""     ,learning_rate=2e5     ,logging_steps=1000     ,overwrite_output_dir=True     ,num_train_epochs=10     ,per_device_train_batch_size=8     ,prediction_loss_only=True     ,gradient_accumulation_steps=4      ,gradient_checkpointing=True     ,bf16=True CC(Can users move generated computation  graph manually in the forward pass from GPU to CPU? And then back to GPU for backward)  ,optim=""adafactor"" ) trainer = tr.Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=train_data ) ``` Also `gradient_checkpointing` never works. Strange.  **Also, is it using all 8 GPU's?**  Versions Versions torch==1.11.0+cu113  torchvision==0.12.0+cu113   torchaudio==0.11.0+cu113  transformers==4.17.0",2022-03-22T05:26:15Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74522,"I think it's possible that you are not using the config/setup properly when training the model given that even batch_size=4 caused OOM, could you submit issue directly to the HuggingFace repo https://github.com/huggingface/transformers? We don't have too much context on model specific issue tbh. They might have a better idea what's wrong with your issue. There's possibility that there's fragmented memory that is not being released properly, although I doubted about it as it seems you might not using all GPUs. But if you think there's fragmented memory, one suggestion is to use `torch.cuda.empty_cache()` to release some memory, you can refer to this post on some previous discussions https://discuss.pytorch.org/t/abouttorchcudaemptycache/34232/20"
transformer,[PyTorch] _addm_activation native function for matmul/bias/activation fusion,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) Here's an extended version of addmm that takes advantage of cublasLt's fused addmm + relu/gelu support. Differential Revision: D35019612",2022-03-21T19:45:29Z,cla signed,closed,0,13,https://github.com/pytorch/pytorch/issues/74490,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74490**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f2448c136c (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,,"> Do we want to expose them in native_functions.yaml if they are private and intended to be called only from some other publicly exposed functions? Don't we need to do that for testing? Also, do we want to keep this private? Fused matmulbias{r,g}elu  seems like something people might want (but what I do I know about AI research).","Yeah, we need it to be able to test from python, and yes, fused matmulbiasgelu/relu is a useful op, but requirements are higher for publicly available op, such as you are expected to provide a gradient computation for it, and documentation. We still can have them in native_functions as private ops starting with `_`, we have a few of those already. ","hey  this is looking green, can I have a review?",". ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it?","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I'll see what I can do.","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively).","> > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? >  > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning.","> > > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? > >  > >  > > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). >  > Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning. OK I've verified that `test_addmm_repro_possible_cublas_gelu_bug_cuda_float32` (and bfloat16, and float64) will fail as advertised if and only if the following patch is applied: ```  a/fbcode/caffe2/aten/src/ATen/native/cuda/Blas.cpp +++ b/fbcode/caffe2/aten/src/ATen/native/cuda/Blas.cpp @@ 257,7 +257,7 @@                self.data_ptr(),                result_>data_ptr(),                result_ld, if 0 +if !0                activation_to_gemm_and_blas_arg(activation)  else                // GELU is not supported (and does not compile!) prior @@ 315,7 +315,7 @@  // gating activation_to_gemm_and_blas_arg above; here we are manually  // performing a postGELU because we weren't able to use the GELU  // epilogue above. if !0 +if !!0    if (useLtInterface && activation == Activation::GELU) {      result_ = c10::MaybeOwned::owned(at::gelu(*result_));    } ``` That said, I agree it is suspicious that it doesn't fail when you try via cublas and it does fail when I try via my new codepath, so I am going to look through this diff again to make sure I haven't done something silly like doublegelu.","> I am going to look through this diff again to make sure I haven't done something silly like doublegelu. Nope, didn't find anything silly. We know relu is working, and we're functioning correctly for gelu and set up to do the fusion when we figure out correctness, so I'm going to go ahead and land this.","https://gist.github.com/swolchok/f550a9dfbab879cc514912d5f5ab59af has the version of test_linalg.py with the repro test for gelu epilogue not working properly, in case old versions of this PR aren't accessible after I update it.",Thanks for the repro; I've isolated it to cublaslt and have passed the repro on.
,RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .," 🐛 Describe the bug I am fine tuning masked language model from XLM Roberta large on google machine specs. I made couple of experiments and was strange to see few results. `I think Pytorch is not functioning properly. ` I am using pretrained Hugging face model.  I am using  https://huggingface.co/xlmrobertalarge I am not using fairseq or anything. ``` tokenizer = tr.XLMRobertaTokenizer.from_pretrained(""xlmrobertalarge"",local_files_only=True) model = tr.XLMRobertaForMaskedLM.from_pretrained(""xlmrobertalarge"", return_dict=True,local_files_only=True) model.gradient_checkpointing_enable() included as new line ``` Here is `NvidiaSMI` ``` b'Tue Mar 22 05:06:40 2022 \\n++\\n  ``` **Training Code** ``` training_args = tr.TrainingArguments(      output_dir='****'     ,logging_dir='****'         directory for storing logs     ,save_strategy=""epoch""     ,run_name=""****""     ,learning_rate=2e5     ,logging_steps=1000     ,overwrite_output_dir=True     ,num_train_epochs=10     ,per_device_train_batch_size=8     ,prediction_loss_only=True     ,gradient_accumulation_steps=4      ,gradient_checkpointing=True     ,bf16=True CC(Can users move generated computation  graph manually in the forward pass from GPU to CPU? And then back to GPU for backward)  ,optim=""adafactor"" ) trainer = tr.Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=train_data ) ``` Also `gradient_checkpointing` never works. Strange.  **Also, is it using all 8 GPU's?**  Versions Versions torch==1.11.0+cu113  torchvision==0.12.0+cu113   torchaudio==0.11.0+cu113  transformers==4.17.0",2022-03-22T05:26:15Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74522,"I think it's possible that you are not using the config/setup properly when training the model given that even batch_size=4 caused OOM, could you submit issue directly to the HuggingFace repo https://github.com/huggingface/transformers? We don't have too much context on model specific issue tbh. They might have a better idea what's wrong with your issue. There's possibility that there's fragmented memory that is not being released properly, although I doubted about it as it seems you might not using all GPUs. But if you think there's fragmented memory, one suggestion is to use `torch.cuda.empty_cache()` to release some memory, you can refer to this post on some previous discussions https://discuss.pytorch.org/t/abouttorchcudaemptycache/34232/20"
transformer,[PyTorch] _addm_activation native function for matmul/bias/activation fusion,"  CC([PyTorch] Add test for allmasked case for native softmax)  CC([PyTorch] Run test_transformerencoderlayer_gelu on CUDA)  CC([PyTorch] Run test_transformerencoderlayer on CUDA)  CC([PyTorch] Add NestedTensor support functions for transformers)  CC([PyTorch] NestedTensor kernels for {r,g}elu{,_})  CC([PyTorch] Add & use inplace gelu)  CC([PyTorch] _addm_activation native function for matmul/bias/activation fusion) Here's an extended version of addmm that takes advantage of cublasLt's fused addmm + relu/gelu support. Differential Revision: D35019612",2022-03-21T19:45:29Z,cla signed,closed,0,13,https://github.com/pytorch/pytorch/issues/74490,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74490**  * :page_facing_up: &nbsp;**Preview Python docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit f2448c136c (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,,"> Do we want to expose them in native_functions.yaml if they are private and intended to be called only from some other publicly exposed functions? Don't we need to do that for testing? Also, do we want to keep this private? Fused matmulbias{r,g}elu  seems like something people might want (but what I do I know about AI research).","Yeah, we need it to be able to test from python, and yes, fused matmulbiasgelu/relu is a useful op, but requirements are higher for publicly available op, such as you are expected to provide a gradient computation for it, and documentation. We still can have them in native_functions as private ops starting with `_`, we have a few of those already. ","hey  this is looking green, can I have a review?",". ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it?","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I'll see what I can do.","> . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively).","> > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? >  > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning.","> > > . ,  says he couldn't reproduce faillure with gelu activation with the sizes that you provided, do you have a test that would reproduce it? > >  > >  > > I have included a repro in test_linalg.py (which I am happy to remove before checkin since it's huge). It should fail on CUDA 11.4 if the `if 0` and `if !0` in Blas.cpp are flipped (to `if !0` and `if 0` respectively). >  > Needs a further update before you will be able to run it, but I have to run. I will fix it up tomorrow morning. OK I've verified that `test_addmm_repro_possible_cublas_gelu_bug_cuda_float32` (and bfloat16, and float64) will fail as advertised if and only if the following patch is applied: ```  a/fbcode/caffe2/aten/src/ATen/native/cuda/Blas.cpp +++ b/fbcode/caffe2/aten/src/ATen/native/cuda/Blas.cpp @@ 257,7 +257,7 @@                self.data_ptr(),                result_>data_ptr(),                result_ld, if 0 +if !0                activation_to_gemm_and_blas_arg(activation)  else                // GELU is not supported (and does not compile!) prior @@ 315,7 +315,7 @@  // gating activation_to_gemm_and_blas_arg above; here we are manually  // performing a postGELU because we weren't able to use the GELU  // epilogue above. if !0 +if !!0    if (useLtInterface && activation == Activation::GELU) {      result_ = c10::MaybeOwned::owned(at::gelu(*result_));    } ``` That said, I agree it is suspicious that it doesn't fail when you try via cublas and it does fail when I try via my new codepath, so I am going to look through this diff again to make sure I haven't done something silly like doublegelu.","> I am going to look through this diff again to make sure I haven't done something silly like doublegelu. Nope, didn't find anything silly. We know relu is working, and we're functioning correctly for gelu and set up to do the fusion when we figure out correctness, so I'm going to go ahead and land this.","https://gist.github.com/swolchok/f550a9dfbab879cc514912d5f5ab59af has the version of test_linalg.py with the repro test for gelu epilogue not working properly, in case old versions of this PR aren't accessible after I update it.",Thanks for the repro; I've isolated it to cublaslt and have passed the repro on.
transformer,mask parameter in Transformer support 3D tensor but not mentioned," 📚 Documentation (Add a clear and concise description of what the documentation content issue is. A link to any relevant https://pytorch.org page is helpful if you have one.) First, I read doc in this https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html page, which redirect me to the doc on this https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html page. The notes about ""src_mask"" parameter under ""forwardshape"" section says it only support shape of (S, S). However when I track down the source code I found ""src_mask"" is actually the ""attn_mask"" on this https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html page which support 3D tensor. Then I tested feeding 3D mask to TransformerEncoderLayer and it works. Please check. ",2022-03-20T01:27:11Z,module: docs actionable,closed,0,1,https://github.com/pytorch/pytorch/issues/74612,"Thanks for the report, ! We'd accept a PR expanding the supported shapes listed in the `nn.Transformer` documentation: * `src_mask`  supports shape `(S, S)` or `(N * num_heads, S, S)` * `tgt_mask`  supports shape `(T, T)` or `(N * num_heads, T, T)` "
yi,Copying tensors sometimes doesn't copy last N elements when using cuda graphs," 🐛 Describe the bug Copying tensors sometimes doesn't work when using cuda graphs. Occasionally the last N elements aren't copied. See repro: https://gist.github.com/tomconerlyanth/4970244782fd8e02dd99ea8ca8797eec  Versions tc0 (1m) ~/code$ python collect_env.py Collecting environment information... PyTorch version: 1.10.0a0+gitee11606 Is debug build: False CUDA used to build PyTorch: 11.4 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: version 3.22.3 Libc version: glibc2.27 Python version: 3.9.7 (default, Sep 16 2021, 13:09:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.4.15683.273.amzn2.x86_64x86_64withglibc2.27 Is CUDA available: True CUDA runtime version: 11.4.152 GPU models and configuration: GPU 0: NVIDIA A100SXM440GB GPU 1: NVIDIA A100SXM440GB GPU 2: NVIDIA A100SXM440GB GPU 3: NVIDIA A100SXM440GB GPU 4: NVIDIA A100SXM440GB GPU 5: NVIDIA A100SXM440GB GPU 6: NVIDIA A100SXM440GB GPU 7: NVIDIA A100SXM440GB Nvidia driver version: 470.57.02 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.2.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.2.4 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.921 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.2 [pip3] pytorchmemlab==0.2.4 [pip3] torch==1.10.0a0+gitee11606 [pip3] torchops==0.0.1 [pip3] torchtyping==0.1.4 [conda] blas                      1.0                         mkl [conda] mkl                       2021.4.0           h06a4308_640 [conda] mklinclude               2022.0.1           h06a4308_117 [conda] mklservice               2.4.0            py39h7f8727e_0 [conda] mkl_fft                   1.3.1            py39hd3c417c_0 [conda] mkl_random                1.2.2            py39h51133e4_0 [conda] mypy                      0.921                    pypi_0    pypi [conda] mypyextensions           0.4.3                    pypi_0    pypi [conda] numpy                     1.21.2           py39h20f2e39_0 [conda] numpybase                1.21.2           py39h79a1101_0 [conda] pytorchmemlab            0.2.4                    pypi_0    pypi [conda] torch                     1.10.0a0+gitee11606          pypi_0    pypi [conda] torchops                 0.0.1                     dev_0     [conda] torchtyping               0.1.4                    pypi_0    pypi ",2022-03-18T18:26:17Z,high priority module: dependency bug triaged module: correctness (silent) module: cuda graphs,closed,0,2,https://github.com/pytorch/pytorch/issues/74419,"Reproes with a pure c++ outside of pytorch, thanks  for the reproduction https://gist.github.com/tomconerlyanth/c6e3bc8caddaa1bb628ce98593a26484","Thanks for the report  and  for the support in isolating the issue. It should have been fixed by our CUDA team already so I'll close the issue. Please reopen, if you are still running into it. "
rag,"UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead."," 🐛 Describe the bug ```python class CrossEntropyLoss2d(nn.Module):     def __init__(self, weight=None, size_average=True, ignore_index=255):         super(CrossEntropyLoss2d, self).__init__()         self.nll_loss = nn.NLLLoss(weight, size_average, ignore_index)     def forward(self, inputs, targets):         return self.nll_loss(torch.log(inputs), targets)   ``` **UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.** When I don't pay attention to this warning, it will cause loss to be `nan`! I think this is where improvements are needed, including the official API DOC.  Versions ``` PyTorch version: 1.10.1+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.1 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04) 9.4.0 Clang version: 10.0.04ubuntu1  CMake version: version 3.21.3 Libc version: glibc2.31 Python version: 3.8.3 (default, Jul  2 2020, 16:21:59)  [GCC 7.3.0] (64bit runtime) Python platform: Linux5.8.063genericx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration:  GPU 0: NVIDIA GeForce RTX 2080 GPU 1: NVIDIA GeForce RTX 2080 Nvidia driver version: 495.44 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.18.5 [pip3] numpydoc==1.1.0 [pip3] torch==1.10.1 [conda] blas                      1.0                         mkl   [conda] mkl                       2020.1                      217   [conda] mklservice               2.3.0            py38he904b0f_0   [conda] mkl_fft                   1.1.0            py38h23d657b_0   [conda] mkl_random                1.1.1            py38h0573a6f_0   [conda] mypyextensions           0.4.3                    pypi_0    pypi [conda] numpy                     1.18.5           py38ha1c710e_0   [conda] numpybase                1.18.5           py38hde5b4d6_0   [conda] numpydoc                  1.1.0                      py_0   [conda] torch                     1.10.1                   pypi_0    pypi ``` ",2022-03-18T08:19:12Z,module: nn triaged,open,0,1,https://github.com/pytorch/pytorch/issues/74411,"Hi, Thanks for your feedback. Could you give some more details on what you're trying to do and when do you get this problem? Some combinations of arguments can give you nan indeed but that's not related to you using these args or not."
rag,[ci] Pull some stragglers into the consolidated workflows,"Stack from ghstack:  CC([ci] Pull some stragglers into the consolidated workflows)  CC(Workflow consolidation for GitHub actions)  CC([ci] inline display_ec2 script) Migrate some of the oneoff workflows into the main workflows, according to how they want to be dispatched",2022-03-16T23:27:23Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/74351,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74351**  * :x: Python docsfailed to build  * :x: C++ docsfailed to build  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 5f5c07691a (more details on the Dr. CI page):  * **9/9** failures introduced in this PR   :detective: 7 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5873804100?check_suite_focus=true) pull / linuxdocs / builddocs (python) (1/7) **Step:** ""Build python docs"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
rag,[ci] Pull some stragglers into the consolidated workflows,"Stack from ghstack:  CC([ci] Pull some stragglers into the consolidated workflows)  CC(Workflow consolidation for GitHub actions)  CC([ci] inline display_ec2 script) Migrate some of the oneoff workflows into the main workflows, according to how they want to be dispatched",2022-03-16T23:27:23Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/74351,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74351**  * :x: Python docsfailed to build  * :x: C++ docsfailed to build  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :question:Need help or want to give feedback on the CI? Visit our **office hours**  :pill: CI failures summary and remediations As of commit 5f5c07691a (more details on the Dr. CI page):  * **9/9** failures introduced in this PR   :detective: 7 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5873804100?check_suite_focus=true) pull / linuxdocs / builddocs (python) (1/7) **Step:** ""Build python docs"" (full log    This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
,Training MLM model XLM Roberta large on google machine specs not fast," 🐛 Describe the bug I am fine tuning masked language model from `XLM Roberta large` on google machine specs.  I made couple of experiments and was strange to see few results.  ``` ""a2highgpu4g"" ,accelerator_count=4, accelerator_type=""NVIDIA_TESLA_A100"" on 4,12,672 data batch size 4 Running ( 4 data*4 GPU=16 data points) ""a2highgpu4g"" ,accelerator_count=4 , accelerator_type=""NVIDIA_TESLA_A100""on 4,12,672 data batch size 8 failed  ""a2highgpu4g"" ,accelerator_count=4, accelerator_type=""NVIDIA_TESLA_A100"" on 4,12,672 data batch size 16 failed ""a2highgpu4g"" ,accelerator_count=4.,accelerator_type=""NVIDIA_TESLA_A100"" on 4,12,672 data batch size 32 failed ``` I was not able to train model with `batch size ` more than 4 on  of GPU's. It stopped in midway. Here is the code I am using. ``` training_args = tr.TrainingArguments(      disable_tqdm=True,     output_dir='/home/pc/Bert_multilingual_exp_TCM/results_mlm_exp2',      overwrite_output_dir=True,     num_train_epochs=2,     per_device_train_batch_size=4,      per_device_train_batch_size      per_gpu_train_batch_size     prediction_loss_only=True     ,save_strategy=""no""     ,run_name=""MLM_Exp1""     ,learning_rate=2e5     ,logging_dir='/home/pc/Bert_multilingual_exp_TCM/logs_mlm_exp1'         directory for storing logs     ,logging_steps=40000     ,logging_strategy='no' ) trainer = tr.Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=train_data ) ``` **My Questions** How can I train with larger batch size on `a2highgpu4g` machine? Which parameters can I include in `TrainingArguments` so that training is fast and occupies small memory? Thanks in advance.  Versions ``` torch==1.11.0+cu113  torchvision==0.12.0+cu113   torchaudio==0.11.0+cu113  transformers==4.17.0 ``` ",2022-03-16T15:18:26Z,oncall: distributed triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/74302," can you share more details about what implementation you are using? is it the fairseq implementation or huggingface implementation or sth else? did you download the pretained model from torchhub? For different implementation of the models like fairseq or huggingface, we recommend contacting or submitting issues to their repo directly to get a faster and more accurate response :)",Any help on this? More info here  CC(RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .),">  can you share more details about what implementation you are using? is it the fairseq implementation or huggingface implementation or sth else? did you download the pretained model from torchhub? >  > For different implementation of the models like fairseq or huggingface, we recommend contacting or submitting issues to their repo directly to get a faster and more accurate response :) More detailed description here  CC(RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB with 8 Ampere GPU's .)"
transformer,Loading of packaged model object fails with torch==1.11.0," 🐛 Describe the bug The new release of PyTorch 1.11.0 seems to break the loading of packaged objects. ```python import pathlib from torch import package import transformers model = transformers.AutoModelForSequenceClassification.from_pretrained(     ""sentencetransformers/paraphrasemultilingualmpnetbasev2"",     num_labels=5, ) _RESOURCE_NAME = ""models"" _PACKAGE_NAME = ""frozen_model"" _FILE_NAME = ""frozen_model.pt"" def freeze_model(model, save_path):     save_path.mkdir(parents=True, exist_ok=True)     file_path = (save_path / _FILE_NAME)     with package.PackageExporter(file_path) as exp:         exp.extern(""transformers.**"")         exp.extern(""torch.**"")         exp.save_pickle(_PACKAGE_NAME, _RESOURCE_NAME, model) def load_model(path):     importer = package.PackageImporter(path)     runner = importer.load_pickle(_PACKAGE_NAME, _RESOURCE_NAME)     return runner freeze_model(model, pathlib.Path(""./out"")) load_model(pathlib.Path(""./out/frozen_model.pt"")) ``` The code example above works with `torch==1.10.0` but fails with `torch==1.11.0`.  Versions ``` Collecting environment information... PyTorch version: 1.11.0+cu102 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 9.4.01ubuntu1~20.04) 9.4.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.31 Python version: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.13.035genericx86_64withglibc2.17 Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.910 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.4 [pip3] torch==1.11.0 [conda] mypy                      0.910              pyhd3eb1b0_0   [conda] mypy_extensions           0.4.3                    py38_0   [conda] numpy                     1.21.4                   pypi_0    pypi [conda] torch                     1.11.0                   pypi_0    pypi ```",2022-03-16T10:37:57Z,oncall: package/deploy imported,closed,0,1,https://github.com/pytorch/pytorch/issues/74290,Seems like this PR solves the issue. https://github.com/pytorch/pytorch/pull/80917
yi,remove _lazy_init() in rebuild full params,"  CC(remove _lazy_init() in rebuild full params)  CC(make sharding strategy configurable and support zero2 algorithm) remove _lazy_init() in rebuild full params, which was introduced in https://github.com/pytorch/pytorch/pull/73300. _lazy_init() synces streams whenever before rebuilding full params. This sync is not needed when BackwardPrefetch.BACKWARD_PRE is enabled. Differential Revision: D34907823",2022-03-15T22:00:38Z,oncall: distributed cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/74263,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74263**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit b7371bec9b (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"> Stamp to unblock, but don't quite understand the PR comment. >  > It says that this sync is not needed when BackwardPrefetch.PRE is enabled, but by default this is not enabled. So would we need the sync in that case? _lazy_init() will sync streams, since we've synced streams outside _rebuild_full_params() as needed, no need to call _lazy_init() to sync streams inside _rebuild_full_params()","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Create secure credential storage for metrics credentials and associated documentation on how to regenerate them if needed,/pytorchdevinfra,2022-03-15T20:21:20Z,module: ci triaged,open,0,1,https://github.com/pytorch/pytorch/issues/74256,I've added a section to our internal wiki
transformer,Hugging face code with Pytorch throwing error when running using vertex-ai," 🐛 Describe the bug I am training a NLP Hugging face model in vertexai with custom image.  The same code works in local machine. Here is my code and the error. ```python   import torch   import numpy as np   import pandas as pd   from transformers import BertTokenizer, BertForSequenceClassification   import transformers as tr   from sentence_transformers import SentenceTransformer   from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM   from transformers import AdamW   from transformers import AutoTokenizer   from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,BertForMaskedLM   from transformers import DataCollatorForLanguageModeling   from scipy.special import softmax   import scipy   import random   import pickle   import os   print(""package imported completed"")   os.environ['TRANSFORMERS_OFFLINE']='1'   os.environ['HF_MLFLOW_LOG_ARTIFACTS']='TRUE'   print(""env setup completed"")   print( tr.__version__)   device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')   print(""Using"", device)   torch.backends.cudnn.deterministic = True     tr.trainer_utils.set_seed(0)   print(""here"")   tokenizer = tr.XLMRobertaTokenizer.from_pretrained(""xlmrobertalarge"",local_files_only=True)   model = tr.XLMRobertaForMaskedLM.from_pretrained(""xlmrobertalarge"", return_dict=True,local_files_only=True)   model.to(device)   print(""Model loaded successfully"")   df=pd.read_csv(""gs://****bucket***/data.csv"")    print(""read csv"")    ,engine='openpyxl',sheet_name=""master_data""   train_df=df.text.tolist()   print(len(train_df))   train_df=list(set(train_df))   train_df = [x for x in train_df if str(x) != 'nan']   train_df=train_df[:50]   print(""Length of training data is \\n "",len(train_df))   print(""DATA LOADED successfully"")   train_encodings = tokenizer(train_df, truncation=True, padding=True, max_length=512, return_tensors=""pt"")   print(""encoding done"")   data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)   print(""data collector done"")   class SEDataset(torch.utils.data.Dataset):       def __init__(self, encodings):           self.encodings = encodings       def __getitem__(self, idx):           item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}           return item       def __len__(self):           return len(self.encodings[""attention_mask""])   train_data = SEDataset(train_encodings)   print(""train data created"")   training_args = tr.TrainingArguments(       output_dir='gs://****bucket***/results_mlm_exp1',        overwrite_output_dir=True,       num_train_epochs=2,       per_device_train_batch_size=4,        per_device_train_batch_size        per_gpu_train_batch_size       prediction_loss_only=True        ,save_strategy=""epoch""        ,run_name=""MLM_Exp1""       ,learning_rate=2e5        logging_dir='gs://****bucket***/logs_mlm_exp1',             directory for storing logs        logging_steps=32000,   )   trainer = tr.Trainer(       model=model,       args=training_args,       data_collator=data_collator,       train_dataset=train_data,   )   print(""training to start"")   trainer.train()   print(""model training finished"")   trainer.save_model(""gs://****bucket***/model_mlm_exp1"")   print(""training finished"") ``` The error that I get is:     None	INFO	train data created     None	INFO	training to start     None	ERROR	0% 0/8 [00:09<?, ?it/s] Most of them are warning but still my code stops with error.  Versions Using FROM usdocker.pkg.dev/vertexai/training/tfgpu.28:latest as base image ``` Transformer: 4.10.3 torch:  1.7.1+cu101 Using cuda ``` ",2022-03-15T10:31:00Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/74238,"I don't know if this is a PyTorch problem, this issue might be better off in the huggingface transformers repo."
transformer,[WIP] fix: no duplicate final LayerNorm in Transformer, Description This pull request solves CC(Successive Layer Normalization in nn.Transformer) and CC(Two consecutive nn.LayerNorm are used in transformer model when norm_first is False) by implementing the third solution (the minimal and backwardcompatible one) among the ones listed hereissuecomment1066609080).,2022-03-15T09:42:00Z,open source cla signed Stale,closed,0,3,https://github.com/pytorch/pytorch/issues/74237,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74237**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 282617b95e (more details on the Dr. CI page):  * **8/8** failures introduced in this PR   :detective: 8 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5557945955?check_suite_focus=true) linuxxenialcuda11.3py3.7gcc7 / test (default, 2, 2, linux.4xlarge.nvidia.gpu) (1/8) **Step:** ""Test"" (full log  :repeat: rerun)   20220315T18:36:13.5026477Z [  FAILED  ] TransformerTest.Transformer  ``` 20220315T18:36:13.4902214Z [ RUN      ] OperationTest.Cross 20220315T18:36:13.4902942Z [       OK ] OperationTest.Cross (4 ms) 20220315T18:36:13.4903725Z [ RUN      ] OperationTest.Linear_out 20220315T18:36:13.4904519Z [       OK ] OperationTest.Linear_out (0 ms) 20220315T18:36:13.4905334Z [] 3 tests from OperationTest (5 ms total) 20220315T18:36:13.4905807Z  20220315T18:36:13.4906360Z [] Global test environment teardown 20220315T18:36:13.5024398Z [==========] 971 tests from 45 test suites ran. (46032 ms total) 20220315T18:36:13.5025048Z [  PASSED  ] 970 tests. 20220315T18:36:13.5025625Z [  FAILED  ] 1 test, listed below: 20220315T18:36:13.5026477Z [  FAILED  ] TransformerTest.Transformer 20220315T18:36:13.5027106Z  20220315T18:36:13.5027485Z  1 FAILED TEST 20220315T18:36:13.5609775Z + cleanup 20220315T18:36:13.5610132Z + retcode=1 20220315T18:36:13.5610433Z + set +x 20220315T18:36:13.5642115Z [error]Process completed with exit code 1. 20220315T18:36:13.5867487Z [group]Run  ir => recursive include all files in pattern 20220315T18:36:13.5868173Z [36;1m ir => recursive include all files in pattern[0m 20220315T18:36:13.5868695Z [36;1m7z a ""testjsons$Env:FILE_SUFFIX.zip"" ir'!test\\*.json'[0m 20220315T18:36:13.5887586Z shell: C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.EXE command "". '{0}'"" ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","Sure, , thanks for the feedback! You are right. I would be glad to propose another solution if it can somehow help.","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,fix: no duplicate final LayerNorm in Transformer, Description This pull request solves CC(Successive Layer Normalization in nn.Transformer) and CC(Two consecutive nn.LayerNorm are used in transformer model when norm_first is False) by implementing the third solution (the minimal and backwardcompatible one) among the ones listed hereissuecomment1066609080).,2022-03-15T08:48:17Z,open source cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/74233,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74233**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 788bd42713 (more details on the Dr. CI page):  * **30/30** failures introduced in this PR   :detective: 30 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5550986772?check_suite_focus=true) winvs2019cpupy3 / test (default, 2, 2, windows.4xlarge) (1/30) **Step:** ""Upload test statistics"" (full log  :repeat: rerun)   20220315T09:28:17.6386265Z RuntimeError: test_cpp_api_parity failed!  ``` 20220315T09:28:17.1942086Z  20220315T09:28:17.1942268Z FAILED (errors=1, skipped=550, expected failures=20) 20220315T09:28:17.1942500Z  20220315T09:28:17.1942634Z Generating XML reports... 20220315T09:28:17.3366764Z Generated XML report: testreports/pythonunittest/test_cpp_api_parity/TESTTestCppApiParity20220315092810.xml 20220315T09:28:17.6379569Z Traceback (most recent call last): 20220315T09:28:17.6380031Z   File ""test/run_test.py"", line 1047, in  20220315T09:28:17.6382676Z     main() 20220315T09:28:17.6383014Z   File ""test/run_test.py"", line 1025, in main 20220315T09:28:17.6385801Z     raise RuntimeError(err_message) 20220315T09:28:17.6386265Z RuntimeError: test_cpp_api_parity failed! 20220315T09:28:17.8473018Z  20220315T09:28:17.8473575Z real	22m9.082s 20220315T09:28:17.8473956Z user	64m4.126s 20220315T09:28:17.8474213Z sys	8m26.887s 20220315T09:28:17.8474404Z + cleanup 20220315T09:28:17.8474564Z + retcode=1 20220315T09:28:17.8474710Z + set +x 20220315T09:28:17.8520140Z [error]Process completed with exit code 1. 20220315T09:28:17.8570141Z [group]Run  Ensure the working directory gets chowned back to the current user 20220315T09:28:17.8570469Z [36;1m Ensure the working directory gets chowned back to the current user[0m ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",Closed as the pull request branch was created from an outdated fork. CC([WIP] fix: no duplicate final LayerNorm in Transformer) replaces this pull request.
transformer,Preserve codegen on fx graph in transformer,Summary: Use the codegen on the original graph module for the new graph module produced by transformer. Test Plan: Added a unit test: test_custom_codegen_with_transformer Differential Revision: D34867938,2022-03-14T20:23:15Z,fb-exported cla signed module: fx,closed,0,3,https://github.com/pytorch/pytorch/issues/74189,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74189**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit f05360766d (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34867938,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,Tracing model parameter shapes without instantiating the model parameters," 🚀 The feature, motivation and pitch  Request We would like a way to get a model's parameter shapes without instantiating them. The following is an example call signature. ```python  MyModel is an `nn.Module`  instantiate model the usual way model = MyModel(*args, **kwargs)  we propose a function on `nn.Module` called `trace_shapes`  trace shapes without instantiating shapes = MyModel.trace_shapes(*args, **kwargs)  `shapes` is a dict with key = param names, value = param shapes for name, param in model.named_parameters():   assert(param.shape == shapes[name]) ```  Motivation Hi, I'm a maintainer for the mup package. This repo allows one to implement in their models a special parametrization called maximal update parametrization, or muP, that has the special property that narrow and wide networks share the same optimal hyperparameters (like learning rate, initialization, etc). This is demonstrated below on a Transformer trained with adam, where on the left we have the pytorch default parametrization and the right we have muP. !image This property in particular can be used to tune hyperparameters for extremely large neural networks like GPT3 that is too expensive to train more than once, by just tuning a tiny version of it.  Current Implementation To implement muP in a model, we need to annotate the parameter shapes with two pieces of information. For each dimension in a shape, these are 1) is it a width dimension (which will be scaled up or down)? 2) what is the ""base value"" of this dimension?  our implementation is designed to replicate the pytorch default when the dimension is equal to its base value, so as to provide backward compatibility. To obtain 1) and 2), we ask the user to instantiate a ""base model"" and a ""delta model"". The delta model is supposed to vary all ""width"" dimensions compared to the base model. Then, by calling ```python set_base_shapes(model, base_model, delta=delta_model) ``` the parameters in `model` are annotated by 1) and 2) which are determined by 1) a dimension is a ""width"" dimension if it varies between `base_model` and `delta_model`, and 2) its base value is given by the value in `base_model`.  Issues with Current Implementation The current implementation is not too bad if `base_model` and `delta_model` are relatively small, but when they are big, their instantiations can cost nontrivial time and memory. Worse yet, they might need multiple GPUs to instantiate while `model` can be a small model that fit on a single GPU. Therefore, it'd be much more preferable to just obtain the shapes of `base_model` and `delta_model` without instantiating.  Alternatives See `Current Implementation` section above.  Additional context _No response_ ",2022-03-13T04:49:16Z,feature module: nn triaged,open,9,12,https://github.com/pytorch/pytorch/issues/74143,"Hey , meta tensors may help solve your problem. They are a special type of tensor that maintain shape information but do not allocate storage for the data. Example: ```python t = torch.randn((5, 3), device='meta')   allocates no memory print(t.shape)   prints torch.Size([5, 3]) ``` Note that all `torch.nn` modules have a `device` constructor kwarg that defines their parameters as meta tensors: ```python m = nn.Linear(3, 3, device='meta') print(m.weight.device)   device='meta' in = torch.randn((5, 3), device='meta') out = m(in)   out shape: torch.Size([5, 3]); out device: 'meta' ``` See here for more info. The obvious caveat is that any userdefined modules should also define a way to place their parameters on `device='meta'` to work like this.","Hi , we tested `device='meta'` with our package and it seems to work fine as is, which is great news. We have added a blurb in our README about this. However, looking around at some popular github repos, I'm not sure how prevalent is it for models to pass down `device` flags to its submodules. For example, just looking at Huggingface's BERT, this doesn't seem to be the case. So, before this `device` convention is universally adopted by the community, I'm still wondering whether it makes sense to have an automatic way of inferring parameter shapes without needing to change the code of a module.",? Also , the new Deferred Module Initialization feature that we are about to release in an outoftree package will exactly address your particular problem. I would be happy to give you access to our private repo if you want to try it out before we publicly release it.," Hi! I'm the other maintainer of the mup package. Thanks for your response, and we are happy to try this new feature! Please let us know how we can access the private repo.","No problem! I am on PTO today and have limited internet access. Expect to receive an invitation email to the “torchdistx” repo tomorrow morning. If you are on PyTorch’s Slack channel, happy to chat there as well.","  I just sent you an invite to the torchdistx repo. Please reach out to me either over PyTorch's Slack channel or over balioglu at fb.com to chat about the feature. Since the docs are not ""rendered"" yet, you might need some help to locate them.", Thanks! I just asked to be added to torch slack since I don't use FB. Will reach out to you over there once I get in.,  Happy to share that torchdistx is now public: https://github.com/pytorch/torchdistx Check out the docs for Deferred Module Initialization and Fake Tensor which I believe will address your problem. Please let me know if you have any feedback!,Awesome!   Trying this out right now., deferred_init works like a charm for the most part! Though we just encountered an issue with a model that has an unsqueeze call in its forward pass. `*** NotImplementedError: `aten::unsqueeze` cannot be run with fake tensor(s) because the meta backend has no kernel for it. Please file an issue if you want it to be supported.` I assume this is expected since the kernel isn't implemented. Should we file a separate issue if we want to have it supported?,"Hey , great! Happy to hear that it ""almost"" worked. Yes, unfortunately `unsqueeze` does not support the meta backend yet. I have also came across it in a few HG models. The good news though is that it should be fairly straightforward to add meta support for it. Please create a separate issue and ."
yi,for_blob Tensor building API runs into an issue when specifying device," 🐛 Describe the bug https://github.com/pytorch/pytorch/pull/74113/files uses `for_blob` API to build up a tensor, but changing it to the following: ``` at::Tensor param_size_tensor =         at::for_blob(param_size_data_ptr>data(), {data_size})             .context(                 param_size_data_ptr,                 [](void* ctx) {                   delete static_cast*>(ctx);                 })             .options(at::TensorOptions().dtype(at::kLong).device(params[0].device()))             .make_tensor(); ``` results in the following error: ``` RuntimeError: index_ == 1  index_ >= 0 INTERNAL ASSERT FAILED at ""caffe2/c10/core/Device.h"":128, please report a bug to PyTorch. Device index must be 1 or nonnegative, got 2 Exception raised from validate at caffe2/c10/core/Device.h:128 (most recent call first):  0  c10::get_backtraceabi:cxx11  1  std::_Function_handler, std::allocator > (), c10::(anonymous namespace)::GetFetchStackTrace()::$_0>::_M_invoke(std::_Any_data const&)  2  c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >)  3  c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string, std::allocator > const&)  4  c10::detail::torchInternalAssertFail(char const*, char const*, unsigned int, char const*, std::__cxx11::basic_string, std::allocator > const&)  5  c10::Device::validate()  6  c10::Device::Device(c10::DeviceType, signed char)  7  at::cuda::getDeviceFromPtr(void*)  8  at::cuda::detail::CUDAHooks::getDeviceFromPtr(void*) const  9  at::Context::getDeviceFromPtr(void*, c10::DeviceType)  10 at::TensorMaker::make_tensor()  11 c10d::verify_params_across_processes(c10::intrusive_ptr > const&, std::vector > const&, c10::optional > const&) ``` trying ` at::kCUDA` has the same result and putting `.device(...)` before the dtype has the same result.  To reproduce, simply patch above PR and make above mentioned change.  Versions main ",2022-03-11T18:06:58Z,module: cpp triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/74114, ,varma in your code snippet I see that you are deallocating the tensor data using ``` delete static_cast*>(ctx); ``` which means that it resides on the host memory. Do you know why the `device` is reported as `CUDA`? Another question; do you remember which version of CUDA you used for this code?,Check out CC(Fix how we handle host memory in CUDA getDeviceFromPtr).
transformer,"lazy_tensor Input tensor is not a lazy tensor: Background_Matting, nvidia_deeprecommender, speech_transformer",,2022-03-11T13:47:14Z,triaged module: lazy,closed,0,1,https://github.com/pytorch/pytorch/issues/74100,deprioritize
rag,Lazy tensors do not have storage: tts_angular,,2022-03-11T13:26:46Z,triaged module: lazy,closed,0,2,https://github.com/pytorch/pytorch/issues/74099,"The error is triggered by a call to `tensor.storage()` at https://github.com/pytorch/pytorch/blob/bb49d0d54c3875e59769a070017706966f6501a3/aten/src/ATen/native/cudnn/RNN.cppL1510. In the current design of LTC, there is no guarantee that a lazy tensor has a backed storage, thus the assertion in `storage()` is intentional. Supporting `storage()` requires a substantial change to LTC, and the value of it is arguable given only very few kernels are calling `storage()` directly. Without supporting `storage()`, one way to solve the issue here is to force the relevant kernel, `lstm.input`, to go through the fallback path, meaning all the inputs are materialized before entering the kernel. I have tried to do that following the example of `normal_` in LTC, however, what makes things complicated is that `lstm.input` is a CompositeImplicitAutograd kernel. We need to register `lstm.input` as AutogradLazy, and handle it similarly to `max_pool3d` in LTC.",deprioritize
transformer,Two consecutive nn.LayerNorm are used in transformer model when norm_first is False," 🐛 Describe the bug See the code from the master https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL62L67 https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL340L345 When `norm_first` is False, the last layer of the encoder layer is `LayerNorm()`. The problem is that it always puts a LayerNorm in the encoder, see https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL204L205 The consequence is the output of the last encoder layer is fed into another layernorm, so two consectuive layer norm layers are used here. Also, the output of the ith encoder layer is used as the input for the next LayerNorm layer in (i+1)th encoder layer.  **Expected behavior**: When norm_first is False, the Encoder should not contain a layer norm layer.  Versions The master ",2022-03-11T06:42:21Z,module: nn triaged,open,0,4,https://github.com/pytorch/pytorch/issues/74092,"I'd like to propose a solution for this issue. There are different possibilities, though  I can think of three of them that are quite different:  **backwardincompatible solutions:**    **if a final normalization is always  even when ```norm_first=True```  supposed to be applied to the output of the last encoder/decoder layer:** ⇒ an additional parameter ```norm_first``` should be required by ```TransformerEncoder```/```TransformerDecoder``` as well to apply the final normalization only when ```norm_first=True```    **else:** ⇒ since the final normalization carried out inside ```TransformerEncoder```/```TransformerDecoder``` looks redundant because alredy carried out internally to ```TransformerEncoderLayer```/```TransformerDecoderLayer```, it could simply be removed and ```TransformerEncoder```/```TransformerDecoder``` would not need the initialization parameter ```norm```  **backwardcompatible solutions:**    ```None``` should be passed as argument for the parameter ```norm``` of ```TransformerEncoder```/```TransformerDecoder``` when instantiating them inside the ```Transformer``` class, in place of the additional ```LayerNorm``` instance The second (backwardincompatible) solution looks more coherent, as users would not expect a final normalization to be carried out at the end of the encoder/decoder anyway even when specifying ```norm_first=True```, but it would remove the possibility of having a final normalization different from the ones used inside ```TransformerEncoderLayer```/```TransformerDecoderLayer```. The third solution (the backwardcompatible one) is the simplest one and solves the issue with minimal changes. Any feedback?",Is this a duplicate of CC(Successive Layer Normalization in nn.Transformer)?,"> Any feedback? One thing I'm unsure about with the proposed backwardcompatible solution is how this would work with previously serialized models. By default, LayerNorm has learnable parameters (i.e. `elementwise_affine=True`) that end up in the serialized form of the model. Running `new_model.load_state_dict(old_state_dict)` will fail because of the extra `state_dict` key. I think to be truly backwards compatible, the fix needs to be optin rather than changing the default behavior of Transformer, at least at first.","Thanks for the feedback, , you are right. If you are suggesting to go  at least for the moment  for a backwardcompatible solution, then how about simply never applying ```self.norm``` inside the ```forward``` method of ```TransformerEncoder```/```TransformerDecoder```? It should be truly backwardcompatible and give the desired behavior, but at the expense of keeping unused ```norm``` attributes for ```TransformerEncoder```/```TransformerDecoder``` that do not take part in the forward and backward propagations at all, and this would mean that when ```TransformerEncoder```/```TransformerDecoder``` are directly instantiated by users ```norm``` won't have any effect. Do you have more reasonable backwardcompatible solutions to suggest?"
transformer,Tracing model parameter shapes without instantiating the model parameters," 🚀 The feature, motivation and pitch  Request We would like a way to get a model's parameter shapes without instantiating them. The following is an example call signature. ```python  MyModel is an `nn.Module`  instantiate model the usual way model = MyModel(*args, **kwargs)  we propose a function on `nn.Module` called `trace_shapes`  trace shapes without instantiating shapes = MyModel.trace_shapes(*args, **kwargs)  `shapes` is a dict with key = param names, value = param shapes for name, param in model.named_parameters():   assert(param.shape == shapes[name]) ```  Motivation Hi, I'm a maintainer for the mup package. This repo allows one to implement in their models a special parametrization called maximal update parametrization, or muP, that has the special property that narrow and wide networks share the same optimal hyperparameters (like learning rate, initialization, etc). This is demonstrated below on a Transformer trained with adam, where on the left we have the pytorch default parametrization and the right we have muP. !image This property in particular can be used to tune hyperparameters for extremely large neural networks like GPT3 that is too expensive to train more than once, by just tuning a tiny version of it.  Current Implementation To implement muP in a model, we need to annotate the parameter shapes with two pieces of information. For each dimension in a shape, these are 1) is it a width dimension (which will be scaled up or down)? 2) what is the ""base value"" of this dimension?  our implementation is designed to replicate the pytorch default when the dimension is equal to its base value, so as to provide backward compatibility. To obtain 1) and 2), we ask the user to instantiate a ""base model"" and a ""delta model"". The delta model is supposed to vary all ""width"" dimensions compared to the base model. Then, by calling ```python set_base_shapes(model, base_model, delta=delta_model) ``` the parameters in `model` are annotated by 1) and 2) which are determined by 1) a dimension is a ""width"" dimension if it varies between `base_model` and `delta_model`, and 2) its base value is given by the value in `base_model`.  Issues with Current Implementation The current implementation is not too bad if `base_model` and `delta_model` are relatively small, but when they are big, their instantiations can cost nontrivial time and memory. Worse yet, they might need multiple GPUs to instantiate while `model` can be a small model that fit on a single GPU. Therefore, it'd be much more preferable to just obtain the shapes of `base_model` and `delta_model` without instantiating.  Alternatives See `Current Implementation` section above.  Additional context _No response_ ",2022-03-13T04:49:16Z,feature module: nn triaged,open,9,12,https://github.com/pytorch/pytorch/issues/74143,"Hey , meta tensors may help solve your problem. They are a special type of tensor that maintain shape information but do not allocate storage for the data. Example: ```python t = torch.randn((5, 3), device='meta')   allocates no memory print(t.shape)   prints torch.Size([5, 3]) ``` Note that all `torch.nn` modules have a `device` constructor kwarg that defines their parameters as meta tensors: ```python m = nn.Linear(3, 3, device='meta') print(m.weight.device)   device='meta' in = torch.randn((5, 3), device='meta') out = m(in)   out shape: torch.Size([5, 3]); out device: 'meta' ``` See here for more info. The obvious caveat is that any userdefined modules should also define a way to place their parameters on `device='meta'` to work like this.","Hi , we tested `device='meta'` with our package and it seems to work fine as is, which is great news. We have added a blurb in our README about this. However, looking around at some popular github repos, I'm not sure how prevalent is it for models to pass down `device` flags to its submodules. For example, just looking at Huggingface's BERT, this doesn't seem to be the case. So, before this `device` convention is universally adopted by the community, I'm still wondering whether it makes sense to have an automatic way of inferring parameter shapes without needing to change the code of a module.",? Also , the new Deferred Module Initialization feature that we are about to release in an outoftree package will exactly address your particular problem. I would be happy to give you access to our private repo if you want to try it out before we publicly release it.," Hi! I'm the other maintainer of the mup package. Thanks for your response, and we are happy to try this new feature! Please let us know how we can access the private repo.","No problem! I am on PTO today and have limited internet access. Expect to receive an invitation email to the “torchdistx” repo tomorrow morning. If you are on PyTorch’s Slack channel, happy to chat there as well.","  I just sent you an invite to the torchdistx repo. Please reach out to me either over PyTorch's Slack channel or over balioglu at fb.com to chat about the feature. Since the docs are not ""rendered"" yet, you might need some help to locate them.", Thanks! I just asked to be added to torch slack since I don't use FB. Will reach out to you over there once I get in.,  Happy to share that torchdistx is now public: https://github.com/pytorch/torchdistx Check out the docs for Deferred Module Initialization and Fake Tensor which I believe will address your problem. Please let me know if you have any feedback!,Awesome!   Trying this out right now., deferred_init works like a charm for the most part! Though we just encountered an issue with a model that has an unsqueeze call in its forward pass. `*** NotImplementedError: `aten::unsqueeze` cannot be run with fake tensor(s) because the meta backend has no kernel for it. Please file an issue if you want it to be supported.` I assume this is expected since the kernel isn't implemented. Should we file a separate issue if we want to have it supported?,"Hey , great! Happy to hear that it ""almost"" worked. Yes, unfortunately `unsqueeze` does not support the meta backend yet. I have also came across it in a few HG models. The good news though is that it should be fairly straightforward to add meta support for it. Please create a separate issue and ."
yi,for_blob Tensor building API runs into an issue when specifying device," 🐛 Describe the bug https://github.com/pytorch/pytorch/pull/74113/files uses `for_blob` API to build up a tensor, but changing it to the following: ``` at::Tensor param_size_tensor =         at::for_blob(param_size_data_ptr>data(), {data_size})             .context(                 param_size_data_ptr,                 [](void* ctx) {                   delete static_cast*>(ctx);                 })             .options(at::TensorOptions().dtype(at::kLong).device(params[0].device()))             .make_tensor(); ``` results in the following error: ``` RuntimeError: index_ == 1  index_ >= 0 INTERNAL ASSERT FAILED at ""caffe2/c10/core/Device.h"":128, please report a bug to PyTorch. Device index must be 1 or nonnegative, got 2 Exception raised from validate at caffe2/c10/core/Device.h:128 (most recent call first):  0  c10::get_backtraceabi:cxx11  1  std::_Function_handler, std::allocator > (), c10::(anonymous namespace)::GetFetchStackTrace()::$_0>::_M_invoke(std::_Any_data const&)  2  c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string, std::allocator >)  3  c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string, std::allocator > const&)  4  c10::detail::torchInternalAssertFail(char const*, char const*, unsigned int, char const*, std::__cxx11::basic_string, std::allocator > const&)  5  c10::Device::validate()  6  c10::Device::Device(c10::DeviceType, signed char)  7  at::cuda::getDeviceFromPtr(void*)  8  at::cuda::detail::CUDAHooks::getDeviceFromPtr(void*) const  9  at::Context::getDeviceFromPtr(void*, c10::DeviceType)  10 at::TensorMaker::make_tensor()  11 c10d::verify_params_across_processes(c10::intrusive_ptr > const&, std::vector > const&, c10::optional > const&) ``` trying ` at::kCUDA` has the same result and putting `.device(...)` before the dtype has the same result.  To reproduce, simply patch above PR and make above mentioned change.  Versions main ",2022-03-11T18:06:58Z,module: cpp triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/74114, ,varma in your code snippet I see that you are deallocating the tensor data using ``` delete static_cast*>(ctx); ``` which means that it resides on the host memory. Do you know why the `device` is reported as `CUDA`? Another question; do you remember which version of CUDA you used for this code?,Check out CC(Fix how we handle host memory in CUDA getDeviceFromPtr).
transformer,"lazy_tensor Input tensor is not a lazy tensor: Background_Matting, nvidia_deeprecommender, speech_transformer",,2022-03-11T13:47:14Z,triaged module: lazy,closed,0,1,https://github.com/pytorch/pytorch/issues/74100,deprioritize
rag,Lazy tensors do not have storage: tts_angular,,2022-03-11T13:26:46Z,triaged module: lazy,closed,0,2,https://github.com/pytorch/pytorch/issues/74099,"The error is triggered by a call to `tensor.storage()` at https://github.com/pytorch/pytorch/blob/bb49d0d54c3875e59769a070017706966f6501a3/aten/src/ATen/native/cudnn/RNN.cppL1510. In the current design of LTC, there is no guarantee that a lazy tensor has a backed storage, thus the assertion in `storage()` is intentional. Supporting `storage()` requires a substantial change to LTC, and the value of it is arguable given only very few kernels are calling `storage()` directly. Without supporting `storage()`, one way to solve the issue here is to force the relevant kernel, `lstm.input`, to go through the fallback path, meaning all the inputs are materialized before entering the kernel. I have tried to do that following the example of `normal_` in LTC, however, what makes things complicated is that `lstm.input` is a CompositeImplicitAutograd kernel. We need to register `lstm.input` as AutogradLazy, and handle it similarly to `max_pool3d` in LTC.",deprioritize
transformer,Two consecutive nn.LayerNorm are used in transformer model when norm_first is False," 🐛 Describe the bug See the code from the master https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL62L67 https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL340L345 When `norm_first` is False, the last layer of the encoder layer is `LayerNorm()`. The problem is that it always puts a LayerNorm in the encoder, see https://github.com/pytorch/pytorch/blob/9a5c3ca23ff5f01c78c32ec710ba05942baeb019/torch/nn/modules/transformer.pyL204L205 The consequence is the output of the last encoder layer is fed into another layernorm, so two consectuive layer norm layers are used here. Also, the output of the ith encoder layer is used as the input for the next LayerNorm layer in (i+1)th encoder layer.  **Expected behavior**: When norm_first is False, the Encoder should not contain a layer norm layer.  Versions The master ",2022-03-11T06:42:21Z,module: nn triaged,open,0,4,https://github.com/pytorch/pytorch/issues/74092,"I'd like to propose a solution for this issue. There are different possibilities, though  I can think of three of them that are quite different:  **backwardincompatible solutions:**    **if a final normalization is always  even when ```norm_first=True```  supposed to be applied to the output of the last encoder/decoder layer:** ⇒ an additional parameter ```norm_first``` should be required by ```TransformerEncoder```/```TransformerDecoder``` as well to apply the final normalization only when ```norm_first=True```    **else:** ⇒ since the final normalization carried out inside ```TransformerEncoder```/```TransformerDecoder``` looks redundant because alredy carried out internally to ```TransformerEncoderLayer```/```TransformerDecoderLayer```, it could simply be removed and ```TransformerEncoder```/```TransformerDecoder``` would not need the initialization parameter ```norm```  **backwardcompatible solutions:**    ```None``` should be passed as argument for the parameter ```norm``` of ```TransformerEncoder```/```TransformerDecoder``` when instantiating them inside the ```Transformer``` class, in place of the additional ```LayerNorm``` instance The second (backwardincompatible) solution looks more coherent, as users would not expect a final normalization to be carried out at the end of the encoder/decoder anyway even when specifying ```norm_first=True```, but it would remove the possibility of having a final normalization different from the ones used inside ```TransformerEncoderLayer```/```TransformerDecoderLayer```. The third solution (the backwardcompatible one) is the simplest one and solves the issue with minimal changes. Any feedback?",Is this a duplicate of CC(Successive Layer Normalization in nn.Transformer)?,"> Any feedback? One thing I'm unsure about with the proposed backwardcompatible solution is how this would work with previously serialized models. By default, LayerNorm has learnable parameters (i.e. `elementwise_affine=True`) that end up in the serialized form of the model. Running `new_model.load_state_dict(old_state_dict)` will fail because of the extra `state_dict` key. I think to be truly backwards compatible, the fix needs to be optin rather than changing the default behavior of Transformer, at least at first.","Thanks for the feedback, , you are right. If you are suggesting to go  at least for the moment  for a backwardcompatible solution, then how about simply never applying ```self.norm``` inside the ```forward``` method of ```TransformerEncoder```/```TransformerDecoder```? It should be truly backwardcompatible and give the desired behavior, but at the expense of keeping unused ```norm``` attributes for ```TransformerEncoder```/```TransformerDecoder``` that do not take part in the forward and backward propagations at all, and this would mean that when ```TransformerEncoder```/```TransformerDecoder``` are directly instantiated by users ```norm``` won't have any effect. Do you have more reasonable backwardcompatible solutions to suggest?"
rag,update script to calculate operator coverage ,update model generation script to calculate operator coverage ,2022-03-10T01:08:44Z,cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/74005,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/74005**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit cd20356ce6 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. , merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[BE Hackathon][DataPipe] Automatically generate datapipe.pyi via CMake,Stack from ghstack:  CC([BE Hackathon][DataPipe] Automatically generate datapipe.pyi via CMake)  CC([DataPipe] Separating DataPipes from Dataset into different files) Automatically generate `datapipe.pyi` via CMake and removing the generated .pyi file from Git. Users should have the .pyi file locally after building for the first time. I will also be adding an internal equivalent diff for buck. Differential Revision: D34868001,2022-03-09T22:57:25Z,module: data cla signed release notes: dataloader topic: not user facing,closed,0,10,https://github.com/pytorch/pytorch/issues/73991,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73991**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 1fb559a545 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,> The part about the build system update LGTM >  > You will most likely have to update your script to run fine from where it runs now (relative imports do help for these). I removed a dependency and I think it should work fine now,"  Should I be removing the generated .pyi file from git VCS?  Here is the trade off (should only impact PyTorch developers): 1. Remove the file from Git  the .pyi will need be generated by running `python setup.py develop` or similar **every time** you checkout a new branch 2. Keep the file in Git  when you go to a new branch, the Git version of the .pyi file will exist. When you run `python setup.py develop` and that will overwrite the Git version with the generated version. The Git version might be out of sync (generated version will overwrite the Git version).       We keep the file, should we add it to `.gitignore` or not?  I am currently keeping the .pyi file in Git but adding it to `.gitignore`. Maybe keeping it in Git and **not** in `.gitignore` is the best idea?","> Remove the file from Git  the .pyi will need be generated by running python setup.py develop or similar every time you checkout a new branch It's annoying because functional API is pure python rather than the binding from C++ to Python. And, that's why I prefer generating pyi file whenever `functional_datapipe` is invoked. Even new DataPipe is implemented, the pyi file should be updated at that time. Then, even though users are using `python setup.py develop`, they should be able to use these APIs. > I am currently keeping the .pyi file in Git but adding it to `.gitignore` If you want to generate pyi via CMake, this is better.","> Remove the file from Git  the .pyi will need be generated by running python setup.py develop or similar every time you checkout a new branch Not sure to follow. My understanding if the file is not in git and is in gitignore: whatever was there before you change branch will remain there. So yes, after the first clone you will have to do a compilation to get the file. And when you change branch (or modify python code locally) that pyi file might get stale until you recompile. If my understanding is correct then removing it and ignoring it sounds like the right move. Having a file here will potentially lead to conflicts. And a stale pyi is not a big issue during development.",> Sorry for coming back on the review but following the discussions here I think we want to actually remove `torch/utils/data/datapipe.pyi` from the repo. The same way it is done for `torch/_C/_VariableFunctions.pyi`. I will be removing that and also add an internal diff for buck," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,"RuntimeError: approximate argument must be either none or tanh: hf_Bart, hf_Bert, hf_Longformer, timm_vision_transformer",,2022-03-09T14:48:42Z,,closed,0,1,https://github.com/pytorch/pytorch/issues/73963,This relates to the gelu/gelu_backward declaration changes at https://github.com/pytorch/pytorch/pull/61439
rag,[ao] Removing memoryless observer args for MovingAverage,"Stack from ghstack:  CC([ao] Removing memoryless observer args for MovingAverage) The original implementation of memoryless observers used MinMaxObservers and a memoryless argument to manipulate the behavior of the observer such that it wouldn't keep track of previously observed min and max's. It was later pointed out that this was equivalent to a movingaverageobserver with averaging_constant=1 which is requires less overhead and no 1 off args (memoryless) so this PR refactors the memoryless arg and uses MovingAverage observers instead, although the memoryless adjective is still used, a complete definintion was also added to clarify error messages given these changes. TestPlan python test/test_quantization.py TestQuantizeEagerQAT python test/test_quantization.py TestObserver Differential Revision: D34732080",2022-03-08T22:16:57Z,cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/73947,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73947**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 01c1537c0d (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,[Model Averaging] Add a reference to hierarchical SGD,"Add a reference. Also fix the comment: unlike `averagers.py`, currently this is not a base class that can inherit many subclasses. Proposal:  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD))",2022-03-05T05:01:28Z,oncall: distributed triaged open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/73823,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73823**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 674907c24b (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,[Quant][test] Added test to check if fp16 packing->unpacking yields the same result as to(torch.float16).to(torch.float32) [reland],"Stack from ghstack:  CC([Quant][test] Added test to check if fp16 packing>unpacking yields the same result as to(torch.float16).to(torch.float32) [reland]) Summary: A test was added in test_quantized_op.py that checks whether the fp16 packing and subsequent unpacking of a given fp32 tensor produces the same result as to(torch.float16).to(torch.float32) Test Plan: in pytorch main directory, execute ``` python test/test_quantization.py TestDynamicQuantizedOps.test_linear_prepack_fp16_numerics ``` Reviewed By: jerryzh168 Pulled By: dzdang fbshipitsourceid: 5da453e5db4801dde196424282140726c8a4ef1f (cherry picked from commit ac8910e7feb4eebf677c99f287d48915165a87bf) Differential Revision: D34650372",2022-03-04T20:05:51Z,cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/73808,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73808**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 27211f7351 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Catch overflows in calculating storage byte size,"Fixes CC(Segmentation fault in col2im) In the issue the output tensor's shape is `[2, 4, 536870912, 536870912]` which results in a `numel()` slightly below the point of overflow. When the storage is created it does `numel() * 8` which overflows and a much smaller storage is allocated than required.",2022-03-03T02:23:44Z,module: error checking triaged open source cla signed release notes: cpp topic: improvements,closed,1,18,https://github.com/pytorch/pytorch/issues/73719,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73719**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit 85abfb1b01 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,Thanks for the patch! We will want to benchmark this before merging it though as  mentioned that similar approach before had significant impact.  what would be a good set of benchmark to do here?,"In my callgrind benchmark I see a 0.1% increase in instruction count.  ```python from torch.utils.benchmark import Timer import timeit t = Timer(     stmt=""at::empty({2, 4, 4, 4});"",     num_threads=1,     language='c++',     timer=timeit.default_timer ) stats = t.collect_callgrind() print(stats) ``` I think past objections have been to checking for overflow in `compute_numel()` which is probably more costly because the check is called inside a loop; whereas this check only happens once. Also note that unlike `compute_numel()` this doesn't get called when tensor views are created.","For a general solution, the checks unfortunately have to be done in a loop because the overflow can happen in any iteration in a loop, and then final result will appear to be within limits.  suggested using intrinsic at least on systems that support it  CC(Segmentation fault in max_pool3d_with_indices)issuecomment1054553201   can you look at adopting this approach? The PR as is improves some cases (where numel doesn't overflow, but storage size does), but even small increase in number of instruction for this limited case doesn't seem worth it.  ",Can you please post instruction count benchmark? Do we need some tests for this behavior? Running them only on linux should be safe I think,"Okay, I've switched it over to check for overflow at every stage of the computation when `__builtin_mul_overflow` is available. Using the same benchmark as before I now get around a 1% increase in instruction count. ","It looks like there are some build issues to work out. Before I dig into it, perhaps we should decide whether any increase is okay because it's likely unavoidable. ","I think 36 instructions is an acceptable cost for a general solution? , ",I'm OK with it because we are about to add symbolic ints which will also have this kind of cost.,+1 to this being fine. It looks like the `mul`/`imul` instruction sets the bit anyway on x86 (https://godbolt.org/z/rbhPb5K9K) so it only adds one *highly* predictable branch instruction which doesn't appear to affect the runtime in the slightest. (https://quickbench.com/q/T10cMIrQtlUMo0qccGKXw24zwdY) ARM adds a whopping *two* extra instructions.,"There's CC(Add overflow check to `TensorImpl::compute_numel`) that fixes numel computation in a very similar way, and doesn't have build issues  should we merge it first?", you gonna import this?, [edit] it's still a draft.  is there anything that prevents you from publishing it?, ping, PTAL," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.", merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training," 🚀 The feature, motivation and pitch We are working on training large transformer and DNN. Any approach for saving GPU memory is valuable.  [https://arxiv.org/pdf/2202.13808v1.pdf] DropIT seems to be fitting into pytorch tensor management area.  Will be good to explore if we can offer this feature in pytorch training. Cheers  Dr.  Alternatives _No response_  Additional context _No response_",2022-03-03T00:48:13Z,feature module: memory usage triaged,open,0,1,https://github.com/pytorch/pytorch/issues/73709,We generally wait for research to stabilize before incorporating it into PyTorch proper. Was this published in feb/2022?
yi,[Quant][test] Added test to check if fp16 packing->unpacking yields the same result as to(torch.float16).to(torch.float32),"Stack from ghstack:  CC([Quant][test] Added test to check if fp16 packing>unpacking yields the same result as to(torch.float16).to(torch.float32)) Summary: A test was added in test_quantized_op.py that checks whether the fp16 packing and subsequent unpacking of a given fp32 tensor produces the same result as to(torch.float16).to(torch.float32) Test plan: in pytorch main directory, execute ``` python test/test_quantization.py TestDynamicQuantizedOps.test_linear_prepack_fp16_numerics ``` Differential Revision: D34599476",2022-03-02T21:12:06Z,cla signed Reverted,closed,0,6,https://github.com/pytorch/pytorch/issues/73685,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73685**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit ad807c58bf (more details on the Dr. CI page):  * **6/6** failures introduced in this PR   :detective: 6 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5449668241?check_suite_focus=true) linuxbionicpy3.7clang9 / test (default, 2, 2, linux.2xlarge) (1/6) **Step:** ""Test"" (full log  :repeat: rerun)   20220307T15:25:42.6987215Z RuntimeError: test_quantization failed!  ``` 20220307T15:25:42.0421759Z Generated XML report: testreports/pythonunittest/test_quantization/TESTquantization.core.test_workflow_module.TestRecordHistogramObserver20220307151430.xml 20220307T15:25:42.0428046Z Generated XML report: testreports/pythonunittest/test_quantization/TESTquantization.core.test_quantized_module.TestReferenceQuantizedModule20220307151430.xml 20220307T15:25:42.0446281Z Generated XML report: testreports/pythonunittest/test_quantization/TESTquantization.bc.test_backward_compatibility.TestSerialization20220307151430.xml 20220307T15:25:42.0464934Z Generated XML report: testreports/pythonunittest/test_quantization/TESTquantization.core.test_quantized_module.TestStaticQuantizedModule20220307151430.xml 20220307T15:25:42.0479669Z Generated XML report: testreports/pythonunittest/test_quantization/TESTquantization.fx.test_subgraph_rewriter.TestSubgraphRewriter20220307151430.xml 20220307T15:25:42.6981767Z Traceback (most recent call last): 20220307T15:25:42.6982089Z   File ""test/run_test.py"", line 1047, in  20220307T15:25:42.6984676Z     main() 20220307T15:25:42.6984863Z   File ""test/run_test.py"", line 1025, in main 20220307T15:25:42.6986990Z     raise RuntimeError(err_message) 20220307T15:25:42.6987215Z RuntimeError: test_quantization failed! 20220307T15:25:42.9584484Z  20220307T15:25:42.9584781Z real	59m30.919s 20220307T15:25:42.9585129Z user	150m28.444s 20220307T15:25:42.9585447Z sys	10m56.210s 20220307T15:25:42.9585673Z + cleanup 20220307T15:25:42.9585845Z + retcode=1 20220307T15:25:42.9586005Z + set +x 20220307T15:25:42.9630447Z [error]Process completed with exit code 1. 20220307T15:25:42.9721978Z [group]Run  Ensure the working directory gets chowned back to the current user 20220307T15:25:42.9722347Z [36;1m Ensure the working directory gets chowned back to the current user[0m ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This pull request has been **reverted** by 8a47d9cf869ceb44580a5e333741382323b7e637. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk)."
yi,(torch/elastic) add documentation clarifying that torchrun is a console script to torch.distributed.run,"Summary: resolves  CC([docs] distributed docs still mention/recommend deprecated torch.distributed.launch) Simply clarifies that `torchrun` is a console script that invokes `python m torch.distributed.run`. Test Plan: N/A doc change only, letting github CI validate that the docs build correctly. Differential Revision: D34558538",2022-03-01T19:54:10Z,oncall: distributed fb-exported cla signed,closed,0,22,https://github.com/pytorch/pytorch/issues/73598,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73598**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit fc5640a0ae (more details on the Dr. CI page):  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base 71aa3ab020 since Mar 02   :construction: 1 ongoing upstream failure: These were probably **caused by upstream breakages** that are **not fixed yet**. * linuxxenialpy3.7clang7asan / test (default, 1, 3, linux.2xlarge) since Mar 02 (e6afa4f771)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D34558538,This pull request was **exported** from Phabricator. Differential Revision: D34558538,"What is `NUM_TRAINERS`? In academic environments often we don't know beforehand how many jobs will be run per node. Often someone will run a job taking 4 GPUs without knowing if other people take all 4 remanining GPUs in one job, or 2 jobs by 2 GPUs or no jobs at all for some time",A link in the docs to the github source code of `torchrun` would be awesome,"> What is `NUM_TRAINERS`? In academic environments often we don't know beforehand how many jobs will be run per node. Often someone will run a job taking 4 GPUs without knowing if other people take all 4 remanining GPUs in one job, or 2 jobs by 2 GPUs or no jobs at all for some time Its a placeholder env var that you pass to `nproc_per_node` (the number of local workers to start for this particular job)",> A link in the docs to the github source code of `torchrun` would be awesome done,> Its a placeholder env var that you pass to `nproc_per_node` (the number of local workers to start for this particular job) Is it mandatory? In practice the person running the first job may not know what other jobs will be run in the future on the same machine,> > Its a placeholder env var that you pass to `nproc_per_node` (the number of local workers to start for this particular job) >  > Is it mandatory? In practice the person running the first job may not know what other jobs will be run in the future on the same machine no its just a way to document that you can pass whatever integer you want to `nproc_per_node` without having to hard code a specific number. Same can be achieved by having documented it as `nproc_per_node=k`,i think it'd better be specified explicitly that it's not mandatory (to have the simplest recipe for this most frequent scenario),"> done hmm, seems this change is not pushed to the PR yet...","> A link in the docs to the github source code of `torchrun` would be awesome done > i think it'd better be specified explicitly that it's not mandatory (to have the simplest recipe for this most frequent scenario) you need to specify `nproc_per_node`, this tells the launcher how many ""copies"" of your trainer to launch (all the copies belong to a single process group, aka job)",This pull request was **exported** from Phabricator. Differential Revision: D34558538,"Right, so in practice this is ~NGPUS, right?",So far I kind of understood that torchrun somehow runs torch.distributed.run script. Is it actually just a symlink? Or does it have more functionality? From this phrasing it is not clear: `is a console script to the main module torch.distributed.run` ,"> Right, so in practice this is ~NGPUS, right? yes","> So far I kind of understood that torchrun somehow runs torch.distributed.run script. Is it actually just a symlink? Or does it have more functionality? From this phrasing it is not clear: `is a console script to the main module torch.distributed.run` its literally called ""console script"" for standard python packaging: https://packaging.python.org/en/latest/specifications/entrypoints/useforscripts","I see! outside of python packaging, `console_script` doesn't have a very precise meaning and could contain extra functionality. For such docs IMO it doesn't hurt to be a bit more verbose, not everyone is wellversed in python packaging terminology...",This pull request was **exported** from Phabricator. Differential Revision: D34558538,"> I see! outside of python packaging, `console_script` doesn't have a very precise meaning and could contain extra functionality. For such docs IMO it doesn't hurt to be a bit more verbose, not everyone is wellversed in python packaging terminology... yea, I added a hyperlink.",This pull request was **exported** from Phabricator. Differential Revision: D34558538,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,`torch.result_type` does not yield correct type in `torch.autocast()` enabled regions," 🐛 Describe the bug  Description `torch.result_type` incorrectly computes `float32` output within a `torch.autocast()` enabled region  Repro ```python import torch  Creates some tensors in default dtype (here assumed to be float32) print('Original dtypes') a_float32 = torch.rand((8, 8), device=""cuda"") b_float32 = torch.rand((8, 8), device=""cuda"") c_float32 = torch.rand((8, 8), device=""cuda"") d_float32 = torch.rand((8, 8), device=""cuda"") x_float16 = torch.rand((8, 8), device=""cuda"").half() print(a_float32.dtype) print(b_float32.dtype) print(c_float32.dtype) print(d_float32.dtype) print('\\nOutput dtypes within autocast enabled region') with torch.autocast(device_type='cuda'):      torch.mm is on autocast's list of ops that should run in float16.      Inputs are float32, but the op runs in float16 and produces float16 output.      No manual casts are required.     e_float16 = torch.mm(a_float32, b_float32)     assert e_float16.dtype == x_float16.dtype      Also handles mixed input types     f_float16 = torch.mm(d_float32, e_float16)     assert e_float16.dtype == x_float16.dtype      torch.result_type is unaware of autocast's list and fail to compute the right dtype     i_wrong_float = torch.result_type(a_float32, b_float32)     assert i_wrong_float == x_float16.dtype     j_wrong_float = torch.result_type(d_float32, e_float16)     assert j_wrong_float == x_float16.dtype print('\\nOutput dtypes outside autocast region')  After exiting autocast, calls f_float16.float() to use with d_float32 g_float32 = torch.mm(d_float32, f_float16.float()) assert g_float32.dtype == d_float32.dtype  Outside autocast enabled region torch.result_type works as expected h_float32 = torch.result_type(d_float32, f_float16.float()) assert h_float32 == d_float32.dtype ``` I am willing to help on the fix  Versions  Versions ``` Collecting environment information... PyTorch version: 1.12.0a0+git1e7a4d6 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.4 LTS (x86_64) GCC version: (Ubuntu 8.4.03ubuntu2) 8.4.0 Clang version: 15.0.0++20220222052859+cedc23bc86121~exp1~20220222172957.167 CMake version: version 3.19.6 Libc version: glibc2.31 Python version: 3.9.7 (default, Sep 16 2021, 13:09:58)  [GCC 7.5.0] (64bit runtime) Python platform: Linux5.13.030genericx86_64withglibc2.31 Is CUDA available: True CUDA runtime version: 11.3.109 GPU models and configuration:  GPU 0: Quadro K620 Nvidia driver version: 510.47.03 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.3.2 HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==0.812 [pip3] mypyextensions==0.4.3 [pip3] numpy==1.20.0 [pip3] torch==1.12.0a0+git1e7a4d6 [pip3] torchvision==0.13.0.dev20220210+cu111 [conda] blas                      1.0                         mkl   [conda] magmacuda111             2.5.2                         1    pytorch [conda] magmacuda113             2.5.2                         1    pytorch [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklinclude               2022.0.1           h06a4308_117   [conda] mklservice               2.4.0            py39h7f8727e_0   [conda] mkl_fft                   1.3.0            py39h42c9631_2   [conda] mkl_random                1.2.2            py39h51133e4_0   [conda] mypy                      0.782                    pypi_0    pypi [conda] mypy_extensions           0.4.3            py39hf3d152e_4    condaforge [conda] numpy                     1.21.4                   pypi_0    pypi [conda] numpybase                1.21.2           py39h79a1101_0   [conda] torch                     1.12.0a0+git1e7a4d6           dev_0     [conda] torchvision               0.13.0.dev20220210+cu111          pypi_0    pypi ``` ",2022-02-28T21:33:43Z,triaged module: amp (automated mixed precision),closed,0,4,https://github.com/pytorch/pytorch/issues/73538,"This is expected behavior, as `result_type` outputs the output type of pointwise operation on its inputs subject to pytorch type promotion rules https://pytorch.org/docs/master/generated/torch.result_type.html?highlight=result_typetorch.result_type. `matmul`, even without autocast, doesn't comply to these type promotion rules. ","> This is expected behavior, as `result_type` outputs the output type of pointwise operation on its inputs subject to pytorch type promotion rules https://pytorch.org/docs/master/generated/torch.result_type.html?highlight=result_typetorch.result_type. `matmul`, even without autocast, doesn't comply to these type promotion rules. Thanks for the input . From the point of view of the users, does it make sense to compute output's `dtype` based solely on type promotion rules? My understanding is that users want to know the output dtype, regardless of what feature/capability is enabled. By reading the function description (aka `Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors`) it seems the aforementioned expectation includes Autocast.  Maybe Autocast didn't exist when `result_type` was implemented (didn't check that), thus the mention of type promotion doc and no mention of Autocast? If indeed this is the desired behavior, I would suggest (or even push a PR) adding a warning note in the doc highlighting the `result_type`'s result may be incorrect when running on a Autocast enabled region.","But it's not just autocastenabled region, e.g. `mm` on float16/float32 won't work, but `result_type` for float16/float32 will happily output `float32`. Regular pointwise operations (for which `result_type` generally predicts correct result type) will run the same whether in the autocastenabled region or outside of it, difference is only for ops like `mm`, and `result_type` has only tensor args, not operation arg, so it cannot predict output dtype that depends on the operation. "," yes, ops that do not accept mixed precision out of the box (most of them?) will raise an exception related to the type mismatch. But the same op wrapped by autocast would work properly. This in particular shouldn't be a problem for `result_type` On the other hand, you brough a great point! `result_type` only take tensors as input, not operator [name]. What do think think about adding a string `op_name` as optional argument, defaulting to None, in which users could leverage type inference based on operator name when autocast is enabled? My end game is having a way for users to know what is the result type of an operator execution without actually executing it. With this information, they could make informed decisions in their training script or pytorch extension (ONNX runtime). Another alternative would have a dedicated API, say `torch.autocast_result_type(opname: str, *inputs)`? What do you think?"
agent,"PyTorch for ROCm on a Supported Device Throws ""hipErrorNoBinaryForGpu"""," 🐛 Describe the bug I am trying to install PyTorch with ROCm support on a Unibap iX10 ADS  which uses a slightly modified version of Ubuntu 18.04 server with an AMD Ryzen Embedded V1605B with Radeon Vega Gfx GPU  and whether I install from source of via pip the error ```hipErrorNoBinaryForGpu: Unable to find code object for all current devices!``` is thrown when I try to use CUDA, even though C HIP code compiles and runs perfectly well.  Using the environment variable AMD_LOG_LEVEL=6 I have found that it seems like it tries to look for an incorrect device; I am including a few test script outputs. To summarize them, square.cpp is a HIP file that squares the elements in an array and compiles and runs without issues; based on the output log, it seems to look for the correct device ""AMD Ryzen Embedded V1605B with Radeon Vega Gfx."" PyTest.py is a simple python script that does nothing but import torch and ```print(torch.cuda.is_available())``` and throws the error before crashing. The output log shows that it looks for the device ""amdgcnamdamdhsagfx902:xnack+,"" which is a part of my device  rocminfo lists it as the ISA of Agent 2  though it seems like it is incorrect, since square does not seem to look for it.  I am also including the output of rocminfo for more information on my system. pyTestOutput.txt rocminfo.txt squareOutput.txt  Versions Collecting environment information... ""hipErrorNoBinaryForGpu: Unable to find code object for all current devices!"" Aborted (core dumped) ",2022-02-28T21:11:21Z,module: rocm triaged,closed,1,4,https://github.com/pytorch/pytorch/issues/73534,"I am facing same issue but for **amdgcnamdamdhsagfx1010:xnack  [Not Found]** ``` >>> import torch >>> torch.cuda.is_available() :3:rocdevice.cpp            :414 : 34831946688 us: 30   : [tid:0x7f2900de80c0] Initializing HSA stack. :3:comgrctx.cpp             :33  : 34831974083 us: 30   : [tid:0x7f2900de80c0] Loading COMGR library. :3:rocdevice.cpp            :205 : 34831979970 us: 30   : [tid:0x7f2900de80c0] Numa selects cpu agent[0]=0x559781b73f90(fine=0x559785447460,coarse=0x559785436740) for gpu agent=0x559785456360 :3:rocdevice.cpp            :1591: 34831980834 us: 30   : [tid:0x7f2900de80c0] HMM support: 1, xnack: 0, direct host access: 40 :4:rocdevice.cpp            :1887: 34831980946 us: 30   : [tid:0x7f2900de80c0] Allocate hsa host memory 0x7f27820d8000, size 0x28 :4:rocdevice.cpp            :1887: 34831981672 us: 30   : [tid:0x7f2900de80c0] Allocate hsa host memory 0x7f2755700000, size 0x101000 :4:rocdevice.cpp            :1887: 34831982652 us: 30   : [tid:0x7f2900de80c0] Allocate hsa host memory 0x7f2755500000, size 0x101000 :4:runtime.cpp              :82  : 34831982956 us: 30   : [tid:0x7f2900de80c0] init :3:hip_context.cpp          :50  : 34831982976 us: 30   : [tid:0x7f2900de80c0] Direct Dispatch: 1 :1:hip_code_object.cpp      :460 : 34831983045 us: 30   : [tid:0x7f2900de80c0] hipErrorNoBinaryForGpu: Unable to find code object for all current devices! :1:hip_code_object.cpp      :461 : 34831983061 us: 30   : [tid:0x7f2900de80c0]   Devices: :1:hip_code_object.cpp      :464 : 34831983075 us: 30   : [tid:0x7f2900de80c0]     amdgcnamdamdhsagfx1010:xnack  [Not Found] :1:hip_code_object.cpp      :468 : 34831983088 us: 30   : [tid:0x7f2900de80c0]   Bundled Code Objects: :1:hip_code_object.cpp      :485 : 34831983100 us: 30   : [tid:0x7f2900de80c0]     hostx86_64unknownlinux  [Unsupported] :1:hip_code_object.cpp      :483 : 34831983120 us: 30   : [tid:0x7f2900de80c0]     hipv4amdgcnamdamdhsagfx1030  [code object v4 is amdgcnamdamdhsagfx1030] :1:hip_code_object.cpp      :483 : 34831983138 us: 30   : [tid:0x7f2900de80c0]     hipv4amdgcnamdamdhsagfx900  [code object v4 is amdgcnamdamdhsagfx900] :1:hip_code_object.cpp      :483 : 34831983155 us: 30   : [tid:0x7f2900de80c0]     hipv4amdgcnamdamdhsagfx906  [code object v4 is amdgcnamdamdhsagfx906] :1:hip_code_object.cpp      :483 : 34831983172 us: 30   : [tid:0x7f2900de80c0]     hipv4amdgcnamdamdhsagfx908  [code object v4 is amdgcnamdamdhsagfx908] :1:hip_code_object.cpp      :483 : 34831983198 us: 30   : [tid:0x7f2900de80c0]     hipv4amdgcnamdamdhsagfx90a  [code object v4 is amdgcnamdamdhsagfx90a] ""hipErrorNoBinaryForGpu: Unable to find code object for all current devices!"" Aborted (core dumped) ```","Can confirm the same issue on gfx1030 device using PyTorch 1.11.0 (RoCM 5.20) Docker: ``` >>> import torch >>> torch.cuda.is_available() ""hipErrorNoBinaryForGpu: Unable to find code object for all current devices!"" Aborted (core dumped) ```","Hi  , I'm able to confirm gfx1030 is working as expected on my local dev box:  reference dockerfile: ```  Build from a base rocm docker image FROM rocm/devubuntu20.04:latest  Install PyTorch 1.12 whl package fetched from pytorch.org  RUN pip3 install torch torchvision torchaudio extraindexurl https://download.pytorch.org/whl/rocm5.1.1 ```  Sample output using the docker container built with it: ``` ~/dockerfile_navi21$ drun 1a92498f05d7 rootsjc284:/ cd rootsjc284:~ ls rootsjc284:~ import torch bash: import: command not found rootsjc284:~ python3 Python 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import torch >>> torch.cuda.is_available() True ``` You can refer to the following document on how to properly install PyTorchROCm on supported GPUs: https://docs.amd.com/bundle/Deep_learning_Guide_5.2/page/Frameworks_Installation_Guide.html At this moment we have `gfx900, gfx906, gfx908, gfx90a, gfx1030` binary supported in PyTorch WHL package and the dependency libraries. ","Hello all, As clear stated by Unibap, only their versions of ROCm, pyTorch, Tensorflow, pyHIP, etc will work on their products based on AMD APUs. Do not expect the reglar ROCm or higher order software to work on APUs. They are not officially supported by AMD. Contact the vendor."
rag,`storage` does support `complex32` tensor," 🐛 Describe the bug `storage` and `storage_type` does support `complex32` tensor ```python input = torch.rand([1, 1], dtype=torch.complex32) input.storage()  RuntimeError: unsupported Storage type ``` Interestingly, `storage_offset` supports `complex32` tensor ```python input = torch.rand([1, 1], dtype=torch.complex32) input.storage_offset()  0 ```  Versions pytorch: 1.10.1 ",2022-02-28T11:23:57Z,triaged module: complex module: half,closed,0,3,https://github.com/pytorch/pytorch/issues/73502,Hey! Note that complex32 was disabled for the upcoming release and should not be used at the moment as it is WIP,"Hi, based on the replyissuecomment1035337126), `complex32` may be supported in future version, so I think we need to pay some attention on it",Ho yes for sure! Just wanted to let you know that now might be a bit early to use it as there are still quite a few rough edges.
chat,Windows workflows frequently failing with timeout," Current Status Mitigated  Error looks like Example failure looks like: https://github.com/pytorch/pytorch/runs/5346428448?check_suite_focus=true  Incident timeline (all times pacific) Started on 2/22/22 around 12pm PST, with this commit: https://hud2.pytorch.org/pytorch/pytorch/commit/53faf78143bbca3157dfef41dedf76eb0ba0b9f2    Around 2/22/22, windows CI jobs started to fail with timeout errors such as https://github.com/pytorch/pytorch/runs/5346428448?check_suite_focus=true. It looks like this issue started with this commit: https://hud2.pytorch.org/pytorch/pytorch/commit/53faf78143bbca3157dfef41dedf76eb0ba0b9f2 which may have increased the duration of windows tests resulting in this timeout, though this is yet to be determined.    e.g.  2/22 12:22pm Incident began  2/22 12:54pm Manually detected by OSS CI oncall, pinged in chat  [not yet root caused]  [not yet mitigated]  [not yet closed]   User impact  Windows jobs are failing, causing noise on PRs and potentially masking real windows breakages  Root cause To be filled out after mitigation  Mitigation  sent a PR to bump windows test timeout: https://github.com/pytorch/pytorch/pull/73490 SEV is mitigated after rebalancing the shards appropriately with CC(Remove PR/trunk distinction as it isn't accurate + fix win sharding) and CC(Update XNNPACK), see 's comment here:  CC(Windows tests frequently timeout)issuecomment1054348060 [todo] try to revert https://hud2.pytorch.org/pytorch/pytorch/commit/53faf78143bbca3157dfef41dedf76eb0ba0b9f2  Prevention/followups  to be filled out after investigation",2022-02-28T02:59:36Z,triaged ci: sev,closed,0,2,https://github.com/pytorch/pytorch/issues/73495,Should we close this one and continue discussion in  CC(Windows tests frequently timeout) ?,Followup:   Raise an alert when test runtime is 75+% of the specified limit   [BE] Why do we need to limits: one on the workflow and another on the job
yi,"dbr quant: insert activation obs explicitly, instead of relying on hooks","Stack from ghstack:  CC(dbr quant: enable reference module support for torch.qint32) dbr quant: enable reference module support for torch.qint32 * * CC(dbr quant: insert activation obs explicitly, instead of relying on hooks) dbr quant: insert activation obs explicitly, instead of relying on hooks** Summary: Before this PR, DBR quant reused the Eager mode quantization machinery to insert activation observers. This was done for speed of developing the prototype. A drawback of this is that the activation observers are not present in DBR's data structures and live on the modules instead. This PR refactors DBR quant to stop using Eager mode quantization observer insertion for activations, and instead create and track the activation observers in DBR's data structures. This has a couple of benefits: 1. activation observers are now created the same way in DBR for modules and functions 2. we can remove some technical debt due to fixing (1) 3. this will make it easier to support reference modules in a future PR The reason (3) is true is because the current design of reference modules assumes that the activation observer lives on the framework (like in FX graph mode quantization). This PR starts to adhere to that assumption. Test plan: ``` python test/test_quantization.py k DBR ``` Differential Revision: D34520758",2022-02-28T02:26:36Z,cla signed release notes: quantization topic: not user facing,closed,0,6,https://github.com/pytorch/pytorch/issues/73492,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73492**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit a3f4a48c38 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," ,  , would you be able to help review this?"," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
agent,(torch/elastic) skip logging structured error info if error_file is not set,"Summary: resolves  CC([torchelastic] properly format (or don't log) trace info in structured error when the agent process is killed prematurely) This `log.error` is not necessary (and its also not humanfriendly formatted) because we end up reraising the same exception after recording the exception into an error_file (if present). Eventually python should handle this error the way it handles any other errors and will write the trace info into the console. This additional logging produces duplicate error console prints, which affects all users whose schedulers do not set `TORCHELASTIC_ERROR_FILE` env var when calling `torch.distributed.run`. Test Plan: Induce an error on the agent process by `kill 15 $AGENT_PID` ``` python m torch.distributed.run \\    nproc_per_node 2 \\    nnodes 1:1 \\    rdzv_backend c10d \\   rdzv_endpoint localhost:29500 \\   monitor_interval 3 test.py ``` Produces {F704936697} In contrast to the duplicated error before: {F704936729} Differential Revision: D34501852",2022-02-26T20:00:03Z,oncall: distributed fb-exported cla signed release notes: distributed (ddp) topic: bug fixes,closed,1,5,https://github.com/pytorch/pytorch/issues/73477,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73477**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 2334765989 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34501852,This pull request was **exported** from Phabricator. Differential Revision: D34501852,This pull request was **exported** from Phabricator. Differential Revision: D34501852,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,different results on each batch_size in torch==1.10.2+cu113 on RTX 3080," 🐛 Describe the bug on each batch size, the result is different, I tried on googlecolab(with there default pytorch) the outputs were consistent. ``` from PIL import Image import requests import torch import os from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel urls = ['http://images.cocodataset.org/val2017/000000039769.jpg' , 'http://images.cocodataset.org/teststuff2017/000000000448.jpg','http://images.cocodataset.org/teststuff2017/000000000128.jpg'] get_image = lambda x: Image.open(requests.get(x, stream=True).raw) images = [get_image(i) for i in urls] images = images * 10 torch.use_deterministic_algorithms(True) model = CLIPVisionModel.from_pretrained(""openai/clipvitbasepatch32"").to(""cuda"").eval() processor = CLIPProcessor.from_pretrained(""openai/clipvitbasepatch32"") inputs = processor(images=images, return_tensors=""pt"") with torch.no_grad():       output_clip_images = model(**inputs.to(""cuda"")) inputs = processor(images=images[0], return_tensors=""pt"") with torch.no_grad():       output_clip_image = model(**inputs.to(""cuda"")) close = torch.isclose(output_clip_image[1][0], output_clip_images[1][0]) close.sum(), close.shape ```  > (tensor(11, device='cuda:0'), torch.Size([768])) the same code on google colab but with `torch.use_deterministic_algorithms(False)` because it raises an error  gives: > (tensor(768, device='cuda:0'), torch.Size([768])) ``` np.testing.assert_almost_equal(output_clip_image[1][0].cpu().detach().numpy(), output_clip_images[1][0].cpu().detach().numpy()) ``` ``` AssertionError:  Arrays are not almost equal to 7 decimals Mismatched elements: 768 / 768 (100%) Max absolute difference: 0.00241792 Max relative difference: 0.5413908  x: array([3.5340825e01,  4.3769903e02, 7.5632817e01, 8.6524528e01,         5.1617998e01, 8.6147100e01,  3.0487695e01,  1.0068034e+00,         4.8502433e01,  4.2452109e01,  2.1260881e+00, 6.3165623e01,...  y: array([3.5306537e01,  4.2240802e02, 7.5647628e01, 8.6631018e01,         5.1585835e01, 8.6233985e01,  3.0509090e01,  1.0060844e+00,         4.8487094e01,  4.2465281e01,  2.1263967e+00, 6.2989092e01,... ```  Versions ``` Collecting environment information... PyTorch version: 1.10.2+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: elementary OS 6.1 Jólnir (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.29 Python version: 3.7.9 (default, Feb 20 2022, 21:45:03)  [GCC 9.3.0] (64bit runtime) Python platform: Linux5.13.030genericx86_64withdebianbullseyesid Is CUDA available: True CUDA runtime version: 10.1.243 GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080 Laptop GPU Nvidia driver version: 510.47.03 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.21.5 [pip3] torch==1.10.2+cu113 [pip3] torchaudio==0.10.2+cu113 [pip3] torchvision==0.11.3+cu113 [conda] Could not collect ```",2022-02-26T18:38:13Z,triaged module: numerical-reproducibility,open,0,3,https://github.com/pytorch/pytorch/issues/73475,"Hi, This is very much possible if an early layer is not deterministic and leads to small errors that then get amplified by the rest of the network. You can try to use the https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html?highlight=deterministictorch.use_deterministic_algorithms flag to force determinism. Also note that models that have batchnorm or similar models change their behavior after every forward when they are in training mode.","> flag to force determinism. even this didn't work and the results are way more different.  On google Colab(K80) even without determinism still gives the same results. I traced the operations, and even a simple addition like `residual + hidden_states` gives different results on different batch_size ",> the same code on google colab but with torch.use_deterministic_algorithms(False) because it raises an error If that raised an error that means that the algorithm is indeed nondeterministic. Note that nondeterministic doesn't mean that it will always be different. Only that it can be (depending on a lot of things including version of dependency/hardware/input values/etc.
agent,[torchelastic] properly format (or don't log) trace info in structured error when the agent process is killed prematurely," 🐛 Describe the bug When running with `torchrun` and: 1. `TORCHELASTIC_ERROR_FILE` env var is not set at the agent level 2. The agent process (process that is running `torchrun`) either raises an error or is killed prematurely due to a signal Then we end up seeing an ugly unformatted trace info (`extraInfo.py_callstack`) from the error log of the structured error of the agent process (see screenshot below) !image What should happen is that we either format the trace info in the structured error when logging or just skip the error logging and let python's error handling/console printing do the job. REPRO STEPS: 1. create a `test.py` with a single line `import time; time.sleep(6000)` 2. `torchrun nproc_per_node 2 nnodes 1:1 rdzv_backend c10d rdzv_endpoint localhost:29500 monitor_interval 3 test.py` 3. grab the PID of CC(Don't support legacy Python) and run `kill 15` on it  4. Observe the unformatted error traceback (as shown in the screenshot). The logging is done on this line: https://github.com/pytorch/pytorch/blob/df11e2d6f9782fc3995e17ef09a5ef3812da041d/torch/distributed/elastic/multiprocessing/errors/error_handler.pyL36 ADDITIONAL LINKS: 1) (Original user report): https://github.com/pytorch/pytorch/pull/65041issuecomment1051368205  Versions Collecting environment information... PyTorch version: N/A Is debug build: N/A CUDA used to build PyTorch: N/A ROCM used to build PyTorch: N/A OS: Ubuntu 16.04.7 LTS (x86_64) GCC version: (Ubuntu 5.4.06ubuntu1~16.04.12) 5.4.0 20160609 Clang version: Could not collect CMake version: version 3.16.3 Libc version: glibc2.23 Python version: 3.8.5 (default, Mar 24 2021, 22:32:23)  [GCC 5.4.0 20160609] (64bit runtime) Python platform: Linux4.4.01102awsx86_64withglibc2.17 Is CUDA available: N/A CUDA runtime version: Could not collect GPU models and configuration: GPU 0: Tesla V100SXM216GB GPU 1: Tesla V100SXM216GB GPU 2: Tesla V100SXM216GB GPU 3: Tesla V100SXM216GB Nvidia driver version: 440.33.01 cuDNN version: /usr/local/cuda10.1/targets/x86_64linux/lib/libcudnn.so.7.6.2 HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.20.3 [pip3] pytorchlightning==1.3.5 [pip3] torchaudio==0.9.0 [pip3] torchelastic==0.2.3.dev0 [pip3] torchmetrics==0.3.2 [pip3] torchserve==0.4.0 [pip3] torchvision==0.10.0+cu111 [pip3] torchx==0.1.1.dev0 [conda] blas                      1.0                         mkl [conda] cudatoolkit               10.2.89              hfd86e86_1 [conda] mkl                       2021.3.0           h06a4308_520 [conda] torch                     1.12.0.dev20220225+cu102          pypi_0    pypi",2022-02-26T01:16:43Z,triaged module: elastic oncall: r2p,closed,0,13,https://github.com/pytorch/pytorch/issues/73465,"Please see these 2 log files for examples of interleaving and too many errors replayed for each node and/or process: 181Bn48pp12bf162182654.txt 181Bn48pp6bf161305.txt This setup is 48 nodes of 8 gpus each: ``` $ grep ERROR 181Bn48pp12bf162182654.out wc l 429 ``` Both exemplify what I have been referring to in various issues. Some errors are dumped once per node (first `wc l`, others for each process, 2nd `wc l`) and you can see that at times it's impossible to unravel the Tracebacks as they are interleaved, ` 181Bn48pp12bf162182654.out` is the better example of interleaving. sometimes it's almost perfect, other times it's much much worse interleaving. If need be I can dig up more examples. If you would like any more input about the setup please let me know but otherwise it's: ``` export LAUNCHER=""python u m torch.distributed.run \\     nproc_per_node $GPUS_PER_NODE \\     nnodes $NNODES \\     rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \\     rdzv_backend c10d \\     max_restarts 0 \\     "" ``` Thank you!","Looks like you have a single .out file for all the nodes. Are you observing intra or inter node trace interleaving? To make things better intranode, you could try passing `tee 3` (prefixes local rank to each line of stdout and stderr). For intranode... this isn't really controlled by us (neither is intranode, but passing `tee` helps a bit).  The scheduler (I'm assuming SLURM in your case from the srun logs) should have a separate stream per node, I'm assuming you're tee'ing those back to a single console? You could also try using workerperslurmtask, in which case SLURM would create separate output streams for each worker (in this case you wouldn't even need `torch.distributed.run`)","Well, there are typically 2 types of failures: 1. something happened in all processes  e.g. broken code  and they all die at once  same error 2. something happens in one process  e.g. OOM  and then all processes get killed  currently in either event we end up with n_errors == n_gpus, whereas the ideal situation would be, that only a set of these errors is printed. So if it's the same error on all processes it just says  all processes died with the following error, or there were 2 errors, etc. But I suppose the heuristics of comparing the different backtraces could be complicated.","I see, yep this is something we've been wanting to fix. Just to be clear, (besides the duplicate error logs) this isn't a regression from torch1.8 right? AFAIK this has been an issue with the fact that torch runs multiple workers on the same node as different processes.","> Looks like you have a single .out file for all the nodes. Are you observing intra or inter node trace interleaving? From my manual experiments it's typically an intranode issue, please see how I resolve it here: https://github.com/pytorch/pytorch/pull/61803 and propose to add it to torch.distributed for normal prints  as they suffer the same issue, but somehow this needs to be extended to the python exception manager so that when it dumps the traceback it goes through a `flock`'ed print. > To make things better intranode, you could try passing `tee 3` (prefixes local rank to each line of stdout and stderr). For intranode... this isn't really controlled by us (neither is intranode, but passing `tee` helps a bit). I will give it a try  thank you! > The scheduler (I'm assuming SLURM in your case from the srun logs) should have a separate stream per node, I'm assuming you're tee'ing those back to a single console? You could also try using workerperslurmtask, in which case SLURM would create separate output streams for each worker (in this case you wouldn't even need `torch.distributed.run`) SLURM it is. The problem is that if I don't pipe it into a single file it becomes much harder to monitor problems.  So ideally I want a single log file I can `tail f` that tells me the important events but not duplicated 300 times. And then it's fine to have additional log files.","ok, I tried `tee 3` and it prefixes local rank but how does it help?  Are you suggesting that it'd be helpful if I get an intrainterleaved traceback and then I'd grep for a single local rank, and thus be able to extract the noninterleaved traceback? That's doable but that's doing a whole bunch of steps, when I quickly need to determine the cause of failure and act on it. Typically I have the cursor on the last good line of the `tail f file` while it continues spitting data or use console's search to find the place before the crash, and I want to get the normal traceback right there. But now I have to copynpaste this to another file, grep this file, create new file and only then I get to the traceback. This is very painfully inefficient. but doable.",Here is another related issue  CC([elastic launcher] redirects/tee support for global rank),"> I see, yep this is something we've been wanting to fix. Just to be clear, (besides the duplicate error logs) this isn't a regression from torch1.8 right? AFAIK this has been an issue with the fact that torch runs multiple workers on the same node as different processes. This is definitely not a regression and when I tried with 1.11tobe it looked as if someone worked on it and more often than not the tracebacks were either noninterleaved or close to being noninterleaved. Except when it completely fails and gets very interleaved. So perhaps the ""working"" was just a coincidence.  If I try to reproduce it with a small example on a single node 95% of time I get perfect noninterleaved tracebacks. But you have seen now the situation from the logs I attached:  CC([torchelastic] properly format (or don't log) trace info in structured error when the agent process is killed prematurely)issuecomment1052474028","yep I worked on it based on our conversation after torch1.9. The error summary output is supposed to help you quickly figure out what happened by looking at the last 50100 lines (tail 100) of the console output of each node. This is why the “Root Cause” error is reported last. Looks like I missed the edgecase where the agent itself fails or is killed by the scheduler, then the code follows a different branch and error summary doesn’t kick in.  This should help with the duplicate traces: https://github.com/pytorch/pytorch/pull/73477issuecomment1052536699 I’ll make a separate PR to also write the agent failures using the same error summary format as the worker failures. ","Thanks a lot for making it better. > The error summary output is supposed to help you quickly figure out what happened by looking at the last 50100 lines (tail 100) of the console output of each node. That would be amazing! currently I have to scroll up for 5 min until I find the actual cause, since usually at the end I get many totally unrelated errors which happen due to earlier errors. And I get hundreds of these at the end: ``` [3]:   time      : 20220223_20:31:04   host      : jeanzayiam11ib0   rank      : 83 (local_rank: 3)   exitcode  : 1 (pid: 92407)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [4]:   time      : 20220223_20:31:04   host      : jeanzayiam11ib0   rank      : 84 (local_rank: 4)   exitcode  : 1 (pid: 92408)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html [5]:   time      : 20220223_20:31:04   host      : jeanzayiam11ib0   rank      : 85 (local_rank: 5)   exitcode  : 1 (pid: 92409)   error_file:    traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ```",ah adding ``to your main function will make it so that the exception traces appear in that summary instead of the “To enable trace back…”. ,Additionally could the `tee` preamble make a bit of white space after it and be less noisy by itself with square brackets? Currently it's overbearing: ``` [default4]:[Rank 324] (after 1 iterations) memory (MB)  max reserved: 30904.0 ```,"also if I orchestrate a failure MegatronDeespeed I consistently get a bunch of segfaults with, so my log ends with: ```  Root Cause (first observed failure): [0]:   time      : 20220226_22:02:07   host      : jeanzayiam39ib0   rank      : 304 (local_rank: 0)   exitcode  : 1 (pid: 181845)   error_file: /tmp/torchelastic_78rus22c/none_h9w4ezyp/attempt_0/0/error.json   traceback : Traceback (most recent call last):     File ""/gpfswork/rech/six/commun/conda/py38pt111/lib/python3.8/sitepackages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper       return f(*args, **kwargs)     File ""/gpfswork/rech/six/commun/code/tr8b104B/MegatronDeepSpeed/pretrain_gpt.py"", line 241, in main       die   NameError: name 'die' is not defined ============================================================   File ""/gpfswork/rech/six/commun/conda/py38pt111/l  File ""/gpfswork/rech/six/commun/conda/  File ""/gpfswork/rech/six/commun/conda/py38pt111/lib/python3.8/sitepacka  File ""/gpfswork/rech/six/commun/conda/py38pt111/lib/python3.8/runpy.py""  File ""/gpfswork/rech/six/commun/conda/py38pt111/lib/pythsrun: error: jeanzayiam21: task 20: Exited with exit code 1 srun: error: jeanzayiam29: task 28: Exited with exit code 1 srun: error: jeanzayiam35: task 34: Segmentation fault (core dumped) srun: error: jeanzayiam08: task 7: Exited with exit code 1 srun: error: jeanzayiam31: task 30: Exited with exit code 1 srun: error: jeanzayiam41: task 40: Exited with exit code 1 srun: error: jeanzayiam14: task 13: Exited with exit code 1 srun: error: jeanzayiam15: task 14: Exited with exit code 1 srun: error: jeanzayiam26: task 25: Exited with exit code 1 srun: error: jeanzayiam06: task 5: Exited with exit code 1 srun: error: jeanzayiam37: task 36: Exited with exit code 1 srun: error: jeanzayiam24: task 23: Segmentation fault (core dumped) srun: error: jeanzayiam17: task 16: Segmentation fault (core dumped) srun: error: jeanzayiam46: task 45: Exited with exit code 1 srun: error: jeanzayiam23: task 22: Segmentation fault (core dumped) srun: error: jeanzayiam03: task 2: Exited with exit code 1 srun: error: jeanzayiam47: task 46: Exited with exit code 1 srun: error: jeanzayiam13: task 12: Exited with exit code 1 srun: error: jeanzayiam42: task 41: Exited with exit code 1 srun: error: jeanzayiam19: task 18: Exited with exit code 1 srun: error: jeanzayiam33: task 32: Segmentation fault (core dumped) srun: error: jeanzayiam25: task 24: Exited with exit code 1 srun: error: jeanzayiam43: task 42: Exited with exit code 1 srun: error: jeanzayiam45: task 44: Exited with exit code 1 srun: error: jeanzayiam36: task 35: Exited with exit code 1 srun: error: jeanzayiam40: task 39: Exited with exit code 1 srun: error: jeanzayiam04: task 3: Exited with exit code 1 srun: error: jeanzayiam12: task 11: Exited with exit code 1 srun: error: jeanzayiam16: task 15: Exited with exit code 1 srun: error: jeanzayiam32: task 31: Exited with exit code 1 srun: error: jeanzayiam09: task 8: Exited with exit code 1 srun: error: jeanzayiam38: task 37: Exited with exit code 1 srun: error: jeanzayiam34: task 33: Exited with exit code 1 srun: error: jeanzayiam28: task 27: Exited with exit code 1 srun: error: jeanzayiam20: task 19: Exited with exit code 1 srun: error: jeanzayiam30: task 29: Exited with exit code 1 srun: error: jeanzayiam10: task 9: Exited with exit code 1 srun: error: jeanzayiam27: task 26: Exited with exit code 1 srun: error: jeanzayiam48: task 47: Exited with exit code 1 srun: error: jeanzayiam39: task 38: Exited with exit code 1 srun: error: jeanzayiam07: task 6: Segmentation fault (core dumped) srun: error: jeanzayiam02: task 1: Segmentation fault (core dumped) srun: error: jeanzayiam22: task 21: Segmentation fault (core dumped) srun: error: jeanzayiam44: task 43: Segmentation fault (core dumped) srun: error: jeanzayiam18: task 17: Segmentation fault (core dumped) srun: error: jeanzayiam11: task 10: Segmentation fault (core dumped) srun: error: jeanzayiam01: task 0: Exited with exit code 1 ``` where the bt is: ``` (gdb) bt CC(未找到相关数据)  0x000014557cdfb9bf in sigsuspend () from /lib64/libpthread.so.0 CC(Matrix multiplication operator)  0x0000000000000000 in ?? () (gdb) thread apply all bt Thread 3 (LWP 181629): warning: Section `.regxstate/181629' in core file too small. CC(未找到相关数据)  0x000014557cb15a41 in process_entry () from /lib64/libc.so.6 CC(Matrix multiplication operator)  0x0000000000000000 in ?? () Thread 2 (LWP 181583): warning: Section `.regxstate/181583' in core file too small. CC(未找到相关数据)  0x000014557cdf9bd6 in sem_close () from /lib64/libpthread.so.0 CC(Matrix multiplication operator)  0x000055cdf1a75d30 in ?? () at /tmp/build/80754af9/pythonsplit_1634043551344/work/Modules/_threadmodule.c:64 CC(Don't support legacy Python)  0x00001454e4000b60 in ?? () CC(PEP8)  0x0000000000000000 in ?? () Thread 1 (LWP 181630): CC(未找到相关数据)  0x000014557cdfb9bf in sigsuspend () from /lib64/libpthread.so.0 CC(Matrix multiplication operator)  0x0000000000000000 in ?? () ``` perhaps it's totally unrelated to the launcher and something that this framework causes..."
yi,Fix CUDA error when multiplying sparse hybrid tensors with zero dense dimensions,Fixes  CC(TestSparse misses out on TestCase.setUp() + thus disabling doesn't work)   CC(Fix CUDA error when multiplying sparse hybrid tensors with zero dense dimensions) Differential Revision: D34478521,2022-02-25T16:09:48Z,module: sparse open source cla signed release notes: sparse topic: bug fixes,closed,1,3,https://github.com/pytorch/pytorch/issues/73428,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73428**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit dd6b42f062 (more details on the Dr. CI page):  * **1/2** failures introduced in this PR * **1/2** broken upstream at merge base 590685dc6e on Feb 25 from  7:37am to  9:03am   1 failure *not* recognized by patterns:    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * linuxxenialpy3.7clang7onnx / test (default, 1, 2, linux.2xlarge) on Feb 25 from  7:37am to  9:03am (590685dc6e  3c45fc8e20)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
transformer,ONNX export failed for torch_scatter ops in TAPAS model," 🐛 Describe the bug I am trying to convert the TAPAS model from HuggingFace into ONNX format. However, it has torchscatter dependency which cannot be export to ONNX. For easy replication:  ```     import torch     from transformers import TapasForQuestionAnswering     model_name = ""google/tapasbasefinetunedwtq""     model = TapasForQuestionAnswering.from_pretrained(model_name, torchscript=True)     bs = 1     seq_len = 512     dummy_inputs = (torch.ones(bs, seq_len, dtype=torch.long), torch.ones(bs, seq_len), torch.zeros(bs, seq_len, 7, dtype=torch.long))     torch.onnx.export(         model,         dummy_inputs,         args.save,         export_params=True,         opset_version=13,         input_names=[""input_ids"", ""attention_mask"", ""token_type_ids""],         output_names=[""logits"", ""logits_aggregation""],         dynamic_axes={""input_ids"": [0, 1], ""token_type_ids"": [0, 1], ""attention_mask"": [0, 1], ""logits"": [0, 1], ""logits_aggregation"": [0, 1]},     ) ``` And I am getting the following error msg: ``` Traceback (most recent call last):   File ""export.py"", line 21, in      torch.onnx.export(   File ""/opt/conda/lib/python3.8/sitepackages/torch/onnx/__init__.py"", line 301, in export     return utils.export(model, args, f, export_params, verbose, training,   File ""/opt/conda/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 118, in export     _export(model, args, f, export_params, verbose, training, input_names, output_names,   File ""/opt/conda/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 728, in _export     _model_to_graph(model, args, verbose, input_names,   File ""/opt/conda/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 503, in _model_to_graph     graph = _optimize_graph(graph, operator_export_type,   File ""/opt/conda/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 232, in _optimize_graph     graph = torch._C._jit_pass_onnx(graph, operator_export_type)   File ""/opt/conda/lib/python3.8/sitepackages/torch/onnx/__init__.py"", line 358, in _run_symbolic_function     return utils._run_symbolic_function(*args, **kwargs)   File ""/opt/conda/lib/python3.8/sitepackages/torch/onnx/utils.py"", line 1192, in _run_symbolic_function     raise RuntimeError(""ONNX export failed on an operator with unrecognized namespace {}::{}. "" RuntimeError: ONNX export failed on an operator with unrecognized namespace torch_scatter::scatter_min. If you are trying to export a custom operator, make sure you registered it with the right domain and version. ```  Versions Collecting environment information... PyTorch version: 1.11.0a0+bfe5ad2 Is debug build: False CUDA used to build PyTorch: 11.6 ROCM used to build PyTorch: N/A OS: Ubuntu 20.04.3 LTS (x86_64) GCC version: (Ubuntu 9.3.017ubuntu1~20.04) 9.3.0 Clang version: Could not collect CMake version: version 3.21.3 Libc version: glibc2.31 Python version: 3.8.12  (default, Oct 12 2021, 21:59:51)  [GCC 9.4.0] (64bit runtime) Python platform: Linux5.4.01066awsx86_64withglibc2.10 Is CUDA available: True CUDA runtime version: 11.6.55 GPU models and configuration: GPU 0: Tesla T4 Nvidia driver version: 450.142.00 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.3.2 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.3.2 HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] numpy==1.22.0 [pip3] pytorchquantization==2.1.2 [pip3] torch==1.11.0a0+bfe5ad2 [pip3] torchscatter==2.0.9 [pip3] torchtensorrt==1.1.0a0 [pip3] torchtext==0.12.0a0 [pip3] torchvision==0.12.0a0 [conda] magmacuda110             2.5.2                         5    local [conda] mkl                       2019.5                      281    condaforge [conda] mklinclude               2019.5                      281    condaforge [conda] numpy                     1.22.0           py38h6ae9a64_0    condaforge [conda] pytorchquantization      2.1.2                    pypi_0    pypi [conda] torch                     1.11.0a0+bfe5ad2          pypi_0    pypi [conda] torchscatter             2.0.9                    pypi_0    pypi [conda] torchtensorrt            1.1.0a0                  pypi_0    pypi [conda] torchtext                 0.12.0a0                 pypi_0    pypi [conda] torchvision               0.12.0a0                 pypi_0    pypi",2022-02-24T21:22:01Z,module: onnx triaged,closed,1,5,https://github.com/pytorch/pytorch/issues/73388,Here is some documentation to get you started on exporting custom operators not in the default namespaces https://pytorch.org/docs/stable/onnx.htmlcustomoperators,"I also have this problem, for `torch_scatter::scatter_max` when trying to export a PyG GAT model to ONNX. Did anyone find a way to support `torch_scatter` ops during ONNX export? A related issue https://github.com/rusty1s/pytorch_scatter/issues/181 and followup https://github.com/pygteam/pytorch_geometric/discussions/3514 aren't particularly helpful.","For TAPAS model at least, the following naive scatter implementation can be used to replace the PyTorch Geometric's scatter ops and let onnx export succeed. Note: This work was intended to make TAPAS running in TensorRT, rather than in PyTorch  in PyTorch it's definitely much slower than PyG, but in TensorRT the endtoend is great for inference. ```     def scatter_naive1D(src, index, dim, dim_size, reduce):          Assume 1D tensor.          assert dim == 0 and src.dim() == 1 and index.dim() == 1, ""Only support 1D scatter!""         def reduce_op(l, op):             result = 0             if op == 'min':                 result = torch.min(l)             elif op == 'max':                 result = torch.max(l)             elif op == 'sum':                 result = torch.sum(l)             elif op == 'mean':                 result = torch.mean(l)             else:                 assert False, 'Reduce op not supported!'             return result          output = torch.zeros(dim_size, dtype=src.dtype, device=src.device)         index_range = torch.max(index) + 1         bucket = [[] for _ in range(index_range)]         for i in range(len(src)):             bucket[index[i]].append(src[i])         for i in range(len(bucket)):             if bucket[i]:                 bucket[i] = reduce_op(torch.as_tensor(bucket[i], dtype=src.dtype, device=src.device), reduce)             else:                 bucket[i] = 0         output[:len(bucket)] = torch.as_tensor(bucket, dtype=src.dtype, device=src.device)         return output ```","Thanks , unfortunately I need 2D `scatter_max`. I'm looking into whether we can get this supported directly, see https://github.com/onnx/onnx/issues/4322 and https://github.com/pygteam/pytorch_geometric/issues/728.","The following repro ````python import torch from transformers import TapasForQuestionAnswering model_name = ""google/tapasbasefinetunedwtq"" model = TapasForQuestionAnswering.from_pretrained(model_name, torchscript=True) bs = 1 seq_len = 512 dummy_inputs = (     torch.ones(bs, seq_len, dtype=torch.long),     torch.ones(bs, seq_len),     torch.zeros(bs, seq_len, 7, dtype=torch.long), ) torch.onnx.export(     model,     dummy_inputs,     ""scatter.onnx"",     export_params=True,     opset_version=13,     input_names=[""input_ids"", ""attention_mask"", ""token_type_ids""],     output_names=[""logits"", ""logits_aggregation""],     dynamic_axes={         ""input_ids"": [0, 1],         ""token_type_ids"": [0, 1],         ""attention_mask"": [0, 1],         ""logits"": [0, 1],         ""logits_aggregation"": [0, 1],     }, ) ```` Produces ````bash /home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py:2039: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input_ids   warnings.warn( /home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py:2039: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input token_type_ids   warnings.warn( /home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py:2039: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input attention_mask   warnings.warn( /home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py:2039: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input logits   warnings.warn( /home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py:2039: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input logits_aggregation   warnings.warn( /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1613: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   self.indices = torch.as_tensor(indices) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1614: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   self.num_segments = torch.as_tensor(num_segments, device=indices.device) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1719: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   batch_size = torch.prod(torch.tensor(list(index.batch_shape()))) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1795: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   [torch.as_tensor([1], dtype=torch.long), torch.as_tensor(vector_shape, dtype=torch.long)], dim=0 /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1798: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   flat_values = values.reshape(flattened_shape.tolist()) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1804: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   dim_size=int(flat_index.num_segments), /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1811: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   torch.as_tensor(index.batch_shape(), dtype=torch.long), /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1812: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   torch.as_tensor([index.num_segments], dtype=torch.long), /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1813: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   torch.as_tensor(vector_shape, dtype=torch.long), /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1818: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   output_values = segment_means.view(new_shape.tolist()) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1746: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   batch_shape = torch.as_tensor( /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1750: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   num_segments = torch.as_tensor(num_segments)   create a rank 0 tensor (scalar) containing num_segments (e.g. 64) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1761: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   new_shape = [int(x) for x in new_tensor.tolist()] /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1764: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   multiples = torch.cat([batch_shape, torch.as_tensor([1])], dim=0) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1765: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   indices = indices.repeat(multiples.tolist()) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:321: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   torch.as_tensor(self.config.max_position_embeddings  1, device=device), position  first_position /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1273: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   indices=torch.min(row_ids, torch.as_tensor(self.config.max_num_rows  1, device=row_ids.device)), /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1278: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   indices=torch.min(column_ids, torch.as_tensor(self.config.max_num_columns  1, device=column_ids.device)), /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1976: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   column_logits += CLOSE_ENOUGH_TO_LOG_ZERO * torch.as_tensor( /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:1981: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   column_logits += CLOSE_ENOUGH_TO_LOG_ZERO * torch.as_tensor( /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:2017: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   labels_per_column, _ = reduce_sum(torch.as_tensor(labels, dtype=torch.float32, device=labels.device), col_index) /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:2040: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   torch.as_tensor(labels, dtype=torch.long, device=labels.device), cell_index /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:2047: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   column_mask = torch.as_tensor( /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:2072: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   selected_column_id = torch.as_tensor( /home/thiagofc/miniconda3/envs/dev1cu113py39/lib/python3.9/sitepackages/transformers/models/tapas/modeling_tapas.py:2077: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.   selected_column_mask = torch.as_tensor( Traceback (most recent call last):   File ""/home/thiagofc/dev/pytorch_repros/stale_repro.py"", line 15, in      torch.onnx.export(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 504, in export     _export(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 1529, in _export     graph, params_dict, torch_out = _model_to_graph(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 1115, in _model_to_graph     graph = _optimize_graph(   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 663, in _optimize_graph     graph = _C._jit_pass_onnx(graph, operator_export_type)   File ""/home/thiagofc/dev/github/pytorchdev1/torch/onnx/utils.py"", line 1909, in _run_symbolic_function     raise errors.UnsupportedOperatorError( torch.onnx.errors.UnsupportedOperatorError: ONNX export failed on an operator with unrecognized namespace torch_scatter::scatter_min. If you are trying to export a custom operator, make sure you registered it with the right domain and version. ``` meaning it does not recognize `torch_scatter::scatter_min` as an operator. Indeed, `torch.onnx.export` only supports operators with domain `aten::*`.  As pointed out by , a custom operator is needed to support this scenario as documented at https://pytorch.org/docs/master/onnx.htmlcustomoperators"
rag,Integrate Hierarchical Model Averaging with PostLocalSGDOptimizer," 🚀 The feature, motivation and pitch See https://github.com/pytorch/pytorch/pull/73285discussion_r814232287 Since the current periodic model averager is a special case (2level) hierarchical model averaging, and it can be embedded into `PostLocalSGDOptimizer`, the new hierarchical model averaging module should be able to run in the same way.  Alternatives _No response_  Additional context _No response_ ",2022-02-24T21:01:15Z,oncall: distributed,closed,0,1,https://github.com/pytorch/pytorch/issues/73382,"Another nice usability improvement is renaming `PostLocalSGDOptimizer` as `ModelAveragingOptimizer`, since it can not only support post local SGD, but also (post) hierarchical SGD. cc: varma  "
agent,[Dynamic RPC] Allow for optional world_size argument in init_rpc,"Stack from ghstack:  CC([Dynamic RPC] Add graceful shutdown for dynamic RPC members) [Dynamic RPC] Add graceful shutdown for dynamic RPC members  CC([Dynamic RPC] Allow existing ranks to communicate with newly joined ranks) [Dynamic RPC] Allow existing ranks to communicate with newly joined ranks  CC([Dynamic RPC] Allow newly joined ranks to communicate with existing ranks) [Dynamic RPC] Allow newly joined ranks to communicate with existing ranks * * CC([Dynamic RPC] Allow for optional world_size argument in init_rpc) [Dynamic RPC] Allow for optional world_size argument in init_rpc** This PR which allows for optional `world_size` argument in init_rpc. This makes changes in rendezvous to allow for `NoneType` for world_size and creates a new code path when initializing TensorPipe agent for init_rpc. The TensorPipe agent is protected by a critical section enforced using the store, so that only one node can create a TPAgent at a time. This PR does not yet enable RPC commands between ranks. Previously: ```python os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '29500' init_rpc(""worker0"", world_size=1, rank=0) ``` Now (only rank is needed): ```python os.environ['MASTER_ADDR'] = 'localhost' os.environ['MASTER_PORT'] = '29500' init_rpc(""worker0"", rank=0) ``` Tp run the added tests: `pytest test/distributed/rpc/test_tensorpipe_agent.py vsk test_init_rpc_without_world_size` `pytest test/distributed/rpc/test_tensorpipe_agent.py vsk test_init_dynamic_and_static_rpc_group` Differential Revision: D34621651",2022-02-24T19:46:36Z,oncall: distributed cla signed release notes: distributed (rpc) topic: new features,closed,0,6,https://github.com/pytorch/pytorch/issues/73372,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73372**  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit ce6a56dfaf (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"Huang has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","> And we were thinking it is because TensorPipe used the same pipe for request and response.  is this correct assumption? I don't know what's the context for this, and this doesn't really depend on TensorPipe but on how the RPC agent chooses to use TensorPipe. Last time I checked this was the case:  The A>B pipe was used for A sending requests to B and B sending responses to A  The B>A pipe (which is separate!) was used for B sending requests to A and A sending responses to B","Huang has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Huang has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.", retest this please
rag,[Model Averaging] Support hierarchical model averaging,"Implement hierarchical model averaging proposed in  CC([RFC] Hierarchical Model Averaging (Hierarchical SGD)). Unit tests are added. Since I don't have access to 4GPU machines in opensource environment, expect that the branch with the prefix of `ciall` can run the test that requires 4 GPUs. In the future, the internals of `PeriodicModelAveraging` can be simplified as an implementation of a specialized hierarchical model averaging, where `period_group_size_dict` only has a pair of period and world size.",2022-02-23T07:08:53Z,oncall: distributed open source cla signed release notes: distributed (ddp) topic: new features,closed,0,4,https://github.com/pytorch/pytorch/issues/73285,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73285**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 1a233963d4 (more details on the Dr. CI page):  * **2/2** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5327261059?check_suite_focus=true) winvs2019cuda11.3py3 / test (default, 1, 2, windows.8xlarge.nvidia.gpu) (1/1) **Step:** ""Test"" (full log   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",failures are unerlated
transformer,LazyLinear with equal in_features and out_features," 🚀 The feature, motivation and pitch I find the LazyLinear module very useful in a particular case of applying an MLP after some feature extractor (e.g., Transformer of some convnet). In this case, the output dimensionality of the feature extractor depends on the choice of a backbone net, so `LazyLinear` allows one not to think about passing the dimensionality to an MLP constructor. However, it is common practice to build MLPs with multiple hidden layers of the same size: `feature_extractor (768) > Linear(768, 768) > ReLU > Linear(768, n_out)`. For instance, this is done in the RoBERTa classification head. Here I discovered that I still need to pass (and calculate) the output dimensionality of the feature extractor since I need to define the `out_features` parameter of the `LazyLinear`. I think it would be great to have a subclass of `LazyLinear` that infers the number of input features and creates a square parameter matrix.  Alternatives I think now it is possible to derive a subclass of LazyLinear and override `initialize_parameters` method.  Additional context _No response_ ",2022-02-21T12:13:41Z,module: nn triaged needs research,open,0,1,https://github.com/pytorch/pytorch/issues/73172,"Hey , thanks for the request! As you mentioned, it should be straightforward to create such a square layer by subclassing `LazyLinear` or `LazyModuleMixin`, but please feel free to post here if you run into problems doing so. Note that, to reduce maintenance costs, we try to minimize the set of modules we provide in PyTorch core to those that are widely used or expected to be present. If this request becomes more popular, we can consider providing such a module in torch.nn."
yi,Fix overflow check in `geometry_is_contiguous`,  CC(Fix overflow check in `geometry_is_contiguous`)  CC(Remove native_functions.yaml dependency from CUDA distributions) The existing check isn't safe for 32bit `size_t` because the max 64bit int will overflow. Differential Revision: D34524229,2022-02-20T20:03:06Z,open source cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/73162,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73162**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 82aa437d4c (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
yi,How do we handle metadata-modifying in-place operators (like `squeeze_`) with `__torch_dispatch__`?,"``` import torch from torch.utils._pytree import tree_map aten = torch.ops.aten class LoggingTensor(torch.Tensor):     elem: torch.Tensor     __slots__ = ['elem']          def __new__(cls, elem, *args, **kwargs):          The wrapping tensor (LoggingTensor) shouldn't hold any          memory for the class in question, but it should still          advertise the same device as before         r = torch.Tensor._make_wrapper_subclass(   type: ignore[attrdefined]             cls, elem.size(),             strides=elem.stride(), storage_offset=elem.storage_offset(),              TODO: clone storage aliasing             dtype=elem.dtype, layout=elem.layout,             device=elem.device, requires_grad=kwargs.get(""requires_grad"", False)         )          ...the real tensor is held as an element on the tensor.         r.elem = elem.detach() if r.requires_grad else elem         return r     def __repr__(self):         return f""LoggingTensor({self.elem})""          def __torch_dispatch__(cls, func, types, args=(), kwargs=None):         def unwrap(e):             return e.elem if isinstance(e, LoggingTensor) else e         def wrap(e):             return LoggingTensor(e) if isinstance(e, torch.Tensor) else e          no_dispatch is only needed if you use enable_python_mode.          It prevents infinite recursion.         rs = tree_map(wrap, func(*tree_map(unwrap, args), **tree_map(unwrap, kwargs)))         return rs a = LoggingTensor(torch.randn(1, 5)) print(a.shape, a.elem.shape) a.squeeze_(0) print(a.shape, a.elem.shape) ``` This doesn't modify the tensor's shape, as it's inplace. Usually, we can address this by checking whether an operator is inplace or not (https://github.com/pytorch/functorch/blob/main/functorch/_src/python_key.pyL123), but that only allows us to modify the underlying `elem` tensor. How do we update the metadata here? cc:     ",2022-02-20T00:56:15Z,triaged module: __torch_dispatch__,open,0,3,https://github.com/pytorch/pytorch/issues/73150,Need to synchronize the metadata after the operation. If we had a tag for these cases you could only do the synchronization for operators which are known to modify metadata (there aren't that many). Does `resize_` work?,> Need to synchronize the metadata after the operation. Is there a way of doing this now? > Does resize_ work? No,"Here's how we did it for CompositeCompliantTensor. I don't know if this works in general, but it would be nice to have some blessed way of doing this: https://github.com/pytorch/pytorch/blob/932adf26e4b88e97425021aca492c4efd156e1e8/torch/testing/_internal/composite_compliance.pyL136L165"
rag,Support writing tensorboard traces to AWS S3 (and other cloud storage services) in profiler," 🚀 The feature, motivation and pitch I would like to write profiler logs directly to S3 so that they can be read from tensorboard remotely. Details of datasets and model runs can already be logged in a tensorboard format and saved directly to S3 with a call like ``` import torch log_dir = ""s3://bucket/prefix/"" writer = torch.utils.tensorboard.SummaryWriter(log_dir=log_dir) writer.add_scalar(""test"", 1) writer.close() ``` Those logs can be displayed running `tensorboard logdir=s3://bucket/prefix/`. _Profiling_ logs are handled somewhat differently, and while the pytorch profiler tb_plugin instructions show how to _read_ logs from S3 into a tensorboard instance, currently it doesn't seem possible to _write_ logs to S3 while profiling. Specifically, an example like: ``` with torch.profiler.profile(on_trace_ready=torch.profiler.tensorboard_trace_handler(""s3://bucket/prefix"")) as prof:     ...     prof.step() ``` does not write logs to S3 but instead tries to create a local dir of the same name and raises an error `ValueError: No logger registered for the s3 protocal prefix`. (I am unsure whether this should be filed as a bug or a feature request, but since the documentation doesn't spell out that this should be possible I'd consider it the former)  Alternatives A current workaround could be to write all tensorboard logs to a local dir, then copy to S3 with boto. But partial writing to S3 is already supported (in `SummaryWriter`, but not `profiler` AFAICT), and it would smoother if it was supported uniformly.  Additional context _No response_ ",2022-02-19T00:23:03Z,feature triaged module: tensorboard OSS contribution wanted,open,5,4,https://github.com/pytorch/pytorch/issues/73131,"This was briefly raised on discuss.pytorch.org earlier, link for reference: https://discuss.pytorch.org/t/canpytorchprofileradds3support/117397",This would be a great feature to have!," You might want to take a look at https://github.com/pytorch/kineto/blob/6968a24e3f2e72263e25d69a1a7f57dd0dd83181/libkineto/src/ActivityLoggerFactory.hL29 In principle, libkineto supports adding new protocols in this way, and IIRC this is already used internally in Meta for writing to remote storage. The error message you saw comes from the same file.",Any updates..?
rag,Disable test history as it's fragile,Related to CC(Improve test_test_history.py),2022-02-18T18:16:27Z,cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/73093,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73093**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 87860946a1 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   1 failure *not* recognized by patterns:   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. , merge this please, merge this,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
yi,Fix serialization and deepcopying for wrapper subclasses,"Stack from ghstack:  CC(Fix serialization and deepcopying for wrapper subclasses) This PR addresses broken support for serialization and deepcopy on wrapper subclasses. In particular, the old implementations of `Tensor.__reduce_ex__` and `Tensor.__deepcopy__` fail by assuming the tensor type has storage, which is not the case for wrapper subclasses. To fix this: * For serialization, `Tensor.__reduce_ex__` is expanded to return a new reconstruction function `_rebuild_wrapper_subclass` (that calls `_make_wrapper_subclass`) for cases for tensor subclasses where no storage exists. * For deepcopying, `Tensor.__deepcopy__` is expanded to call `clone()` for cases where no storage exists. The goal is to provide reasonable default implementations usable for cases where a custom tensor type uses `_make_wrapper_subclass()` and at most sets some additional copyable attributes. If anything more complex is needed, users are expected to write their own `__deepcopy__` / `__reduce_ex__` implementations for their type. Note that custom implementations for the above magic methods MUST handle copying the `_is_param` attribute that may be present on the custom tensor type instance (if we go this route for custom tensors as parameters). Differential Revision: D34342718",2022-02-18T15:32:01Z,cla signed release notes: python_frontend topic: bug fixes,closed,0,9,https://github.com/pytorch/pytorch/issues/73078,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/73078**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit fa2325fc07 (more details on the Dr. CI page):  * **4/4** failures introduced in this PR   :detective: 4 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5321769366?check_suite_focus=true) linuxxenialcuda11.3py3.7gcc7 / test (default, 2, 2, linux.4xlarge.nvidia.gpu) (1/4) **Step:** ""Unknown"" (full log  :repeat: rerun)   20220224T18:14:55.0770319Z   test_add_done_ca...arg() takes 0 positional arguments but 1 was given  ``` 20220224T18:14:55.0754496Z  20220224T18:14:55.0755487Z For more information about alternatives visit: ('https://numba.pydata.org/numbadoc/latest/cuda/overview.html', 'cudatoolkitlookup')[0m 20220224T18:14:55.0756810Z   warnings.warn(errors.NumbaWarning(msg)) 20220224T18:14:55.0757709Z C:\\Jenkins\\Miniconda3\\lib\\sitepackages\\numba\\cuda\\envvars.py:17: NumbaWarning: [1m 20220224T18:14:55.0759130Z Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\nvvm\\libdevice. 20220224T18:14:55.0760105Z  20220224T18:14:55.0761093Z For more information about alternatives visit: ('https://numba.pydata.org/numbadoc/latest/cuda/overview.html', 'cudatoolkitlookup')[0m 20220224T18:14:55.0762261Z   warnings.warn(errors.NumbaWarning(msg)) 20220224T18:14:55.0762803Z ok (0.773s) 20220224T18:14:55.0764344Z   test_add_done_callback_maintains_callback_order (__main__.TestFuture) ... ok (0.003s) 20220224T18:14:55.0770319Z   test_add_done_callback_no_arg_error_is_ignored (__main__.TestFuture) ... [E pybind_utils.h:201] Got the following error when running the callback: TypeError: no_arg() takes 0 positional arguments but 1 was given 20220224T18:14:55.0771719Z ok (0.001s) 20220224T18:14:55.0789163Z   test_add_done_callback_simple (__main__.TestFuture) ... ok (0.002s) 20220224T18:14:55.0856959Z   test_chained_then (__main__.TestFuture) ... ok (0.006s) 20220224T18:14:55.1891573Z   test_collect_all (__main__.TestFuture) ... ok (0.103s) 20220224T18:14:55.1902818Z   test_done (__main__.TestFuture) ... ok (0.001s) 20220224T18:14:55.1922619Z   test_done_exception (__main__.TestFuture) ... ok (0.002s) 20220224T18:14:55.1949143Z   test_interleaving_then_and_add_done_callback_maintains_callback_order (__main__.TestFuture) ... ok (0.003s) 20220224T18:14:55.1964977Z   test_interleaving_then_and_add_done_callback_propagates_error (__main__.TestFuture) ... [E pybind_utils.h:201] Got the following error when running the callback: ValueError: Expected error 20220224T18:14:55.1965930Z  20220224T18:14:55.1966818Z At: ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","> I think we will want to extend this testing to cover more common subclass implementation. But this PR seems to be a step in the right direction. True, the custom tensors as parameters PR I'm working on has much more extensive tests run across currently 5 different subclass impls.", merge this please,Merge failed due to Command `git C /home/runner/work/pytorch/pytorch cherrypick x 3fed15854337d0dd5b8c7980ee41e82ca7a32985` returned nonzero exit code 1 Automerging test/test_python_dispatch.py Automerging torch/_tensor.py CONFLICT (content): Merge conflict in torch/_tensor.py Raised by https://github.com/pytorch/pytorch/actions/runs/1893983006, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Rename `Typed/UntypedStorage` to `_Typed/_UntypedStorage` (#72540),"Cherrypicking https://github.com/pytorch/pytorch/pull/72540 for 1.11 release Rename UntypedStorage and TypedStorage to _UntypedStorage and _TypedStorage. At this point, users do not need to interact directly with these classes, and they are not documented yet. (cherry picked from commit 329238f612a9d92586bb0e5b33bcc45a0ec6936b)",2022-02-16T14:58:12Z,cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/72914,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72914**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit bc0f664f72 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5218316415?check_suite_focus=true) linuxbionicpy3.7clang9 / test (xla, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220216T15:26:10.3655393Z AttributeError: module 'torch' has no attribute 'complex32'  ``` 20220216T15:26:10.3650495Z   File ""/opt/conda/lib/python3.7/runpy.py"", line 263, in run_path 20220216T15:26:10.3650894Z     pkg_name=pkg_name, script_name=fname) 20220216T15:26:10.3651335Z   File ""/opt/conda/lib/python3.7/runpy.py"", line 96, in _run_module_code 20220216T15:26:10.3651590Z     mod_name, mod_spec, pkg_name, script_name) 20220216T15:26:10.3651837Z   File ""/opt/conda/lib/python3.7/runpy.py"", line 85, in _run_code 20220216T15:26:10.3652052Z     exec(code, run_globals) 20220216T15:26:10.3653768Z   File ""/var/lib/jenkins/workspace/xla/test/pytorch_test_base.py"", line 465, in  20220216T15:26:10.3654360Z     class XLATestBase(DeviceTypeTestBase): 20220216T15:26:10.3654694Z   File ""/var/lib/jenkins/workspace/xla/test/pytorch_test_base.py"", line 468, in XLATestBase 20220216T15:26:10.3654995Z     torch.half, torch.complex32, torch.complex64, torch.complex128 20220216T15:26:10.3655393Z AttributeError: module 'torch' has no attribute 'complex32' 20220216T15:26:10.5276385Z + cleanup 20220216T15:26:10.5276752Z + retcode=1 20220216T15:26:10.5277016Z + set +x 20220216T15:26:10.5322984Z [error]Process completed with exit code 1. 20220216T15:26:10.5382712Z [group]Run  Ensure the working directory gets chowned back to the current user 20220216T15:26:10.5383059Z [36;1m Ensure the working directory gets chowned back to the current user[0m 20220216T15:26:10.5383370Z [36;1mdocker run rm v ""$(pwd)"":/v w /v ""${ALPINE_IMAGE}"" chown R ""$(id u):$(id g)"" .[0m 20220216T15:26:10.5693773Z shell: /usr/bin/bash e {0} 20220216T15:26:10.5693960Z env: 20220216T15:26:10.5694161Z   BUILD_ENVIRONMENT: linuxbionicpy3.7clang9 ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
transformer,DISABLED test_transformer_module_apply (__main__.TestApply),"Platforms: linux     This test was disabled because it is failing in CI. See recent examples and the most recent     workflow logs.     Over the past 3 hours, it has been determined flaky in 1 workflow(s) with     2 red and 2 green. ",2022-02-16T09:39:54Z,oncall: distributed triaged module: flaky-tests skipped module: fsdp,closed,0,4,https://github.com/pytorch/pytorch/issues/72908,"Note that the last few pytorch bot comments refer the the same instance. This was due to a mistake in my query, which should be fixed by https://github.com/pytorch/testinfra/pull/200"," For oncall: distributed issues could we avoid adding triaged label, to make sure we discuss it during our distributed oncall meeting? Thank you! ",Ah okay. I will remove the bot functionality to add triaged as well for any of the labels as well.,"Cannot reproduce, possibly should have been fixed by https://github.com/pytorch/pytorch/pull/73314"
yi,Type signature for tools.codegen.api.lazy.isValueType is a bit suspect," 🐛 Describe the bug Current implementation looks like this: ``` def isValueType(typ: Union[Type, BaseCType, OptionalCType, ConstRefCType, MutRefCType,                            ListCType, ArrayRefCType, ArrayCType, VectorCType, TupleCType]) > bool:     """"""     Given a type, determine if it is a Valuelike type.  This is equivalent to     being Tensorlike, but assumes the type has already been transformed.     """"""     if isinstance(typ, BaseCType):         return typ.type == valueT     elif isinstance(typ, (OptionalCType, ListCType, VectorCType)):         return isValueType(typ.elem)     else:         return False ``` There are two problems: 1. Each CType is listed out explicitly. This is bad because if a new CType is added this site has to be manually updated. Instead, the CType union should have been used instead. This should be an easy fix and I can submit the patch for it. 2. This function is overloaded; it takes both Type and CType, which are semantically different concepts. To make matters worse, a it is never possible for a Type to return true. Calling isValueType on a Type is a code smell; it shouldn't be permitted in the signature at all.    Versions master",2022-02-15T14:20:26Z,triaged module: codegen,closed,0,0,https://github.com/pytorch/pytorch/issues/72852
yi,[DataPipe] Improve .pyi generation,Stack from ghstack:  CC([DataPipe] Improve .pyi generation) Make two functions more flexible and usable from a different repo. Differential Revision: D34227912,2022-02-15T00:03:14Z,module: data cla signed release notes: dataloader,closed,0,4,https://github.com/pytorch/pytorch/issues/72829,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72829**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 49e7c71b33 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
rag,Rename undocumented storage types,"Rename `UntypedStorage` and `TypedStorage` to `_UntypedStorage` and `_TypedStorage`. At this point, users do not need to interact directly with these classes, and they are not documented yet.",2022-02-14T18:28:53Z,open source cla signed,closed,0,1,https://github.com/pytorch/pytorch/issues/72802,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72802**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 4974722d7c (more details on the Dr. CI page):  * **2/2** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5188999970?check_suite_focus=true) linuxbionicpy3.7clang9 / test (xla, 1, 1, linux.2xlarge) (1/1) **Step:** ""Test"" (full log   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "
yi,Update lazy_ir.py from lazy_tensor_staging,"Summary: This diff contains changes from several PRs landed to lazy_tensor_staging branch.  generating 'fallback' overrides for each codegenned op, useful for debugging  supports operators which are missing aten:: symbols for op names, instead using their string counterpart  makes the IR class a base class instead of hardcoding the assumption of TS Test Plan: tested on lazy_tensor_staging branch Differential Revision: D34178476",2022-02-11T19:19:54Z,fb-exported cla signed Reverted topic: not user facing release notes: lazy,closed,0,7,https://github.com/pytorch/pytorch/issues/72730,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72730**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 0e717a8787 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   1 failure *not* recognized by patterns:   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34178476,This pull request was **exported** from Phabricator. Differential Revision: D34178476,This pull request was **exported** from Phabricator. Differential Revision: D34178476,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This pull request has been **reverted** by 889f3f48b2ef0fb27cff5a1a474d7c316fd7b5d4. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk).","This pull request has been **reverted** by 889f3f48b2ef0fb27cff5a1a474d7c316fd7b5d4. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk)."
transformer,[Vulkan] Implement GRU operator,"Summary: Implemented GRU operator in the Vulkan GPU backend: * This is an initial implementation to support an internal model. * Internal name for GRU is `aten::gru.input` * There should be 2 weights and 2 biases per layer. See GRU >> Variables section * For num_layers=1 the weights should contain [weight_ih, weight_hh, bias_ih, bias_hh] (4 elements) * Need to reshape input and hidden state to 2D since Vulkan `mm` and `addmm` ops accept only 2D dim * By design, all weights and biases should be on the CPU where input sequence and hidden state should be on the Vulkan GPU. * Input arguments and return values:     * `input_vk`: input tensor of shape (L, N, H_in) when batch_first=False or (N, L, H_in) when batch_first=True containing the features of the input sequence     * `hx_vk`: initial hidden state for each element in the batch. tensor of shape (D * num_layers, N, H_out)     * `output`: tensor of shape (N, L, D * H_out)) when batch_first=True     * `h_n`: tensor of shape (D * num_layers, N, H_out)     * where         * L = sequence length         * N = batch size         * D = 2 if bidirectional=True otherwise 1         * H_in = input_size ( of expected features in the input x)         * H_out = hidden_size ( of features in the hidden state h) * This initial implementation has some limitations:     * Tensor dim should be 3 for input sequence and hidden state.     * has_biases=True     * train=False     * bidirectional=False     * batch_first=True     * dropout=0.0     * D=1 since bidirectional=False     * N=1 (batch size)     * L=1 (sequence length) * GRU highlevel python code: ``` import torch from torch import nn import numpy as np import math H_in = 10 H_out = 10 num_layers = 2 D = 1 gru = nn.GRU(H_in, H_out, num_layers) input = torch.randn(1, 1, H_in) h0 = torch.randn(D * num_layers, 1, H_out) output, h_n = gru(input, h0) print(output) print(h_n) print(gru._all_weights)  the same result can be calculated directly x = input output = x h_n = [] for i in range(num_layers):     h = h0[i]     W_ih, W_hh, b_ih, b_hh = gru._flat_weights[i * 4 : (i + 1) * 4]     W_ir, W_iz, W_in = W_ih.split(H_in)     W_hr, W_hz, W_hn = W_hh.split(H_in)     b_ir, b_iz, b_in = b_ih.split(H_in)     b_hr, b_hz, b_hn = b_hh.split(H_in)     r = torch.sigmoid(x @ W_ir.T + b_ir + h @ W_hr.T + b_hr)     z = torch.sigmoid(x @ W_iz.T + b_iz + h @ W_hz.T + b_hz)     n = torch.tanh(x @ W_in.T + b_in + r * (h @ W_hn.T + b_hn))     h = (1  z) * n + z * h     x = h     output = x     h_n.append(h[0]) print(output) print(h_n) ``` * References     * PyTorch Docs > torch.nn > GRU     * Dive into Deep Learning > 9.1. Gated Recurrent Units (GRU)     * Gated Recurrent Unit (GRU) With PyTorch     * From GRU to Transformer Test Plan: Build & test on Android: ``` cd ~/fbsource buck build c ndk.custom_libcxx=false c pt.enable_qpl=0 //xplat/caffe2:pt_vulkan_api_test_binAndroid\\androidarm64 showoutput adb push buckout/gen/xplat/caffe2/pt_vulkan_api_test_binAndroid\\androidarm64 /data/local/tmp/vulkan_api_test adb shell ""/data/local/tmp/vulkan_api_test"" ``` Test result on Android (Google Pixel 5): ``` [ RUN      ] VulkanAPITest.gru_mclareninputs_success [       OK ] VulkanAPITest.gru_mclareninputs_success (59 ms) [ RUN      ] VulkanAPITest.gru_invalidinputs_exceptions [       OK ] VulkanAPITest.gru_invalidinputs_exceptions (17 ms) ``` Differential Revision: D33995221",2022-02-11T01:34:40Z,fb-exported cla signed release notes: vulkan topic: new features,closed,0,3,https://github.com/pytorch/pytorch/issues/72692,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72692**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 8d4de67255 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33995221,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
llm,[tools_common] Don't remove underscores from call_module targets in get_acc_ops_name,Test Plan: CI. Reviewed By: wushirong Differential Revision: D34148357,2022-02-10T18:49:03Z,fb-exported cla signed module: fx release notes: fx topic: bug fixes,closed,0,5,https://github.com/pytorch/pytorch/issues/72664,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72664**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 5f2a0d2946 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D34148357,This pull request was **exported** from Phabricator. Differential Revision: D34148357,This pull request was **exported** from Phabricator. Differential Revision: D34148357,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
transformer,upstream `apex.normalization.FusedRMSNorm`," 🚀 The feature, motivation and pitch All T5 models and their derivatives (t5, mt5, t0, etc.) use `RMSNorm`, instead of `LayerNorm`. The former is a subset of the latter, it only scales and doesn't shift. The original need was a discovery that all HF Transformers t5based models were somewhat slow under mixed precision, because of ""manual"" implementation of `T5LayerNorm` where manual up/down casting was causing a significant bottleneck. While researching this I have run into other users who wanted to use a fast `RMSNorm` (but didn't save the references) NVIDIA/apex recently implemented `apex.normalization.FusedRMSNorm` but building apex is far from easy for a lay person.  I have benchmarked it in an ensemble and it gives a pretty significant gain  about 10% improvement on the full backtoback application. https://github.com/huggingface/transformers/pull/14656  so clearly multiple times faster on just the norm part. So to ease user's path to faster t5based models if possible it'd be great to have this subset functionality of `LayerNorm` available in pytorch. It's already in the nvfused branch: https://github.com/csarofeen/pytorch/pull/1428 I will see if I can find other users who may want a fast `RMSNorm` Thank you! ",2022-02-10T01:41:39Z,feature module: nn triaged module: norms and normalization,open,12,42,https://github.com/pytorch/pytorch/issues/72643,"(if it's a variant of LayerNorm, should it be supported by LayerNorm natively, e.g. if weight is not None and bias is None, it should call this new fused kernel?)","From https://arxiv.org/abs/1910.07467 > When the mean of summed inputs is zero, RMSNorm is exactly equal to LayerNorm.  !snapshot_10 So it's the same as LayerNorm, but: 1. mean is always 0. 2. no bias  only weight  which is how it's done in T5  perhaps biasenabling should be configurable?","> if it's a variant of LayerNorm, should it be supported by LayerNorm natively, e.g. if weight is not None and bias is None, it should call this new fused kernel?  the problem is that you also want to ignore the average term (E[x]) there? So that would need to be a new boolean flag to enable that. Not sure if that is better than adding a new `RMSNorm` function.","It'd be awesome to have this available in PyTorch!! It was used again, in this big paper: https://arxiv.org/pdf/2112.11446.pdf","Another big model using it: https://arxiv.org/abs/2302.13971   I can help add it if necessary, but in terms of design choices which is better:   adding boolean flag to enable them in LayerNorm?   Creating a new `RMSNorm` class?  Also I've noticed that there seems to be a test for a fused version: https://github.com/pytorch/pytorch/blame/5dd52e250f66a5e3377eb39228cd929871f1eb5d/test/functorch/test_memory_efficient_fusion.pyL155","Hey! Sorry for the delay in answering, it is a tricky one. Given the similarity with LayerNorm I think we want to move forward as follows:  Add a `no_bias` kwarg to LayerNorm which can be used to enable this behavior.  Make sure the doc for this arg mentions RMSNorm explicitly so that it is discoverable by that name.  Using torch.compile should gives you the same performance as the fused implementation from apex. So we don't want to add the fused implementation in PyTorch.","> Using torch.compile should gives you the same performance as the fused implementation from apex. So we don't want to add the fused implementation in PyTorch. `torch.compile` is very very very very far from being ready for general use. Most of my attempts at using it failed. You can see multiple bug reports I have filed about it. It only works in very specific situations. But if this request is put on hold for another year it might come through. Which is fine too, since we do have `apex`. It's just a big pain to install.","`torch.compile` along with `JIT` before it has historically not worked well for NLP models which tend to be more dynamic. It'd be nice to prioritize some core development until compilers like that are stabilized.  tbh, if I could just use `torch.compile` for oneoff modules, that'd be sick. It feels like overkill to try to compile an entire model. ","Just fyi, RMSNorm isn't just LayerNorm without bias (otherwise we could have use the functional method that allows to not pass bias). You need also to remove mean estimation. Totally fine with the plan to use `torch.compile` as the main way forward. I'll just use `apex` version for now.", (also `bias=False` naming is probably better (to match nn.Linear's arguments' naming),"yes `bias` is not great as it is usually just the bias weight but here it also means the centering is also removed (you don't remove the average bias). Do you think `rms_only=True` would be a better name? > tbh, if I could just use torch.compile for oneoff modules, that'd be sick. It feels like overkill to try to compile an entire model.  this is the whole point of torch.compile (and the big difference with existing jit in ML frameworks) it is designed to work with partial graphs and small pieces! What I heard from  is that in the case of RMSNorm, torch.compile is able to generate a good compiled version. So wrapping just that one layer implementing it will give you a fused version.","Thank you for clarifying that one doesn't have to compile the whole model to fuse just one component, albanD. That's great! I disagree that this should be left to users. You want pytorch to be the winning framework, correct? Make it great out of the box. Expecting users to figure out that they need to `compile` `RMSNorm` to make it faster is not a great strategy, IMHO. If it works why not `compile` it by default then and give an option to opt out?","I tend to agree with  as pytorch core/domain libraries is an important source of idioms that are adopted by the community. So if RmsNorm module can be implemented trivially in core by torch.compiling around simple impl, it's great to have it in core + tests + perf tests. Then this could also be a showcase/doc reference of usecase where torch.compile works great (kind of dogfooding).","Yeah, if compiling works, why not offer outofthebox PyTorch modules that have been compiled together? Especially, modules that are used in stateoftheart models.  Most stateoftheart models cannot be built in PyTorch because they require many layers that are not readily available and are hard to implement. The community is needing to use libraries like `apex` which are hard to use. "," the problem is that we need to do a zerotoone of having torch.compile inside regular torch library code. This is a reasonable thing to want to do, but there hasn't really been any emphasis on it (since most of the effort has been on torch.compile with full libraries). Because there is no emphasis on this style of use case, there are big perf gaps (e.g., guard evaluation overhead matters a lot more in this regime). So to make your suggestion into reality, we need to also spend some time making this work well. Or we just enlist the OSS community's help in just adding the FusedRMSNorm into the framework directly and kick the can a few more months.","> I disagree that this should be left to users. You want pytorch to be the winning framework, correct? Make it great out of the box. This is definitely the long term vision where compile will be always there, optimizing what it can and leaving the rest as python code. In that world, users won't have to figure out they need compile. They just always use it and will get the perf out of the box. Right now we're in a weird transition period though as, as you said, this is not stable enough to be in this alwayson state. And so the question is still open on how to best spend our ressources: add more fused oneoff kernels that will be obsolete when compile is the default or work on making compile the default. I think our current stance is to still add these fused kernels for things that are critical right now or that compile will not handle well any time soon (SDPA, adamw for example) and focus the rest of our efforts to making compile the default. To come back to this particular issue, adding the new flag to be able to do RMSNorm is a definite ""yes we want it"" but adding the fused implementation is more ""we would accept a simple PR but no one on the core team is working on it"".","I think, having certain things in core that are torch.compile'd demonstrates to the users maturity of the technology, so it's a good goal to have by itself. Plus, in core you could have automated perf tests comparing it against legacy apex fused kernels. If wanted, this stuff could go into some separate experimental pytorch package (from where kernels can graduate into core). As long as it's built/released/tested along with the rest of pytorch, it's already strictly better than apex. Once pytorch core has torch.compile'd kernels, it will show true commitment to the technology","If things are unstable, yet, it'd be wasteful to allocate resources to do such porting because an automatic solution is ""imminent"", then I think it's perfectly fine to close this feature request and tell the user to use `apex` if they want speed  a user who can figure out how to compile `RMSNorm` will surely be able to build `apex`, so I don't really see any advantage here if it's not something available out of the box.","We **unify** the `LayerNorm` and `RMSNorm` in PreNormalization Transformers in our paper https://arxiv.org/abs/2305.14858. The arithmetic equivalence allows us to convert PreLayerNorm Transformers into PreRMSNorm models without impact on the model functionality. Considering that RMSNorm offers superior efficiency compared to LayerNorm in theory, we believe that providing an official RMSNorm API would greatly benefit the community, allowing them to harness this improvement in both training and inference effectively. We also release our implementation https://github.com/ZixuanJiang/prermsnormtransformer for reference. Thanks for your consideration.","Following up on the discussion here, from further discussion, it seems like `LayerNorm` is given by  $y = \\frac{x  \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$ where the $\\epsilon$ is added for numerical stability and `RMSNorm` is given by  $y = \\frac{x}{ RMS(x)} * \\gamma$ (with maybe an optional $+  \\beta$ per this commentissuecomment1035594258))  Where  $Var(x) = \\frac{1}{n}\\sum\\limits_{i=1}^{n}  (x_i  E[x])^2$ and  $RMS(x) =  \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}   (x_i)^2}$ ($\\epsilon$ could also be used for numerical stability in this calculation) so `RMSNorm` defers from `LayerNorm` on 3 counts  1) don't subtract `E[x]` from the numerator 2) use `RMS(x)` rather than $\\sqrt{Var(x) + \\epsilon}$ in the denominator 3) (perhaps) don't learn an elementwise affine bias  If my understanding here is correct, we would accept the addition of `RMSNorm` as a new `torch.nn.Module` that is separate from `LayerNorm`",It also seems that there's a RMSNorm impl in flashattention (fused with dropout): https://github.com/DaoAILab/flashattention/blob/4f285b354796fb17df8636485b9a04df3ebbb7dc/flash_attn/ops/rms_norm.pyL11,"Am I also understanding correctly that semantically `RMSNorm(x) := F.normalize(x, dim = dim, p = 2, eps = eps) * sqrt(D) * gamma`? modulo different treatment of eps (F.normalize clamps the denom by eps from below, and rmsnorm adds eps to the underthesqrt expression)",There is also an impl of RMSNorm in Triton at https://github.com/kakaobrain/trident/blob/main/trident/kernel/rms_norm.py  maybe can be incorporated into core?,+1,Any progress on this issue ? cc:  ,as a matter of fact the rmsnorm from FasterTransformer is much faster than Apex' https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/kernels/layernorm_kernels.cu,"Could you please share some factual information to support this claim, ?", I can try to make a unitary snippet but can tell you in this PR https://github.com/OpenNMT/OpenNMTpy/pull/2539/files it had a huge impact on inference tok/sec for a Mistral7B LM for instance. (separately from the other change kv_cache from flash2 that had also an impact).,Related:    CC(Compiling RMSNorm Triton Kernal gives error),Should probably be closed now
transformer,Add naive native FFN in aten,"Summary: This is an alias of feed forward network in transformer. We would like to use this API in the prototype of the new transformer encoder. Notice this diff might not bring any perf improvement since it is just a combination of aten ops, but we can improve its kernels later. fp16 is not supported in this version, due to the poor support of Half type in current aten::baddbmm. We should improve this in our future implemented fast FFN kernel. Test Plan: buck build mode/opt c fbcode.enable_gpu_sections=true caffe2/test:nn && buckout/gen/caffe2/test/nn\\binary.par r test_ffn Differential Revision: D33800859",2022-02-09T00:57:07Z,fb-exported cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/72564,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72564**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit d9bdecd5a8 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33800859
rag,add reduce package function to storage.py in order to deperecate presistent_id in OSS,Stack from ghstack:  CC(remove torch from package importer and add fbcode shims for BC)  CC(remove torch from package exporter)  CC(add reduce package function to storage.py in order to deperecate presistent_id in OSS)  CC([pkg] add generic ZipFile Reader/Writer) This PR also adds changes in order to allow for torchscript model serialization without importing torch into torch.package.PackageImporter or torch.package.PackageExporter. This allows us to remove the dependency of torch in torch.package.PackageImporter or torch.package.PackageExporter in the PRs on top of this. Differential Revision: D34096804,2022-02-09T00:47:16Z,oncall: jit cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/72563,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72563**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 0d528f14a9 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   1 failure *not* recognized by patterns:   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."," has imported this pull request.  If you are a Facebook employee, you can view this diff on Phabricator."
yi,Allow specifying tags for aten operators in native_functions.yaml,"Stack from ghstack:  CC(Allow specifying tags for aten operators in native_functions.yaml) This PR: 1. Adds a new yaml file. For each new tag, we will require an entry in this file along with a mandatory description. 2. There are checks to ensure that any tags added for native_functions are valid tags (i.e., they exist in tags.yaml) 3. Each NativeFunction object holds tags as a set of strings (this logic can be found in model.py) Most file changes are to update the call to `parse_native_yaml` to include the path to `native_functions.yaml` as well as `tags.yaml`",2022-02-08T22:27:12Z,cla signed Reverted release notes: composability,closed,0,11,https://github.com/pytorch/pytorch/issues/72549,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72549**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * Need help or want to give feedback on the CI? Visit our office hours  :pill: CI failures summary and remediations As of commit bc975370b0 (more details on the Dr. CI page):  * **1/1** failures introduced in this PR   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5697626730?check_suite_focus=true) trunk / linuxbionicrocm4.5py3.7distributed / test (distributed, 1, 1, linux.rocm.gpu) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220325T21:06:01.8892354Z AssertionError: Losses differ between local optimizer and ZeRO  ``` 20220325T21:06:01.8881332Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_internal/common_distributed.py"", line 96, in wrapper 20220325T21:06:01.8882318Z     return func(*args, **kwargs) 20220325T21:06:01.8883534Z   File ""/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py"", line 912, in test_local_optimizer_parity 20220325T21:06:01.8884543Z     check_step() 20220325T21:06:01.8885871Z   File ""/var/lib/jenkins/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py"", line 869, in check_step 20220325T21:06:01.8886992Z     msg=""Losses differ between local optimizer and ZeRO"", 20220325T21:06:01.8888453Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_comparison.py"", line 1331, in assert_close 20220325T21:06:01.8889355Z     msg=msg, 20220325T21:06:01.8890570Z   File ""/opt/conda/lib/python3.7/sitepackages/torch/testing/_comparison.py"", line 1084, in assert_equal 20220325T21:06:01.8891498Z     raise error_metas[0].to_error() 20220325T21:06:01.8892354Z AssertionError: Losses differ between local optimizer and ZeRO 20220325T21:06:01.8892914Z  20220325T21:06:01.8892928Z  20220325T21:06:01.8892941Z  20220325T21:06:01.8893645Z  20220325T21:06:01.8894431Z Ran 48 tests in 326.346s 20220325T21:06:01.8894738Z  20220325T21:06:01.8895059Z FAILED (errors=2, skipped=23, unexpected successes=6) 20220325T21:06:01.8895440Z  20220325T21:06:01.8895642Z Generating XML reports... 20220325T21:06:01.8972699Z Generated XML report: testreports/pythonunittest/distributed.optim.test_zero_redundancy_optimizer/TESTTestZeroRedundancyOptimizerDistributed20220325210035.xml ```   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","oops, didn't see not for review. Unsubscribing for now, please re","These tags still aren't exposed from Python, is that right? It would be nice to parse in the descriptions so that we can then generate appropriate `__doc__` on the Python side representation (seems better than slinging strings around in Python).","> These tags still aren't exposed from Python, is that right? It would be nice to parse in the descriptions so that we can then generate appropriate `__doc__` on the Python side representation (seems better than slinging strings around in Python). No they aren't yet but the next PR in the stack exposes the tags through the torch.ops API.", merge this please, merge this please,Merge failed due to PR 72549 does not match merge rules Raised by https://github.com/pytorch/pytorch/actions/runs/2041367071, merge this please,"Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.", revert this,Reverting as this change broke internal build systems. Significant build system changes still have to go thru the old land path
rag,Rename `Typed/UntypedStorage` to `_Typed/_UntypedStorage`,"Rename `UntypedStorage` and `TypedStorage` to `_UntypedStorage` and `_TypedStorage`. At this point, users do not need to interact directly with these classes, and they are not documented yet.",2022-02-08T19:41:55Z,module: internals triaged open source cla signed,closed,0,5,https://github.com/pytorch/pytorch/issues/72540,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72540**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit f0a7f47da2 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"> I am wondering about BC behavior of this renaming. Does this mean that any storage (and thus Tensor) saved before this PR won't be able to be loaded after this PR? Before this PR, when a `TypedStorage` is saved, it's saved in a backward and forward compatible format (assuming the two versions of pytorch support all the same dtypes) so that old versions of pytorch can load it into the corresponding `Storage` class. For the original PR CC([pytorch][PR] Reimplement Python storages as wrappers around an untyped storage), I did some BC/FC testing with this script to make sure of that: https://github.com/kurtamohler/pytorchperftestscripts/blob/master/untypedStorage/serialization_compat_test.py I just reran it now on this PR, and there do appear to be a couple small issues. I'll see what's going on and fix it","> I just reran it now on this PR, and there do appear to be a couple small issues. I'll see what's going on and fix it Turns out that these were just issues with my script, and I've fixed them. I had to make it recognize the `_TypedStorage` name change. Also, the test case for JIT serialization of the `nn.Linear` module was failing because the JIT code that it compiles into has changed slightly since 1.10, a change unrelated to `TypedStorage`. I decided to just use `nn.Conv` instead. I ran the script using the `save` option in pytorch 1.10, then ran it with the `load` option in pytorch 1.10, the current master branch, and the branch for this PR to check that saving from pytorch 1.10 is compatible with all three of these different versions of python. These all passed. I also saved from the master branch and this PR's branch, and those were also fully compatible. So it looks like changing `TypedStorage` to `_TypedStorage` won't affect serialization BC/FC, at least not for all the cases that my script checks"," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey . You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label."
finetuning,[FSDP][BE] Remove multiple param logic in FSDP code," 🚀 The feature, motivation and pitch Currently we only shard based on flat_param, eventually once it is decided that FSDP will not support nonflat param based training, we should remove/simplify all code such as https://github.com/pytorch/pytorch/blob/master/torch/distributed/fsdp/fully_sharded_data_parallel.pyL275 that assumes that there might be multiple parameters.  We can probably do this in the future once we've investigated all various use cases such as finetuning and are confident that we won't need non flat param approach, simplifying the code base by quite a bit.   Alternatives _No response_  Additional context _No response_ ",2022-02-08T15:54:38Z,oncall: distributed triaged better-engineering module: fsdp,closed,1,1,https://github.com/pytorch/pytorch/issues/72520,"Instead, we are refactoring to readd this logic because we want to support nonrecursive wrapping / functionallike. The only difference is that we should do everything in terms of `FlatParamHandle` instead of `FlatParameter`."
yi,Added missing antialias argument to functional.pyi.in,Description:  Added missing antialias argument to functional.pyi.in  mypy is happy if checking `interpolate` method with antialias argument Related torchvision issue: https://github.com/pytorch/vision/pull/5329,2022-02-07T11:57:59Z,open source cla signed Reverted release notes: nn topic: not user facing,closed,0,7,https://github.com/pytorch/pytorch/issues/72420,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72420**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit d09f662f38 (more details on the Dr. CI page):  * **1/2** failures introduced in this PR * **1/2** broken upstream at merge base 4eb277ac61 from Feb 05 until Feb 07   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5093608423?check_suite_focus=true) winvs2019cuda11.3py3 / test (force_on_cpu, 1, 1, windows.4xlarge) (1/1) **Step:** ""Test"" (full log  :repeat: rerun)   20220207T15:17:31.2425017Z RuntimeError: test_spectral_ops failed!  ``` 20220207T15:17:30.8912795Z FAILED (errors=1, skipped=2, expected failures=3) 20220207T15:17:30.8913235Z  20220207T15:17:30.8913678Z Generating XML reports... 20220207T15:17:30.8914744Z Generated XML report: testreports\\distgloo\\test_spectral_ops\\TESTTestFFTCPU20220207151724.xml 20220207T15:17:30.8916603Z Generated XML report: testreports\\distgloo\\test_spectral_ops\\TESTTestFFTDocExamplesCPU20220207151724.xml 20220207T15:17:31.2422174Z Traceback (most recent call last): 20220207T15:17:31.2423074Z   File ""run_test.py"", line 1114, in  20220207T15:17:31.2423441Z     main() 20220207T15:17:31.2423842Z   File ""run_test.py"", line 1092, in main 20220207T15:17:31.2424547Z     raise RuntimeError(err_message) 20220207T15:17:31.2425017Z RuntimeError: test_spectral_ops failed! 20220207T15:17:31.5208498Z  20220207T15:17:31.5209175Z (base) C:\\actionsrunner\\_work\\pytorch\\pytorch\\test>popd 20220207T15:17:31.5213971Z  20220207T15:17:31.5214444Z (base) C:\\actionsrunner\\_work\\pytorch\\pytorch>if ERRORLEVEL 1 exit /b 1  20220207T15:17:31.5244209Z + cleanup 20220207T15:17:31.5244561Z + retcode=1 20220207T15:17:31.5244822Z + set +x 20220207T15:17:31.5283207Z [error]Process completed with exit code 1. 20220207T15:17:31.5435062Z [group]Run  ir => recursive include all files in pattern 20220207T15:17:31.5435775Z 36;1m ir => recursive include all files in pattern[0m ```    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * [linuxxenialpy3.7gcc5.4 / test (backwards_compat, 1, 1, linux.2xlarge) from Feb 05 until Feb 07 (1edf6f5647  d8c3ab11ae)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.","Hey 5. You've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found here for the 'release notes: ...' and here for the 'topics: ...'. For changes that are 'topic: not user facing' there is no need for a release notes label.","This pull request has been **reverted** by 127bf42ee7a7a1a50473b0b7e17ceb69ee5ebb7f. To reland this change, follow these steps.", do you know why this PR is reverted ?,It was though to be the root cause of some CI issue. But it was not. So the revert has been reverted. This is (should be) in master again.,"This pull request has been **reverted** by 127bf42ee7a7a1a50473b0b7e17ceb69ee5ebb7f. To reland this change, please open another pull request, assignthe same reviewers, fix the CI failures that caused the revert and make sure that the failing CI runs on the PR by applying the proper ciflow label (e.g., ciflow/trunk)."
yi,Make cholesky_inverse documentation mention that batching is supported," 🚀 The feature, motivation and pitch Dear PyTorch developers, hi, I am a ML researcher and a PyTorch programmer working heavily in the prediction of Gaussian distribution with full coverariance matrix. In my work, I need to use NN to predict the cholesky decomposition of the covariance matrix, and then operate it quite often.  One of my operation is to compute the inverse of the covariance matrix in a batched manner. I found an implementation using the cholesky, which makes me excited. But later I found it only supports 2d input, so I would like to ask a new feature to make it support batched input [..., 2d].  Thanks a lot in advance. Best, Ge Li  Alternatives _No response_  Additional context _No response_ ",2022-02-04T14:39:04Z,module: docs triaged enhancement module: linear algebra,closed,0,5,https://github.com/pytorch/pytorch/issues/72334,"Even though we should mentoin this somewhere, you can imitate cholesky inv and cholesky solve via  `linalg.solve_triangular`. ~~This is because `LL^TX = B` can be solved via two `solve_triangular`s, one with `L` and the other one with `L^T`. Set `B=Id` and you have the inverse.~~ Even better, you can do `X = L^{1} = solve_triangular(L, Id)` and then your inverse is given by `X^T`. As always, I'd very much recommend you not to use the inverse and then multiply, but doing solves agianst hte matrix that you want to multiply to later on. ","Both `torch.cholesky_inverse` and `torch.cholesky_solve` support batched inputs and they use the same backend function so their performance should roughly be the same. It's an oversight in the documentation that it specifies only 2D inputs. ```py In [1]: import torch In [2]: a = torch.randn(3, 2, 2, dtype=torch.float64) In [3]: a = a @ a.mT  make symmetric In [4]: L = torch.linalg.cholesky(a)  Factorize the batched matrix In [5]: a_inv = torch.cholesky_solve(torch.eye(2, dtype=L.dtype), L)  compute the batched inverse one way In [6]: torch.allclose(a_inv, a.inverse()) Out[6]: True In [7]: a_inv2 = torch.cholesky_inverse(L)  compute the batched inverse the other way In [8]: torch.allclose(a_inv2, a_inv) Out[8]: True ```","Hi, thanks a lot for your answer and support.  I did not know these cholesky's functions by heart before.  Maybe one other question: Do the cholesky_inverse and cholesky_solve functions offer same numerical stabilities?  We know that normally doing the solve() instead of inverse() can offer better numerical stabilities. ","As Lezcano mentioned above if the intention is to multiply the inverse by some other matrix, often it should be more efficient to use the solve function to do that. Internally, `torch.cholesky_inverse(L)` computes the result by calling `torch.cholesky_solve(eye(L.shape[0]), L)`. Leaving performance aside, I don't know whether the accuracy would differ much. This answer on Math.StackExchange suggests that the forward error of using the inverse explicitly or solving the linear system is about the same.","Thanks again for your answer, I have no more doubt. Please feel free to close this issue, or maybe after the documentation has been updated.  :+1:  Best, Ge Li"
transformer,Transformer Initialization,"While you took care of this in the tutorial on Transformers and `nn.Transformer`. I just used `nn.TransformerEncoder` and realized that this won't initialize parameters in a sensible way on its own. One would create an encoder like this: ``` encoder_layer = TransformerEncoderLayer(...) transformer_encoder = TransformerEncoder(encoder_layer, nlayers)  ``` This will yield a Transformer that has the same initialization in all layers, which I think rarely is what you want in neural networks, so it is unexpected. You always need to initialize from the outside again. This is not the usual case in PyTorch and not documented, see https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.htmltorch.nn.TransformerEncoder I am not sure which way to better resolve this, through a changed `nn.TransformerEncoder` API, just documentation or whether you are working on a rewrite anyways already. Thus, I raised this as an issue first.  Versions This is a problem in the current version (1.10). This bug does not need to be reproduced, but can be seen relatively easy from the source code of `nn.TransformerEncoder`. ",2022-02-03T10:08:48Z,high priority module: docs module: nn triaged actionable,open,8,4,https://github.com/pytorch/pytorch/issues/72253,We would accept a patch that fixes this. I suspect that making it conform to the initialization requirements of the rest of the library is what makes the most sense.,"This occurs because `TransformerEncoder` / `TransformerDecoder` have the unfortunate distinction of being the only modules that accept a sublayer and **clone** it N times. In contrast, the RNN modules create the N layers internally and initialize their weights from the same distribution, but not with the exact same values. It would be BCbreaking to change this behavior, and there is a longerterm redesign in progress already. For now, we'd accept a PR adding a warning to the docs indicating that explicit manual initialization is recommended.",Was digging through an opensource VALLE implementation and found this exact bug had gone unnoticed. I think it really deserves a loud warning in the docs at the very least (short of a redesign). ,bumping priority to update the doc
yi,RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward," 🐛 Describe the bug I am reopening the issue  CC(RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.) with the following additional informations: When training a network which contains parametrization in DP (Distributed Parallel) and the caching mode (torch.nn.utils.parametrize.cached), a race condition occurs at a random point in the training causing the error: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. Anomaly detection points to the following culprit: ``` File ""/mnt/task_runtime/src/dibnn_ext/nn/modules.py"", line 832, in forward     weight=self.weight.to(input.device, copy=True) * self.gain, bias=self.bias,   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/utils/parametrize.py"", line 339, in get_parametrized     return get_cached_parametrization(parametrization)   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/utils/parametrize.py"", line 324, in get_cached_parametrization     tensor = parametrization()   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/modules/module.py"", line 1102, in _call_impl     return forward_call(*input, **kwargs)   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/utils/parametrize.py"", line 260, in forward     x = self0   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/modules/module.py"", line 1102, in _call_impl     return forward_call(*input, **kwargs)   File ""/mnt/task_runtime/src/dibnn_ext/nn/parametrization.py"", line 36, in forward     weight = DF.weight_standardization(weight, eps=self.eps)   File ""/mnt/task_runtime/src/dibnn_ext/nn/functional.py"", line 660, in weight_standardization     weight = (weight  mean) / (torch.sqrt(var * fan_in + eps))  (function _print_stack) ``` The training step is defined as follow: ``` def training_step(...):    ...    with torch.nn.utils.parametrize.cached():             pred = self.forward(data.detach(), data2.detach())             loss = torch.abs(pred  data).mean()     return { 'loss': loss } ``` I suspect the following: The training step is handle per thread and per GPU, but the caching mechanism is global. What can happen is GPU 0 starts to enter to the forward pass and cache the weights, GPU 1 enters later in the game but GPU 0 has finished its backward pass and the gradients of the weights cached are not longer available, this causes issue on GPU 1 since its backward pass now is invalid. This also explain why the following line is required to transfer weights cached to the current valid device/GPU when caching is used: ```     def forward(self, input: torch.Tensor) > torch.Tensor:         return TF.conv2d(input,                          weight=self.weight.to(input, copy=True) * self.gain, bias=self.bias,   Due to the cache, weights can be on a different device than input                          stride=self._stride,                          padding=self._padding,                          dilation=self._dilation,                          groups=self._groups) ``` Attached a dummy example to reproduce the issue using DP and autograddetectanomaly. Full example to reproduce the issue ```python from typing import Optional, List, Dict import os import argparse import numpy as np import torch import torch.nn as nn import torch.nn.functional as TF import torch.nn.utils.parametrize as TP import torchvision from torch.utils.data import DataLoader from torchvision.datasets import MNIST import pytorch_lightning as pl def weight_standardization(weight: torch.Tensor, eps: float = 1e6):     """"""     Weight standardization as defined by:     Characterizing signal propagation to close the performance gap in unnormalized ResNets, https://arxiv.org/abs/2101.08692     :param weight: weight to standardize     :param eps: epsilon to ensure safe division     :return: weight to standardized     """"""     fan_in = nn.init._calculate_fan_in_and_fan_out(weight)[0]     var, mean = torch.var_mean(weight, dim=list(range(1, weight.ndim)), keepdim=True)     weight = (weight  mean) / (torch.sqrt(var * fan_in + eps))     return weight class WeightStandardization(nn.Module):     def __init__(self, eps: float=1e6):         super().__init__()         self.eps = eps     def forward(self, weight: torch.Tensor) > torch.Tensor:         weight = weight_standardization(weight, eps=self.eps)         return weight class SWSConv2d(nn.Module):     r""""""     2D Conv layer with Scaled Weight Standardization.     Characterizing signal propagation to close the performance gap in unnormalized ResNets     https://arxiv.org/abs/2101.08692     """"""     def __init__(self,                  in_channels: int, out_channels: int, kernel_size: int,                  stride: int=1, padding: int=0, padding_mode: str='zeros', dilation=1, groups: int=1,                  bias: bool=True):         super().__init__()         self._stride = stride         self._padding = padding         self._padding_mode = padding_mode         self._dilation = dilation         self._groups = groups         self.weight = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size, kernel_size))         self.bias = nn.Parameter(torch.empty(out_channels)) if bias else None         self.gain = nn.Parameter(torch.ones(out_channels, 1, 1, 1))          Init weights         k = groups / (in_channels * kernel_size * kernel_size)         torch.nn.init.uniform_(self.weight, k, k)         if bias:             torch.nn.init.uniform_(self.bias, k, k)         self.register_parametrization()     def register_parametrization(self):         if not TP.is_parametrized(self, 'weight'):             TP.register_parametrization(self, 'weight', WeightStandardization(eps=1e6))     def remove_parametrization(self, leave_parametrized: bool=True):         if TP.is_parametrized(self, 'weight'):             TP.remove_parametrizations(self, 'weight', leave_parametrized=leave_parametrized)     def forward(self, input: torch.Tensor) > torch.Tensor:         return TF.conv2d(input,                          weight=self.weight.to(input, copy=True) * self.gain, bias=self.bias,   Due to the cache, weights can be on a different device than input                          stride=self._stride,                          padding=self._padding,                          dilation=self._dilation,                          groups=self._groups) class FeatHeadLayer(nn.Module):     def __init__(self,                  in_channels: int,                  out_channels: int = 16,                  hidden_features: int = 24):         super().__init__()         activation = nn.LeakyReLU(negative_slope=0.1)         self._in_channels = in_channels          TODO: Improve hidden channels as multiple of 16         self._feat_head = nn.Sequential(             SWSConv2d(in_channels, hidden_features, 3, stride=1, bias=True, padding=1), activation,             SWSConv2d(hidden_features, out_channels, 1, bias=True)         )     def forward(self, x: torch.Tensor) > torch.Tensor:         return self._feat_head(x) class RegressionLayer(nn.Module):     def __init__(self):         super().__init__()         self._regres = SWSConv2d(32, 1, 1)     def forward(self, input: torch.Tensor) > torch.Tensor:         pred = self._regres(input)         return pred class DummyModel(nn.Module):     def __init__(self):         super().__init__()         self._features_head = FeatHeadLayer(in_channels=1, out_channels=24)         self._cv = SWSConv2d(24 * 2, 32, 1)         self._regression_layer = RegressionLayer()     def forward(self, x1: torch.Tensor, x2: torch.Tensor) > torch.Tensor:         feat0_mc2 = self._features_head(x1)         feat1_mc2 = self._features_head(x2)         corr2 = self._cv(torch.cat((feat0_mc2, feat1_mc2), dim=1))         pred = self._regression_layer(corr2)         return pred class MNISTDataModule(pl.core.LightningDataModule):     def __init__(self, hparams):         super().__init__()         self._hparams = hparams         self._transform = torchvision.transforms.Compose([             torchvision.transforms.ToTensor(),             torchvision.transforms.Normalize(                 (0.1307,), (0.3081,))         ])     def prepare_data(self):          Download the datasets         MNIST(os.getcwd(), train=True, download=True, transform=None)         MNIST(os.getcwd(), train=False, download=True, transform=None)     def train_dataloader(self):         import torch.utils.data         num_samples = self.hparams.num_steps * self.hparams.batch_size         dataset = MNIST(os.getcwd(), train=True, download=False, transform=self._transform)   Only access to the dataset previously downloaded         sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=num_samples)         loader = DataLoader(dataset, sampler=sampler, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)         return loader     def val_dataloader(self):         dataset = MNIST(os.getcwd(), train=False, download=False, transform=self._transform)         loader = DataLoader(dataset, batch_size=1, num_workers=0)         return loader          def add_specific_args(parser: argparse.ArgumentParser):         g_training = parser.add_argument_group('training')         g_training.add_argument('numsteps', dest='num_steps', type=int, default=1000)         g_training.add_argument('batchsize', dest='batch_size', type=int, default=64)         g_training.add_argument('numworkers', dest='num_workers', type=int, default=16) class MNISTModel(pl.core.LightningModule):     def __init__(self, hparams):         super().__init__()         self.save_hyperparameters(hparams)         self.model = DummyModel()     def forward(self, x1, x2):         pred = self.model(x1, x2)         return pred     def training_step(self, batch, batch_idx):         data = batch[0]         target = batch[1]         data2 = torch.randn(data.size())         with torch.nn.utils.parametrize.cached():             pred = self.forward(data.detach(), data2.detach())             loss = torch.abs(pred  data).mean()         return { 'loss': loss }     def validation_step(self, batch, batch_idx, *args):         data = batch[0]         target = batch[1]         data2 = torch.randn(data.size())         pred = self.forward(data, data2)         loss = torch.abs(pred  data).mean()         return {             'loss': loss.item(),             'batch': batch,             'pred': pred         }     def test_step(self, batch, batch_idx, *args):         return self.validation_step(batch, batch_idx, *args)     def validation_visualization_groundtruth(self, batch, batch_idx, dataloader_idx: int):         viz_dict = dict()         data = batch[0]         data_img = data.detach().cpu().squeeze(0).numpy().transpose((1, 2, 0))         viz_dict['data'] = np.clip(data_img * 255.0, 0.0, 255.0).astype(np.uint8)         return viz_dict     def configure_optimizers(self):         from torch.optim.sgd import SGD         optim = SGD(self.parameters(), lr=self.hparams.learning_rate, momentum=self.hparams.momentum)         return {             'optimizer': optim         }          def add_specific_args(parser: argparse.ArgumentParser):         default_log_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '71062', 'log')         parser.add_argument('logpath', dest='log_path', type=str, default=default_log_path)         parser.add_argument('numepochs', dest='num_epochs', type=int, default=100)         g_training = parser.add_argument_group('training')         g_training.add_argument('learningrate', dest='learning_rate', type=float, default=0.01)         g_training.add_argument('momentum', dest='momentum', type=float, default=0.5) if __name__ == '__main__':     args_parser = argparse.ArgumentParser(description='MNIST example')     MNISTModel.add_specific_args(args_parser)     MNISTDataModule.add_specific_args(args_parser)     hparams = args_parser.parse_args()     os.makedirs(hparams.log_path, exist_ok=True)     import pytorch_lightning as pl     import pytorch_lightning.loggers     import pytorch_lightning.plugins     model = MNISTModel(hparams)     data_module = MNISTDataModule(hparams)     pl.seed_everything(42)     num_epochs = hparams.num_epochs     trainer = pl.trainer.Trainer(default_root_dir=hparams.log_path,                                  enable_checkpointing=True,                                  enable_model_summary=False,                                  strategy=pl.plugins.DataParallelPlugin(parallel_devices=None),                                  accelerator='gpu',                                  devices='auto',                                  min_epochs=num_epochs, max_epochs=num_epochs,                                  num_sanity_val_steps=0,                                  check_val_every_n_epoch=1,                                  val_check_interval=1.0,                                  limit_val_batches=100,                                  limit_test_batches=50,                                  detect_anomaly=True)      Run training     trainer.fit(model, datamodule=data_module)      Test loop     trainer.test(model, dataloaders=data_module.val_dataloader()) ```  Versions Versions Collecting environment information... PyTorch version: 1.10.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: Could not collect Libc version: glibc2.10 Python version: 3.7.7 (default, Mar 23 2020, 22:36:06) [GCC 7.3.0] (64bit runtime) Python platform: Linux4.19.561.el7.x86_64x86_64withdebianbustersid Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: Tesla V100SXM232GB GPU 1: Tesla V100SXM232GB GPU 2: Tesla V100SXM232GB GPU 3: Tesla V100SXM232GB GPU 4: Tesla V100SXM232GB GPU 5: Tesla V100SXM232GB GPU 6: Tesla V100SXM232GB GPU 7: Tesla V100SXM232GB Nvidia driver version: 418.40.04 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] numpy==1.21.2 [pip3] pytorchlightning==1.5.5 [pip3] torch==1.10.0+cu113 [pip3] torchaudio==0.10.0+cu113 [pip3] torchmetrics==0.6.2 [pip3] torchvision==0.11.1+cu113 [conda] blas 1.0 mkl [conda] cudatoolkit 11.3.1 h2bc3f7f_2 [conda] libblas 3.9.0 12_linux64_mkl condaforge [conda] libcblas 3.9.0 12_linux64_mkl condaforge [conda] liblapack 3.9.0 12_linux64_mkl condaforge [conda] mkl 2021.4.0 h06a4308_640 [conda] mklservice 2.4.0 py37h7f8727e_0 [conda] mkl_fft 1.3.1 py37hd3c417c_0 [conda] mkl_random 1.2.2 py37h51133e4_0 [conda] numpy 1.19.5 pypi_0 pypi [conda] numpybase 1.21.2 py37h79a1101_0 [conda] pytorchcpu 1.1.0 py37he1b5a44_0 condaforge [conda] pytorchlightning 1.5.5 pyhd8ed1ab_0 condaforge [conda] pytorchmutex 1.0 cuda pytorch [conda] torch 1.10.0+cu113 pypi_0 pypi [conda] torchaudio 0.10.0+cu113 pypi_0 pypi [conda] torchmetrics 0.6.2 pyhd8ed1ab_0 condaforge [conda] torchvision 0.11.1+cu113 pypi_0 pypi ",2022-01-29T19:25:30Z,module: autograd triaged module: nn.utils.parametrize,open,0,2,https://github.com/pytorch/pytorch/issues/72041,"As mentioned in the previous PR, a minimal repro would certainly help, as the current code is too complex to reason about.","That being said, I do think that the multiprocessing + caching may not work well.  "
yi,[warnings][caffe2] Fix asserts yielding -Wstring-conversion warnings,"Summary: Find and replace `assert(!""` with `assert(false && ""` Excludes headers and paths that contain ""thirdparty"" or ""external"" Clang raises a `Wstringconversion` warning when treating a string as a boolean.  This is not uncommon for asserts though (e.g. `assert(!""should never happen"")`).  Clang does permit `expr && ""string""` though in order to support these assertion use cases. Test Plan: ci pass Differential Revision: D33823092",2022-01-28T19:13:27Z,oncall: jit fb-exported cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/72013,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/72013**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 4d683ba99f (more details on the Dr. CI page):  * **1/1** failures possibly\\* introduced in this PR     * **1/1** nonscanned failure(s)   ci.pytorch.org: 1 failed * **Failed:** `pr/pytorchlinuxbionicrocm4.5py3.7`  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33823092
yi,torch/monitor: update pyi definitions,Summary: This updates the .pyi definitions to match the pybind interfaces. Test Plan: ``` pyre ``` CI Differential Revision: D33830311,2022-01-27T23:06:17Z,fb-exported cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/71950,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71950**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 60c539dc42 (more details on the Dr. CI page):  * **5/10** failures introduced in this PR * **5/10** broken upstream at merge base 0c3bc426a8 on Jan 28 from  8:59am to 11:43am   :detective: 5 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/4985325785?check_suite_focus=true) winvs2019cpupy3 / test (default, 1, 2, windows.4xlarge) (1/5) **Step:** ""Test"" (full log  :repeat: rerun)   20220128T19:32:36.8027657Z RuntimeError: test_ops failed!  ``` 20220128T19:32:32.8717791Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestGradientsMETA20220128191112.xml 20220128T19:32:33.0356192Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestJitCPU20220128191112.xml 20220128T19:32:33.1079270Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestMathBitsCPU20220128191112.xml 20220128T19:32:33.1933858Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestJitMETA20220128191112.xml 20220128T19:32:33.2970403Z Generated XML report: testreports/pythonunittest/test_ops/TESTTestMathBitsMETA20220128191112.xml 20220128T19:32:36.8021852Z Traceback (most recent call last): 20220128T19:32:36.8022166Z   File ""test/run_test.py"", line 1101, in  20220128T19:32:36.8025069Z     main() 20220128T19:32:36.8025363Z   File ""test/run_test.py"", line 1079, in main 20220128T19:32:36.8027275Z     raise RuntimeError(err_message) 20220128T19:32:36.8027657Z RuntimeError: test_ops failed! 20220128T19:32:37.0183607Z  20220128T19:32:37.0184152Z real	39m23.127s 20220128T19:32:37.0184392Z user	82m27.821s 20220128T19:32:37.0184576Z sys	5m38.390s 20220128T19:32:37.0184739Z + cleanup 20220128T19:32:37.0184904Z + retcode=1 20220128T19:32:37.0185068Z + set +x 20220128T19:32:37.0237393Z [error]Process completed with exit code 1. 20220128T19:32:37.0317527Z [group]Run  Ensure the working directory gets chowned back to the current user 20220128T19:32:37.0317911Z 36;1m Ensure the working directory gets chowned back to the current user[0m ```    :construction: 5 fixed upstream failures: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * [linuxxenialpy3.7clang7onnx / test (default, 2, 2, linux.2xlarge) on Jan 28 from 10:20am to 11:05am (c5df294940  c5df294940)     * :repeat: rerun * linuxxenialpy3.7gcc7 / test (default, 1, 2, linux.2xlarge) on Jan 28 from  8:59am to 11:27am (f499ab9cef  cb823d9f07)     * :repeat: rerun * linuxbionicpy3.7clang9 / test (default, 1, 2, linux.2xlarge) on Jan 28 from  8:59am to 11:27am (f499ab9cef  cb823d9f07)     * :repeat: rerun * linuxxenialpy3.7clang7onnx / test (default, 1, 2, linux.2xlarge) on Jan 28 from 10:20am to 11:43am (c5df294940  765669e1b9)     * :repeat: rerun * linuxxenialpy3.7gcc5.4 / test (default, 1, 2, linux.2xlarge) on Jan 28 from  8:59am to 11:27am (f499ab9cef  cb823d9f07)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D33830311,This pull request was **exported** from Phabricator. Differential Revision: D33830311,This pull request was **exported** from Phabricator. Differential Revision: D33830311,This pull request was **exported** from Phabricator. Differential Revision: D33830311,Try rebasing on latest master to make the build errors go away.,This pull request was **exported** from Phabricator. Differential Revision: D33830311,"* linuxbionicpy3.7clang9 / test (xla, 1, 1, linux.2xlarge) (1/2) * linuxbionicpy3.7clang9 / test (noarch, 1, 1, linux.2xlarge) (2/2) are broken in trunk and unrelated to this PR"
yi,Fix hardcoded `TsNode` in `lazy_ir.py` ,"There are a number of places where TorchScript specific classes are hardcoded into the lazy tensor autogen code. Fixing those instances to make them more general.  There is a hardcoded instance of `TsNode` in `tools/codegen/dest/lazy_ir.py`. Fixing that to be `self.node_base` instead.  There are hardocded instances of `TSOpVector` and `TSLoweringContext` as well which were replaced by overridable fields in the dataclass. With these changes, a custom `LazyIR` subclass can be passed into the `gen_lazy_tensor.run` to custom the autogen for lazy tensors. CC:  ",2022-01-27T16:59:22Z,open source cla signed,closed,8,7,https://github.com/pytorch/pytorch/issues/71921,"Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71921**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit e69285a0a1 (more details on the Dr. CI page):  * **9/9** failures introduced in this PR   :detective: 7 new failures recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/4987509119?check_suite_focus=true) linuxbionicpy3.7clang9 / test (default, 2, 2, linux.2xlarge) (1/7) **Step:** ""Test"" (full log   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!, Another gentle reminder to please review this at your earliest convenience,"> The original plan was to use this infra to develop wellsupported bridges into TS and XLA. (Possibly also MLIR) but not use it as a point for each vendor to integrate to directly. That's more than fine. I was actually planning on using this change to do the autogen for torchmlir. I wanted to reuse as much of the existing autogen stuff as possible. Hence the reason for this PR. > We'll approve this, but wanted to point out that the codegen infra isn't a stable API, and we still plan on changing it some to accommodate improvements to LTC.  Understood. Though, any future changes I think should be made more general and not reference anything specific like it is now with TS. > Do you think that longer term you would be able to share an integration point with one of the common IRs, or you think you really want to have a direct integration here for various reasons? The plan is to share an integration point with torchmlir. But as mentioned above, to use the autogen for torchmlir, the changes in this PR removing the TS hardcoded elements are required.",> future changes I think should be made more general and not reference anything specific like it is now with TS yea agreed. OK sounds good.  thanks for clarifying!, What are the next steps to getting this merged? I don't have write access and can't merge it myself
rag,decouple storage and serializer from exporter,"Stack from ghstack:  CC(add zip reader)  CC(remove importer's reliance on load_tensor, break bc on map_location by using None)  CC(decouple storage and serializer from exporter)",2022-01-26T19:10:57Z,oncall: jit cla signed Stale,closed,0,2,https://github.com/pytorch/pytorch/issues/71873,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71873**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit df365a75a5 (more details on the Dr. CI page):  * **2/2** failures introduced in this PR   2 failures *not* recognized by patterns:   This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
transformer,Regression in multi-node training speed with Transformers + PyTorch," 🐛 Describe the bug We have seen a regression in multinode training speed with the Huggingface Transformers CLM test case happening at PyTorch commit 38ac9e6 which reverts commit 3957ed4 ""[DDP] Disable reducer hooks from running outside of DDP backwards"". Between those two commits I get around 28 `train_samples_per_second` (as reported by the CLM test) using two nodes with 4xNVIDIA A100 GPUs each. Afterwards speed drops to around 10 `train_samples_per_second`. This can be reliably reproduced every time. My testing script can be found here, although it's a bit difficult to reproduce at it requires building the singularity containers and a cluster with at least two nodes: https://github.com/mvsjober/pytorchclmtestcase/blob/main/pytorch_clm_test.sh The PyTorch container has been created with this build recipe. It's a CentosOS 7.7 image with CUDA 11.1, cuDNN 8.1.0, NCCL 2.8.4.  Versions Collecting environment information... PyTorch version: 1.10.0a0+git38ac9e6 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: CentOS Linux 7 (Core) (x86_64) GCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.12) Clang version: Could not collect CMake version: version 2.8.12.2 Libc version: glibc2.17 Python version: 3.8.11 (default, Sep  1 2021, 12:33:46)  [GCC 9.3.1 20200408 (Red Hat 9.3.12)] (64bit runtime) Python platform: Linux3.10.01160.41.1.el7.x86_64x86_64withglibc2.2.5 Is CUDA available: True CUDA runtime version: 11.3.109 GPU models and configuration: Could not collect Nvidia driver version: Could not collect cuDNN version: Probably one of the following: /usr/lib64/libcudnn.so.8.1.0 /usr/lib64/libcudnn_adv_infer.so.8.1.0 /usr/lib64/libcudnn_adv_train.so.8.1.0 /usr/lib64/libcudnn_cnn_infer.so.8.1.0 /usr/lib64/libcudnn_cnn_train.so.8.1.0 /usr/lib64/libcudnn_ops_infer.so.8.1.0 /usr/lib64/libcudnn_ops_train.so.8.1.0 HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] numpy==1.22.1 [pip3] torch==1.10.0a0+git38ac9e6 [pip3] torchaudio==0.9.1 [pip3] torchtext==0.10.1 [pip3] torchvision==0.10.1+cu111 [conda] Could not collect ",2022-01-26T12:57:06Z,high priority triage review oncall: distributed triaged module: regression module: ddp,open,0,3,https://github.com/pytorch/pytorch/issues/71855,Adding high priority due to perf regression ," is the regression still happening? also wondering what is the QPS before commit 3957ed4?  If 3957ed4 does help the QPS improvement, that mostly means this commit disabled the grad sync when directly using the local module wrapped by DDP, is that what you want in the application? "," Yes, regression is still happening. I just ran my test with the most recent release and I got around 8 `train_samples_per_second` and I also tried to most recent commit (which was c6f1bbc) and I got around 13. Compared to around 28 after commit 3957ed4. And yes, before that ""magic"" commit it was also low. According to my notes with the previous commit 5a4282d it was 10.36. I'm not sure regarding the grad sync, the application is the Huggingface Transformers CLM test case so perhaps this should be brought up with them?"
rag,Remove state_dict from AveragedModel and use buffers instead,Fixes  CC(Improve use_state_dict in AveragedModel)),2022-01-25T14:08:35Z,module: optimizer cla signed,closed,0,2,https://github.com/pytorch/pytorch/issues/71763,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71763**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit dab9741598 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
rag,[acc_tracer] Add test coverage for retracing,"Summary: Added coverage for reshape specifically which required a fix. The problem for `acc_ops.reshape` as best as I understand:  `torch.reshape` requires the `shape` arg to be a `tuple` of `ints`  If `torch.reshape` is passed a `tuple` where the first element is not an `int`, it throws a TypeError e.g. `TypeError: reshape(): argument 'shape' (position 2) must be tuple of ints, not tuple`   If the `shape` we're reshaping to is an FX Proxy then this type error will be thrown. This happens when the first element of the `shape` tuple is a Proxy because it's inputdependent.  As a workaround we use `tensor.reshape` instead of `torch.reshape`, which doesn't do equivalent type checking for a `tuple` of `ints`. Also remove unnecessary `acc_utils.get_field_from_acc_out_ty()` with cast to `TensorMetadata`. Test Plan: Added test coverage Differential Revision: D33760455",2022-01-25T05:32:05Z,fb-exported cla signed module: fx,closed,0,6,https://github.com/pytorch/pytorch/issues/71752,  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71752**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 905952fc4f (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33760455,This pull request was **exported** from Phabricator. Differential Revision: D33760455,This pull request was **exported** from Phabricator. Differential Revision: D33760455,This pull request was **exported** from Phabricator. Differential Revision: D33760455,This pull request was **exported** from Phabricator. Differential Revision: D33760455
rag,add move constructor for serialized storage context,  CC([torch::deploy] remove asserts from deploy)  CC(add move constructor for serialized storage context)  CC([pkg] detect bad packages while interning),2022-01-24T23:52:42Z,oncall: jit cla signed Stale,closed,0,2,https://github.com/pytorch/pytorch/issues/71731,"  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71731**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 2e34756dfc (more details on the Dr. CI page):  * **7/8** failures introduced in this PR * **1/8** broken upstream at merge base ada6f3b447 on Jan 24 from  3:26pm to  4:18pm   :detective: 1 new failure recognized by patterns The following CI failures do not appear to be due to upstream breakages:  ![See GitHub Actions build](https://github.com/pytorch/pytorch/runs/5396599235?check_suite_focus=true) Test tools / test (1/1) **Step:** ""Test tools"" (full log    :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * Lint / quickchecks on Jan 24 from  3:26pm to  4:18pm (64162588d1  cda6f40151)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ","Looks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. Feel free to remove the `Stale` label if you feel this was a mistake. If you are unable to remove the `Stale` label please contact a maintainer in order to do so. If you want the bot to never mark this PR stale again, add the `nostale` label.`Stale` pull requests will automatically be closed after 30 days of inactivity."
llm,[FX] Support call_method in NormalizeArgs," 🚀 The feature, motivation and pitch Currently NormalizeArgs only supports normalizing `call_function`s and `call_module`s. Ideally it would support `call_method`s as well. Example code: ```python import torch import torch.fx class TestModule(torch.nn.Module):     def forward(self, x):         return x.flatten(), torch.flatten(x) mod = TestModule() gm = torch.fx.symbolic_trace(mod) print(gm.graph) ``` Output shows flatten args for `call_method` and `call_function` not normalized with standard FX tracing (all kwargs usage or default values), as expected: ``` graph():     %x : [users=2] = placeholder[target=x]     %flatten : [users=1] = call_methodtarget=flatten, kwargs = {})     %flatten_1 : [users=1] = call_functiontarget=torch.flatten, kwargs = {})     return (flatten, flatten_1) ``` Now when we normalize we see that the `call_function` is normalized with all kwargs and default values, but the `call_method` is not. ```python from torch.fx.experimental.normalize import NormalizeArgs gm = NormalizeArgs(gm).transform() print(gm.graph) ``` ``` graph():     %x : [users=2] = placeholder[target=x]     %flatten : [users=1] = call_methodtarget=flatten, kwargs = {})     %flatten_1 : [users=1] = call_functiontarget=torch.flatten, kwargs = {input: %x, start_dim: 0, end_dim: 1})     return (flatten, flatten_1) ```  Alternatives _No response_  Additional context _No response_",2022-01-24T18:44:37Z,triaged module: fx,open,0,0,https://github.com/pytorch/pytorch/issues/71715
rag,Fix unused variable warning in AveragePool2d,Differential Revision: D33692619,2022-01-20T22:08:42Z,fb-exported cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/71585," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/rbarnes/pytorch/blob/1c85f83b10ca584a68ad09e08eaf15110968a949/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71585**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 348f210429 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33692619,This pull request was **exported** from Phabricator. Differential Revision: D33692619
yi,Allow specifying num_samples to RandomSampler even when replacement=False,"Fixes CC(Allow specifying num_samples to RandomSampler when replacement=False) CC(Allow specifying num_samples to RandomSampler in any case(38032)) Hi, I modified the RandomSampler to satisfy the requirement of CC(Allow specifying num_samples to RandomSampler when replacement=False). I also added and deleted some test cases in the test/test_dataloader.py to match with the new requirement.",2022-01-20T18:49:34Z,triaged open source cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/71568," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/PM25/pytorch/blob/88ede0f27ac90bc51c9dd2cca60e30ee4de90f70/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ","Hi !  Thank you for your pull request and welcome to our community.   Action Required In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71568**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 9ff8361f54 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,"ok, no problem. I have just added the test cases."," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",Thanks for the review!
yi,torch.jit.script failed to compile nn.MultiheadAttention when specifying the kdim and vdim parameters.," 🐛 Describe the bug nn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum code to reproduce the behavior: ``` import torch mha0 = torch.nn.MultiheadAttention(256, 4) script_mha0 = torch.jit.script(mha0) print(""Successful compiled mha0"") mha1 = torch.nn.MultiheadAttention(256, 4, kdim=128, vdim=128) script_mha1 = torch.jit.script(mha1) print(""Successful compiled mha1"") ``` Full error stack: ``` Successful compiled mha0 Traceback (most recent call last):   File ""tests/bin/jit_mha.py"", line 9, in      script_mha1 = torch.jit.script(mha1)   File ""/storage09/yuxinyuan02/anaconda3/envs/pytorch/lib/python3.6/sitepackages/torch/jit/_script.py"", line 1097, in script     obj, torch.jit._recursive.infer_methods_to_compile   File ""/storage09/yuxinyuan02/anaconda3/envs/pytorch/lib/python3.6/sitepackages/torch/jit/_recursive.py"", line 412, in create_script_module     return create_script_module_impl(nn_module, concrete_type, stubs_fn)   File ""/storage09/yuxinyuan02/anaconda3/envs/pytorch/lib/python3.6/sitepackages/torch/jit/_recursive.py"", line 478, in create_script_module_impl     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)   File ""/storage09/yuxinyuan02/anaconda3/envs/pytorch/lib/python3.6/sitepackages/torch/jit/_recursive.py"", line 355, in create_methods_and_properties_from_stubs     concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults) RuntimeError:  multi_head_attention_forward(Tensor query, Tensor key, Tensor value, int embed_dim_to_check, int num_heads, Tensor in_proj_weight, Tensor? in_proj_bias, Tensor? bias_k, Tensor? bias_v, bool add_zero_attn, float dropout_p, Tensor out_proj_weight, Tensor? out_proj_bias, bool training=True, Tensor? key_padding_mask=None, bool need_weights=True, Tensor? attn_mask=None, bool use_separate_proj_weight=False, Tensor? q_proj_weight=None, Tensor? k_proj_weight=None, Tensor? v_proj_weight=None, Tensor? static_k=None, Tensor? static_v=None) > ((Tensor, Tensor?)): Expected a value of type 'Tensor' for argument 'in_proj_weight' but instead found type 'NoneType'. :   File ""/storage09/yuxinyuan02/anaconda3/envs/pytorch/lib/python3.6/sitepackages/torch/nn/modules/activation.py"", line 1020         if not self._qkv_same_embed_dim:             attn_output, attn_output_weights = F.multi_head_attention_forward(                                                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ < HERE                 query, key, value, self.embed_dim, self.num_heads,                 self.in_proj_weight, self.in_proj_bias, ```  Versions PyTorch version: 1.9.0 Is debug build: False CUDA used to build PyTorch: 10.2 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.5 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: Could not collect CMake version: Could not collect Libc version: glibc2.17 Python version: 3.6.13  (default, Jun  4 2021, 14:25:59)  [GCC 7.5.0] (64bit runtime) Python platform: Linux4.15.0142genericx86_64withdebianbustersid Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration: GPU 0: GeForce GTX 1080 Ti Nvidia driver version: 460.73.01 cuDNN version: Probably one of the following: /usr/lib/x86_64linuxgnu/libcudnn.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.0.4 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.0.4 HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] mypyextensions==0.4.3 [pip3] numpy==1.19.2 [pip3] torch==1.9.0 [pip3] torchaudio==0.9.0a0+33b2469 [pip3] torchtext==0.10.0 [pip3] torchvision==0.10.0 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               10.2.89              hfd86e86_1   [conda] ffmpeg                    4.3                  hf484d3e_0    pytorch [conda] mkl                       2020.2                      256   [conda] mklservice               2.3.0            py36he8ac12f_0   [conda] mkl_fft                   1.3.0            py36h54f3939_0   [conda] mkl_random                1.1.1            py36h0573a6f_0   [conda] mypyextensions           0.4.3                    pypi_0    pypi [conda] numpy                     1.19.2           py36h54aff64_0   [conda] numpybase                1.19.2           py36hfa32c7d_0   [conda] pytorch                   1.9.0           py3.6_cuda10.2_cudnn7.6.5_0    pytorch [conda] torchaudio                0.9.0                      py36    pytorch [conda] torchtext                 0.10.0                     py36    pytorch [conda] torchvision               0.10.0               py36_cu102    pytorch",2022-01-19T07:52:50Z,oncall: jit,open,0,1,https://github.com/pytorch/pytorch/issues/71470,"I think the issue comes from the following annotation in torch.nn.functional.multi_head_attention_forward https://github.com/pytorch/pytorch/blob/b56ba296b1cb5d65a3fe2e33cc1d910481baa644/torch/nn/functional.pyL5084 When specifying kdim & vdim,  `use_separate_proj_weight` becomes true, and `in_proj_weight` becomes None. Changing `in_proj_weight: Tensor` to `in_proj_weight: Optional[Tensor]`, and add a new assert statement here https://github.com/pytorch/pytorch/blob/b56ba296b1cb5d65a3fe2e33cc1d910481baa644/torch/nn/functional.pyL5231 should fix the issue."
rag,Remove simd qualifier for pragma omp loop in upsample_nearest_op.h,"Summary: Fixes ```       6 aienv/aienv_ig_reels_base:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning]       6 deep_entity_classification/si_dec_gnn:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning]       6 feed_recommendation_infra/multifeed_execution_graph_service_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning]      12 mobile_cv/mobilevision_experimental:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning]      30 mobile_cv/mobilevision_xraymobilev2_detection_caffe2:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning]      42 aienv/aienv:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning]     128 feed_recommendation_infra/multifeed_recagg_dev:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning]     136 fluent2/fblearner_flow_projects_fluent2_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning]    1338 f6/f6_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [Werror,Wpassfailed=transformwarning] ``` Test Plan: Sandcastle Reviewed By: luciang Differential Revision: D33641869",2022-01-19T01:03:22Z,fb-exported cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/71462," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/rbarnes/pytorch/blob/de3fd864f455569c644a74a4395f26942b3ca859/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71462**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit de3fd864f4 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33641869
rag,is_alias_of changes for storageless tensors,"The is_alias_of() method should check has_storage() before returning true/false for storageless tensors. Also, it should be possible to override the default behavior by making it virtual. Fixes CC(is_alias_of support for storageless tensors)",2022-01-17T06:18:32Z,triaged open source cla signed,closed,0,8,https://github.com/pytorch/pytorch/issues/71378," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/sujoysaraswati/pytorch/blob/7f68c29626d171e23eccf2ca9c162c9d41fd9dd6/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71378**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 7f68c29626 (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,"Hey! From more internal discussion on this, I think the main point are:  We should rename is_alias_of to has_same_storage. This is a better description of what it does and ensure its semantic is clear.  The places where this function is used assumes that the given Tensor is a strided Tensor. The rest of the code around it assumes that the Tensor is strided to implement its feature. So allowing this function to be virtual doesn't help if it is not properly looking like a strided Tensor.  In your case, if your Tensor is properly strided, then it should expose all these things properly (and thus wouldn't need it to be virtual).","Hi , in our case we have lazy tensors and they can be strided/view tensors. We do the bookkeeping at our side to create the tensors in the device and run ops with the strided tensor semantics. However, since this is a lazy tensor, there is no storage attached so we would not be able to expose a storage for them and use has_same_storage functionality. This is why we wanted to create it as a virtual so that we can refer to our bookkeeping and return true/false based on whether they are strided or not. Is there anything wrong to keep the strided tensor bookkeeping inside the backend bridge for a lazy tensor? It feels a bit restrictive, as we do create lazy tensors with strided/view semantics, but we can't have an API like is_alias_of that can be overridden and hence we are blocked on DDP usage. > Hey! >  > From more internal discussion on this, I think the main point are: >  > * We should rename is_alias_of to has_same_storage. This is a better description of what it does and ensure its semantic is clear. > * The places where this function is used assumes that the given Tensor is a strided Tensor. The rest of the code around it assumes that the Tensor is strided to implement its feature. So allowing this function to be virtual doesn't help if it is not properly looking like a strided Tensor.  In your case, if your Tensor is properly strided, then it should expose all these things properly (and thus wouldn't need it to be virtual).","For the DDP use in particular my understanding is the following: `is_alias_of` is only one of the functions that is being used there to make their bucketting system work. They also need advanced viewing/striding functions etc. So you do need to provide the full ""strided Tensor API"" to be able to use that feature. The fact that there is a storage, size, stride, etc are just pieces of that API. If you do have all of these, I don't see why you cannot have a Storage object here since you do have one. Note that when I talk about Storage object here, I mean the ""Storage"" objects stored by your TensorImpl. This doesn't have to be the PyTorch vanilla Storage though. Advanced features like Functionalization have a special Storage class that they set on their custom TensorImpl. In your case, you can have a LazyStorage object that will allow you to use all the Tensor methods asis without issues."," I want to connect you with  from our side working on DDP design for lazy tensor core.  It's still in early stages, but we'll want to solve the same things so let's work together on a solution.",">  Thanks for the suggestions  . Currently we have the lazy tensor as storageless (similar to torch/XLA), so the TensorImpl doesn't use a storage object and has_storage()/storage() will return false/raise error. I will sync up with  about the Lazy Tensor Core and how it plans to use any LazyStorage.  ","Hi  ,   we have followed your suggestion and used a dummy storage for our lazy tensor. With this, I am abandoning this PR. "
rag,is_alias_of support for storageless tensors," 🚀 The feature, motivation and pitch Motivation:  Torch DDP code uses is_alias_of() for two tensors to check if they share the same storage. The current is_alias_of() API on the tensor checks if the underlying Storage class for both the tensors has a common storage_impl_. This is also not a virtual method, so not an overridable API for the tensor currently. For backends like HPU, that use lazy storageless tensors and uses DDP for distributed training, the is_alias_of() API doesn't work.  Pitch:  The is_alias_of() API by default should check if the tensor has storage, before comparing the storage_impl_. This should ensure for storageless tensors, the default API doesn't fail with no storage error. Also, the is_alias_of() API needs to be virtual, so that a backend can override it and provide its own implementation for  is_alias_of(). The backend can provide a result by using its own mechanism to detect alias tensors that are storageless. For HPU lazy storageless tensors, the backend would determine if they tensors are sharing storage internally on the device and return a result accordingly.  Alternatives _No response_  Additional context _No response_",2022-01-17T06:18:14Z,triaged module: partial aliasing module: ddp module: lazy,open,0,0,https://github.com/pytorch/pytorch/issues/71377
yi,"Trying to run build_libtorch.py getting, ""No module named 'typing_extensions'"," 🐛 Describe the bug File ""/home/jet/pytorch/tools/codegen/gen.py"", line 3, in      from typing_extensions import Literal ModuleNotFoundError: No module named 'typing_extensions' CMake Error at cmake/Codegen.cmake:243 (message):   Failed to get generated_headers list Call Stack (most recent call first):   caffe2/CMakeLists.txt:2 (include)  Versions Python 3.8.12   aarch64 Jetpack 4.6 Nvidia Jetson AGX Xavier",2022-01-16T01:53:54Z,triaged,closed,0,3,https://github.com/pytorch/pytorch/issues/71360,"Hi, This is my first time contributing to OSS. Please correct me if I missed something.  This appears to be a setup issue. At a top level, this error suggests a dependency is missing from your python environment. I see a couple of possibilities: 1. Install the dependencies(conda install) OR 2. Find suitable prebuilt PyTorch pip wheel installers for your setup Background: I found build_libtorch.sh here.  the script content says: ```  Script for installing libtorch from scratch (without any python dependencies)  Note that you need to have all the dependencies needed before running this script! (Read README.md in the main pytorch repo) ``` Which leads us to the docs, specifically the Install dependencies topic: ``` conda install astunparse numpy ninja pyyaml mkl mklinclude setuptools cmake cffi typing_extensions ... ``` Also, there is a note just above this command, which may apply to you: `If you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are available here(https://devtalk.nvidia.com/default/topic/1049071/jetsonnano/pytorchforjetsonnano/) ` Notes to self: typing_extensions module was included  since Feb 2021.","Hi . I agree with  that it seems like this is a dependency install issue. If you're trying to build pytorch/pytorch, instructions for what to install are here. If this doesn't work, it would be great to get some information on what's happeningwhat command are you trying to run, what have you done before, etc?",I installed typing_extensions for python version 3.8. I was able to build libtorch on the AGX Xavier with python version 3.8.   Python versions 3.6 and 3.9 did not work.   `sudo python3.8 tools/build_libtorch.py`
rag,[RFC] Hierarchical Model Averaging (Hierarchical SGD)," 🚀 The feature, motivation and pitch  Infeasibility of Frequent Global Synchronization Nowadays powerful industry users (not necessarily in FAANG) like Cruise, Microsoft, and Tesla are moving towards scaling largescale training to 100+ or even 1K+ GPUs. DDP, as the most welladopted data parallelism implementation in PyTorch, fully synchronizes the gradients of all the processes at each iteration by default, which may lead to a low scaling efficiency in this case (let's assume that it may not be feasible to afford highbandwidth network environment like InfiniBand/EFA or highend GPUs for many companies). The key is that, the global synchronization at each iteration can become infeasible at a very large scale, at least for some industry users. Therefore, we may need to consider replacing the default SyncSGD with a combination of frequent partial synchronization and infrequent global synchronization. One solution I contributed to PyTorch is postlocalSGD), based on model averaging paradigm, as implemented in this module. Now it seems that the ownership is transferred to varma.  Hierarchical Model Averaging One effective approach is **hierarchical model averaging**, namely **hierarchical SGD** (ICLR'2021, AAAI'2022), which can be viewed as an extended version of local SGD. Local SGD essentially shows a twolevel hierarchy. An oversimplified example is that, given 32 GPUs on 8 machines, every 4 GPUs on the same machine is organized as a sub process group, 8 subgroups in total. Each subgroup runs allreduce at every iteration. In the meantime, a global allreduce is executed every 8 or 16 iterations. Additionally, some other tricks are needed to ensure convergence efficiency. If powerful users plan to scale to at least hundreds of GPUs, a twolevel hierarchy may not be high enough to support infrequent global synchronizations. Therefore, we can consider executing allreduce in a more hierarchical way, which can be userdefined. For example, we can imagine a 4level hierarchy like this:  Level I: 4 GPUs on the same machine sync every iteration;  Level II: 32 GPUs sync every 8 iterations;  Level III: 128 GPUs sync every 16 iterations;  Level IV: 512 GPUs sync every 32 iterations. To avoid the potential data race, at each iteration, there will be at most one synchronization, which should be at the highest level that triggers allreduce. Note that recently Facebook has published a relevant paper on KDD'21, although it is deployed on CPU clusters, so I guess this feature (built as a generic PyTorch module) can also be beneficial for Facebook itself.  Benefits 1. Improving the scaling efficiency, especially if the model is relatively large and can cause nontrivial data transfer cost. 2. Smoothing out potentially large computation variance and hence mitigating stragglers, based on the assumption that stragglers occur randomly among all the processes at each iteration. The mitigation effect is similar to gradient accumulation, but hierarchical model averaging still runs optimizer step at every iteration, it can often give a higher accuracy than a simple gradient accumulation implementation.  Alternatives As mentioned above, gradient accumulation can be an alternate approach. However, there can be a few disadvantages: 1. Usually the  of accumulated steps before each synchronization cannot be too large due to the potential model quality regression. 2. Gradient accumulation still runs global allreduce, which can be less communicationefficient.  Additional context This can be considered as a DDP feature request. To facilitate the triage: Labels: feature, module:ddp, oncall:distributed. CC PyTorch Distributed DDP developers:    varma ",2022-01-14T20:20:53Z,oncall: distributed feature module: ddp,closed,2,8,https://github.com/pytorch/pytorch/issues/71325,"Thanks for the suggestion ! I was wondering if our current DDP communication hooks are generic enough for you to try out these different ideas and implement them on your end in the mean time? It does make sense to have native support in PyTorch for some of these ideas, but we would like to primarily make DDP hackable/pluggable such that ideas like these can be prototyped easily by users themselves and then we can incorporate some of these techniques as builtin features into DDP itself. I guess since this idea also involves model averaging, we might need a generic pluggable model averaging component as well here? ","> I was wondering if our current DDP communication hooks are generic enough for you to try out these different ideas and implement them on your end in the mean time?  I don't expect PTD to allocate anyone to implement this in the near future. Actually I plan to work on the implementation at Cruise. IMO, the current DDP should be already flexible enough for supporting this idea  in other words, I don't request any enablement work from DDP. > I guess since this idea also involves model averaging, we might need a generic pluggable model averaging component as well here? We will see if that is necessary. Will need some code reviews from PTD when iterating over the implementation, in order to better incorporate into PyTorch.",Maybe related: https://twitter.com/m_ryabinin/status/1481994243761713153,"> Maybe related: https://twitter.com/m_ryabinin/status/1481994243761713153 Thanks  for the pointer! It's an interesting read. I think this paper focuses more on a faulttolerant allreduce implementation over extremely heterogeneous and extremely unreliable (potentially **preemptible**) devices. In the authors' words, the feasible environment is **spot instances or volunteer PCs**. Specifically, the heterogeneous experimental setups in the paper are: 1) 81 heterogeneous GPUs across 64 servers and **across 2 data centers**, and **additional latency is injected**.  2) 66 **preemptible** heterogeneous GPUs, and 34 of them are various devices rented on a public marketplace, spread **across 3 continents**. Honestly, I don't think the above extreme heterogeneity and unreliability would be practical for almost all the decent industry users. Except for the extreme setup in the paper, I really doubt Moshpit SGD that requires tworound allreduce involving all the nodes could still outperform more commonly used model averaging approaches or Gossip SGD, and I will be also very surprised if Yandex will have a feasible production environment to adopt Moshpit SGD. Note that if the paper targets a federated learning setup, I don't think PyTorch DDP is a fair comparison, since it's hard to imagine that allreduce will be used for federated learning. By contrast, this RFC targets a much more common scenario in industry  a large number of homogeneous GPUs: each machine usually has multiple GPUs (so the  of machines are much smaller than the  of GPUs), and these machines are usually in the same cluster.","Yeah, I mostly referred to its separation of nodes into groups and aggregation happening within those groups, so maybe that aspect is related to the proposed hierarchical grouping / aggregation.","Thanks for the proposal Yi! I wonder if this can be incorporated into the model averaging library, or if we can directly use ModelAverager/PeriodicModelAverager to implement this idea (both of the interfaces look quite generic and pluggable): https://github.com/pytorch/pytorch/tree/master/torch/distributed/algorithms/model_averaging","> Yeah, I mostly referred to its separation of nodes into groups and aggregation happening within those groups, so maybe that aspect is related to the proposed hierarchical grouping / aggregation. Thanks ! Both ideas are indeed relevant in this sense.","> Thanks for the proposal Yi! I wonder if this can be incorporated into the model averaging library, or if we can directly use ModelAverager/PeriodicModelAverager to implement this idea (both of the interfaces look quite generic and pluggable): https://github.com/pytorch/pytorch/tree/master/torch/distributed/algorithms/model_averaging Yes, I am working on prototyping this idea based on the existing model averaging foundation."
rag,[RFC] Cross-Process Performance Analysis: Straggler Detection," 🚀 The feature, motivation and pitch  Motivation: Limitation of Existing Profiling Approach To conduct PyTorch distributed training performance analysis, currently a recommended way is profiling by using PyTorch profiler or Nsight. As a result, every profiled process can output a trace file, and users usually only need to take one or two representative files to analyze (mostly for the case of data parallelism). However, one limitation of such approach is no easy support for crossprocess performance analysis. A prominent example is **straggler detection**, because it's hard to identify straggling by only looking into a small number of individual trace files, and if the training runs at a relatively large scale, it's also impractical for users to eyeball a large number of trace files.  Impact Note that as we scale out trainings to more and more machines nowadays, straggler issues can become increasingly common, especially given that the most welladopted PyTorch distributed training implementation, DDP is highly sensitive to stragglers  the allreducebased gradient synchronization has to wait for the slowest process at every iteration. I believe that straggler issues can occur in various cases as DDP training is scaled out, and detecting and mitigating stragglers can have a high impact for improving largescale training performance. A few cases: 1. Embedding table lookup can be expensive with large embeddings, especially these embedding tables may need to be partitioned across machines and hence incur alltoall communication overheads. Such workloads may have a large variance across processes in the forward pass and hence cause stragglers. This can be found in training largescale recommendation systems. (This one can be quite relevant to Ads companies like Meta: , varma) 2. If a massive amount of training data is used for training a relatively small model, the I/O cost can become a bottleneck. In this case, stragglers can occur if the data size of the input examples/records have a very large variance. Reading the input data from (potentially multihomed) cloud storage may make this even worse. (This can be common when there is a strict lowlatency requirement, and the performance of opensource data loader is not good enough.) 3. If a customized data loader has a very computationally expensive `transform` phase, and a high variance in the computation amount of onthefly data transformation may also contribute to straggling. On the other hand, PyTorch users may not be even aware of such pattern (example)).  Detailed Requests Can PyTorch provide an easy way for straggler detection? Particularly: 1. It should be easy to install such a detection component, and there should be no intrusive code change. 2. Straggling can be defined programmatically, and a metric can be reported to measure the extent of straggling. 3. Report whether the stragglers are the same across different iterations. If so, the straggling is more likely to be host/machinespecific, or it may imply the lack of sufficient data shuffling, etc. With such feature, it will be much easier for industry powerful users to run fleetwide straggler detections, and PyTorch users can be more alert to stragglers.  Possible Implementations 1. We can develop a new type of debugging DDP communication hook. In the hook, before allreduce is kicked off, first measure the start time in the hook state, and then summarize different start times across processes at the same iteration. However, in the hook, it's impossible to also measure the total runtime of an iteration. To evaluate the extent of straggling, may need to bring some side inputs (e.g., regarding the total runtime of an iteration) when parsing a hook state that records allreduce starts times. 2. We can build an analysis layer on top of existing DDP logging (`_get_ddp_logging_data()`), which has already collected all the raw data needed for this purpose. CC: , varma   Alternatives As mentioned above, currently there isn't an easy way to detect stragglers. After all, it's impractical to eyeball many trace files generated by profilers for the case of largescale training.  Additional context This can be considered as a DDP feature request. To facilitate the triage: 1) Labels: **feature**, **module:ddp**, **oncall:distributed**. 2) CC PyTorch Distributed DDP developers:    varma  ",2022-01-14T09:31:25Z,high priority triage review oncall: distributed feature module: c10d module: ddp,open,2,8,https://github.com/pytorch/pytorch/issues/71303,"I wonder if we need to develop something entirely new here, or if we just need to take something like `_get_ddp_logging_data` and make the data/insights more relevant/easily digestible by end users. For example, straggler detection can be handled with `_get_ddp_logging_data` (admittedly in a not very user friendly manner) today.  I know you mentioned ease of use, will the use case not be satisfied by using DDP APIs such as `get_logging_data` or enabling this detection with TORCH_DISTRIBUTED_DEBUG?  Longer term, ideally outputs from something like `_get_ddp_logging_data` should be visible in Tensorboard for OSS users. There is already a distributed profiling view developed by Microsoft: https://pytorch.org/blog/pytorchprofiler1.9released/, we could possibly look into adding to this. ","> will the use case not be satisfied by using DDP APIs such as get_logging_data or enabling this detection with TORCH_DISTRIBUTED_DEBUG? I think both routes could work, so I don't have any preference here. Personally I would not prefer the name of **TORCH_DISTRIBUTED_DEBUG** mode in this context, because this mode is mainly designed for debugging distributed training failures that normally lead to a certain uninformative error (e.g., NCCL timeout). I am not sure if we want to view straggler as a **bug** here. This may cause some user confusion. > Longer term, ideally outputs from something like _get_ddp_logging_data should be visible in Tensorboard for OSS users. This can be a good idea. Just check if we can have any more lightweight solution before reaching this stage. Additionally, would love to see if we can have some analytics on top of the raw data to summarize the extent of straggling. This can help a lot on a fleetwide performance analysis. > There is already a distributed profiling view developed by Microsoft: https://pytorch.org/blog/pytorchprofiler1.9released/, we could possibly look into adding to this. This can be a good entry point. I have used this distributed view on TensorBoard, and I can confirm it does not have any support on straggler detection yet.","I guess the approach might depend on the scope. If you only care about DDP stragglers, having a custom comm hook totally makes sense. If you would like to cover all collectivebased features (DDP/ZeRO/FSDP), having a context manager might be more generic. It can be sth like below, where `sd.summary()` can tell how much time is wasted due to straggler on each process. ```python with torch.distributed.straggler_detection() as sd:      some training iterations if rank == 0:     print(sd.summary()) ``` ","> I guess the approach might depend on the scope. If you only care about DDP stragglers, having a custom comm hook totally makes sense. If you would like to cover all collectivebased features (DDP/ZeRO/FSDP), having a context manager might be more generic. It can be sth like below, where `sd.summary()` can tell how much time is wasted due to straggler on each process. >  > ```python > with torch.distributed.straggler_detection() as sd: >      some training iterations >  > if rank == 0: >     print(sd.summary()) > ``` Thanks for the suggestion !  > I guess the approach might depend on the scope. If you only care about DDP stragglers, having a custom comm hook totally makes sense. Currently my end will only need to target DDP scope. > If you would like to cover all collectivebased features (DDP/ZeRO/FSDP), having a context manager might be more generic. Personally I probably will prefer an alternate approach, which can be a separate module to run some offline analysis over traces collected by torch profiler. A few reasons: 1. In practice, users may want to run profiling first instead of directly running straggler detection, so it would be nice if we can incorporate such a new context manager into profiler context manager. We still have the advantage of not being restricted to any specific collective communication. 2. Microsoft folks working on profiler told me that they plan to upstream a module PT1.11, which can provide an API to conveniently export perf stats from trace files. This makes such route more viable in the future. 3. This approach can not only support straggler detection, but also other tracebased analysis as well. On the other hand, I think a debug DDP comm hook (or some analysis based on DDP logging data) for straggler detection can still make sense. 1. It's more lightweight than enabling profiling, which is often more than adding a context manager for multinode training in practice. Profiling often requires other setup like uploading trace files to a cloud storage w/o exceeding the rate limit. 2. It's much easier for industrial users to deploy a DDP comm hook (or enable DDP logging) than to add a context manager out of training loop fleet wide, because setting up a DDP comm hook or (DDP logging) can be totally decoupled from the training loop, which means such setup could be done in one place. In contrast, for industrial users every use case usually has a different training loop in different files, and hence the workload will be much larger, unless a higherlevel API (probably based on Lightning or Ignite) has unified these loops in one place.",Adding hipri as straggler detection has come up quite a bit both internally and for use cases in OSS,Did some other PR meet this need? Trying to track down some stragglers but it seems discussion of them in general is rare across the PyTorch ecosystem.,The way we do this internally is we have a high performance structured logging system (Scuba) and we send data from all ranks there and then can look for patterns across multiple ranks. Not sure what the best OSS equivalents here are though.,
transformer,Use SLEEF functions for erf/exp on macOS ARM64,"The NEON `Vectorized` implementation does not use SLEEF functions in order to compile on mobile platforms. However, SLEEF is already compiled on macOS ARM64 and is safe to use on that platform. Since transformers rely heavily on `erf` (GELU) and `exp` (softmax), using the SLEEF implementations of `erf`/`exp` can provide large speedups of transformers on Apple M1 Macs. For instance, we have found spaCy transformer models to be 20% faster when using the SLEEF implementation of these functions. This PR is a followup to CC(Use SLEEF functions for NEON vectors on macOS ARM64), but with a more limited scope as suggested in https://github.com/pytorch/pytorch/pull/70354pullrequestreview852154370. Profile of a tranformer model before this change:  Most time is spent in `erff`/`expf`. Profile of the same transformer model after this change:  Time spent in `erff` is reduced from 1.94min to 1.18min, time spent in `expf` is reduced from 60s to 8s.",2022-01-14T08:44:21Z,open source cla signed,closed,0,4,https://github.com/pytorch/pytorch/issues/71302," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/danieldk/pytorch/blob/62506db82b777a642edcb45e4192aa53a5d9678d/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/71302**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 62506db82b (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator.",Superseded by merging of CC(Use SLEEF functions for NEON vectors on macOS ARM64).
transformer,ONNX: export custom model to ONNX leads to different output," 🐛 Describe the bug Hi all, I have finetuned a `sentencetransformers/LaBSE` on a binary sentence classification task and I am now trying to export it to ONNX. However, the ONNX model's outputs do not match the PyTorch ones.    Steps to reproduce the behavior: ```python import numpy as np import onnxruntime as rt import torch import torch.nn as nn import transformers from transformers import AutoTokenizer, AutoConfig, AutoModel from transformers import convert_graph_to_onnx class LabseForClassification(nn.Module):     def __init__(self, config):         super(LabseForClassification, self).__init__()         self.config = config         self.num_labels = config.num_labels         self.labse = AutoModel.from_config(config)         self.classifier = nn.Linear(768, config.num_labels)     def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):         model_output = self.labse(             input_ids=input_ids,             token_type_ids=token_type_ids,             attention_mask=attention_mask         )         embeddings = model_output[1]         embeddings = nn.functional.normalize(embeddings)         logits = self.classifier(embeddings)         return torch.softmax(logits, dim=1) TEXT = """"""LOS ANGELES (KTLA) – In high school football, the talent gap between future pros and average Joes can be quite wide indeed, and lopsided scores are nothing new. But a 1060 win? That will draw some attention. That was the score at the game between Inglewood High and Inglewood Morningside in Southern California over the weekend. Despite scoring 59 points in the first quarter alone, Inglewood High head coach Mil’Von James declined to play backups and was initially reticent to use a running clock to shorten the game, according to Inglewood Morningside football coach Brian Collins. Inglewood High even went for a twopoint conversion pass, instead of the traditional onepoint kick attempt, after scoring to take a tripledigit lead, which Collins told the Los Angeles Times was “a classless move.” “I told them, ‘Go play St. John Bosco and Mater Dei,'” Collins said in reference to two of the area’s powerhouse high schools that recently produced the starting quarterbacks at toptier programs Clemson University and the University of Alabama. James has not responded to an email seeking comment on the game. In a statement provided to the Times’ Eric Sondheimer, the California Interscholastic Federation Southern Section, which governs most Southern California high school sports, said the 1060 score “does not represent” the organization’s ideals of character. “The CIFSS condemns, in the strongest terms, results such as these,” the statement read. Other high school coaches were similarly incensed. Matt Poston, head coach at Tesoro High School in Las Flores, said he hoped he was “reading this wrong” when he looked at the score. “We’re supposed to be teaching young men life lessons through the game. What message was this staff teaching last night? Sad,” Poston wrote on Twitter. Legendary basketball sportscaster Dick Vitale also weighed in on Twitter. Sportswriter Nick Harris highlighted some of the most eyepopping stats, calling the game “a beatdown for the ages.” While 1060 is a score rarely seen at any level of football, it’s not the largest margin of victory. The most lopsided football score of all time is widely considered to be Georgia Tech’s 2220 win over Cumberland in 1916, when Cumberland had discontinued its football program but was forced to play the game, putting together a squad of fraternity brothers and other students."""""" ENTITY = ""Inglewood"" TEXT = TEXT.replace(ENTITY, ""[MASK]"") if __name__ == ""__main__"":     tokenizer = AutoTokenizer.from_pretrained(""sentencetransformers/LaBSE"", use_fast=True)     config = AutoConfig.from_pretrained(""sentencetransformers/LaBSE"", num_labels=2)     model_raw = LabseForClassification(config)     model_raw.load_state_dict(torch.load(""outputs/SAL/pytorch_model.bin"", map_location=""cpu""))     model_raw.eval()     model_pipeline = transformers.Pipeline(model=model_raw, tokenizer=tokenizer)     with torch.no_grad():         input_names, output_names, dynamic_axes, tokens = convert_graph_to_onnx.infer_shapes(             model_pipeline,             ""pt""         )         ordered_input_names, model_args = convert_graph_to_onnx.ensure_valid_input(             model_pipeline.model, tokens, input_names         )     del dynamic_axes[""output_0""]   Delete unused output     output_names = [""probs""]     dynamic_axes[""probs""] = {0: 'batch'}     torch.onnx.export(         model_raw,         model_args,         f=""test.onnx"",         input_names=input_names,         output_names=output_names,         dynamic_axes=dynamic_axes,         do_constant_folding=True,         opset_version=12,     )     sess = rt.InferenceSession(""test.onnx"")     inputs_np = tokenizer(TEXT, return_tensors=""np"")     probs_onnx = sess.run(None, {         ""input_ids"": inputs_np[""input_ids""],         ""attention_mask"": inputs_np[""attention_mask""],         ""token_type_ids"": inputs_np[""token_type_ids""]     })     inputs = tokenizer(TEXT, return_tensors=""pt"")     probs = model_raw(**inputs)     assert np.allclose(         probs_onnx[0].squeeze(),         probs.squeeze().detach().numpy(),         atol=1e6,     ) ```  Also, I'm getting this warning, which I suspect to be the reason of the outputs divergence: ``` /Users/jules/Desktop/datanai/.venv/training/lib/python3.9/sitepackages/transformers/modeling_utils.py:1967: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!   assert all( WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. ``` Am I doing something wrong here? Please let me know if you need anything else from my side.  Versions PyTorch version: 1.10.0 Is debug build: False CUDA used to build PyTorch: None ROCM used to build PyTorch: N/A OS: macOS 10.14.6 (x86_64) GCC version: Could not collect Clang version: Could not collect CMake version: Could not collect Libc version: N/A Python version: 3.9.9 (main, Jan  6 2022, 16:18:49)  [Clang 10.0.1 (clang1001.0.46.4)] (64bit runtime) Python platform: macOS10.14.6x86_64i38664bit Is CUDA available: False CUDA runtime version: No CUDA GPU models and configuration: No CUDA Nvidia driver version: No CUDA cuDNN version: No CUDA HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] numpy==1.21.1 [pip3] torch==1.10.0 [pip3] torchvision==0.11.1 [conda] Could not collect Additional: transformers==4.15.0 onnx==1.9.0 onnxruntime==1.9.0",2022-01-12T10:19:45Z,module: onnx triaged onnx-needs-info,closed,0,2,https://github.com/pytorch/pytorch/issues/71207,Can you please try to reproduce using the latest pytorch nightly?,"Hi  The warnings above are actually harmless and can be ignored. The numerical differences can only be address with a repro, as requested in Feb 2 by a colleague Since a repro was not provided, we’ve gone ahead and closed this issue because it is stale. If you still believe this issue is relevant, please feel free to reopen the issue and we will triage it as necessary. Please specify in a comment any updated information you may have so that we can address it effectively. We encourage you to try the latest pytorchpreview (nightly) version to see if it has resolved the issue. Thanks, ONNX Converter team"
transformer,traced module fails on second execution," 🐛 Describe the bug I have a model that I save usint torch.jit.trace and torch.jit.save. When I load it, and execute it, the first execution works correctly, but the second one fails with the following error ``` RuntimeError                              Traceback (most recent call last)  in  > 1 out = model(inputs) ~/.local/lib/python3.8/sitepackages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)    1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks    1101                 or _global_forward_hooks or _global_forward_pre_hooks): > 1102             return forward_call(*input, **kwargs)    1103          Do not call functions when jit is used    1104         full_backward_hooks, non_full_backward_hooks = [], [] RuntimeError: _Map_base::at ``` The model consists of a normalizing flow conditioned on a transformer. When tracing the transformer part alone it doesn't causes this error, but when tracing the full model, it does!  Versions Checked with torch 1.8.0 and torch 1.10.0. Happens both on CPU and GPU",2022-01-11T18:22:14Z,oncall: jit,open,0,3,https://github.com/pytorch/pytorch/issues/71165,"Update: it seems to be happening only when the forward method has a keyword argument. When I change my forward method from `def forward(self, x, cond, sldj, reverse=False):` to `def forward(self, x, cond, sldj):` the error goes away! Update 2: actually that was on a minimal example I had, but on the full code, doing that, it still fails...","it seems the error also happens when i return only one element of the tuple a module returns.  This works: `trace = torch.jit.trace(lambda x,y: model.output_mod_glows0, (noise,latent), check_trace=False)` And this fails `trace = torch.jit.trace(lambda x,y: model.output_mod_glows0[0], (noise,latent), check_trace=False)` This seems to be the main failure mode: when an intermediate tensor is not used for the output", can you provide a repro script for this?
rag,Rollup: forward-mode AD operator coverage,"NOTE: See also the tracker) for forwardoverreverse formulas:  Sometimes implementing a forward AD formula will also implicitly support its forwardoverreverse formula, make sure to comment/mark as completed on that issue as well to keep it up to date. These are ops that currently have `supports_forward_ad=False` (usually implicitly) in their OpInfo. Because not all ops have OpInfos yet, some ops that don't support forward AD may be missing from this list. **Please file an issue or comment here if there is an op you'd like to see supported, but is not on this list**. A complete list of formulas is included at the bottom. For a short tutorial on adding a forward AD formula see:  CC(Forward mode AD for linear algebra functions) If you take a brief look at the list below, you'll notice that formulas in `derivatives.yaml` don't generally correspond onetoone with OpInfo entries, so it is sometimes hard to know just from staring at the op name, which formula is needed to support it. To figure out what formula corresponds to a particular op, set `supports_forward_ad=True` in the OpInfo and run the test `test_forward_mode_AD`, the error will tell you which formula is needed, e.g. `forward ad is not implemented for `. Sometimes a single formula will support multiple ops. After implementing the formula in `dervatives.yaml`, it may be useful to run all the forward AD tests to reveal if any more ops have been supported by side effect. High priority (*in order of priority*; please start working from the top)   [x] nn.functional.binary_cross_entropy (DO THIS FIRST; needs OpInfo) https://github.com/pytorch/pytorch/pull/77755  [x] nn.functional.binary_cross_entropy_with_logits (DO THIS FIRST; needs OpInfo) (already there, but https://github.com/pytorch/pytorch/pull/77755 simplifies implementation)  [x] clamp https://github.com/pytorch/pytorch/pull/74042  [x] log_softmax https://github.com/pytorch/pytorch/pull/73741  [x] norm https://github.com/pytorch/pytorch/pull/70253, https://github.com/pytorch/pytorch/pull/74205  [x] dist https://github.com/pytorch/pytorch/pull/74205  [x] atan2 https://github.com/pytorch/pytorch/pull/75027  [x] nn.functional.embedding    [x] nn.functional.glu https://github.com/pytorch/pytorch/pull/77186  [x] nn.functional.prelu    [x] nn.functional.multi_head_attention_forward  [x] nn.functional.dropout ()  [x] nn.functional.{dropout2d, dropout3d}  [x] nn.functional.l1_loss  [x] nn.functional.smooth_l1_loss  [ ] nn.functional.conv3d (needs OpInfo)  [x] fmod https://github.com/pytorch/pytorch/pull/69908  [x] softmax https://github.com/pytorch/pytorch/pull/73741  [x] nn.functional.softmin https://github.com/pytorch/pytorch/pull/73741  [x] nn.functional.cross_entropy https://github.com/pytorch/pytorch/pull/73741  [x] nn.functional.normalize https://github.com/pytorch/pytorch/pull/74205  [x] nn.functional.bilinear    [x] nn.functional.rrelu    [x] nn.functional.logsigmoid    [x] `__rsub__` https://github.com/pytorch/pytorch/pull/75326  [x] rsub https://github.com/pytorch/pytorch/pull/75326  [x] polar https://github.com/pytorch/pytorch/pull/75326  [x] `__getitem__` https://github.com/pytorch/pytorch/pull/69909  [x] put  [ ] cdist  [x] renorm https://github.com/pytorch/pytorch/pull/100798  [x] logsumexp https://github.com/pytorch/pytorch/pull/73741  [x] logcumsumexp https://github.com/pytorch/pytorch/pull/100629  [ ] nn.functional.embedding_bag  [ ] nn.functional.grid_sample  [x] amax https://github.com/pytorch/pytorch/pull/77975  [x] amin https://github.com/pytorch/pytorch/pull/77975  [x] nanmean https://github.com/pytorch/pytorch/pull/77975  [x] nansum https://github.com/pytorch/pytorch/pull/77975  [ ] nn.functional.ctc_loss (?)  [x] nn.functional.nll_loss https://github.com/pytorch/pytorch/pull/73741  [x] nn.functional.pairwise_distance https://github.com/pytorch/pytorch/pull/74205  [x] nn.functional.mse_loss https://github.com/pytorch/pytorch/pull/71026  [x] nn.functional.huber_loss https://github.com/pytorch/pytorch/pull/71026  [x] nn.functional.kl_div https://github.com/pytorch/pytorch/pull/71026 Linalg ops (old tracker:  CC(Forward mode AD for linear algebra functions))  [x] linalg.det https://github.com/pytorch/pytorch/pull/79487  [ ] linalg.cond  [x] linalg.eigvals https://github.com/pytorch/pytorch/pull/70527  [x] linalg.lstsq (already done, see old tracker  need to filter out nongrad oriented OpInfo)  [x] linalg.norm https://github.com/pytorch/pytorch/pull/74205  [x] linalg.matrix_norm (TODO: , investigate failure)  [x] linalg.slogdet https://github.com/pytorch/pytorch/pull/79743  [x] linalg.vector_norm https://github.com/pytorch/pytorch/pull/74205  [x] cholesky_inverse https://github.com/pytorch/pytorch/pull/75033  [x] lu_unpack https://github.com/pytorch/pytorch/pull/67833  [x] svd  https://github.com/pytorch/pytorch/pull/70253  [x] linalg.svd  https://github.com/pytorch/pytorch/pull/70253  [x] linalg.svdvals https://github.com/pytorch/pytorch/pull/70253  [x] svd_lowrank (composite op, supported by linalg.svd)  [x] pca_lowrank (composite op, supported by linalg.svd)  [x] logdet https://github.com/pytorch/pytorch/pull/79743 Low frequency:  [ ] igamma  [ ] igammac  [ ] special.zeta  [ ] to_sparse  [ ] nn.functional.one_hot FFT ops (these correspond to 3 formulas in `derivatives.yaml`: _fft_r2c, _fft_c2r, _fft_c2c):  PR in progress: https://github.com/pytorch/pytorch/pull/75326  [x] fft.fft  [x] fft.fft2  [x] fft.fftn  [x] fft.hfft  [x] fft.hfft2  [x] fft.hfftn  [x] fft.rfft  [x] fft.rfft2  [x] fft.rfftn  [x] fft.ifft  [x] fft.ifft2  [x] fft.ifftn  [x] fft.ihfft  [x] fft.ihfft2  [x] fft.ihfftn  [x] fft.irfft  [x] fft.irfft2  [x] fft.irfftn  [x] stft  [x] istft Masked ops (these don't have their own formulas, just implement the formula for the nonmasked version and you get covered here for free):  [x] _masked.amax https://github.com/pytorch/pytorch/pull/77975  [x] _masked.amin https://github.com/pytorch/pytorch/pull/77975  [x] _masked.norm https://github.com/pytorch/pytorch/pull/74205  [x] _masked.softmax https://github.com/pytorch/pytorch/pull/73741  [x] _masked.log_softmax https://github.com/pytorch/pytorch/pull/73741  [x] _masked.softmin https://github.com/pytorch/pytorch/pull/73741  [x] _masked.normalize https://github.com/pytorch/pytorch/pull/74205 Deprecated ops ( CC(Deprecations tracking issue)):  cholesky  solve  symeig  eig It may not be entirely useful, but expand the below list (TODO: not up to date) to see a list of remaining formulas. Implementing a formula that ends in  `_backward`  will support forwardoverreverse autograd.    [click to expand]  ``` _sparse_addmm affine_grid_generator acosh_ asinh_ atanh_ atan2 cholesky cholesky_inverse clamp.Tensor _coalesce polar logcumsumexp _ctc_loss _det_lu_based_helper dist _fused_dropout native_dropout native_dropout_backward eig fake_quantize_per_tensor_affine_cachemask _fake_quantize_per_tensor_affine_cachemask_tensor_qparams _fake_quantize_learnable_per_tensor_affine fake_quantize_per_channel_affine_cachemask _fake_quantize_learnable_per_channel_affine _fused_moving_avg_obs_fq_helper geqrf grid_sampler_2d grid_sampler_3d _grid_sampler_2d_cpu_fallback igamma igammac index.Tensor https://github.com/pytorch/pytorch/pull/69909 special_zeta special_zeta.self_scalar special_zeta.other_scalar logdet logsumexp lstsq lu_unpack amax amin native_batch_norm_backward native_layer_norm_backward nextafter norm.Scalar norm.ScalarOpt_dim norm.ScalarOpt_dtype norm.ScalarOpt_dim_dtype linalg_vector_norm _pdist_forward _pdist_backward _euclidean_dist _cdist_forward _cdist_backward ormqr put_ renorm slogdet linalg_slogdet solve std_mean.correction rsub.Tensor rsub.Scalar nansum nansum.dim_IntList _svd_helper symeig one_hot to_dense to_sparse to_sparse.sparse_dim to_mkldnn _unique unique_dim unique_consecutive unique_dim_consecutive _unique2 var_mean.correction _weight_norm_cuda_interface sparse_mask _sparse_coo_tensor_with_dims_and_tensors _sparse_sum.dim _standard_gamma _standard_gamma_grad values _trilinear binary_cross_entropy binary_cross_entropy_backward embedding embedding_dense_backward _embedding_bag _embedding_bag_dense_backward embedding_renorm_ multilabel_margin_loss_forward nll_loss_forward nll_loss2d_forward elu_ celu_ glu log_sigmoid_forward _log_softmax _sparse_log_softmax prelu prelu_backward rrelu_with_noise rrelu_with_noise_ _softmax _sparse_softmax _sparse_sparse_matmul _upsample_bicubic2d_aa.vec elu_backward glu_backward kl_div_backward l1_loss_backward log_sigmoid_backward _log_softmax_backward_data leaky_relu_backward max_unpool2d_backward mse_loss_backward nll_loss_backward nll_loss2d_backward rrelu_with_noise_backward smooth_l1_loss_backward huber_loss_backward softplus_backward _softmax_backward_data soft_margin_loss_backward _upsample_bicubic2d_aa_backward.vec _cudnn_ctc_loss cudnn_grid_sampler cudnn_affine_grid_generator cudnn_batch_norm_backward _cudnn_rnn _cudnn_rnn_backward miopen_batch_norm_backward miopen_rnn miopen_rnn_backward mkldnn_linear mkldnn_max_pool2d mkldnn_max_pool3d mkldnn_adaptive_avg_pool2d _mkldnn_reshape _fft_r2c _fft_c2r _fft_c2c _thnn_fused_lstm_cell _thnn_fused_gru_cell _pack_padded_sequence segment_reduce _pin_memory _test_warn_in_autograd ```  A follow up tracker can be created to track the completion of any remaining formulas if necessary. Alternatively, any additions to the list above are also welcome. See also: Tracker for `supports_fwgrad_bwgrad=False` for forwardoverreverse Hessian coverage (TODO). ",2022-01-10T18:23:09Z,module: autograd triaged actionable module: forward ad,open,0,6,https://github.com/pytorch/pytorch/issues/71117,"I added a few operations that will have AD support once some PRs are merged, and I also removed a few duplicated functions. It's worth noting that this list includes some operations that will be removed in the next version, such as `symeig` and similar linalg functions.","Oh, awesome! Thank you! Now we have list of small PRs inbetween large things :) There are some linear algebra functions on this list that are going to be deprecated, so they could get excluded. Most torch.linalg functions support forward AD.","Updated to exclude the deprecated linear algebra functions that will be deprecated, thanks!","About linalg: `linalg.lstsq` has forward AD implemented. svd/pcalowrank should be able to support forward AD as they are composite operations relying on mm and svd, for which `linalg.svd` now has forward AD support, unless legacy svd is used...","NB: 's old stack at https://github.com/pytorch/pytorch/pull/69211 has many of the OpInfos that we need to test the above (binary_cross_entropy{_with_logits}, l1_loss, etc) but I don't know what the status of it is"," wanted to land the stack at some point, but it seems we both have forgotten about it. Want to give this another shot?"
yi,RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.," 🐛 Describe the bug When using modules having torch.nn.utils.parametrize like: ``` class SWSConv2d(nn.Module):     r""""""     2D Conv layer with Scaled Weight Standardization.     Characterizing signal propagation to close the performance gap in unnormalized ResNets     https://arxiv.org/abs/2101.08692     """"""     def __init__(self,                  in_channels: int, out_channels: int, kernel_size: int,                  stride: int=1, padding: int=0, padding_mode: str='zeros', dilation=1, groups: int=1,                  bias: bool=True):         super().__init__()         self._stride = stride         self._padding = padding         self._padding_mode = padding_mode         self._dilation = dilation         self._groups = groups         self.weight = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size, kernel_size))         self.bias = nn.Parameter(torch.empty(out_channels)) if bias else None         self.gain = nn.Parameter(torch.ones(out_channels, 1, 1, 1))          Init weights         k = groups / (in_channels * kernel_size * kernel_size)         torch.nn.init.uniform_(self.weight, k, k)         if bias:             torch.nn.init.uniform_(self.bias, k, k)     def forward(self, input: torch.Tensor) > torch.Tensor:         return TF.conv2d(input,                          weight=self.weight.to(input.device, copy=True) * self.gain, bias=self.bias,                          stride=self._stride,                          padding=self._padding,                          dilation=self._dilation,                          groups=self._groups) ``` Optimizing training performance by using: ```  Avoid to recompute parametrization for weights shared with torch.nn.utils.parametrize.cached():   pred = self.model.forward(img0, img1, solver_num_iters=solver_num_iters) ``` Training failed with the following error: RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. Anomaly detection points to the following culprit: ``` File ""/mnt/task_runtime/src/dibnn_ext/nn/modules.py"", line 832, in forward     weight=self.weight.to(input.device, copy=True) * self.gain, bias=self.bias,   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/utils/parametrize.py"", line 339, in get_parametrized     return get_cached_parametrization(parametrization)   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/utils/parametrize.py"", line 324, in get_cached_parametrization     tensor = parametrization()   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/modules/module.py"", line 1102, in _call_impl     return forward_call(*input, **kwargs)   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/utils/parametrize.py"", line 260, in forward     x = self0   File ""/miniconda/lib/python3.7/sitepackages/torch/nn/modules/module.py"", line 1102, in _call_impl     return forward_call(*input, **kwargs)   File ""/mnt/task_runtime/src/dibnn_ext/nn/parametrization.py"", line 36, in forward     weight = DF.weight_standardization(weight, eps=self.eps)   File ""/mnt/task_runtime/src/dibnn_ext/nn/functional.py"", line 660, in weight_standardization     weight = (weight  mean) / (torch.sqrt(var * fan_in + eps))  (function _print_stack) ``` It seems the cached weights of a parametrize module do not have gradients available when they are used multiple time in a model.  Versions Collecting environment information... PyTorch version: 1.10.0+cu113 Is debug build: False CUDA used to build PyTorch: 11.3 ROCM used to build PyTorch: N/A OS: Ubuntu 18.04.6 LTS (x86_64) GCC version: (Ubuntu 7.5.03ubuntu1~18.04) 7.5.0 Clang version: 6.0.01ubuntu2 (tags/RELEASE_600/final) CMake version: Could not collect Libc version: glibc2.10 Python version: 3.7.7 (default, Mar 23 2020, 22:36:06)  [GCC 7.3.0] (64bit runtime) Python platform: Linux4.19.561.el7.x86_64x86_64withdebianbustersid Is CUDA available: True CUDA runtime version: Could not collect GPU models and configuration:  GPU 0: Tesla V100SXM232GB GPU 1: Tesla V100SXM232GB GPU 2: Tesla V100SXM232GB GPU 3: Tesla V100SXM232GB GPU 4: Tesla V100SXM232GB GPU 5: Tesla V100SXM232GB GPU 6: Tesla V100SXM232GB GPU 7: Tesla V100SXM232GB Nvidia driver version: 418.40.04 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Versions of relevant libraries: [pip3] numpy==1.21.2 [pip3] pytorchlightning==1.5.5 [pip3] torch==1.10.0+cu113 [pip3] torchaudio==0.10.0+cu113 [pip3] torchmetrics==0.6.2 [pip3] torchvision==0.11.1+cu113 [conda] blas                      1.0                         mkl   [conda] cudatoolkit               11.3.1               h2bc3f7f_2   [conda] libblas                   3.9.0            12_linux64_mkl    condaforge [conda] libcblas                  3.9.0            12_linux64_mkl    condaforge [conda] liblapack                 3.9.0            12_linux64_mkl    condaforge [conda] mkl                       2021.4.0           h06a4308_640   [conda] mklservice               2.4.0            py37h7f8727e_0   [conda] mkl_fft                   1.3.1            py37hd3c417c_0   [conda] mkl_random                1.2.2            py37h51133e4_0   [conda] numpy                     1.19.5                   pypi_0    pypi [conda] numpybase                1.21.2           py37h79a1101_0   [conda] pytorchcpu               1.1.0            py37he1b5a44_0    condaforge [conda] pytorchlightning         1.5.5              pyhd8ed1ab_0    condaforge [conda] pytorchmutex             1.0                        cuda    pytorch [conda] torch                     1.10.0+cu113             pypi_0    pypi [conda] torchaudio                0.10.0+cu113             pypi_0    pypi [conda] torchmetrics              0.6.2              pyhd8ed1ab_0    condaforge [conda] torchvision               0.11.1+cu113             pypi_0    pypi ",2022-01-09T02:18:39Z,module: autograd triaged module: nn.utils.parametrize,closed,0,10,https://github.com/pytorch/pytorch/issues/71062,"This model does not have any `nn.utils.Parametrization`s that I can see. Where did you use parametrizations? Parametrised weights can indeed be used several times in the model, and that's exactly what `cached` is for. As a side note, I reckon you've already tried this, but does the training succeed when you do not register the parametrisations?","Thanks Lezcano  for your reply. I cannot share the model but I can reproduce the issue consistently with different models. Here is how I use the parametrization. ``` import torch import torch.nn as nn import torch.nn.functional as TF def weight_standardization(weight: torch.Tensor, eps: float = 1e6):     """"""     Weight standardization as defined by:     Characterizing signal propagation to close the performance gap in unnormalized ResNets, https://arxiv.org/abs/2101.08692     :param weight: weight to standardize     :param eps: epsilon to ensure safe division     :return: weight to standardized     """"""     fan_in = nn.init._calculate_fan_in_and_fan_out(weight)[0]     var, mean = torch.var_mean(weight, dim=list(range(1, weight.ndim)), keepdim=True)     weight = (weight  mean) / (torch.sqrt(var * fan_in + eps))     return weight class WeightStandardization(nn.Module):     def __init__(self, eps: float=1e6):         super().__init__()         self.eps = eps     def forward(self, weight: torch.Tensor) > torch.Tensor:         weight = DF.weight_standardization(weight, eps=self.eps)         return weight class SWSConv2d(nn.Module):     r""""""     2D Conv layer with Scaled Weight Standardization.     Characterizing signal propagation to close the performance gap in unnormalized ResNets     https://arxiv.org/abs/2101.08692     """"""     def __init__(self,                  in_channels: int, out_channels: int, kernel_size: int,                  stride: int=1, padding: int=0, padding_mode: str='zeros', dilation=1, groups: int=1,                  bias: bool=True):         super().__init__()         self._stride = stride         self._padding = padding         self._padding_mode = padding_mode         self._dilation = dilation         self._groups = groups         self.weight = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size, kernel_size))         self.bias = nn.Parameter(torch.empty(out_channels)) if bias else None         self.gain = nn.Parameter(torch.ones(out_channels, 1, 1, 1))          Init weights         k = groups / (in_channels * kernel_size * kernel_size)         torch.nn.init.uniform_(self.weight, k, k)         if bias:             torch.nn.init.uniform_(self.bias, k, k)         TP.register_parametrization(self, 'weight', DP.WeightStandardization(eps=1e6))     def forward(self, input: torch.Tensor) > torch.Tensor:         return TF.conv2d(input,                          weight=self.weight.to(input.device, copy=True) * self.gain, bias=self.bias,                          stride=self._stride,                          padding=self._padding,                          dilation=self._dilation,                          groups=self._groups) class DummyModel(nn.Module):     def __init__(self,                  input_dim: int,                  output_dim: int,                  hidden_dim: int):         super().__init__()         self.blocks = nn.Sequential(             SWSConv2d(input_dim, hidden_dim, 3, bias=True, activation=TF.ReLU()),             SWSConv2d(hidden_dim, output_dim, 1, bias=True         )     def forward(self, input) > torch.Tensor:         pred = self.blocks(input)         return pred ``` Overall the training works WITHOUT the parametrization (standard Conv2D instead of SWSConv2d) but an ideal scenario because the SWS helps the convergence. I have in my models a siamese network for feature extraction which reuses the same weights 2 times. The training also works WITH the parametrization SWSConv2d and with torch.nn.utils.parametrize.cached(): commented. It only failed with the cache enabled randomly during training. I suspect the backward pass try to recompute gradients on SWSConv2d already computed in the past. I do not know if we can force to keep the gradients in a cached parametrization?","What seems odd to me from thatis the line `self.weight.to(input.device, copy=True)`. Why do you move it to a different device, rather than creating `nn.Sequential` in the correct device or moving the whole model to the correct device via `to(device)`?","That's a good point. I had to include this line because when I use the cache it appears the weights are save from one GPU but used to another GPU. Overall, I move the model to the correct devices but during training when I use cached parametrization the weights can be in another GPU.",I am using DDP with mixed precision training on 8 GPU. Not sure how the weights should be cached?, + caches + multi GPU setups. I'm not sure of what's the behaviour of `parametrization`s together with DDP.,The issue is easier to reproduce with DP and autograddetectanomaly. I am wondering if the problem is simply the cache is global and not thread safe?,"Can you share what your training loop looks like as well please? Or even better a full example we can run to reproduce this? Note that the cached result is computed in a differentiable way. So if you do multiple backwards within the cached context, you are trying to backward multiple times indeed so I would expect this error.","Unfortunately this is a lot of code I cannot share but I will try to create a minimal example to reproduce the issue. I am using Pytorch Lightning for the training loop. The network is a simple feed forward with a siamnese branch, and to be exact, the issue only occurs when the cache is enabled.","I found the issue by working on a minimal example. The issue was simply caching the forward pass not including the computation of the loss: ``` def training_step(...):   with torch.nn.utils.parametrize.cached():     pred = self.model.forward(img0, img1, solver_num_iters=solver_num_iters)   ...   loss = compute_loss(batch, pred, ...) ``` I think we can close this ticket. ^^"
rag,[Model Averaging] Update the documentation of PeriodicModelAverager,"Here 20 is a bad example, since the warmup step is set as 100. 200 iterations will make much more sense. ",2022-01-07T06:00:21Z,oncall: distributed open source cla signed,closed,1,5,https://github.com/pytorch/pytorch/issues/70974," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/wayi1/pytorch/blob/48f296813d3e241f402a14b4bf62bdf4a70332fe/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ","Hi !  Thank you for your pull request.  We **require** contributors to sign our **Contributor License Agreement**, and yours needs attention. You currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.  Process In order for us to review and merge your suggested changes, please sign at . **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA. Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it. If you have received this in error or have any questions, please contact us at cla.com. Thanks!",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/70974**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 48f296813d (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,"varma has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
agent,DISABLED test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA),"Platforms: rocm ``` 00:39:54   test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA) ... Memory exception on virtual address 0x7f1216360000, node id 4 : Page not present 00:39:54 Address does not belong to a known buffer 00:39:54 Memory access fault by GPU node4 (Agent handle: 0x55b39d917fc0) on address 0x7f1216360000. Reason: Page not present or supervisor privilege. ``` ",2022-01-06T05:03:40Z,module: rocm skipped,closed,0,2,https://github.com/pytorch/pytorch/issues/70890,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA)` * Platforms for which to skip the test: rocm Within ~15 minutes, `test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA)` will be disabled in PyTorch CI for these platforms: rocm. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.  ``` Platforms: caseinsensitive, list, of, platforms ``` We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows.","Closing, duplicate of  CC(DISABLED test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA))"
agent,DISABLED test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA),"Platforms: rocm https://ci.pytorch.org/jenkins/job/pytorchbuilds/job/pytorchlinuxbionicrocm4.5py3.7test1/45//console ``` 16:39:42   test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA) ... Memory exception on virtual address 0x7fc5a5360000, node id 2 : Page not present 16:39:42 Address does not belong to a known buffer 16:39:42 Memory access fault by GPU node2 (Agent handle: 0x56050fa4e6f0) on address 0x7fc5a5360000. Reason: Page not present or supervisor privilege. 16:39:42 Traceback (most recent call last): 16:39:42   File ""test/run_test.py"", line 1096, in  16:39:42     main() 16:39:42   File ""test/run_test.py"", line 1074, in main 16:39:42     raise RuntimeError(err_message) 16:39:42 RuntimeError: test_reductions failed! Received signal: SIGIOT ``` ",2022-01-06T00:49:47Z,module: rocm skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/70883,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA)` * Platforms for which to skip the test: rocm Within ~15 minutes, `test_median_nan_values_cuda_float64 (__main__.TestReductionsCUDA)` will be disabled in PyTorch CI for these platforms: rocm. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.  ``` Platforms: caseinsensitive, list, of, platforms ``` We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows."
agent,DISABLED test_median_nan_values_cuda_float32 (__main__.TestReductionsCUDA),Platforms: rocm ``` test_median_nan_values_cuda_float32 (__main__.TestReductionsCUDA) ... Memory access fault by GPU node2 (Agent handle: 0x55d98dbd81a0) on address 0x7fba787b0000. Reason: Page not present or supervisor privilege.  test_reductions failed! Received signal: SIGIOT ``` ,2022-01-05T23:13:24Z,module: rocm skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/70878,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_median_nan_values_cuda_float32 (__main__.TestReductionsCUDA)` * Platforms for which to skip the test: rocm Within ~15 minutes, `test_median_nan_values_cuda_float32 (__main__.TestReductionsCUDA)` will be disabled in PyTorch CI for these platforms: rocm. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.  ``` Platforms: caseinsensitive, list, of, platforms ``` We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows."
agent,DISABLED test_median_nan_values_cuda_float16 (__main__.TestReductionsCUDA),"Platforms: rocm ``` test_median_nan_values_cuda_float16 (__main__.TestReductionsCUDA) ... Memory exception on virtual address 0x7f765efd8000, node id 2 : Page not present Address does not belong to a known buffer Memory access fault by GPU node2 (Agent handle: 0x56076309ef10) on address 0x7f765efd8000. Reason: Page not present or supervisor privilege. 20210928 20:11:15,199 INFO  test_reductions failed! Received signal: SIGIOT ``` ",2022-01-05T23:04:26Z,module: rocm module: tests skipped,closed,0,1,https://github.com/pytorch/pytorch/issues/70876,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_median_nan_values_cuda_float16 (__main__.TestReductionsCUDA)` * Platforms for which to skip the test: rocm Within ~15 minutes, `test_median_nan_values_cuda_float16 (__main__.TestReductionsCUDA)` will be disabled in PyTorch CI for these platforms: rocm. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.  ``` Platforms: caseinsensitive, list, of, platforms ``` We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows."
chat,[pytorch][aten][cuda] move CUDAGeneratorImpl.h to ATen/cuda,"Summary: This patch moves a CUDAspecific file, `CUDAGeneratorImpl.h` to `ATen/cuda` as the following TODO comment in  `CUDAGeneratorImpl.h` suggests: ``` // TODO: this file should be in ATen/cuda, not top level ``` Differential Revision: D33414890",2022-01-04T23:23:24Z,fb-exported cla signed,closed,0,7,https://github.com/pytorch/pytorch/issues/70650," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/shintaroiwasaki/pytorch/blob/79dbfd6cddab5f24b358769f7036fa0bf7540823/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ",  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/70650**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 79dbfd6cdd (more details on the Dr. CI page):  :green_heart: :green_heart: **Looks good so far! There are no failures yet.** :green_heart: :green_heart:  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ,This pull request was **exported** from Phabricator. Differential Revision: D33414890,This pull request was **exported** from Phabricator. Differential Revision: D33414890,This pull request was **exported** from Phabricator. Differential Revision: D33414890,This pull request was **exported** from Phabricator. Differential Revision: D33414890,"iwasaki has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
chat,[pytorch][aten][cuda] fix LpNormFunctor,"Summary: `&` has lower precedence than `==`, so `==` will be evaluated first. This behavior should not be intended. This patch fixes it. Test Plan: 🧐  Carefully check the change. Differential Revision: D33397964",2022-01-04T15:42:08Z,fb-exported cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/70601," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/shintaroiwasaki/pytorch/blob/0a3484acab514fddea5bb05f0fbecbb9b4258567/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/70601**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * [:leftwards_arrow_with_hook: &nbsp;[fbonly] Rerun with SSH instructions](https://github.com/pytorch/pytorch/wiki/DebuggingusingwithsshforGithubActions)  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 0a3484acab (more details on the Dr. CI page):  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base d35fc409ad from Jan 04 until Jan 05   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * linuxxenialpy3.7clang7asan / test (default, 1, 3, linux.2xlarge) from Jan 04 until Jan 05 (d35fc409ad  1681323ddc)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. ",This pull request was **exported** from Phabricator. Differential Revision: D33397964
yi,torch.cat without copying memory," 🚀 The feature, motivation and pitch  Principle Today, the concatenation implemented on pytorch consists in the allocation of a new tensor. I would like to know if it is possible to realize a concatenation of contiguous and/or noncontiguous tensors without memory duplication.  Example 1 : contiguous concatenation The following code is executed with allocation of a new tensor `concatenated_tensor`:  ``` import torch  tensor1 = torch.rand(2,3,4) tensor2 = torch.rand(2,3,4) concatenated_tensor = torch.cat(tensor1, tensor2) ``` I'd like to enable the same scenario, but have `concatenated_tensor` as a view of `tensor1` and `tensor2`. In terms of UX, I don't know what to propose.  Note: since I'm a new pytorch user, maybe the word ""view"" is not appropriate. The lowlevel idea is to consider `concatenated_tensor` as a list of pointers to tensors.  Example 2 : noncontiguous concatenation  Next, I would like to enable the following scenario, if possible:  ``` import torch  tensor1 = torch.rand(2,3,4).expand(10, 1, 1) tensor2 = torch.rand(2,3,4).expand(10, 1, 1) concatenated_tensor = torch.cat(tensor1, tensor2) ```  Alternatives _No response_  Additional context Discussed in CC(torch.utils.data.DataLoader  returned views support) with . See discussion/34609. ",2022-01-04T15:22:32Z,triaged module: numpy module: viewing and reshaping,closed,0,3,https://github.com/pytorch/pytorch/issues/70600,"> I would like to know if it is possible to realize a concatenation of contiguous and/or noncontiguous tensors without memory duplication. The answers of  on https://discuss.pytorch.org/t/concatenatetensorswithoutmemorycopying/34609/13 explain how difficult this is. It does not fit the strided model of a tensor at all. It'd be a ton of work  I think it's safe to say this won't happen. I'll let someone else decide, but I propose to close this issue.","No problem, we'll close this issue.","nestedtensor https://github.com/pytorch/nestedtensor might be doing what you want. Also, if you don't need autograd, `_foreach_*` ops support operations on the lists of tensors, so you don't need to concatenate them in advance."
transformer,[docs] Transformer: no batch dim support doc update,,2022-01-04T10:42:07Z,open source cla signed,closed,0,3,https://github.com/pytorch/pytorch/issues/70597," CI Flow Status  :atom_symbol: CI Flow Ruleset  Version: `v1` Ruleset  File: https://github.com/kshitij12345/pytorch/blob/03d52811f38fa54def33114ab6eefed281e6a1d9/.github/generatedciflowruleset.json PR ciflow labels: `ciflow/default`   You can add a comment to the PR and tag  with the following commands:  ```sh  ciflow rerun, ""ciflow/default"" will always be added automatically  ciflow rerun  ciflow rerun with additional labels ""l "", which is equivalent to adding these labels manually and trigger the rerun  ciflow rerun l ciflow/scheduled l ciflow/slow ```  For more information, please take a look at the CI Flow Wiki. ","  :link: Helpful links  * :test_tube: &nbsp;**See artifacts and rendered test results at hud.pytorch.org/pr/70597**  * :page_facing_up: &nbsp;**Preview docs built from this PR**  * :page_facing_up: &nbsp;**Preview C++ docs built from this PR**  * :wrench: &nbsp;Optin to CIFlow to control what jobs run on your PRs  :pill: CI failures summary and remediations As of commit 03d52811f3 (more details on the Dr. CI page):  :white_check_mark: **None of the CI failures appear to be your fault** :green_heart:  * **1/1** broken upstream at merge base d35fc409ad on Jan 04 from  1:31am to  6:07pm   :construction: 1 fixed upstream failure: These were probably **caused by upstream breakages** that were **already fixed**.   Please rebase on the viable/strict branch (expand for instructions)  If your commit is older than `viable/strict`, run these commands: ``` git fetch https://github.com/pytorch/pytorch viable/strict git rebase FETCH_HEAD ```  * linuxxenialpy3.7clang7asan / test (default, 1, 3, linux.2xlarge) on Jan 04 from  1:31am to  6:07pm (d35fc409ad  4d08db0cb2)     * :repeat: rerun  This comment was automatically generated by Dr. CI (expand for details). Please report bugs/suggestions to the (internal) Dr. CI Users group. Click here  to manually regenerate this comment. "," has imported this pull request. If you are a Meta employee, you can view this diff on Phabricator."
rag,[feature request] Exponential moving average (EMA) of a tensor across a dimension," 🚀 The feature, motivation and pitch If the dimension size is high, Python loop may be inefficient and maybe inplace computation graph (for efficiency) could confuse TorchScript. In `pandas` this exists as `ewm()` function: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html In NumPy various reimpl methods exist (recurrent; truncated based on convolution): https://stackoverflow.com/questions/42869495/numpyversionofexponentialweightedmovingaverageequivalenttopandasewm  Alternatives Special APIs for returning convolution weights, e.g. https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter1d.html, and the same for exponential weights. Then have a recipe/example in docs of using these weights to create an approximate exponential moving average via convolution. Although maybe given that gaussian / ewm filtering is very popular, both APIs would be useful.  Additional context _No response_ ",2022-01-02T09:06:31Z,feature triaged module: numpy,open,0,1,https://github.com/pytorch/pytorch/issues/70555,probably some API designs for such function already exist in PyTorch time series packages...
agent,DISABLED test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest),Platforms: mac This test was disabled because it is failing on master (recent examples). ,2022-01-01T03:26:48Z,high priority triage review oncall: distributed module: flaky-tests module: rpc skipped,open,0,11,https://github.com/pytorch/pytorch/issues/70546,"Hello there! From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI. The information I have parsed is below: * Test name: `test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest)` * Platforms for which to skip the test: mac Within ~15 minutes, `test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest)` will be disabled in PyTorch CI for these platforms: mac. Please verify that your test name looks correct, e.g., `test_cuda_assert_async (__main__.TestCuda)`. To modify the platforms list, please include a line in the issue body, like below. The default action will disable the test for all platforms if no platforms list is specified.  ``` Platforms: caseinsensitive, list, of, platforms ``` We currently support the following platforms: asan, linux, mac, macos, rocm, win, windows.","Forgot to triage this one during my oncall  , let's discuss it in next week's oncall meeting? ", Are there any examples I can look at? The recent examples link doesn't have anything since it is probably outdated now: https://www.torchci.com/failure/test_tensorpipe_set_default_timeout%2C%20TensorPipeTensorPipeAgentRpcTest. I tried the test history tool as well and it looks like there are no reports: https://gist.github.com/pritamdamania87/c2b518e7e37b85ab133b6a8e7bf6e328  ,"Went to our data backend and there were two examples:        https://github.com/pytorch/pytorch/runs/4666724994?check_suite_focus=true        https://github.com/pytorch/pytorch/runs/4564324852?check_suite_focus=true raw logs: https://osscirawjobstatus.s3.amazonaws.com/log/4666724994 https://osscirawjobstatus.s3.amazonaws.com/log/4564324852 On Thu, Mar 3, 2022 at 4:58 PM Pritam Damania ***@***.***> wrote: >   Are there any examples I can look at? The > recent examples link doesn't have anything since it is probably outdated > now: > https://www.torchci.com/failure/test_tensorpipe_set_default_timeout%2C%20TensorPipeTensorPipeAgentRpcTest > . > > I tried the test history tool as well and it looks like there are no > reports: > https://gist.github.com/pritamdamania87/35790a03dafa59cf6fa063335836a101 > >   > > — > Reply to this email directly, view it on GitHub > , > or unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","> Went to our data backend and there were two examples: Thanks a lot, I was wondering if you could share how to get these examples from the data backend? I usually try to use the `test_history` tool, but in this case it looks like for some reason it couldn't pick up these examples.","Looking at https://osscirawjobstatus.s3.amazonaws.com/log/4564324852, it seems like an ""Address already in use error"": ``` 20211217T19:38:54.0780097Z   test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest) ... terminate called after throwing an instance of 'std::runtime_error' 20211217T19:38:54.0783014Z   what():  In listenFromLoop at tensorpipe/transport/uv/uv.h:176 ""rv  + 0x11390 (0x7f84c97b7390 in /lib/x86_64linuxgnu/libpthread.so.0) 20211217T19:38:54.0863209Z frame CC(Don't support legacy Python): syscall + 0x19 (0x7f84c94dd5d9 in /lib/x86_64linuxgnu/libc.so.6) 20211217T19:38:54.0865339Z frame CC(PEP8): std::__atomic_futex_unsigned_base::_M_futex_wait_until(unsigned int*, unsigned int, bool, std::chrono::duration >, std::chrono::duration >) + 0x29 (0x7f84b598a9e5 in /opt/conda/lib/libstdc++.so.6) 20211217T19:38:54.0869913Z frame CC(PEP8): tensorpipe::transport::ListenerBoilerplate::addr() const + 0x25a (0x7f84ba81032a in /opt/conda/lib/python3.6/sitepackages/torch/lib/libtorch_cpu.so) 20211217T19:38:54.0878150Z frame CC(Checklist for Release): tensorpipe::channel::mpt::ContextImpl::ContextImpl(std::vector, std::allocator > >, std::vector, std::allocator > >, std::unordered_map, std::allocator >, std::hash, std::equal_to, std::allocator, std::allocator > > > >) + 0x4f5 (0x7f84ba867605 in /opt/conda/lib/python3.6/sitepackages/torch/lib/libtorch_cpu.so) 20211217T19:38:54.0885496Z frame CC(Remove dampening from SGD): tensorpipe::channel::mpt::ContextImpl::create(std::vector, std::allocator > >, std::vector, std::allocator > >) + 0x503 (0x7f84ba868943 in /opt/conda/lib/python3.6/sitepackages/torch/lib/libtorch_cpu.so) 20211217T19:38:54.0890689Z frame CC(ImportError: No module named _C): tensorpipe::channel::mpt::create(std::vector, std::allocator > >, std::vector, std::allocator > >) + 0xf5 (0x7f84ba801e35 in /opt/conda/lib/python3.6/sitepackages/torch/lib/libtorch_cpu.so) 20211217T19:38:54.0894807Z frame CC(fake commit):  + 0x3675e14 (0x7f84b9320e14 in /opt/conda/lib/python3.6/sitepackages/torch/lib/libtorch_cpu.so) 20211217T19:38:54.0899798Z frame CC(Add Storage.from_buffer): std::_Function_handler > (), std::unique_ptr > (*)()>::_M_invoke(std::_Any_data const&) + 0x1a (0x7f84b932e44a in /opt/conda/lib/python3.6/sitepackages/torch/lib/libtorch_cpu.so) 20211217T19:38:54.0904636Z frame CC(Tensors don't print sometimes): torch::distributed::rpc::TensorPipeAgent::startImpl() + 0xab1 (0x7f84b9327e11 in /opt/conda/lib/python3.6/sitepackages/torch/lib/libtorch_cpu.so) 20211217T19:38:54.0907467Z frame CC(add multiprocessing unit tests, working filedescriptor based solution and OSX): torch::distributed::rpc::RpcAgent::start() + 0xe2 (0x7f84b92ff492 in /opt/conda/lib/python3.6/sitepackages/torch/lib/libtorch_cpu.so) ``` ?","Ah, I think they are missing reports because the upload test stats step doesn't get to do the uploading part. The step fails when trying to access RDS to upload failures, but this should be fixed now after permissions were set.  CC(MacOS tests sometimes failing with ""botocore.exceptions.ClientError: An error occurred (AccessDeniedException)"")  And for the data, you can query the scuba table like https://fburl.com/scuba/opensource_ci_jobs/k53dbso4","> ? I had a dejavu feeling when seeing this and indeed I think this was hit before:  CC(test_backward_accumulate_grads (__main__.TensorPipeDistAutogradTest) is flaky)  CC(DISABLED test_future_wait_twice (__main__.TensorPipeRpcTest)). I gave my thoughts on this in those two issues, do those explanations make sense here?","> I had a dejavu feeling when seeing this and indeed I think this was hit before:  CC(test_backward_accumulate_grads (__main__.TensorPipeDistAutogradTest) is flaky)  CC(DISABLED test_future_wait_twice (__main__.TensorPipeRpcTest)). I gave my thoughts on this in those two issues, do those explanations make sense here?  Thanks for the pointers, looking through those two issues it seems like there are two potential reasons for this: 1. This should be avoidable if PyTorch passes a separate port for each multiplexing lane.  2. This might be what's going on here: we're turning up and tearing down connections at such a rate that all ports are unavailable. Regarding 1. can you point to where exactly we pass in this port? Regarding 2., I do feel this does seem to be unlikely since there are 65k ports usually available. "," Although looking at the code here: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/tensorpipe_agent.cppL456, it seems like we only pick a default address for UV and not port. If so, I'm assuming the port will always be 0 and the OS should pick one for us. In that case, it is indeed weird that we have an ""Address already in use"" error.","Regarding 1, there's no ""explicit"" ports ever being used by TensorPipe. What happens during these tests is that we might use an explicit port to initialize the TCPStore (or maybe not even that, if we use a FileStore) but, once the store is established and passed to the TensorPipe agent, we only ever create sockets bound on ""automatic"" ports (i.e., a port of 0, which tells the kernel to find a free port, allocate it to us and give us its number) and then the number of this port is inserted into the store so that the clients can read it and use it when connecting. Note that there are two cases in which the listen call can return EADDRINUSE (see https://man7.org/linux/manpages/man2/listen.2.html): either the explicit port number that was requested is occupied, or there are no more automatic port numbers to assign. I suspect we're thus hitting this second case. As for how many ports are actually being used by each test, we can try to estimate it but it's a bit fuzzy. Most tests use 4 ranks (though some use more?) and if they all connect to each other that makes 16 connections (because for each pair the A>B and B>A directions need separate connections). Some (or maybe all?) of these connections might eventually be replaced with a faster backend by TensorPipe (e.g., SHM) but they all ""start"" as TCP during the handshake phase. In fact, depending on which backends are being used, a single TensorPipe pipe might end up using multiple connections (in some cases up to 20, though I don't think this is happening in these tests?). So in the worst case we do indeed get a few hundred connections per test (which might be enough to exhaust the ports) but most likely much fewer. I think there's also the issue that closed connections don't immediately make their port available again. The TCP protocol needs to keep each port in a ""limbo"" for some time to ensure that stray packets that might come in late for the previous connection might not accidentally be delivered to the new owner of a port. I think some of these timeouts are tunable (and I think TensorPipe tries to minimize them?) but not all of them. This means that even if a single test doesn't exhaust the ports, running too many quick tests in a sequence might still do that. Finally there's the issue that we might not really have 65k ports to start with. That's the upper theoretical limit, but it can be lowered in the kernel. You can read the `/proc/sys/net/ipv4/ip_local_port_range` file to know the lower and upper bound of the range of ports that the kernel will use in this automatic assignment. On most baremetal machines I'd say this goes from 4k to 65k (i.e., all of them except some ""reserved"" ports). But I think we're using Docker for our tests and perhaps this limits the number of ports that are made available? Or something along those lines?"
